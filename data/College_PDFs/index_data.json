{"chunks": ["\n\nApplied Predictive \nAnalytics\nPrinciples and Techniques for the \nProfessional Data Analyst\nDean Abbott\n\nApplied Predictive Analytics: Principles and Techniques for the Professional Data Analyst\nPublished by\nJohn Wiley & Sons, Inc.\n10475 Crosspoint Boulevard\nIndianapolis, IN 46256\nwww.wiley.com\nCopyright \u00a9 2014 by John Wiley & Sons, Inc., Indianapolis, Indiana\nPublished simultaneously in Canada\nISBN: 978-1-118-72796-6\nISBN: 978-1-118-72793-5 (ebk)\nISBN: 978-1-118-72769-0 (ebk)\nManufactured in the United States of America\n10 9 8 7 6 5 4 3 2 1\nNo part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, \nelectronic, mechanical, photocopying, recording, scanning or otherwise, except as permitted under Sections 107 or \n108 of the 1976 United States Copyright Act, without either the prior written permission of the Publisher, or autho-\nrization through payment of the appropriate per-copy fee to the Copyright Clearance Center, 222 Ro", " United States Copyright Act, without either the prior written permission of the Publisher, or autho-\nrization through payment of the appropriate per-copy fee to the Copyright Clearance Center, 222 Rosewood Drive, \nDanvers, MA 01923, (978) 750-8400, fax (978) 646-8600. Requests to the Publisher for permission should be addressed \nto the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ  07030, (201) 748-6011, fax (201) \n748-6008, or online at http://www.wiley.com/go/permissions.\nLimit of Liability/Disclaimer of Warranty: The publisher and the author make no representations or warranties with \nrespect to the accuracy or completeness of the contents of this work and specifi cally disclaim all warranties, including \nwithout limitation warranties of fi tness for a particular purpose. No warranty may be created or extended by sales or \npromotional materials. The advice and strategies contained herein may not be suitable for every situation. This work \nis sold wi", "particular purpose. No warranty may be created or extended by sales or \npromotional materials. The advice and strategies contained herein may not be suitable for every situation. This work \nis sold with the understanding that the publisher is not engaged in rendering legal, accounting, or other professional \nservices. If professional assistance is required, the services of a competent professional person should be sought. \nNeither the publisher nor the author shall be liable for damages arising herefrom. The fact that an organization or \nWeb site is referred to in this work as a citation and/or a potential source of further information does not mean that \nthe author or the publisher endorses the information the organization or website may provide or recommendations \nit may make. Further, readers should be aware that Internet websites listed in this work may have changed or disap-\npeared between when this work was written and when it is read.\nFor general information on our other product", "eaders should be aware that Internet websites listed in this work may have changed or disap-\npeared between when this work was written and when it is read.\nFor general information on our other products and services please contact our Customer Care Department within the \nUnited States at (877) 762-2974, outside the United States at (317) 572-3993 or fax (317) 572-4002.\nWiley publishes in a variety of print and electronic formats and by print-on-demand. Some material included with \nstandard print versions of this book may not be included in e-books or in print-on-demand. If this book refers to \nmedia such as a CD or DVD that is not included in the version you purchased, you may download this material at \nhttp://booksupport.wiley.com. For more information about Wiley products, visit www.wiley.com.\nLibrary of Congress Control Number: 2013958302\nTrademarks: Wiley and the Wiley logo are trademarks or registered trademarks of John Wiley & Sons, Inc. and/or \nits affi liates, in the United Stat", ".com.\nLibrary of Congress Control Number: 2013958302\nTrademarks: Wiley and the Wiley logo are trademarks or registered trademarks of John Wiley & Sons, Inc. and/or \nits affi liates, in the United States and other countries, and may not be used without written permission. [Insert third-\nparty trademark information] All other trademarks are the property of their respective owners. John Wiley & Sons, \nInc. is not associated with any product or vendor mentioned in this book.\n\nTo Barbara\n\n\nv\nDean Abbott is President of Abbott Analytics, Inc. in San Diego, California. \nDean is an internationally recognized data-mining and predictive analytics \nexpert with over two decades of experience applying advanced modeling and \ndata preparation techniques to a wide variety of real-world problems. He is \nalso Chief Data Scientist of SmarterRemarketer, a startup company focusing \non data-driven behavior  segmentation and attribution.\nDean has been recognized as a top-ten data scientist and one of the top", "so Chief Data Scientist of SmarterRemarketer, a startup company focusing \non data-driven behavior  segmentation and attribution.\nDean has been recognized as a top-ten data scientist and one of the top ten \nmost infl uential people in data analytics. His blog has been recognized as one \nof the top-ten predictive analytics blogs to follow.\nHe is a regular speaker at Predictive Analytics World and other analytics \nconferences. He is on the advisory board for the University of California Irvine \nCertifi cate Program for predictive analytics and the University of California \nSan Diego Certifi cate Program for data mining, and is a regular instructor for \ncourses on predictive modeling algorithms, model deployment, and text min-\ning. He has also served several times on the program committee for the KDD \nConference Industrial Track.\nAbout the Author\n\n\nvii\nWilliam J. Komp has a Ph.D. from the University of Wisconsin\u2013Milwaukee, \nwith a specialization in the fi elds of general relativity and cos", "the KDD \nConference Industrial Track.\nAbout the Author\n\n\nvii\nWilliam J. Komp has a Ph.D. from the University of Wisconsin\u2013Milwaukee, \nwith a specialization in the fi elds of general relativity and cosmology. He has \nbeen a professor of physics at the University of Louisville and Western Kentucky \nUniversity. Currently, he is a research scientist at Humana, Inc., working in the \nareas of predictive analytics and data mining.\nAbout the Technical Editor\n\n\nix\nExecutive Editor\nRobert Elliott\nProject Editor\nAdaobi Obi Tulton\nTechnical Editor\nWilliam J. Komp \nSenior Production Editor\nKathleen Wisor\nCopy Editor\nNancy Rapoport\nManager of Content Development \nand Assembly\nMary Beth Wakefi eld \nDirector of Community Marketing\nDavid Mayhew\nMarketing Manager\nAshley Zurcher\nBusiness Manager\nAmy Knies\nVice President and Executive \nGroup Publisher\nRichard Swadley\nAssociate Publisher\nJim Minatel\nProject Coordinator, Cover\nTodd Klemme\nProofreader\nNancy Carrasco \nIndexer\nJohnna VanHoose Dinse\nCover Desig", "ce President and Executive \nGroup Publisher\nRichard Swadley\nAssociate Publisher\nJim Minatel\nProject Coordinator, Cover\nTodd Klemme\nProofreader\nNancy Carrasco \nIndexer\nJohnna VanHoose Dinse\nCover Designer\nRyan Sneed\nCredits\n\n\nxi\nThe idea for this book began with a phone call from editor Bob Elliott, who pre-\nsented the idea of writing a different kind of predictive analytics book geared \ntoward business professionals. My passion for more than a decade has been \nto teach principles of data mining and predictive analytics to business profes-\nsionals, translating the lingo of mathematics and statistics into a language the \npractitioner can understand. The questions of hundreds of course and work-\nshop attendees forced me to think about why we do what we do in predictive \nanalytics. I also thank Bob for not only persuading me that I could write the \nbook while continuing to be a consultant, but also for his advice on the scope \nand depth of topics to cover.\nI thank my father for encouraging", "k Bob for not only persuading me that I could write the \nbook while continuing to be a consultant, but also for his advice on the scope \nand depth of topics to cover.\nI thank my father for encouraging me in analytics. I remember him teaching \nme how to compute batting average and earned run average when I was eight \nyears old so I could compute my Little League statistics. He brought home reams \nof accounting pads, which I used for playing thousands of Strat-O-Matic baseball \ngames, just so that I could compute everyone\u2019s batting average and earned run \naverage, and see if there were signifi cant differences between what I observed \nand what the players\u2019 actual statistics were. My parents put up with a lot of \npaper strewn across the fl oor for many years.\nI would never have been in this fi eld were it not for Roger Barron, my fi rst \nboss at Barron Associates, Inc., a pioneer in statistical learning methods and \na man who taught me to be curious, thorough, and persistent about data an", "i eld were it not for Roger Barron, my fi rst \nboss at Barron Associates, Inc., a pioneer in statistical learning methods and \na man who taught me to be curious, thorough, and persistent about data analysis. \nHis ability to envision solutions without knowing exactly how they would be \nsolved is something I refl ect on often.\nI\u2019ve learned much over the past 27 years from John Elder, a friend and col-\nleague, since our time at Barron Associates, Inc. and continuing to this day. \nI am very grateful to Eric Siegel for inviting me to speak regularly at Predictive \nAcknowledgments\n\nxii \nAcknowledgments\nAnalytics World in sessions and workshops, and for his advice and encourage-\nment in the writing of this book.\nA very special thanks goes to editors Adaobi Obi Tulton and Nancy Rapoport \nfor making sense of what of I was trying to communicate and making this book \nmore concise and clearer than I was able to do alone. Obviously, I was a math-\nematics major and not an English major. I am especia", "g sense of what of I was trying to communicate and making this book \nmore concise and clearer than I was able to do alone. Obviously, I was a math-\nematics major and not an English major. I am especially grateful for technical \neditor William Komp, whose insightful comments throughout the book helped \nme to sharpen points I was making. \nSeveral software packages were used in the analyses contained in the book, \nincluding KNIME, IBM SPSS Modeler, JMP, Statistica, Predixion, and Orange. \nI thank all of these vendors for creating software that is easy to use. I also want \nto thank all of the other vendors I\u2019ve worked with over the past two decades, \nwho have supplied software for me to use in teaching and research. \nOn a personal note, this book project could not have taken place without the \nsupport of my wife, Barbara, who encouraged me throughout the project, even \nas it wore on and seemed to never end. She put up with me as I spent countless \nnights and Saturdays writing, and cheered ", "\nsupport of my wife, Barbara, who encouraged me throughout the project, even \nas it wore on and seemed to never end. She put up with me as I spent countless \nnights and Saturdays writing, and cheered with each chapter that was submit-\nted. My two children still at home were very patient with a father who wasn\u2019t \nas available as usual for nearly a year.\nAs a Christian, my worldview is shaped by God and how He reveals Himself \nin the Bible, through his Son, and by his Holy Spirit. When I look back at the \ncircumstances and people He put into my life, I only marvel at His hand \nof providence, and am thankful that I\u2019ve been able to enjoy this fi eld for my \nentire career.\n\nxiii\nIntroduction \nxxi\nChapter 1 \nOverview of Predictive Analytics \n1\nWhat Is Analytics? \n3\nWhat Is Predictive Analytics? \n3\nSupervised vs. Unsupervised Learning \n5\nParametric vs. Non-Parametric Models \n6\nBusiness Intelligence  \n6\nPredictive Analytics vs. Business Intelligence \n8\nDo Predictive Models Just State the Obvio", "\n3\nSupervised vs. Unsupervised Learning \n5\nParametric vs. Non-Parametric Models \n6\nBusiness Intelligence  \n6\nPredictive Analytics vs. Business Intelligence \n8\nDo Predictive Models Just State the Obvious? \n9\nSimilarities between Business Intelligence \nand Predictive Analytics \n9\nPredictive Analytics vs. Statistics \n10\nStatistics and Analytics \n11\nPredictive Analytics and Statistics Contrasted \n12\nPredictive Analytics vs. Data Mining \n13\nWho Uses Predictive Analytics? \n13\nChallenges in Using Predictive Analytics \n14\nObstacles in Management \n14\nObstacles with Data \n14\nObstacles with Modeling \n15\nObstacles in Deployment \n16\nWhat Educational Background Is Needed to Become a \nPredictive Modeler? \n16\nChapter 2 \nSetting Up the Problem \n19\nPredictive Analytics Processing Steps: CRISP-DM \n19\nBusiness Understanding \n21\nThe Three-Legged Stool \n22\nBusiness Objectives \n23\nContents\n\nxiv \nContents\nDefi ning Data for Predictive Modeling \n25\nDefi ning the Columns as Measures \n26\nDefi ning the Unit of An", "s Understanding \n21\nThe Three-Legged Stool \n22\nBusiness Objectives \n23\nContents\n\nxiv \nContents\nDefi ning Data for Predictive Modeling \n25\nDefi ning the Columns as Measures \n26\nDefi ning the Unit of Analysis  \n27\nWhich Unit of Analysis? \n28\nDefi ning the Target Variable \n29\nTemporal Considerations for Target Variable \n31\nDefi ning Measures of Success for Predictive Models \n32\nSuccess Criteria for Classifi cation \n32\nSuccess Criteria for Estimation \n33\nOther Customized Success Criteria \n33\nDoing Predictive Modeling Out of Order \n34\nBuilding Models First \n34\nEarly Model Deployment \n35\nCase Study: Recovering Lapsed Donors \n35\nOverview \n36\nBusiness Objectives \n36\nData for the Competition \n36\nThe Target Variables \n36\nModeling Objectives \n37\nModel Selection and Evaluation Criteria \n38\nModel Deployment \n39\nCase Study: Fraud Detection \n39\nOverview \n39\nBusiness Objectives \n39\nData for the Project \n40\nThe Target Variables \n40\nModeling Objectives \n41\nModel Selection and Evaluation Criteria \n41\nMod", "yment \n39\nCase Study: Fraud Detection \n39\nOverview \n39\nBusiness Objectives \n39\nData for the Project \n40\nThe Target Variables \n40\nModeling Objectives \n41\nModel Selection and Evaluation Criteria \n41\nModel Deployment \n41\nSummary \n42\nChapter 3 \nData Understanding \n43\nWhat the Data Looks Like \n44\nSingle Variable Summaries \n44\nMean \n45\nStandard Deviation \n45\nThe Normal Distribution \n45\nUniform Distribution \n46\nApplying Simple Statistics in Data Understanding \n47\nSkewness \n49\nKurtosis \n51\nRank-Ordered Statistics \n52\nCategorical Variable Assessment \n55\nData Visualization in One Dimension \n58\nHistograms \n59\nMultiple Variable Summaries \n64\n\n \nContents \nxv\nHidden Value in Variable Interactions: Simpson\u2019s Paradox \n64\nThe Combinatorial Explosion of Interactions \n65\nCorrelations \n66\nSpurious Correlations \n66\nBack to Correlations \n67\nCrosstabs \n68\nData Visualization, Two or Higher Dimensions \n69\nScatterplots \n69\nAnscombe\u2019s Quartet \n71\nScatterplot Matrices \n75\nOverlaying the Target Variable in Summary", "s \n66\nBack to Correlations \n67\nCrosstabs \n68\nData Visualization, Two or Higher Dimensions \n69\nScatterplots \n69\nAnscombe\u2019s Quartet \n71\nScatterplot Matrices \n75\nOverlaying the Target Variable in Summary  \n76\nScatterplots in More Than Two Dimensions \n78\nThe Value of Statistical Signifi cance \n80\nPulling It All Together into a Data Audit \n81\nSummary \n82\nChapter 4 \nData Preparation \n83\nVariable Cleaning \n84\nIncorrect Values \n84\nConsistency in Data Formats \n85\nOutliers \n85\nMultidimensional Outliers \n89\nMissing Values \n90\nFixing Missing Data \n91\nFeature Creation \n98\nSimple Variable Transformations \n98\nFixing Skew \n99\nBinning Continuous Variables \n103\nNumeric Variable Scaling \n104\nNominal Variable Transformation \n107\nOrdinal Variable Transformations \n108\nDate and Time Variable Features \n109\nZIP Code Features \n110\nWhich Version of a Variable Is Best? \n110\nMultidimensional Features \n112\nVariable Selection Prior to Modeling \n117\nSampling \n123\nExample: Why Normalization Matters for K-Means Cluster", "de Features \n110\nWhich Version of a Variable Is Best? \n110\nMultidimensional Features \n112\nVariable Selection Prior to Modeling \n117\nSampling \n123\nExample: Why Normalization Matters for K-Means Clustering \n139\nSummary \n143\nChapter 5 \nItemsets and Association Rules \n145\nTerminology \n146\nCondition \n147\nLeft-Hand-Side, Antecedent(s) \n148\nRight-Hand-Side, Consequent, Output, Conclusion \n148\nRule (Item Set) \n148\n\nxvi \nContents\nSupport \n149\nAntecedent Support \n149\nConfi dence, Accuracy \n150\nLift \n150\nParameter Settings \n151\nHow the Data Is Organized \n151\nStandard Predictive Modeling Data Format \n151\nTransactional Format \n152\nMeasures of Interesting Rules \n154\nDeploying Association Rules \n156\nVariable Selection \n157\nInteraction Variable Creation \n157\nProblems with Association Rules \n158\nRedundant Rules \n158\nToo Many Rules \n158\nToo Few Rules \n159\nBuilding Classifi cation Rules from Association Rules \n159\nSummary \n161\nChapter 6 \nDescriptive Modeling \n163\nData Preparation Issues with Descriptive ", "Rules \n158\nToo Many Rules \n158\nToo Few Rules \n159\nBuilding Classifi cation Rules from Association Rules \n159\nSummary \n161\nChapter 6 \nDescriptive Modeling \n163\nData Preparation Issues with Descriptive Modeling \n164\nPrincipal Component Analysis \n165\nThe PCA Algorithm \n165\nApplying PCA to New Data \n169\nPCA for Data Interpretation \n171\nAdditional Considerations before Using PCA \n172\nThe Effect of Variable Magnitude on PCA Models \n174\nClustering Algorithms \n177\nThe K-Means Algorithm \n178\nData Preparation for K-Means  \n183\nSelecting the Number of Clusters \n185\nThe Kohonen SOM Algorithm \n192\nVisualizing Kohonen Maps \n194\nSimilarities with K-Means \n196\nSummary \n197\nChapter 7 \nInterpreting Descriptive Models \n199\nStandard Cluster Model Interpretation \n199\nProblems with Interpretation Methods \n202\nIdentifying Key Variables in Forming Cluster Models \n203\nCluster Prototypes \n209\nCluster Outliers \n210\nSummary \n212\nChapter 8 \nPredictive Modeling \n213\nDecision Trees \n214\nThe Decision Tree Landscape \n", "dentifying Key Variables in Forming Cluster Models \n203\nCluster Prototypes \n209\nCluster Outliers \n210\nSummary \n212\nChapter 8 \nPredictive Modeling \n213\nDecision Trees \n214\nThe Decision Tree Landscape \n215\nBuilding Decision Trees \n218\n\n \nContents \nxvii\nDecision Tree Splitting Metrics \n221\nDecision Tree Knobs and Options \n222\nReweighting Records: Priors \n224\nReweighting Records: Misclassifi cation Costs \n224\nOther Practical Considerations for Decision Trees \n229\nLogistic Regression \n230\nInterpreting Logistic Regression Models \n233\nOther Practical Considerations for Logistic Regression \n235\nNeural Networks \n240\nBuilding Blocks: The Neuron \n242\nNeural Network Training \n244\nThe Flexibility of Neural Networks \n247\nNeural Network Settings \n249\nNeural Network Pruning \n251\nInterpreting Neural Networks \n252\nNeural Network Decision Boundaries \n253\nOther Practical Considerations for Neural Networks \n253\nK-Nearest Neighbor \n254\nThe k-NN Learning Algorithm \n254\nDistance Metrics for k-NN \n258\nOther Pr", "ks \n252\nNeural Network Decision Boundaries \n253\nOther Practical Considerations for Neural Networks \n253\nK-Nearest Neighbor \n254\nThe k-NN Learning Algorithm \n254\nDistance Metrics for k-NN \n258\nOther Practical Considerations for k-NN \n259\nNa\u00efve Bayes \n264\nBayes\u2019 Theorem \n264\nThe Na\u00efve Bayes Classifi er \n268\nInterpreting Na\u00efve Bayes Classifi ers \n268\nOther Practical Considerations for Na\u00efve Bayes \n269\n Regression Models \n270\nLinear Regression \n271\nLinear Regression Assumptions \n274\nVariable Selection in Linear Regression \n276\nInterpreting Linear Regression Models \n278\nUsing Linear Regression for Classifi cation \n279\nOther Regression Algorithms \n280\nSummary \n281\nChapter 9 \nAssessing Predictive Models \n283\nBatch Approach to Model Assessment \n284\nPercent Correct Classifi cation \n284\nRank-Ordered Approach to Model Assessment \n293\nAssessing Regression Models \n301\nSummary \n304\nChapter 10 \nModel Ensembles \n307\nMotivation for Ensembles \n307\nThe Wisdom of Crowds \n308\nBias Variance Tradeoff \n309\nBa", " Approach to Model Assessment \n293\nAssessing Regression Models \n301\nSummary \n304\nChapter 10 \nModel Ensembles \n307\nMotivation for Ensembles \n307\nThe Wisdom of Crowds \n308\nBias Variance Tradeoff \n309\nBagging \n311\n\nxviii \nContents\nBoosting \n316\nImprovements to Bagging and Boosting \n320\nRandom Forests \n320\nStochastic Gradient Boosting \n321\nHeterogeneous Ensembles \n321\nModel Ensembles and Occam\u2019s Razor \n323\nInterpreting Model Ensembles \n323\nSummary \n326\nChapter 11 \nText Mining \n327\nMotivation for Text Mining  \n328\nA Predictive Modeling Approach to Text Mining \n329\nStructured vs. Unstructured Data \n329\nWhy Text Mining Is Hard \n330\nText Mining Applications \n332\nData Sources for Text Mining \n333\nData Preparation Steps \n333\nPOS Tagging \n333\nTokens \n336\nStop Word and Punctuation Filters \n336\nCharacter Length and Number Filters \n337\nStemming \n337\nDictionaries \n338\nThe Sentiment Polarity Movie Data Set \n339\nText Mining Features  \n340\nTerm Frequency \n341\nInverse Document Frequency \n344\nTF-IDF \n344\n", "er Length and Number Filters \n337\nStemming \n337\nDictionaries \n338\nThe Sentiment Polarity Movie Data Set \n339\nText Mining Features  \n340\nTerm Frequency \n341\nInverse Document Frequency \n344\nTF-IDF \n344\nCosine Similarity \n346\nMulti-Word Features: N-Grams \n346\nReducing Keyword Features \n347\nGrouping Terms \n347\nModeling with Text Mining Features \n347\nRegular Expressions \n349\nUses of Regular Expressions in Text Mining \n351\nSummary \n352\nChapter 12 \nModel Deployment \n353\nGeneral Deployment Considerations \n354\nDeployment Steps \n355\nSummary \n375\nChapter 13 \nCase Studies \n377\nSurvey Analysis Case Study: Overview \n377\nBusiness Understanding: Defi ning the Problem \n378\nData Understanding \n380\nData Preparation \n381\nModeling \n385\n\n \nContents \nxix\nDeployment: \u201cWhat-If\u201d Analysis \n391\nRevisit Models \n392\nDeployment \n401\nSummary and Conclusions \n401\nHelp Desk Case Study \n402\nData Understanding: Defi ning the Data  \n403\nData Preparation \n403\nModeling \n405\nRevisit Business Understanding \n407\nDeployment \n40", "ployment \n401\nSummary and Conclusions \n401\nHelp Desk Case Study \n402\nData Understanding: Defi ning the Data  \n403\nData Preparation \n403\nModeling \n405\nRevisit Business Understanding \n407\nDeployment \n409\nSummary and Conclusions \n411\nIndex \n413\n\n\nxxi\nThe methods behind predictive analytics have a rich history, combining disci-\nplines of mathematics, statistics, social science, and computer science. The label \n\u201cpredictive analytics\u201d is relatively new, however, coming to the forefront only in \nthe past decade. It stands on the shoulders of other analytics-centric fi elds such \nas data mining, machine learning, statistics, and pattern recognition. \nThis book describes the predictive modeling process from the perspective \nof a practitioner rather than a theoretician. Predictive analytics is both science \nand art. The science is relatively easy to describe but to do the subject justice \nrequires considerable knowledge of mathematics. I don\u2019t believe a good practi-\ntioner needs to understand th", "science \nand art. The science is relatively easy to describe but to do the subject justice \nrequires considerable knowledge of mathematics. I don\u2019t believe a good practi-\ntioner needs to understand the mathematics of the algorithms to be able to apply \nthem successfully, any more than a graphic designer needs to understand the \nmathematics of image correction algorithms to apply sharpening fi lters well. \nHowever, the better practitioners understand the effects of changing modeling \nparameters, the implications of the assumptions of algorithms in predictions, \nand the limitations of the algorithms, the more successful they will be, especially \nin the most challenging modeling projects.\nScience is covered in this book, but not in the same depth you will fi nd in \nacademic treatments of the subject. Even though you won\u2019t fi nd sections describ-\ning how to decompose matrices while building linear regression models, this \nbook does not treat algorithms as black boxes.\nThe book describes wh", "subject. Even though you won\u2019t fi nd sections describ-\ning how to decompose matrices while building linear regression models, this \nbook does not treat algorithms as black boxes.\nThe book describes what I would be telling you if you were looking over my \nshoulder while I was solving a problem, especially when surprises occur in \nthe data. How can algorithms fool us into thinking their performance is excel-\nlent when they are actually brittle? Why did I bin a variable rather than just \ntransform it numerically? Why did I use logistic regression instead of a neural \nnetwork, or vice versa? Why did I build a linear regression model to predict a \nIntroduction\n\nxxii \nIntroduction\nbinary outcome? These are the kinds of questions, the art of predictive model-\ning, that this book addresses directly and indirectly.\nI don\u2019t claim that the approaches described in this book represent the only way \nto solve problems. There are many contingencies you may encounter where the \napproaches described in ", "y and indirectly.\nI don\u2019t claim that the approaches described in this book represent the only way \nto solve problems. There are many contingencies you may encounter where the \napproaches described in this book are not appropriate. I can hear the comments \nalready from other experienced statisticians or data miners asking, \u201cbut what \nabout . . . ?\u201d or \u201chave you tried doing this . . . ?\u201d I\u2019ve found over the years that \nthere are often many ways to solve problems successfully, and the approach you \ntake depends on many factors, including your personal style in solving prob-\nlems and the desires of the client for how to solve the problems. However, even \nmore often than this, I\u2019ve found that the biggest gains in successful modeling \ncome more from understanding data than from applying a more sophisticated \nalgorithm.\nHow This Book Is Organized\nThe book is organized around the Cross-Industry Standard Process Model for \nData Mining (CRISP-DM). While the name uses the term \u201cdata mining,\u201d the ", "ophisticated \nalgorithm.\nHow This Book Is Organized\nThe book is organized around the Cross-Industry Standard Process Model for \nData Mining (CRISP-DM). While the name uses the term \u201cdata mining,\u201d the \nsteps are the same in predictive analytics or any project that includes the build-\ning of predictive or statistical models. \nThis isn\u2019t the only framework you can use, but it a well-documented frame-\nwork that has been around for more than 15 years. CRISP-DM should not be \nused as a recipe with checkboxes, but rather as a guide during your predictive \nmodeling projects.\nThe six phases or steps in CRISP-DM:\n \n1. Business Understanding\n \n2. Data Understanding\n \n3. Data Preparation\n \n4. Modeling\n \n5. Evaluation\n \n6. Deployment\nAfter an introductory chapter, Business Understanding, Data Understanding, \nand Data Preparation receive a chapter each in Chapters 2, 3, and 4. The Data \nUnderstanding and Data Preparation chapters represent more than one-quarter \nof the pages of technical content of ", "ding, \nand Data Preparation receive a chapter each in Chapters 2, 3, and 4. The Data \nUnderstanding and Data Preparation chapters represent more than one-quarter \nof the pages of technical content of the book, with Data Preparation the largest \nsingle chapter. This is appropriate because preparing data for predictive model-\ning is nearly always considered the most time-consuming stage in a predictive \nmodeling project. \n\n \nIntroduction \nxxiii\nChapter 5 describes association rules, usually considered a modeling method, \nbut one that differs enough from other descriptive and predictive modeling \ntechniques that it warrants a separate chapter. \nThe modeling methods\u2014descriptive, predictive, and model ensembles\u2014are \ndescribed in Chapters 6, 8, and 10. Sandwiched in between these are the model-\ning evaluation or assessment methods for descriptive and predictive modeling, \nin Chapters 7 and 9, respectively. This sequence is intended to connect more \ndirectly the modeling methods with how they", "ing evaluation or assessment methods for descriptive and predictive modeling, \nin Chapters 7 and 9, respectively. This sequence is intended to connect more \ndirectly the modeling methods with how they are evaluated. \nChapter 11 takes a side-turn into text mining, a method of analysis of unstruc-\ntured data that is becoming more mainstream in predictive modeling. Software \npackages are increasingly including text mining algorithms built into the soft-\nware or as an add-on package. Chapter 12 describes the sixth phase of predictive \nmodeling: Model Deployment. \nFinally, Chapter 13 contains two case studies: one based on work I did as a \ncontractor to Seer Analytics for the YMCA, and the second for a Fortune 500 \ncompany. The YMCA case study is actually two case studies built into a single \nnarrative because we took vastly different approaches to solve the problem. \nThe case studies are written from the perspective of an analyst as he considers \ndifferent ways to solve the problem, includ", "arrative because we took vastly different approaches to solve the problem. \nThe case studies are written from the perspective of an analyst as he considers \ndifferent ways to solve the problem, including solutions that were good ideas \nbut didn\u2019t work well. \nThroughout the book, I visit many of the problems in predictive modeling \nthat I have encountered in projects over the past 27 years, but the list certainly is \nnot exhaustive. Even with the inherent limitations of book format, the thought \nprocess and principles are more important than developing a complete list of \npossible approaches to solving problems.\nWho Should Read This Book \nThis book is intended to be read by anyone currently in the fi eld of predictive \nanalytics or its related fi elds, including data mining, statistics, machine learn-\ning, data science, and business analytics. \nFor those who are just starting in the fi eld, the book will provide a description \nof the core principles every modeler needs to know. \nThe boo", "hine learn-\ning, data science, and business analytics. \nFor those who are just starting in the fi eld, the book will provide a description \nof the core principles every modeler needs to know. \nThe book will also help those who wish to enter these fi elds but aren\u2019t yet there. \nIt does not require a mathematics background beyond pre-calculus to under-\nstand predictive analytics methods, though knowledge of calculus and linear \nalgebra will certainly help. The same applies to statistics. One can understand \nthe material in this book without a background in statistics; I, for one, have never \ntaken a course in statistics. However, understanding statistics certainly provides \nadditional intuition and insight into the approaches described in this book.\n\nxxiv \nIntroduction\nTools You Will Need\nNo predictive modeling software is needed to understand the concepts in the \nbook, and no examples are shown that require a particular predictive analytics \nsoftware package. This was deliberate because", " predictive modeling software is needed to understand the concepts in the \nbook, and no examples are shown that require a particular predictive analytics \nsoftware package. This was deliberate because the principles and techniques \ndescribed in this book are general, and apply to any software package. When \napplicable, I describe which principles are usually available in software and \nwhich are rarely available in software. \nWhat\u2019s on the Website\nAll data sets used in examples throughout the textbook are included on the \nWiley website at www.wiley.com/go/appliedpredictiveanalytics and in the \nResources section of www.abbottanalytics.com. These data sets include:\n \n\u25a0KDD Cup 1998 data, simplifi ed from the full version\n \n\u25a0Nasadata data set\n \n\u25a0Iris data set\nIn addition, baseline scripts, workfl ows, streams, or other documents specifi c \nto predictive analytics software will be included as they become available. \nThese will provide baseline processing steps to replicate analyses from the ", "fl ows, streams, or other documents specifi c \nto predictive analytics software will be included as they become available. \nThese will provide baseline processing steps to replicate analyses from the \nbook chapters.\nSummary\n My hope is that this book will encourage those who want to be more effective \npredictive modelers to continue to work at their craft. I\u2019ve worked side-by-side \nwith dozens of predictive modelers over the years.\nI always love watching how effective predictive modelers go about solving \nproblems, perhaps in ways I had never considered before. For those with more \nexperience, my hope is that this book will describe data preparation, modeling, \nevaluation, and deployment in a way that even experienced modelers haven\u2019t \nthought of before.  \n\n1\nA small direct response company had developed dozens of programs in coop-\neration with major brands to sell books and DVDs. These affi nity programs \nwere very successful, but required considerable up-front work to develop the \ncr", "y had developed dozens of programs in coop-\neration with major brands to sell books and DVDs. These affi nity programs \nwere very successful, but required considerable up-front work to develop the \ncreative content and determine which customers, already engaged with the brand, \nwere worth the signifi cant marketing spend to purchase the books or DVDs \non subscription. Typically, they fi rst developed test mailings on a moderately \nsized sample to determine if the expected response rates were high enough to \njustify a larger program.\nOne analyst with the company identifi ed a way to help the company become \nmore profi table. What if one could identify the key characteristics of those who \nresponded to the test mailing? Furthermore, what if one could generate a score \nfor these customers and determine what minimum score would result in a high \nenough response rate to make the campaign profi table? The analyst discovered \npredictive analytics techniques that could be used for both purpose", " determine what minimum score would result in a high \nenough response rate to make the campaign profi table? The analyst discovered \npredictive analytics techniques that could be used for both purposes, fi nding \nkey customer characteristics and using those characteristics to generate a score \nthat could be used to determine which customers to mail.\nTwo decades before, the owner of a small company in Virginia had a com-\npelling idea: Improve the accuracy and fl exibility of guided munitions using \noptimal control. The owner and president, Roger Barron, began the process of \nderiving the complex mathematics behind optimal control using a technique \nknown as variational calculus and hired a graduate student to assist him in the \ntask. Programmers then implemented the mathematics in computer code so \nC H A P T E R \n1\nOverview of Predictive Analytics\n\n2 \nChapter 1 \u25a0 Overview of Predictive Analytics\nthey could simulate thousands of scenarios. For each trajectory, the variational \ncalculus m", "code so \nC H A P T E R \n1\nOverview of Predictive Analytics\n\n2 \nChapter 1 \u25a0 Overview of Predictive Analytics\nthey could simulate thousands of scenarios. For each trajectory, the variational \ncalculus minimized the miss distance while maximizing speed at impact as \nwell as the angle of impact. \nThe variational calculus algorithm succeeded in identifying the optimal \nsequence of commands: how much the fi ns (control surfaces) needed to change \nthe path of the munition to follow the optimal path to the target. The concept \nworked in simulation in the thousands of optimal trajectories that were run. \nMoreover, the mathematics worked on several munitions, one of which was \nthe MK82 glide bomb, fi tted (in simulation) with an inertial guidance unit to \ncontrol the fi ns: an early smart-bomb. \nThere was a problem, however. The variational calculus was so computation-\nally complex that the small computers on-board could not solve the problem \nin real time. But what if one could estimate the opt", "re was a problem, however. The variational calculus was so computation-\nally complex that the small computers on-board could not solve the problem \nin real time. But what if one could estimate the optimal guidance commands at \nany time during the fl ight from observable characteristics of the fl ight? After \nall, the guidance unit can compute where the bomb is in space, how fast it is \ngoing, and the distance of the target that was programmed into the unit when it \nwas launched. If the estimates of the optimum guidance commands were close \nenough to the actual optimal path, it would be near optimal and still succeed. \nPredictive models were built to do exactly this. The system was called Optimal \nPath-to-Go guidance.\nThese two programs designed by two different companies seemingly could \nnot be more different. One program knows characteristics of people, such as \ndemographics and their level of engagement with a brand, and tries to predict \na human decision. The second program knows lo", "ot be more different. One program knows characteristics of people, such as \ndemographics and their level of engagement with a brand, and tries to predict \na human decision. The second program knows locations of a bomb in space and \ntries to predict the best physical action for it to hit a target. \nBut they share something in common: They both need to estimate values that \nare unknown but tremendously useful. For the affi nity programs, the models \nestimate whether or not an individual will respond to a campaign, and for the \nguidance program, the models estimate the best guidance command. In this \nsense, these two programs are very similar because they both involve predict-\ning a value or values that are known historically, but are unknown at the time \na decision is needed. Not only are these programs related in this sense, but they \nare far from unique; there are countless decisions businesses and government \nagencies make every day that can be improved by using historic data as an ai", "ese programs related in this sense, but they \nare far from unique; there are countless decisions businesses and government \nagencies make every day that can be improved by using historic data as an aid \nto making decisions or even to automate the decisions themselves. \nThis book describes the back-story behind how analysts build the predictive \nmodels like the ones described in these two programs. There is science behind \nmuch of what predictive modelers do, yet there is also plenty of art, where no \ntheory can inform us as to the best action, but experience provides principles \nby which tradeoffs can be made as solutions are found. Without the art, the sci-\nence would only be able to solve a small subset of problems we face. Without \n\n \nChapter 1 \u25a0 Overview of Predictive Analytics \n3\nthe science, we would be like a plane without a rudder or a kite without a tail, \nmoving at a rapid pace without any control, unable to achieve our objectives.\nWhat Is Analytics?\nAnalytics is the process ", "science, we would be like a plane without a rudder or a kite without a tail, \nmoving at a rapid pace without any control, unable to achieve our objectives.\nWhat Is Analytics?\nAnalytics is the process of using computational methods to discover and report \ninfl uential patterns in data. The goal of analytics is to gain insight and often \nto affect decisions. Data is necessarily a measure of historic information so, by \ndefi nition, analytics examines historic data. The term itself rose to prominence \nin 2005, in large part due to the introduction of Google Analytics. Nevertheless, \nthe ideas behind analytics are not new at all but have been represented by dif-\nferent terms throughout the decades, including cybernetics, data analysis, neural \nnetworks, pattern recognition, statistics, knowledge discovery, data mining, and now \neven data science.\nThe rise of analytics in recent years is pragmatic: As organizations collect more \ndata and begin to summarize it, there is a natural progression", "ge discovery, data mining, and now \neven data science.\nThe rise of analytics in recent years is pragmatic: As organizations collect more \ndata and begin to summarize it, there is a natural progression toward using \nthe data to improve estimates, forecasts, decisions, and ultimately, effi ciency. \nWhat Is Predictive Analytics?\nPredictive analytics is the process of discovering interesting and meaningful \npatterns in data. It draws from several related disciplines, some of which have \nbeen used to discover patterns in data for more than 100 years, including pat-\ntern recognition, statistics, machine learning, artifi cial intelligence, and data \nmining. What differentiates predictive analytics from other types of analytics?\nFirst, predictive analytics is data-driven, meaning that algorithms derive key \ncharacteristic of the models from the data itself rather than from assumptions \nmade by the analyst. Put another way, data-driven algorithms induce models \nfrom the data. The induction proc", "erive key \ncharacteristic of the models from the data itself rather than from assumptions \nmade by the analyst. Put another way, data-driven algorithms induce models \nfrom the data. The induction process can include identifi cation of variables to \nbe included in the model, parameters that defi ne the model, weights or coef-\nfi cients in the model, or model complexity. \nSecond, predictive analytics algorithms automate the process of fi nding the \npatterns from the data. Powerful induction algorithms not only discover coef-\nfi cients or weights for the models, but also the very form of the models. Decision \ntrees algorithms, for example, learn which of the candidate inputs best predict \na target variable in addition to identifying which values of the variables to use \nin building predictions. Other algorithms can be modifi ed to perform searches, \nusing exhaustive or greedy searches to fi nd the best set of inputs and  model \nparameters. If the variable helps reduce model error, the var", "s. Other algorithms can be modifi ed to perform searches, \nusing exhaustive or greedy searches to fi nd the best set of inputs and  model \nparameters. If the variable helps reduce model error, the variable is included \n\n4 \nChapter 1 \u25a0 Overview of Predictive Analytics\nin the model. Otherwise, if the variable does not help to reduce model error, it \nis eliminated. \nAnother automation task available in many software packages and algorithms \nautomates the process of transforming input variables so that they can be used \neffectively in the predictive models. For example, if there are a hundred variables \nthat are candidate inputs to models that can be or should be transformed to \nremove skew, you can do this with some predictive analytics software in a single \nstep rather than programming all one hundred transformations one at a time.\nPredictive analytics doesn\u2019t do anything that any analyst couldn\u2019t accomplish \nwith pencil and paper or a spreadsheet if given enough time; the algorithms, \nw", "ne hundred transformations one at a time.\nPredictive analytics doesn\u2019t do anything that any analyst couldn\u2019t accomplish \nwith pencil and paper or a spreadsheet if given enough time; the algorithms, \nwhile powerful, have no common sense. Consider a supervised learning \ndata set with 50 inputs and a single binary target variable with values 0 and \n1. One way to try to identify which of the inputs is most related to the target \nvariable is to plot each variable, one at a time, in a histogram. The target vari-\nable can be superimposed on the histogram, as shown in Figure 1-1. With 50 \ninputs, you need to look at 50 histograms. This is not uncommon for predic-\ntive modelers to do.\nIf the patterns require examining two variables at a time, you can do so with \na scatter plot. For 50 variables, there are 1,225 possible scatter plots to examine. \nA dedicated predictive modeler might actually do this, although it will take \nsome time. However, if the patterns require that you examine three varia", "ere are 1,225 possible scatter plots to examine. \nA dedicated predictive modeler might actually do this, although it will take \nsome time. However, if the patterns require that you examine three variables \nsimultaneously, you would need to examine 19,600 3D scatter plots in order to \nexamine all the possible three-way combinations. Even the most dedicated mod-\nelers will be hard-pressed to spend the time needed to examine so many plots.\n0\n[\u22120.926\u2013 \u22120.658]\n[\u22120.122\u20130.148]\n[0.688\u20130.951]\n[1.498\u20131.768]\n102\n204\n306\n408\n510\n612\n714\n816\n918\n1020\n1129\nFigure 1-1:  Histogram\nYou need algorithms to sift through all of the potential combinations of inputs \nin the data\u2014the patterns\u2014and identify which ones are the most interesting. The \nanalyst can then focus on these patterns, undoubtedly a much smaller number \nof inputs to examine. Of the 19,600 three-way combinations of inputs, it may \n\n \nChapter 1 \u25a0 Overview of Predictive Analytics \n5\nbe that a predictive model identifi es six of the variables a", "r number \nof inputs to examine. Of the 19,600 three-way combinations of inputs, it may \n\n \nChapter 1 \u25a0 Overview of Predictive Analytics \n5\nbe that a predictive model identifi es six of the variables as the most signifi cant \ncontributors to accurate models. In addition, of these six variables, the top three \nare particularly good predictors and much better than any two variables by \nthemselves. Now you have a manageable subset of plots to consider: 63 instead \nof nearly 20,000. This is one of the most powerful aspects of predictive analyt-\nics: identifying which inputs are the most important contributors to patterns \nin the data. \nSupervised vs. Unsupervised Learning\nAlgorithms for predictive modeling are often divided into two groups: supervised \nlearning methods and unsupervised learning methods. In supervised learning \nmodels, the supervisor is the target variable, a column in the data representing \nvalues to predict from other columns in the data. The target variable is chosen \nto ", " methods. In supervised learning \nmodels, the supervisor is the target variable, a column in the data representing \nvalues to predict from other columns in the data. The target variable is chosen \nto represent the answer to a question the organization would like to answer or \na value unknown at the time the model is used that would help in decisions. \nSometimes supervised learning is also called predictive modeling. The primary \npredictive modeling algorithms are classifi cation for categorical target variables \nor regression for continuous target variables. \nExamples of target variables include whether a customer purchased a product, \nthe amount of a purchase, if a transaction was fraudulent, if a customer stated \nthey enjoyed a movie, how many days will transpire before the next gift a donor \nwill make, if a loan defaulted, and if a product failed. Records without a value \nfor the target variable cannot be used in building predictive models. \nUnsupervised learning, sometimes called d", "onor \nwill make, if a loan defaulted, and if a product failed. Records without a value \nfor the target variable cannot be used in building predictive models. \nUnsupervised learning, sometimes called descriptive modeling, has no tar-\nget variable. The inputs are analyzed and grouped or clustered based on the \nproximity of input values to one another. Each group or cluster is given a label \nto indicate which group a record belongs to. In some applications, such as in \ncustomer analytics, unsupervised learning is just called segmentation because \nof the function of the models (segmenting customers into groups). \nThe key to supervised learning is that the inputs to the model are known but \nthere are circumstances where the target variable is unobserved or unknown. \nThe most common reason for this is a target variable that is an event, decision, \nor other behavior that takes place at a time future to the observed inputs to the \nmodel. Response models, cross-sell, and up-sell models work thi", "his is a target variable that is an event, decision, \nor other behavior that takes place at a time future to the observed inputs to the \nmodel. Response models, cross-sell, and up-sell models work this way: Given \nwhat is known now about a customer, can you predict if they will purchase a \nparticular product in the future?\nSome defi nitions of predictive analytics emphasize the function of algorithms \nas forecasting or predicting future events or behavior. While this is often the \ncase, it certainly isn\u2019t always the case. The target variable could represent an \nunobserved variable like a missing value. If a taxpayer didn\u2019t fi le a return in a \nprior year, predictive models can predict that missing value from other examples \nof tax returns where the values are known. \n\n6 \nChapter 1 \u25a0 Overview of Predictive Analytics\nParametric vs. Non-Parametric Models\nAlgorithms for predictive analytics include both parametric and non-parametric \nalgorithms. Parametric algorithms (or models) assume kno", "ew of Predictive Analytics\nParametric vs. Non-Parametric Models\nAlgorithms for predictive analytics include both parametric and non-parametric \nalgorithms. Parametric algorithms (or models) assume known distributions \nin the data. Many parametric algorithms and statistical tests, although not all, \nassume normal distributions and fi nd linear relationships in the data. Machine \nlearning algorithms typically do not assume distributions and therefore are \nconsidered non-parametric or distribution-free models. \nThe advantage of parametric models is that if the distributions are known, \nextensive properties of the data are also known and therefore algorithms can \nbe proven to have very specifi c properties related to errors, convergence, and \ncertainty of learned coeffi cients. Because of the assumptions, however, the \nanalyst often spends considerable time transforming the data so that these \nadvantages can be realized.\nNon-parametric models are far more fl exible because they do not have", "assumptions, however, the \nanalyst often spends considerable time transforming the data so that these \nadvantages can be realized.\nNon-parametric models are far more fl exible because they do not have under-\nlying assumptions about the distribution of the data, saving the analyst con-\nsiderable time in preparing data. However, far less is known about the data a \npriori, and therefore non-parametric algorithms are typically iterative, without \nany guarantee that the best or optimal solution has been found.\nBusiness Intelligence \nBusiness intelligence is a vast fi eld of study that is the subject of entire books; \nthis treatment is brief and intended to summarize the primary characteristics of \nbusiness intelligence as they relate to predictive analytics. The output of many \nbusiness intelligence analyses are reports or dashboards that summarize inter-\nesting characteristics of the data, often described as Key Performance Indicators \n(KPIs). The KPI reports are user-driven, determined by", "gence analyses are reports or dashboards that summarize inter-\nesting characteristics of the data, often described as Key Performance Indicators \n(KPIs). The KPI reports are user-driven, determined by an analyst or decision-\nmaker to represent a key descriptor to be used by the business. These reports \ncan contain simple summaries or very complex, multidimensional measures. \nInterestingly, KPI is almost never used to describe measures of interest in pre-\ndictive analytics software and conferences.\nTypical business intelligence output is a report to be used by analysts and \ndecision-makers. The following are typical questions that might be answered \nby business intelligence for fraud detection and customer analytics:\nFraud Detection \n \n\u25a0How many cases were investigated last month? \n \n\u25a0What was the success rate in collecting debts?\n \n\u25a0How much revenue was recovered through collections?\n \n\u25a0What was the ROI for the various collection avenues: letters, calls, agents?\n\n \nChapter 1 \u25a0 Overview", "was the success rate in collecting debts?\n \n\u25a0How much revenue was recovered through collections?\n \n\u25a0What was the ROI for the various collection avenues: letters, calls, agents?\n\n \nChapter 1 \u25a0 Overview of Predictive Analytics \n7\n \n\u25a0What was the close rate of cases in the past month? Past quarter? Past year?\n \n\u25a0For debts that were closed out, how many days did it take on average to \nclose out debts? \n \n\u25a0For debts that were closed out, how many contacts with the debtor did it \ntake to close out debt?\nCustomer Analytics\n \n\u25a0What were the e-mail open, click-through, and response rates?\n \n\u25a0Which regions/states/ZIPs had the highest response rates?\n \n\u25a0Which products had the highest/lowest click-through rates?\n \n\u25a0How many repeat purchasers were there last month?\n \n\u25a0How many new subscriptions to the loyalty program were there?\n \n\u25a0What is the average spend of those who belong to the loyalty program? \nThose who aren\u2019t a part of the loyalty program? Is this a signifi cant \ndifference?\n \n\u25a0How many vi", "loyalty program were there?\n \n\u25a0What is the average spend of those who belong to the loyalty program? \nThose who aren\u2019t a part of the loyalty program? Is this a signifi cant \ndifference?\n \n\u25a0How many visits to the store/website did a person have?\nThese questions describe characteristics of the unit of analysis: a customer, \na transaction, a product, a day, or even a ZIP code. Descriptions of the unit of \nanalysis are contained in the columns of the data: the attributes. For fraud detec-\ntion, the unit of analysis is sometimes a debt to be collected, or more generally \na case. For customer analytics, the unit of analysis is frequently a customer but \ncould be a visit (a single customer could visit many times and therefore will \nappear in the data many times). \nNote that often these questions compare directly one attribute of interest with \nan outcome of interest. These questions were developed by a domain expert \n(whether an analyst, program manager, or other subject matter expert) as a \n", "compare directly one attribute of interest with \nan outcome of interest. These questions were developed by a domain expert \n(whether an analyst, program manager, or other subject matter expert) as a \nway to describe interesting relationships in the data relevant to the company. \nIn other words, these measures are user-driven.\nAre these KPIs and reports actionable decisions in and of themselves? The \nanswer is no, although they can be with small modifi cations. In the form of \nthe report, you know what happened and can even identify why it happened \nin some cases. It isn\u2019t a great leap, however, to take reports and turn them into \npredictions. For example, a report that summarizes the response rates for each \nZIP code can then use ZIP as a predictor of response rate. \nIf you consider the reports related to a target variable such as response rate, \nthe equivalent machine learning approach is building a decision stump, a sin-\ngle condition rule that predicts the outcome. But this is a ver", "ports related to a target variable such as response rate, \nthe equivalent machine learning approach is building a decision stump, a sin-\ngle condition rule that predicts the outcome. But this is a very simple way of \napproaching prediction.\n\n8 \nChapter 1 \u25a0 Overview of Predictive Analytics\nPredictive Analytics vs. Business Intelligence\nWhat if you reconstruct the two lists of questions in a different way, one that \nis focused more directly on decisions? From a predictive analytics perspective, \nyou may fi nd these questions are the ones asked. \nFraud Detection\n \n\u25a0What is the likelihood that the transaction is fraudulent?\n \n\u25a0What is the likelihood the invoice is fraudulent or warrants further \ninvestigation?\n \n\u25a0Which characteristics of the transaction are most related to or most pre-\ndictive of fraud (single characteristics and interactions)?\n \n\u25a0What is the expected amount of fraud?\n \n\u25a0What is the likelihood that a tax return is non-compliant?\n \n\u25a0Which line items on a tax return contribu", " of fraud (single characteristics and interactions)?\n \n\u25a0What is the expected amount of fraud?\n \n\u25a0What is the likelihood that a tax return is non-compliant?\n \n\u25a0Which line items on a tax return contribute the most to the fraud score?\n \n\u25a0Historically, which demographic and historic purchase patterns were \nmost related to fraud?\nCustomer Analytics for Predictive Analytics\n \n\u25a0What is the likelihood an e-mail will be opened?\n \n\u25a0What is the likelihood a customer will click-through a link in an e-mail?\n \n\u25a0Which product is a customer most likely to purchase if given the choice?\n \n\u25a0How many e-mails should the customer receive to maximize the likeli-\nhood of a purchase?\n \n\u25a0What is the best product to up-sell to the customer after they purchase \na product?\n \n\u25a0What is the visit volume expected on the website next week?\n \n\u25a0What is the likelihood a product will sell out if it is put on sale?\n \n\u25a0What is the estimated customer lifetime value (CLV) of each customer?\nNotice the differences in the kinds o", "ebsite next week?\n \n\u25a0What is the likelihood a product will sell out if it is put on sale?\n \n\u25a0What is the estimated customer lifetime value (CLV) of each customer?\nNotice the differences in the kinds of questions predictive analytics asks com-\npared to business intelligence. The word \u201clikelihood\u201d appears often, meaning \nwe are computing a probability that the pattern exists for a unit of analysis. In \ncustomer analytics, this could mean computing a probability that a customer \nis likely to purchase a product. \nImplicit in the wording is that the measures require an examination of the \ngroups of records comprising the unit of analysis. If the likelihood an individual \ncustomer will purchase a product is one percent, this means that for every 100 \ncustomers with the same pattern of measured attributes for this customer, \n\n \nChapter 1 \u25a0 Overview of Predictive Analytics \n9\none customer purchased the product in the historic data used to compute the \nlikelihood. The comparable measure in the ", "ttributes for this customer, \n\n \nChapter 1 \u25a0 Overview of Predictive Analytics \n9\none customer purchased the product in the historic data used to compute the \nlikelihood. The comparable measure in the business intelligence lists would be \ndescribed as a rate or a percentage; what is the response rate of customers with \na particular purchase pattern.\nThe difference between the business intelligence and predictive analytics \nmeasures is that the business intelligence variables identifi ed in the questions \nwere, as already described, user driven. In the predictive analytics approach, \nthe predictive modeling algorithms considered many patterns, sometimes all \npossible patterns, and determined which ones were most predictive of the \nmeasure of interest (likelihood). The discovery of the patterns is data driven. \nThis is also why many of the questions begin with the word \u201cwhich.\u201d Asking \nwhich line items on a tax return are most related to noncompliance requires \ncomparisons of the line ite", "ns is data driven. \nThis is also why many of the questions begin with the word \u201cwhich.\u201d Asking \nwhich line items on a tax return are most related to noncompliance requires \ncomparisons of the line items as they relate to noncompliance. \nDo Predictive Models Just State the Obvious?\nOften when presenting models to decision-makers, modelers may hear a familiar \nrefrain: \u201cI didn\u2019t need a model to tell me that!\u201d But predictive models do more \nthan just identify attributes that are related to a target variable. They identify the \nbest way to predict the target. Of all the possible alternatives, all of the attributes \nthat could predict the target and all of the interactions between the attributes, \nwhich combinations do the best job? The decision-maker may have been able \nto guess (hypothesize) that length or residence is a good attribute to predict a \nresponder to a Medicare customer acquisition campaign, but that same person \nmay not have known that the number of contacts is even more pred", "that length or residence is a good attribute to predict a \nresponder to a Medicare customer acquisition campaign, but that same person \nmay not have known that the number of contacts is even more predictive, espe-\ncially when the prospect has been mailed two to six times. Predictive models \nidentify not only which variables are predictive, but how well they predict the \ntarget. Moreover, they also reveal which combinations are not just predictive \nof the target, but how well the combinations predict the target and how much \nbetter they predict than individual attributes do on their own.\nSimilarities between Business Intelligence and Predictive \nAnalytics\nOften, descriptions of the differences between business intelligence and predic-\ntive analytics stress that business intelligence is retrospective analysis, looking \nback into the past, whereas predictive analytics or prospective analysis predict \nfuture behavior. The \u201cpredicting the future\u201d label is applied often to predictive \nanalyt", "ospective analysis, looking \nback into the past, whereas predictive analytics or prospective analysis predict \nfuture behavior. The \u201cpredicting the future\u201d label is applied often to predictive \nanalytics in general and the very questions described already imply this is the \ncase. Questions such as \u201cWhat is the likelihood a customer will purchase . . .\u201d \nare forecasting future behavior. \nFigure 1-2 shows a timeline relating data used to build predictive models \nor business intelligence reports. The vertical line in the middle is the time the \n\n10 \nChapter 1 \u25a0 Overview of Predictive Analytics\nmodel is being built (today). The data used to build the models is always to the \nleft: historic data. When predictive models are built to predict a \u201cfuture\u201d event, \nthe data selected to build the predictive models is rolled back to a time prior to \nthe date the future event is known. \nFor example, if you are building models to predict whether a customer will \nrespond to an e-mail campaign, you begi", "ctive models is rolled back to a time prior to \nthe date the future event is known. \nFor example, if you are building models to predict whether a customer will \nrespond to an e-mail campaign, you begin with the date the campaign cured \n(when all the responses have come in) to identify everyone who responded. \nThis is the date for the label \u201ctarget variable computed based on this date\u201d in \nthe fi gure. The attributes used as inputs must be known prior to the date of the \nmailing itself, so these values are collected to the left of the target variable col-\nlection date. In other words, the data is set up with all the modeling data in the \npast, but the target variable is still future to the date the attributes are collected \nin the timeline of the data used for modeling.\nHowever, it\u2019s important to be clear that both business intelligence and predic-\ntive analytics analyses are built from the same data, and the data is historic in \nboth cases. The assumption is that future behavior to the", "ant to be clear that both business intelligence and predic-\ntive analytics analyses are built from the same data, and the data is historic in \nboth cases. The assumption is that future behavior to the right of the vertical \nline in Figure 1-2 will be consistent with past behavior. If a predictive model \nidentifi es patterns in the past that predicted (in the past) that a customer would \npurchase a product, you assume this relationship will continue to be present \nin the future. \nHistoric Data\nFuture Data;\nwhen model will\nbe deployed\nAttributes\ncomputed\nbased on\nthis date\nAttributes\ncomputed\nbased on\nthis date\nTime\nTarget\nvariable\ncomputed\nbased on\nthis date\nDate the\npredictive\nmodels are\nbuilt\nDate the\npredictive\nmodels are\nbuilt\nFigure 1-2:  Timeline for building predictive models\nPredictive Analytics vs. Statistics\nPredictive analytics and statistics have considerable overlap, with some statisti-\ncians arguing that predictive analytics is, at its core, an extension of statistics. \nPr", "ive Analytics vs. Statistics\nPredictive analytics and statistics have considerable overlap, with some statisti-\ncians arguing that predictive analytics is, at its core, an extension of statistics. \nPredictive modelers, for their part, often use algorithms and tests common \nin statistics as a part of their regular suite of techniques, sometimes without \n\n \nChapter 1 \u25a0 Overview of Predictive Analytics \n11\napplying the diagnostics most statisticians would apply to ensure the models \nare built properly.\nSince predictive analytics draws heavily from statistics, the fi eld has taken \nto heart the amusing quote from statistician and creator of the bootstrap, Brad \nEfron: \u201cThose who ignore Statistics are condemned to reinvent it.\u201d Nevertheless, \nthere are signifi cant differences between typical approaches of the two fi elds. \nTable 1-1 provides a short list of items that differ between the fi elds. Statistics is \ndriven by theory in a way that predictive analytics is not, where many algorithm", "pproaches of the two fi elds. \nTable 1-1 provides a short list of items that differ between the fi elds. Statistics is \ndriven by theory in a way that predictive analytics is not, where many algorithms \nare drawn from other fi elds such as machine learning and artifi cial intelligence \nthat have no provable optimum solution. \nBut perhaps the most fundamental difference between the fi elds is summa-\nrized in the last row of the table: For statistics, the model is king, whereas for \npredictive analytics, data is king.\nTable 1-1: Statistics vs. Predictive Analytics\nSTATISTICS\nPREDICTIVE ANALYTICS\nModels based on theory: There is an \noptimum.\nModels often based on non-parametric \nalgorithms; no guaranteed optimum\nModels typically linear.\nModels typically nonlinear\nData typically smaller; algorithms often \ngeared toward accuracy with small data\nScales to big data; algorithms not as e\ufb03  -\ncient or stable for small data\nThe model is king.\nData is king.\nStatistics and Analytics\nIn spite of the", "ithms often \ngeared toward accuracy with small data\nScales to big data; algorithms not as e\ufb03  -\ncient or stable for small data\nThe model is king.\nData is king.\nStatistics and Analytics\nIn spite of the similarities between statistics and analytics, there is a difference \nin mindset that results in differences in how analyses are conducted. Statistics is \noften used to perform confi rmatory analysis where a hypothesis about a relation-\nship between inputs and an output is made, and the purpose of the analysis is \nto confi rm or deny the relationship and quantify the degree of that confi rmation \nor denial. Many analyses are highly structured, such as determining if a drug \nis effective in reducing the incidence of a particular disease.\nControls are essential to ensure that bias is not introduced into the model, thus \nmisleading the analyst\u2019s interpretation of the model. Coeffi cients of models are \ncritically important in understanding what the data are saying, and therefore \ngreat care ", "uced into the model, thus \nmisleading the analyst\u2019s interpretation of the model. Coeffi cients of models are \ncritically important in understanding what the data are saying, and therefore \ngreat care is taken to transform the model inputs and outputs so they comply \nwith assumptions of the modeling algorithms. If the study is predicting the \neffect of caloric intake, smoking, age, height, amount of exercise, and metabo-\nlism on an individual\u2019s weight, and one is to trust the relative contribution of \neach factor on an individual\u2019s weight, it is important to remove any bias due \nto the data itself so that the conclusions refl ect the intent of the model. Bias in \n\n12 \nChapter 1 \u25a0 Overview of Predictive Analytics\nthe data could result in misleading the analyst that the inputs to the model \nhave more or less infl uence that they actually have, simply because of numeric \nproblems in the data.\nResiduals are also carefully examined to identify departure from a Normal \ndistribution, although ", "ave more or less infl uence that they actually have, simply because of numeric \nproblems in the data.\nResiduals are also carefully examined to identify departure from a Normal \ndistribution, although the requirement of normality lessens as the size of the \ndata increases. If residuals are not random with constant variance, the statisti-\ncian will modify the inputs and outputs until these problems are corrected.\nPredictive Analytics and Statistics Contrasted\nPredictive modelers, on the other hand, often show little concern for fi nal param-\neters in the models except in very general terms. The key is often the predic-\ntive accuracy of the model and therefore the ability of the model to make and \ninfl uence decisions. In contrast to the structured problem being solved through \nconfi rmatory analysis using statistics, predictive analytics often attempts to solve \nless structured business problems using data that was not even collected for the \npurpose of building models; it just happened ", "analysis using statistics, predictive analytics often attempts to solve \nless structured business problems using data that was not even collected for the \npurpose of building models; it just happened to be around. Controls are often \nnot in place in the data and therefore causality, very diffi cult to uncover even \nin structured problems, becomes exceedingly diffi cult to identify. Consider, for \nexample, how you would go about identifying which marketing campaign to \napply to a current customer for a digital retailer. This customer could receive \ncontent from any one of ten programs the e-mail marketing group has identi-\nfi ed. The modeling data includes customers, their demographics, their prior \nbehavior on the website and with e-mail they had received in the past, and their \nreaction to sample content from one of the ten programs. The reaction could be \nthat they ignored the e-mail, opened it, clicked through the link, and ultimately \npurchased the product promoted in the e-mail. P", "o sample content from one of the ten programs. The reaction could be \nthat they ignored the e-mail, opened it, clicked through the link, and ultimately \npurchased the product promoted in the e-mail. Predictive models can certainly \nbe built to identify the best program of the ten to put into the e-mail based on \na customer\u2019s behavior and demographics.  \nHowever, this is far from a controlled study. While this program is going on, \neach customer continues to interact with the website, seeing other promotions. \nThe customer may have seen other display ads or conducted Google searches \nfurther infl uencing his or her behavior. The purpose of this kind of model can-\nnot be to uncover fully why the customer behaves in a particular way because \nthere are far too many unobserved, confounding infl uences. But that doesn\u2019t \nmean the model isn\u2019t useful.\nPredictive modelers frequently approach problems in this more unstructured, \neven casual manner. The data, in whatever form it is found, drives ", " uences. But that doesn\u2019t \nmean the model isn\u2019t useful.\nPredictive modelers frequently approach problems in this more unstructured, \neven casual manner. The data, in whatever form it is found, drives the models. \nThis isn\u2019t a problem as long as the data continues to be collected in a manner \nconsistent with the data as it was used in the models; consistency in the data \nwill increase the likelihood that there will be consistency in the model\u2019s predic-\ntions, and therefore how well the model affects decisions.\n\n \nChapter 1 \u25a0 Overview of Predictive Analytics \n13\nPredictive Analytics vs. Data Mining\nPredictive analytics has much in common with its immediate predecessor, data \nmining; the algorithms and approaches are generally the same. Data mining \nhas a history of applications in a wide variety of fi elds, including fi nance, engi-\nneering, manufacturing, biotechnology, customer relationship management, \nand marketing. I have treated the two fi elds as generally synonymous since \n\u201cpredi", "iety of fi elds, including fi nance, engi-\nneering, manufacturing, biotechnology, customer relationship management, \nand marketing. I have treated the two fi elds as generally synonymous since \n\u201cpredictive analytics\u201d became a popular term.\nThis general overlap between the two fi elds is further emphasized by how \nsoftware vendors brand their products, using both data mining and predictive \nanalytics (some emphasizing one term more than the other). \nOn the other hand, data mining has been caught up in the specter of privacy \nconcerns, spam, malware, and unscrupulous marketers. In the early 2000s, con-\ngressional legislation was introduced several times to curtail specifi cally any \ndata mining programs in the Department of Defense (DoD). Complaints were \neven waged against the use of data mining by the NSA, including a letter sent by \nSenator Russ Feingold to the National Security Agency (NSA) Director in 2006:\nOne element of the NSA\u2019s domestic spying program that has gotten too little ", "ining by the NSA, including a letter sent by \nSenator Russ Feingold to the National Security Agency (NSA) Director in 2006:\nOne element of the NSA\u2019s domestic spying program that has gotten too little atten-\ntion is the government\u2019s reportedly widespread use of data mining technology to \nanalyze the communications of ordinary Americans. Today I am calling on the \nDirector of National Intelligence, the Defense Secretary and the Director of the \nNSA to explain whether and how the government is using data mining technology, \nand what authority it claims for doing so.\nIn an interesting d\u00e9j\u00e0 vu, in 2013, information about NSA programs that sift \nthrough phone records was leaked to the media. As in 2006, concerns about \nprivacy were again raised, but this time the mathematics behind the program, \nwhile typically described as data mining in the past, was now often described \nas predictive analytics.\nGraduate programs in analytics often use both data mining and predictive \nanalytics in their de", "\nwhile typically described as data mining in the past, was now often described \nas predictive analytics.\nGraduate programs in analytics often use both data mining and predictive \nanalytics in their descriptions, even if they brand themselves with one or the other. \nWho Uses Predictive Analytics?\nIn the 1990s and early 2000s, the use of advanced analytics, referred to as data \nmining or computational statistics, was relegated to only the most forward-\nlooking companies with deep pockets. Many organizations were still strug-\ngling with collecting data, let alone trying to make sense of it through more \nadvanced techniques. \nToday, the use of analytics has moved from a niche group in large organizations \nto being an instrumental component of most mid- to large-sized organizations. \n\n14 \nChapter 1 \u25a0 Overview of Predictive Analytics\nThe analytics often begins with business intelligence and moves into predictive \nanalytics as the data matures and the pressure to produce greater benefi t from", "er 1 \u25a0 Overview of Predictive Analytics\nThe analytics often begins with business intelligence and moves into predictive \nanalytics as the data matures and the pressure to produce greater benefi t from \nthe data increases. Even small organizations, for-profi t and non-profi t, benefi t \nfrom predictive analytics now, often using open source software to drive deci-\nsions on a small scale.\nChallenges in Using Predictive Analytics\nPredictive analytics can generate signifi cant improvements in effi ciency, deci-\nsion-making, and return on investment. But predictive analytics isn\u2019t always \nsuccessful and, in all likelihood, the majority of predictive analytics models are \nnever used operationally. \nSome of the most common reasons predictive models don\u2019t succeed can be \ngrouped into four categories: obstacles in management, obstacles with data, \nobstacles with modeling, and obstacles in deployment. \nObstacles in Management\nTo be useful, predictive models have to be deployed. Often, deployment", "es: obstacles in management, obstacles with data, \nobstacles with modeling, and obstacles in deployment. \nObstacles in Management\nTo be useful, predictive models have to be deployed. Often, deployment in of \nitself requires a signifi cant shift in resources for an organization and therefore \nthe project often needs support from management to make the transition from \nresearch and development to operational solution. If program management is \nnot a champion of the predictive modeling project and the resulting models, \nperfectly good models will go unused due to lack of resources and lack of politi-\ncal will to obtain those resources.\nFor example, suppose an organization is building a fraud detection model \nto identify transactions that appear to be suspicious and are in need of fur-\nther investigation. Furthermore, suppose the organization can identify 1,000 \ntransactions per month that should receive further scrutiny from investigators. \nProcesses have to be put into place to distribut", "vestigation. Furthermore, suppose the organization can identify 1,000 \ntransactions per month that should receive further scrutiny from investigators. \nProcesses have to be put into place to distribute the cases to the investigators, \nand the fraud detection model has to be suffi ciently trusted by the investigators \nfor them to follow through and investigate the cases. If management is not fully \nsupportive of the predictive models, these cases may be delivered but end up \ndead on arrival.\nObstacles with Data\nPredictive models require data in the form of a single table or fl at fi le containing \nrows and columns: two-dimensional data. If the data is stored in transactional \ndatabases, keys need to be identifi ed to join the data from the data sources to \nform the single view or table. Projects can fail before they even begin if the keys \ndon\u2019t exist in the tables needed to build the data. \n\n \nChapter 1 \u25a0 Overview of Predictive Analytics \n15\nEven if the data can be joined into a single", "jects can fail before they even begin if the keys \ndon\u2019t exist in the tables needed to build the data. \n\n \nChapter 1 \u25a0 Overview of Predictive Analytics \n15\nEven if the data can be joined into a single table, if the primary inputs or \noutputs are not populated suffi ciently or consistently, the data  is meaningless. \nFor example, consider a customer acquisition model. Predictive models need \nexamples of customers who were contacted and did not respond as well as those \nwho were contacted and did respond. If active customers are stored in one table \nand marketing contacts (leads) in a separate table, several problems can thwart \nmodeling efforts. First, unless customer tables include the campaign they were \nacquired from, it may be impossible to reconstruct the list of leads in a campaign \nalong with the label that the lead responded or didn\u2019t respond to the contact. \nSecond, if customer data, including demographics (age, income, ZIP), is \noverwritten to keep it up-to-date, and the demog", "long with the label that the lead responded or didn\u2019t respond to the contact. \nSecond, if customer data, including demographics (age, income, ZIP), is \noverwritten to keep it up-to-date, and the demographics at the time they were \nacquired is not retained, a table containing leads as they appeared at the time \nof the marketing campaign can never be reconstructed. As a simple example, \nsuppose phone numbers are only obtained after the lead converts and becomes \na customer. A great predictor of a lead becoming a customer would then be \nwhether the lead has a phone number; this is leakage of future data unknown \nat the time of the marketing campaign into the modeling data.\nObstacles with Modeling\nPerhaps the biggest obstacle to building predictive models from the analyst\u2019s \nperspective is overfi tting, meaning that the model is too complex, essentially \nmemorizing the training data. The effect of overfi tting is twofold: The model \nperforms poorly on new data and the interpretation of the", "rfi tting, meaning that the model is too complex, essentially \nmemorizing the training data. The effect of overfi tting is twofold: The model \nperforms poorly on new data and the interpretation of the model is unreliable. \nIf care isn\u2019t taken in the experimental design of the predictive models, the extent \nof model overfi t isn\u2019t known until the model has already been deployed and \nbegins to fail.\nA second obstacle with building predictive models occurs when zealous \nanalysts become too ambitious in the kind of model that can be built with the \navailable data and in the timeframe allotted. If they try to \u201chit a home run\u201d \nand can\u2019t complete the model in the timeframe, no model will be deployed at \nall. Often a better strategy is to build simpler models fi rst to ensure a model \nof some value will be ready for deployment. Models can be augmented and \nimproved later if time allows.\nFor example, consider a customer retention model for a company with an online \npresence. A zealous modeler ", "ue will be ready for deployment. Models can be augmented and \nimproved later if time allows.\nFor example, consider a customer retention model for a company with an online \npresence. A zealous modeler may be able to identify thousands of candidate \ninputs to the retention model, and in an effort to build the best possible model, \nmay be slowed by the sheer combinatorics involved with data preparation and \nvariable selection prior to and during modeling. \nHowever, from the analyst\u2019s experience, he or she may be able to identify 100 \nvariables that have been good predictors historically. While the analyst suspects \nthat a better model could be built with more candidate inputs, the fi rst model \ncan be built from the 100 variables in a much shorter timeframe.\n\n16 \nChapter 1 \u25a0 Overview of Predictive Analytics\nObstacles in Deployment\nPredictive modeling projects can fail because of obstacles in the deployment \nstage of modeling. The models themselves are typically not very complicated \ncompu", "ctive Analytics\nObstacles in Deployment\nPredictive modeling projects can fail because of obstacles in the deployment \nstage of modeling. The models themselves are typically not very complicated \ncomputationally, requiring only dozens, hundreds, thousands, or tens of thou-\nsands of multiplies and adds, easily handled by today\u2019s servers.\nAt the most fundamental level, however, the models have to be able to be \ninterrogated by the operational system and to issue predictions consistent with \nthat system. In transactional systems, this typically means the model has to be \nencoded in a programming language that can be called by the system, such as \nSQL, C++, Java, or another high-level language. If the model cannot be translated \nor is translated incorrectly, the model is useless operationally. \nSometimes the obstacle is getting the data into the format needed for deploy-\nment. If the modeling data required joining several tables to form the single \nmodeling table, deployment must replicate ", "\nSometimes the obstacle is getting the data into the format needed for deploy-\nment. If the modeling data required joining several tables to form the single \nmodeling table, deployment must replicate the same joining steps to build the \ndata the models need for scoring. In some transactional systems with dispa-\nrate data forming the modeling table, complex joins may not be possible in the \ntimeline needed. For example, consider a model that recommends content to be \ndisplayed on a web page. If that model needs data from the historic patterns of \nbrowsing behavior for a visitor and the page needs to be rendered in less than \none second, all of the data pulls and transformations must meet this timeline.\nWhat Educational Background Is Needed to Become a \nPredictive Modeler?\nConventional wisdom says that predictive modelers need to have an academic \nbackground in statistics, mathematics, computer science, or engineering. A \ndegree in one of these fi elds is best, but without a degree, at a", "m says that predictive modelers need to have an academic \nbackground in statistics, mathematics, computer science, or engineering. A \ndegree in one of these fi elds is best, but without a degree, at a minimum, one \nshould at least have taken statistics or mathematics courses. Historically, one \ncould not get a degree in predictive analytics, data mining, or machine learning. \nThis has changed, however, and dozens of universities now offer master\u2019s \ndegrees in predictive analytics. Additionally, there are many variants of analytics \ndegrees, including master\u2019s degrees in data mining, marketing analytics, busi-\nness analytics, or machine learning. Some programs even include a practicum \nso that students can learn to apply textbook science to real-world problems.\nOne reason the real-world experience is so critical for predictive modeling is \nthat the science has tremendous limitations. Most real-world problems have data \nproblems never encountered in the textbooks. The ways in which data ", "erience is so critical for predictive modeling is \nthat the science has tremendous limitations. Most real-world problems have data \nproblems never encountered in the textbooks. The ways in which data can go \nwrong are seemingly endless; building the same customer acquisition models \neven within the same domain requires different approaches to data prepara-\ntion, missing value imputation, feature creation, and even modeling methods. \n\n \nChapter 1 \u25a0 Overview of Predictive Analytics \n17\nHowever, the principles of how one can solve data problems are not endless; the \nexperience of building models for several years will prepare modelers to at least \nbe able to identify when potential problems may arise.\nSurveys of top-notch predictive modelers reveal a mixed story, however. \nWhile many have a science, statistics, or mathematics background, many do \nnot. Many have backgrounds in social science or humanities. How can this be?\nConsider a retail example. The retailer Target was building predict", "cience, statistics, or mathematics background, many do \nnot. Many have backgrounds in social science or humanities. How can this be?\nConsider a retail example. The retailer Target was building predictive models \nto identify likely purchase behavior and to incentivize future behavior with rel-\nevant offers. Andrew Pole, a Senior Manager of Media and Database Marketing \ndescribed how the company went about building systems of predictive mod-\nels at the Predictive Analytics World Conference in 2010. Pole described the \nimportance of a combination of domain knowledge, knowledge of predictive \nmodeling, and most of all, a forensic mindset in successful modeling of what \nhe calls a \u201cguest portrait.\u201d\nThey developed a model to predict if a female customer was pregnant. They \nnoticed patterns of purchase behavior, what he called \u201cnesting\u201d behavior. For \nexample, women were purchasing cribs on average 90 days before the due date. \nPole also observed that some products were purchased at regular i", "rchase behavior, what he called \u201cnesting\u201d behavior. For \nexample, women were purchasing cribs on average 90 days before the due date. \nPole also observed that some products were purchased at regular intervals prior \nto a woman\u2019s due date. The company also observed that if they were able to \nacquire these women as purchasers of other products during the time before \nthe birth of their baby, Target was able to increase signifi cantly the customer \nvalue; these women would continue to purchase from Target after the baby was \nborn based on their purchase behavior before.\nThe key descriptive terms are \u201cobserved\u201d and \u201cnoticed.\u201d This means the mod-\nels were not built as black boxes. The analysts asked, \u201cdoes this make sense?\u201d \nand leveraged insights gained from the patterns found in the data to produce \nbetter predictive models. It undoubtedly was iterative; as they \u201cnoticed\u201d pat-\nterns, they were prompted to consider other patterns they had not explicitly \nconsidered before (and maybe had no", "roduce \nbetter predictive models. It undoubtedly was iterative; as they \u201cnoticed\u201d pat-\nterns, they were prompted to consider other patterns they had not explicitly \nconsidered before (and maybe had not even occurred to them before). This \nforensic mindset of analysts, noticing interesting patterns and making connec-\ntions between those patterns and how the models could be used, is critical to \nsuccessful modeling. It is rare that predictive models can be fully defi ned before \na project and anticipate all of the most important patterns the model will fi nd. \nSo we shouldn\u2019t be surprised that we will be surprised, or put another way, we \nshould expect to be surprised. \nThis kind of mindset is not learned in a university program; it is part of the \npersonality of the individual. Good predictive modelers need to have a forensic \nmindset and intellectual curiosity, whether or not they understand the math-\nematics enough to derive the equations for linear regression.\n\n\n19\nThe most important", "ive modelers need to have a forensic \nmindset and intellectual curiosity, whether or not they understand the math-\nematics enough to derive the equations for linear regression.\n\n\n19\nThe most important part of any predictive modeling project is the very begin-\nning when the predictive modeling project is defi ned. Setting up a predictive \nmodeling project is a very diffi cult task because the skills needed to do it well \nare very broad, requiring knowledge of the business domain, databases, or data \ninfrastructure, and predictive modeling algorithms and techniques. Very few \nindividuals have all of these skill sets, and therefore setting up a predictive \nmodeling project is inevitably a team effort. \nThis chapter describes principles to use in setting up a predictive modeling \nproject. The role practitioners play in this stage is critical because missteps in \ndefi ning the unit of analysis, target variables, and metrics to select models can \nrender modeling projects ineffective. \nPredic", "role practitioners play in this stage is critical because missteps in \ndefi ning the unit of analysis, target variables, and metrics to select models can \nrender modeling projects ineffective. \nPredictive Analytics Processing Steps: CRISP-DM\nThe Cross-Industry Standard Process Model for Data Mining (CRISP-DM) \ndescribes the data-mining process in six steps. It has been cited as the most-\noften used process model since its inception in the 1990s. The most frequently \ncited alternative to CRISP-DM is an organization\u2019s or practitioner\u2019s own process \nmodel, although upon more careful examination, these are also essentially the \nsame as CRISP-DM.\nC H A P T E R \n2\nSetting Up the Problem\n\n20 \nChapter 2 \u25a0 Setting Up the Problem\nOne advantage of using CRISP-DM is that it describes the most commonly \napplied steps in the process and is documented in an 80-page PDF fi le. The \nCRISP-DM name itself calls out data mining as the technology, but the same pro-\ncess model applies to predictive analytic", " \napplied steps in the process and is documented in an 80-page PDF fi le. The \nCRISP-DM name itself calls out data mining as the technology, but the same pro-\ncess model applies to predictive analytics and other related analytics approaches, \nincluding business analytics, statistics, and text mining. \nThe CRISP-DM audience includes both managers and practitioners. For pro-\ngram managers, CRISP-DM describes the steps in the modeling process from \na program perspective, revealing the steps analysts will be accomplishing \nas they build predictive models. Each of the steps can then have its own cost \nestimates and can be tracked by the manager to ensure the project deliverables \nand timetables are met. The last step in many of the sub-tasks in CRISP-DM is \na report describing what decisions were made and why. In fact, the CRISP-DM \ndocument identifi es 28 potential deliverables for a project. This certainly is \nmusic to the program manager\u2019s ears!\nFor practitioners, the step-by-step proces", "re made and why. In fact, the CRISP-DM \ndocument identifi es 28 potential deliverables for a project. This certainly is \nmusic to the program manager\u2019s ears!\nFor practitioners, the step-by-step process provides structure for analysis \nand not only reminds the analyst of the steps that need to be accomplished, but \nalso the need for documentation and reporting throughout the process, which \nis particularly valuable for new modelers. Even for experienced practitioners, \nCRISP-DM describes the steps succinctly and logically. Many practitioners are \nhesitant to describe the modeling process in linear, step-by-step terms because \nprojects almost never proceed as planned due to problems with data and mod-\neling; surprises occur in nearly every project. However, a good baseline is still \nvaluable, especially as practitioners describe to managers what they are doing \nand why they are doing it; CRISP-DM provides the justifi cation for the steps \nthat need to be completed in the process of build", ", especially as practitioners describe to managers what they are doing \nand why they are doing it; CRISP-DM provides the justifi cation for the steps \nthat need to be completed in the process of building models.\nThe six steps in the CRISP-DM process are shown in Figure 2-1: Business \nUnderstanding, Data Understanding, Data Preparation, Modeling, Evaluation, and \nDeployment. These steps, and the sequence they appear in the fi gure, represent \nthe most common sequence in a project. These are described briefl y in Table 2-1.\nTable 2-1: CRISM-DM Sequence\nSTAGE\nDESCRIPTION\nBusiness Understanding\nDe\ufb01 ne the project.\nData Understanding\nExamine the data; identify problems in the data.\nData Preparation\nFix problems in the data; create derived variables.\nModeling\nBuild predictive or descriptive models.\nEvaluation\nAssess models; report on the expected e\ufb00 ects of models.\nDeployment\nPlan for use of models.\n\n \nChapter 2 \u25a0 Setting Up the Problem \n21\nData\nData\nData\nEvaluation\nModeling\nData\nPreparation", "ls.\nEvaluation\nAssess models; report on the expected e\ufb00 ects of models.\nDeployment\nPlan for use of models.\n\n \nChapter 2 \u25a0 Setting Up the Problem \n21\nData\nData\nData\nEvaluation\nModeling\nData\nPreparation\nData\nUnderstanding\nBusiness\nUnderstanding\nDeployment\nFigure 2-1: The CRISP-DM process model\nNote the feedback loops in the fi gure. These indicate the most common ways \nthe typical process is modifi ed based on fi ndings during the project. For example, \nif business objectives have been defi ned during Business Understanding, and \nthen data is examined during Data Understanding, you may fi nd that there \nis insuffi cient data quantity or data quality to build predictive models. In this \ncase, Business Objectives must be re-defi ned with the available data in mind \nbefore proceeding to Data Preparation and Modeling. Or consider a model that \nhas been built but has poor accuracy. Revisiting data preparation to create new \nderived variables is a common step to improve the models.\nBusiness Un", "eparation and Modeling. Or consider a model that \nhas been built but has poor accuracy. Revisiting data preparation to create new \nderived variables is a common step to improve the models.\nBusiness Understanding\nEvery predictive modeling project needs objectives. Domain experts who under-\nstand decisions, alarms, estimates, or reports that provide value to an organization \nmust defi ne these objectives. Analysts themselves sometimes have this expertise, \nalthough most often, managers and directors have a far better perspective on \nhow models affect the organization. Without domain expertise, the defi nitions \nof what models should be built and how they should be assessed can lead to \nfailed projects that don\u2019t address the key business concerns.\n\n22 \nChapter 2 \u25a0 Setting Up the Problem\nThe Three-Legged Stool\nOne way to understand the collaborations that lead to predictive modeling suc-\ncess is to think of a three-legged stool. Each leg is critical to the stool remaining \nstable and fulfi", "hree-Legged Stool\nOne way to understand the collaborations that lead to predictive modeling suc-\ncess is to think of a three-legged stool. Each leg is critical to the stool remaining \nstable and fulfi lling its intended purpose. In predictive modeling, the three legs \nof the stool are (1) domain experts, (2) data or database experts, and (3) predictive \nmodeling experts. Domain experts are needed to frame a problem properly in \na way that will provide value to the organization. Data or database experts are \nneeded to identify what data is available for predictive modeling and how that \ndata can be accessed and normalized. Predictive modelers are needed to build \nthe models that achieve the business objectives.\nConsider what happens if one or more of these three legs are missing. If the \nproblem is not defi ned properly and only modelers and the database adminis-\ntrator are defi ning the problems, excellent models may be built with fantastic \naccuracy only to go unused because the model", "oblem is not defi ned properly and only modelers and the database adminis-\ntrator are defi ning the problems, excellent models may be built with fantastic \naccuracy only to go unused because the model doesn\u2019t address an actual need of \nthe organization. Or in a more subtle way, perhaps the model predicts the right \nkind of decision, but the models are assessed in a way that doesn\u2019t address very \nwell what matters most to the business; the wrong model is selected because \nthe wrong metric for describing good models is used.\nIf the database expert is not involved, data problems may ensue. First, there \nmay not be enough understanding of the layout of tables in the database to \nbe able to access all of the fi elds necessary for predictive modeling. Second, \nthere may be insuffi cient understanding of fi elds and what information they \nrepresent even if the names of the fi elds seem intuitive, or worse still, if the \nnames are cryptic and no data dictionary is available. Third, insuffi cie", "nding of fi elds and what information they \nrepresent even if the names of the fi elds seem intuitive, or worse still, if the \nnames are cryptic and no data dictionary is available. Third, insuffi cient per-\nmissions may preclude pulling data into the predictive modeling environment. \nFourth, database resources may not support the kinds of joins the analyst may \nbelieve he or she needs to build the modeling data. And fi fth, model deployment \noptions envisioned by the predictive modeling team may not be supported by \nthe organization. \nIf the predictive modelers are not available during the business understanding \nstage of CRISP-DM, obstacles outlined in this chapter may result. First, a lack \nof understanding by program managers of what the predictive models can do, \ndriven by hype around predictive modeling, can lead the manager to specify \nmodels that are impossible to actually build. Second, defi ning target variables \nfor predictive modeling may not be undertaken at all or, if don", " predictive modeling, can lead the manager to specify \nmodels that are impossible to actually build. Second, defi ning target variables \nfor predictive modeling may not be undertaken at all or, if done, may be speci-\nfi ed poorly, thwarting predictive modeling efforts. Third, without predictive \nmodelers defi ning the layout of data needed for building predictive models, a \n\n \nChapter 2 \u25a0 Setting Up the Problem \n23\nmodeling table to be used by the modeler may not be defi ned at all or may lack \nkey fi elds needed for the models.\nBusiness Objectives\nAssuming all three types of individuals that make up the three-legged stool \nof predictive modeling are present during the Business Understand stage of \nCRISP-DM, tradeoffs and compromises are not unusual during the hours or \neven days of meetings that these individuals and groups participate in so that \nsolid business and predictive modeling objectives are defi ned. \nSix key issues that should be resolved during the Business Understanding \n", "gs that these individuals and groups participate in so that \nsolid business and predictive modeling objectives are defi ned. \nSix key issues that should be resolved during the Business Understanding \nstage include defi nitions of the following:\n \n\u25a0Core business objectives to be addressed by the predictive models\n \n\u25a0How the business objectives can be quantifi ed\n \n\u25a0What data is available to quantify the business objectives\n \n\u25a0What modeling methods can be invoked to describe or predict the busi-\nness objectives\n \n\u25a0How the goodness of model fi t of the business objectives are quantifi ed \nso that the model scores make business sense\n \n\u25a0How the predictive models can be deployed operationally\nFrequently, the compromises reached during discussions are the result of the \nimperfect environment that is typical in most organizations. For example, data \nthat you would want to use in the predictive models may not be available in a \ntimely manner or at all. Target variables that address the busines", "t is typical in most organizations. For example, data \nthat you would want to use in the predictive models may not be available in a \ntimely manner or at all. Target variables that address the business objectives \nmore directly may not exist or be able to be quantifi ed. Computing resources \nmay not exist to build predictive models in the way the analysts would prefer. \nOr there may not be available staff to apply to the project in the timeframe \nneeded. And these are just a few possible issues that may be uncovered. Project \nmanagers need to be realistic about which business objectives can be achieved \nin the timeframe and within the budget available.\nPredictive modeling covers a wide range of business objectives. Even the term \n\u201cbusiness objectives\u201d is restrictive as modeling can be done for more than just \nwhat you normally associate with a commercial enterprise. Following is a short \nlist of predictive modeling projects. I personally have either built models for, or \nadvised a cust", " for more than just \nwhat you normally associate with a commercial enterprise. Following is a short \nlist of predictive modeling projects. I personally have either built models for, or \nadvised a customer on, building models for each of these projects. \n\n24 \nChapter 2 \u25a0 Setting Up the Problem\nPROJECT\nCustomer acquisition/\nResponse/Lead generation \nCredit card application \nfraud\nMedical image anomaly \ndetection\nCross-sell/Up-sell \nLoan application fraud\nRadar signal, vehicle/aircraft \nidenti\ufb01 cation\nCustomer next product to \npurchase\nInvoice fraud\nRadar, friend-or-foe \ndi\ufb00 erentiation\nCustomer likelihood to \npurchase in N days\nInsurance claim fraud\nSonar signal object identi\ufb01 ca-\ntion (long and short range)\nWebsite\u2014next site to interact \nwith\nInsurance application \nfraud\nOptimum guidance com-\nmands for smart bombs or \ntank shells\nMarket-basket analysis\nMedical billing fraud\nLikelihood for \ufb02 ight to be on \ntime\nCustomer value/Customer \npro\ufb01 tability\nPayment fraud\nInsurance risk of catast", "m-\nmands for smart bombs or \ntank shells\nMarket-basket analysis\nMedical billing fraud\nLikelihood for \ufb02 ight to be on \ntime\nCustomer value/Customer \npro\ufb01 tability\nPayment fraud\nInsurance risk of catastrophic \nclaim\nCustomer segmentation\nWarranty fraud\nWeed tolerance to pesticides\nCustomer engagement with \nbrand\nTax collection likeli-\nhood to pay\nMean time to failure/\nLikelihood to fail\nCustomer attrition/Retention\nNon-\ufb01 ler predicted tax \nliability\nLikelihood of hardware \nfailure due to complexity\nCustomer days to next \npurchase\nPatient likelihood to \nre-admit\nFault detection/Fault \nexplanation\nCustomer satisfaction\nPatient likelihood to \ncomply with medica-\ntion protocols\nPart needed for repair\nCustomer sentiment/\nRecommend to a friend\nCancer detection\nIntrusion detection/\nLikelihood of an intrusion \nevent\nBest marketing creative\nGene expression/\nIdenti\ufb01 cation\nNew hire likelihood to \nsucceed/advance\nCredit card transaction fraud\nPredicted toxicity \n(LD50 or LC50) of \nsubstance\nNew hir", " intrusion \nevent\nBest marketing creative\nGene expression/\nIdenti\ufb01 cation\nNew hire likelihood to \nsucceed/advance\nCredit card transaction fraud\nPredicted toxicity \n(LD50 or LC50) of \nsubstance\nNew hire most desirable \ncharacteristics\nWhile many models are built to predict the behavior of people or things, not \nall are. Some models are built expressly for the purpose of understanding the \nbehavior of people, things, or processes better. For example, predicting \u201cweed \ntolerance to pesticides\u201d was built to test the hypothesis that the weeds were \nbecoming intolerant to a specifi c pesticide. The model identifi ed the primary \n\n \nChapter 2 \u25a0 Setting Up the Problem \n25\ncontributors in predicting success or failure in killing the weeds; this in and of \nitself was insightful. While the likelihood of a customer purchasing a product \nwithin seven days is interesting on its own, understanding why the customer \nis likely to purchase can provide even more value as the business decides how \nbest to", " of a customer purchasing a product \nwithin seven days is interesting on its own, understanding why the customer \nis likely to purchase can provide even more value as the business decides how \nbest to contact the individuals. Or if a Customer Retention model is built with \nhigh accuracy, those customers that match the profi le for retention but attrite \nnevertheless become a good segment for call-center follow-up. \nDe\ufb01 ning Data for Predictive Modeling\nData for predictive modeling must be two-dimensional, comprised of rows \nand columns. Each row represents what can be called a unit of analysis. For cus-\ntomer analytics, this is typically a customer. For fraud detection, this may be \na transaction. For call center analytics, this may refer to an individual call. For \nsurvey analysis, this may be a single survey. The unit of analysis is problem-\nspecifi c and therefore is defi ned as part of the Business Understanding stage \nof predictive modeling.\nIf data for modeling is loaded from fi ", "ay be a single survey. The unit of analysis is problem-\nspecifi c and therefore is defi ned as part of the Business Understanding stage \nof predictive modeling.\nIf data for modeling is loaded from fi les, the actual form of the data is largely \nirrelevant because most software packages support data in a variety of formats:\n \n\u25a0Delimited fl at fi les, usually delimited with commas (.csv fi les), tabs, or \nsome other custom character to indicate where fi eld values begin and end\n \n\u25a0Fixed-width fl at fi les with a fi xed number of characters per fi eld. No \ndelimiters are needed in this format but the exact format for each fi eld \nmust be known before loading the data.\n \n\u25a0Other customized fl at fi les\n \n\u25a0Binary fi les, including formats specifi c to software packages such as SPSS \nfi les (.sav), SAS fi les (.sas7bdat), and Matlab fi les (.mat)\nMost software packages also provide connectivity to databases through \nnative or ODBC drivers so that tables and views can be accessed directly from", "), SAS fi les (.sas7bdat), and Matlab fi les (.mat)\nMost software packages also provide connectivity to databases through \nnative or ODBC drivers so that tables and views can be accessed directly from \nthe software. Some software allows for the writing of simple or even complex \nqueries to access the data from within the software itself, which is very conve-\nnient for several reasons:\n \n\u25a0Data does not have to be saved to disk and loaded into the predictive \nmodeling software, a slow process for large data sets.\n \n\u25a0Data can be maintained in the database without having to provide version \ncontrol for the fl at fi les.\n \n\u25a0Analysts have greater control and fl exibility over the data they pull from \nthe database or data mart.\n\n26 \nChapter 2 \u25a0 Setting Up the Problem\nHowever, you must also be careful that the tables and views being accessed \nremain the same throughout the modeling project and aren\u2019t changing without \nany warning. When data changes without the knowledge of the analyst, models ", "eful that the tables and views being accessed \nremain the same throughout the modeling project and aren\u2019t changing without \nany warning. When data changes without the knowledge of the analyst, models \ncan also change inexplicably. \nDe\ufb01 ning the Columns as Measures\nColumns in the data are often called attributes, descriptors, variables, fi elds, features, \nor just columns. The book will use these labels interchangeably. Variables in the \ndata are measures that relate to or describe the record. For customer analytics, \none attribute may be the customer ID, a second the customer\u2019s age, a third the \ncustomer\u2019s street address, and so forth. The number of attributes in the data \nis limited only by what is measured for the particular unit of analysis, which \nattributes are considered to be useful, and how many attributes can be handled \nby the database or predictive modeling software.\nColumns in the data are measures of that unit of analysis, and for predictive \nmodeling algorithms, the numbe", ", and how many attributes can be handled \nby the database or predictive modeling software.\nColumns in the data are measures of that unit of analysis, and for predictive \nmodeling algorithms, the number of columns and the order of the columns must \nbe identical from record to record. Another way to describe this kind of data \nis that the data is rectangular. Moreover, the meaning of the columns must be \nconsistent. If you are building models based on customer behavior, you are faced \nwith an immediate dilemma: How can you handle customers who have visited \ndifferent numbers of times and maintain the rectangular shape of the data? \nConsider Table 2-2 with two customers of a hotel chain, one of whom has visited \nthree times and the other only once. In the table layout, the column labeled \u201cDate \nof Visit 1\u201d is the date of the fi rst visit the customer made to the hotel property. \nCustomer 100001 has visited only once and therefore has no values for visit 2 \nand visit 3. These can be labele", "e \nof Visit 1\u201d is the date of the fi rst visit the customer made to the hotel property. \nCustomer 100001 has visited only once and therefore has no values for visit 2 \nand visit 3. These can be labeled as \u201cNULL\u201d or just left blank. The fact that they \nare not defi ned, however, can cause problems for some modeling algorithms, \nand therefore you often will not represent multiple visits as separate columns. \nThis issue is addressed in Chapter 4.\nTable 2-2: Simple Rectangular Layout of Data\nCUSTOMER ID\nDATE OF VISIT 1\nDATE OF VISIT 2\nDATE OF VISIT 3\n100001\n5/2/12\nNULL\nNULL\n100002\n6/9/12\n9/29/12\n10/13/12\nThere is a second potential problem with this layout of the data, however. \nThe \u201cDate of Visit 1\u201d is the fi rst visit. What if the pattern of behavior related to \nthe models is better represented by how the customer behaved most recently? \nFor customer 100001, the most recent visit is contained in the column \u201cDate of \n\n \nChapter 2 \u25a0 Setting Up the Problem \n27\nVisit 1,\u201d whereas for customer", "ted by how the customer behaved most recently? \nFor customer 100001, the most recent visit is contained in the column \u201cDate of \n\n \nChapter 2 \u25a0 Setting Up the Problem \n27\nVisit 1,\u201d whereas for customer 100002, the most recent visit is in the column \u201cDate \nof Visit 3.\u201d Predictive modeling algorithms consider each column as a separate \nmeasure, and therefore, if there is a strong pattern related to the most recent \nvisit, the pattern is broken in this representation of the data. Alternatively, you \ncould represent the same data as shown in Table 2-3.\nTable 2-3: Alternative Rectangular Layout of Data\nCUSTOMER ID\nDATE OF VISIT 1\nDATE OF VISIT 2\nDATE OF VISIT 3\n100001\n5/2/12\nNULL\nNULL\n100002\n10/13/12\n9/29/12\n6/9/12\nIn this data, Visit 1 is no longer the fi rst visit but is the most recent visit, Visit \n2 is two visits ago, Visit 3 is three visits ago, and so on. The representation of the \ndata you choose is dependent on which representation is expected to provide \nthe most predictive set of ", " Visit \n2 is two visits ago, Visit 3 is three visits ago, and so on. The representation of the \ndata you choose is dependent on which representation is expected to provide \nthe most predictive set of inputs to models. \nA third option for this customer data is to remove the temporal data com-\npletely and represent the visits in a consistent set of attributes that summarizes \nthe visits. Table 2-4 shows one such representation: The same two customers \nare described by their most recent visit, the fi rst visit, and the number of visits.\nTable 2-4: Summarized Representation of Visits\nCUSTOMER ID\nDATE OF FIRST \nVISIT\nDATE OF MOST \nRECENT VISIT\nNUMBER OF \nVISITS\n100001\n5/2/12\n5/2/12\n1\n100002\n6/9/12\n10/13/12\n3\nUltimately, the representation problems described in Tables 2-3, 2-4, and 2-5 \noccur because this data is inherently three dimensional, not two. There is a tem-\nporal dimension that has to be represented in the row-column format, usually \nby summarizing the temporal dimension into featu", "cause this data is inherently three dimensional, not two. There is a tem-\nporal dimension that has to be represented in the row-column format, usually \nby summarizing the temporal dimension into features of the data. \nDe\ufb01 ning the Unit of Analysis \nPredictive modeling algorithms assume that each record is an independent \nobservation. Independence in this context merely means that the algorithms \ndo not consider direct connections between records. For example, if records 1 \nand 2 are customers who are husband and wife and frequently travel together, \nthe algorithms treat these two no differently than any two people with similar \npatterns of behavior; the algorithms don\u2019t know they are related or connected.\n\n28 \nChapter 2 \u25a0 Setting Up the Problem\nIf the rows are not independent and in some way are connected to each other, \nthe data itself will not be representative of the broad set of patterns in the com-\nplete set of records the model may encounter after it is deployed. \nConsider hospit", "ay are connected to each other, \nthe data itself will not be representative of the broad set of patterns in the com-\nplete set of records the model may encounter after it is deployed. \nConsider hospitality analytics where each record is a customer who visits \nthe property of a hotel. The assumption of independence is satisfi ed because \n(presumably) each customer behaves as a separate, independent entity. This is \nplausible, although there are exceptions to this assumption in the case of busi-\nness conventions and conferences. \nBut what if the modeling data is not truly independent and is built from a \nsingle organization that has a contract with the hotel. Assume also that the \norganization has travel procedures in place so that reservations are always \nplaced in the same way, and visits can only be made for business purposes. \nThese records have a connection with each other, namely in the procedures of \nthe organization. A model built from these records is, of course, biased because ", "can only be made for business purposes. \nThese records have a connection with each other, namely in the procedures of \nthe organization. A model built from these records is, of course, biased because \nthe data comes from a single organization, and therefore may not apply well to \nthe general population. In addition, however, models built from this data will \nidentify patterns that are related to the organization\u2019s procedures rather than \nthe visitors\u2019 behavior had they decided on their own to visit the hotel.\nWhich Unit of Analysis?\nIt isn\u2019t always clear which unit of analysis to use. Suppose you are building \nmodels for the same hospitality organization and that the business objectives \ninclude identifying customer behavior so they can customize marketing cre-\native content to better match the type of visitor they are contacting. Assume \nalso that the objective is to increase the number of visits a customer makes to \nthe property in the next quarter. \nThe fi rst obvious choice is to m", "h the type of visitor they are contacting. Assume \nalso that the objective is to increase the number of visits a customer makes to \nthe property in the next quarter. \nThe fi rst obvious choice is to make the unit of analysis a person: a customer. \nIn this scenario, each record will represent an individual customer, and columns \nwill describe the behavior of that customer, including their demographics and \ntheir behavior at the hotel. If the customer has visited the hotel on multiple \noccasions, that history must be represented in the single record. Some possible \nvariables relate to the most recent visit, how frequently the customer visits, \nand how much the customer spends on the reservation, restaurant, and other \namenities at the hotel. These derived variables will take some effort to defi ne \nand compute. However, each record will contain a complete summary of that \ncustomer\u2019s behavior, and no other record will relate to that particular customer\u2014\neach record is an independent obser", "efi ne \nand compute. However, each record will contain a complete summary of that \ncustomer\u2019s behavior, and no other record will relate to that particular customer\u2014\neach record is an independent observation.\nWhat is obscured by this representation is detail-related to each individual \nvisit. If a customer has undergone a sequence of visits that is trending toward \nhigher or lower spend, rolling up the visits into a single record for the customer \n\n \nChapter 2 \u25a0 Setting Up the Problem \n29\nwill obscure the details unless the details are explicitly revealed as columns in \nthe data. To capture specifi c detail, you must create additional derived variables.\nA second unit of analysis is the visit: Each record contains only informa-\ntion about a single visit and the behavior of the customer during that visit. If a \ncustomer visited ten times, that customer will have ten records in the data and \nthese records would be considered independent events without any immediate \nconnection to one anoth", "g that visit. If a \ncustomer visited ten times, that customer will have ten records in the data and \nthese records would be considered independent events without any immediate \nconnection to one another; the modeling algorithms would not know that this \nparticular customer had visited multiple times, nor would the algorithms know \nthere is a connection between the visits. The effect of an individual customer \nvisiting multiple times is this: The pattern of that customer is weighted by the \nnumber of times the customer visits, making it appear to the algorithms that \nthe pattern exists more often in the data. From a visit perspective, of course, \nthis is true, whereas from a customer perspective it is not true. \nOne way to communicate the connection between visits to individual cus-\ntomers is to include derived variables in the data that explicitly connect the \nhistory of visits. Derived variables such as the number of visits, the average \nspend in the past three visits, and the number ", "is to include derived variables in the data that explicitly connect the \nhistory of visits. Derived variables such as the number of visits, the average \nspend in the past three visits, and the number of days since the last visit can \nall be added. You must take care to avoid leaking future information from the \nderived variable into the record for the visit, however. If you create a derived \nvariable summarizing the number of visits the customer has made, no future \nvisit can be included, only visits whose end dates precede the date of the visit for \nthe record. This precludes creating these derived variables from simple \u201cgroup \nby\u201d operations with an appropriate \u201cwhere\u201d clause. \nUltimately, the unit of analysis selected for modeling is determined by the \nbusiness objectives and how the model will be used operationally. Are decisions \nmade from the model scores based on a transaction? Are they made based on the \nbehavior of a single customer? Are they made based on a single visit, or b", "del will be used operationally. Are decisions \nmade from the model scores based on a transaction? Are they made based on the \nbehavior of a single customer? Are they made based on a single visit, or based \non aggregate behavior of several transactions or visits over a time period? Some \norganizations even build multiple models from the same data with different \nunits of analysis precisely because the unit of analysis drives the decisions you \ncan make from the model.\nDe\ufb01 ning the Target Variable\nFor models that estimate or predict a specifi c value, a necessary step in the \nBusiness Understanding stage is to identify one or more target variables to predict. \nA target variable is a column in the modeling data that contains the values to be \nestimated or predicted as defi ned in the business objectives. The target variable \ncan be numeric or categorical depending on the type of model that will be built.\n\n30 \nChapter 2 \u25a0 Setting Up the Problem\nTable 2-5 shows possible target variables ass", " objectives. The target variable \ncan be numeric or categorical depending on the type of model that will be built.\n\n30 \nChapter 2 \u25a0 Setting Up the Problem\nTable 2-5 shows possible target variables associated with a few projects from \nthe list of projects in the section \u201cBusiness Objectives\u201d that appears earlier in \nthe chapter. \nTable 2-5: Potential Target Variables\nPROJECT\nTARGET VARIABLE \nTYPE\nTARGET VARIABLE VALUES\nCustomer Acquisition\nBinary\n1 for acquired, 0 for non-acquired\nCustomer Value\nContinuous\nDollar value of customer\nInvoice Fraud Detection\nCategorical\nType of fraud (5 levels)\nDays to Next Purchase\nContinuous\nNumber of days\nDays to Next Purchase <= 7\nBinary\n1 for purchased within 7 days, 0 for \ndid not purchase within 7 days\nThe fi rst two items in the table are typical predictive modeling problems: the \nfi rst a classifi cation problem and the second an estimation problem. These will \nbe defi ned in Chapter 8. The third item, Invoice Fraud Detection, could have \nbeen defi", "tive modeling problems: the \nfi rst a classifi cation problem and the second an estimation problem. These will \nbe defi ned in Chapter 8. The third item, Invoice Fraud Detection, could have \nbeen defi ned with a binary target variable (1 for \u201cfraud,\u201d 0 for \u201cnot fraud\u201d) but \nwas instead defi ned with fi ve levels: four types of fraud and one level for \u201cnot \nfraud.\u201d This provides not only an indication as to whether or not the invoice is \nfraudulent, but also a more specifi c prediction of the type of fraud that could \nbe used by the organization in determining the best course of action.\nNote the last two items in the table. Both target variables address the same \nidea, predicting the number of days until the next purchase. However, they are \naddressed in different ways. The fi rst is the more straightforward prediction \nof the actual number of days until the next purchase although some organiza-\ntions may want to constrain the prediction to a 7-day window for a variety of \nreasons. Firs", "re straightforward prediction \nof the actual number of days until the next purchase although some organiza-\ntions may want to constrain the prediction to a 7-day window for a variety of \nreasons. First, they may not care if a customer will purchase 30 or 60 days from \nnow because it is outside of the window of infl uence they may have in their \nprograms. Second, binary classifi cation is generally an easier problem to solve \naccurately. These models do not have to differentiate between someone who \npurchases 14 days from now from someone who purchases 20 days from now: \nIn the binary target variable formulation, these are the same (both have value \n0). The predictive models may predict the binary target variable more accurately \nthan the entire distribution. \nDefi ning the target variable is critically important in a predictive modeling \nproject because it is the only information the modeling algorithms have to uncover \nwhat the modeler and program manager desire from the predictions. ", "is critically important in a predictive modeling \nproject because it is the only information the modeling algorithms have to uncover \nwhat the modeler and program manager desire from the predictions. Algorithms \ndo not have common sense and do not bring context to the problem in the way \nthe modeler and program manager can. The target variable defi nition therefore \nmust describe or quantify as much as possible the business objective itself.\n\n \nChapter 2 \u25a0 Setting Up the Problem \n31\nTemporal Considerations for Target Variable\nFor most modeling projects focused on predicting future behavior, careful con-\nsideration for the timeline is essential. Predictive modeling data, as is the case \nwith all data, is historical, meaning that it was collected in the past. To build a \nmodel that predicts so-called future actions from historic data requires shifting \nthe timeline in the data itself. \nFigure 2-2 shows a conceptualized timeline for defi ning the target variable. \nIf the date the data is ", "so-called future actions from historic data requires shifting \nthe timeline in the data itself. \nFigure 2-2 shows a conceptualized timeline for defi ning the target variable. \nIf the date the data is pulled is the last vertical line on the right, the \u201cData pull \ntimestamp,\u201d all data used for modeling by defi nition must precede that date. \nBecause the timestamp for the defi nition of the target variable value must occur \nafter the last information known from the input variables, the timeline for con-\nstructing the modeling data must be shifted to the left. \nTime\nTimeframe\nfor Model\nInputs\nTime Needed\nto Affect\nDecision\nOptional:\nAdditional Data\nfor Testing\nInput Variable\nTimestamp Min\nInput Variable\nTimestamp Max\nTarget Variable\nTimestamp\nData Pull\nTimestamp\nFigure 2-2: Timeline for defining target variable\nThe \u201cTime Needed to Affect Decision\u201d time gap can be critical in models, \nallowing time for a treatment to mature. For example, if you are building mod-\nels to identify churn, the l", "ng target variable\nThe \u201cTime Needed to Affect Decision\u201d time gap can be critical in models, \nallowing time for a treatment to mature. For example, if you are building mod-\nels to identify churn, the lead time to predict when churn might take place is \ncritical to putting churn mitigation programs in place. You might, therefore, \nwant to predict if churn will occur 30\u201360 days in the future, in which case there \nmust be a 30-day gap between the most recent input variable timestamp and \nthe churn timestamp.\nThe \u201cTimeframe for Model Inputs\u201d range has two endpoints. The \u201cInput \nVariable Timestamp Max\u201d is defi ned by the target variable and time needed \nto affect the decision. However, the minimum time is defi ned by the business \nobjectives and practical considerations. Sometimes maintaining a long temporal \n\n32 \nChapter 2 \u25a0 Setting Up the Problem\nsequence of events is diffi cult and costly. Other times, the perception of domain \nexperts is that information more than several years old is to", "long temporal \n\n32 \nChapter 2 \u25a0 Setting Up the Problem\nsequence of events is diffi cult and costly. Other times, the perception of domain \nexperts is that information more than several years old is too stale and unlikely \nto be valuable in predicting the target variable. \nThe time gap at the right, \u201cAdditional Data for Testing,\u201d can sometimes be \nvaluable during model validation. The best validation data are data that have \nnot been seen by the modeling algorithms and emulate what the model will \nbe doing when it goes live. Data collected subsequent to the target variable \ntimestamp is certainly held-out data. If the model is biased by the particular \ntimeframe when the target variable was computed or created, predictions for \ndata collected after the target variable defi nition will reveal the bias.\nDe\ufb01 ning Measures of Success for Predictive Models\nThe determination of what is considered a good model depends on the particular \ninterests of the organization and is specifi ed as the bu", "l the bias.\nDe\ufb01 ning Measures of Success for Predictive Models\nThe determination of what is considered a good model depends on the particular \ninterests of the organization and is specifi ed as the business success criterion. \nThe business success criterion needs to be converted to a predictive modeling \ncriterion so the modeler can use it for selecting models.\nIf the purpose of the model is to provide highly accurate predictions or deci-\nsions to be used by the business, measures of accuracy will be used. If inter-\npretation of the business is what is of most interest, accuracy measures will not \nbe used; instead, subjective measures of what provides maximum insight may \nbe most desirable. Some projects may use a combination of both so that the most \naccurate model is not selected if a less accurate but more transparent model with \nnearly the same accuracy is available.\nSuccess criteria for classifi cation and estimation models is described in more \ndetail in Chapter 9.\nSuccess Criter", "ss accurate but more transparent model with \nnearly the same accuracy is available.\nSuccess criteria for classifi cation and estimation models is described in more \ndetail in Chapter 9.\nSuccess Criteria for Classi\ufb01 cation\nFor classifi cation problems, the most frequent metrics to assess model accuracy \nis Percent Correct Classifi cation (PCC). PCC measures overall accuracy without \nregard to what kind of errors are made; every error has the same weight. Another \nmeasure of classifi cation accuracy is the confusion matrix, which enumerates \ndifferent ways errors are made, like false alarms and false dismissals. PCC and \nthe confusion matrix metrics are good when an entire population must be scored \nand acted on. For example, if customers who visit a website are to be served \ncustomized content on the site based on their browsing behavior, every visitor \nwill need a model score and a treatment based on that score.\nIf you are treating a subset of the population, a selected population, sor", "ent on the site based on their browsing behavior, every visitor \nwill need a model score and a treatment based on that score.\nIf you are treating a subset of the population, a selected population, sorting \nthe population by model score and acting on only a portion of those entities in \nthe selected group can be accomplished through metrics such as Lift, Gain, ROC, \n\n \nChapter 2 \u25a0 Setting Up the Problem \n33\nand Area under the Curve (AUC). These are popular in customer analytics where \nthe model selects a subpopulation to contact with a marketing message, or in \nfraud analytics, when the model identifi es transactions that are good candidates \nfor further investigation.\nSuccess Criteria for Estimation\nFor continuous-valued estimation problems, metrics often used for assessing \nmodels are R2, average error, Mean Squared Error (MSE), median error, aver-\nage absolute error, and median absolute error. In each of these metrics, you \nfi rst compute the error of an estimate\u2014the actual value min", " average error, Mean Squared Error (MSE), median error, aver-\nage absolute error, and median absolute error. In each of these metrics, you \nfi rst compute the error of an estimate\u2014the actual value minus the predicted \nestimate\u2014and then compute the appropriate statistic based on those errors. \nThe values are then summed over all the records in the data.\nAverage errors can be useful in determining whether the models are biased \ntoward positive or negative errors. Average absolute errors are useful in estimat-\ning the magnitude of the errors (whether positive or negative). Analysts most \noften examine not only the overall value of the success criterion, but also exam-\nine the entire range of predicted values by creating scatterplots of actual target \nvalues versus predicted values or actual target values versus residuals (errors).\nIn principle, you can also include rank-ordered metrics such as AUC and \nGain as candidates to estimate the success criteria, although they often are not \ninclu", "arget values versus residuals (errors).\nIn principle, you can also include rank-ordered metrics such as AUC and \nGain as candidates to estimate the success criteria, although they often are not \nincluded in predictive analytics software for estimation problems. In these \ninstances, you need to create a customized success criterion.\nOther Customized Success Criteria\nSometimes none of the typical success criteria are suffi cient to evaluate pre-\ndictive models because they do not match the business objective. Consider \nthe invoice fraud example described earlier. Let\u2019s assume that the purpose of the \nmodel is to identify 100 invoices per month to investigate from the hundreds of \nthousands of invoices submitted. If the analyst builds a classifi cation model and \nselects the model that maximizes PCC, the analyst can be fooled into thinking \nthat the best model as assessed by PCC is good, even though none of the top 100 \ninvoices are good candidates for investigation. How is this possible?", "zes PCC, the analyst can be fooled into thinking \nthat the best model as assessed by PCC is good, even though none of the top 100 \ninvoices are good candidates for investigation. How is this possible? If there are \n100,000 invoices submitted in a month, you are selecting only 0.1 percent of them \nfor investigation. The model could be perfect for 99.9 percent of the population \nand miss what you care about the most, the top 100.\nIn situations where there are specifi c needs of the organization that lead to \nbuilding models, it may be best to consider customized cost functions. In the \nfraud example, you want to identify a population of 100 invoices that is maximally \nproductive for the investigators. If the worst scenario for the investigators is to \npursue a false alarm, a case that turns out to not be fraudulent at all, the model \n\n34 \nChapter 2 \u25a0 Setting Up the Problem\nshould refl ect this cost in the ranking. What modeling metric does this? No \nmetric addresses this directly, althou", "out to not be fraudulent at all, the model \n\n34 \nChapter 2 \u25a0 Setting Up the Problem\nshould refl ect this cost in the ranking. What modeling metric does this? No \nmetric addresses this directly, although ROC curves are close to the idea. You \ncould therefore select models that maximize the area under the ROC curve at \nthe depth of 100 invoices. However, this method considers true alerts and false \nalarms as equally positive or negative. One solution is to consider the cost of \nfalse alarms greater than the benefi t of a true alert; you can penalize false alarms \nten times as much as a true alert. The actual cost values are domain-specifi c, \nderived either empirically or defi ned by domain experts.\nAnother candidate for customized scoring of models includes Return On \nInvestment (ROI) or profi t, where there is a fi xed or variable cost associated with \nthe treatment of a customer or transaction (a record in the data), and a fi xed or \nvariable return or benefi t if the customer respond", "ofi t, where there is a fi xed or variable cost associated with \nthe treatment of a customer or transaction (a record in the data), and a fi xed or \nvariable return or benefi t if the customer responds favorably. For example, if \nyou are building a customer acquisition model, the cost is typically a fi xed cost \nassociated with mailing or calling the individual; the return is the estimated \nvalue of acquiring a new customer. For fraud detection, there is a cost associated \nwith investigating the invoice or claim, and a gain associated with the successful \nrecovery of the fraudulent dollar amount.\nNote that for many customized success criteria, the actual predicted values \nare not nearly as important as the rank order of the predicted values. If you \ncompute the cumulative net revenue as a customized cost function associated \nwith a model, the predicted probability may never enter into the fi nal report, \nexcept as a means to threshold the population into the \u201cselect\u201d group (that is to ", "customized cost function associated \nwith a model, the predicted probability may never enter into the fi nal report, \nexcept as a means to threshold the population into the \u201cselect\u201d group (that is to \nbe treated) and the \u201cnonselect\u201d group.\nDoing Predictive Modeling Out of Order\nWhile CRISP-DM is a useful guide to follow for predictive modeling projects, \nsometimes there are advantages to deviating from the step-by-step structure to \nobtain insights more quickly in a modeling project. Two useful deviations are \nto build models fi rst and to deploy models before they are completed.\nBuilding Models First\nOne of the biggest advantages of predictive modeling compared to other ways \nto analyze data is the automation that occurs naturally in many algorithms. The \nautomation allows you to include many more input variables as candidates for \ninclusion in models than you could with handcrafted models. This is particu-\nlarly the case with decision trees because, as algorithms, they require minima", "many more input variables as candidates for \ninclusion in models than you could with handcrafted models. This is particu-\nlarly the case with decision trees because, as algorithms, they require minimal \ndata preparation compared to other algorithms.\n\n \nChapter 2 \u25a0 Setting Up the Problem \n35\nBuilding models before Data Preparation has been completed, and sometimes \neven before Data Understanding has been undertaken, is usually problematic. \nHowever, there are also advantages to building models. You can think of it as \nan additional step in Data Understanding. \nFirst, the predictive models will give the modeler a sense for which variables \nare good predictors. Some variables that would be good predictors if they were \nprepared properly would not, of course, show up. Second, building predictive \nmodels early gives the modeler a sense of what accuracy can be expected without \napplying any additional effort: a baseline.\nThird, and perhaps one of the biggest insights that can be gained from ", "ve \nmodels early gives the modeler a sense of what accuracy can be expected without \napplying any additional effort: a baseline.\nThird, and perhaps one of the biggest insights that can be gained from build-\ning models early, is to identify models that predict too well. If models predict \nperfectly or much better than expected, it is an indication that an input vari-\nable contains future information subsequent to the target variable defi nition \nor contains information about the target variable itself. Sometimes this is obvi-\nous; if the address of a customer is only known after they respond to a mailing \nand the customer\u2019s state is included as an input, state values equal to NULL \nwill always be related to non-responders. But sometimes the effect is far more \nsubtle and requires investigation by the modeler to determine why a variable \nis inexplicably a great predictor.\nEarly Model Deployment\nModel deployment can take considerable effort for an organization, involving \nindividuals from", "ion by the modeler to determine why a variable \nis inexplicably a great predictor.\nEarly Model Deployment\nModel deployment can take considerable effort for an organization, involving \nindividuals from multiple departments. If the model is to be deployed as part \nof a real-time transactional system, it will need to be integrated with real-time \ndata feeds. \nThe Modeling stage of CRISP-DM is iterative and may take weeks to months \nto complete. However, even early in the process, the modeler often knows which \nvariables will be the largest contributors to the fi nal models and how those \nvariables need to be prepared for use in the models. Giving these specifi cations \nand early versions of the models to the deployment team can aid in identifying \npotential obstacles to deployment once the fi nal models are built. \nCase Study: Recovering Lapsed Donors\nThis case study provides an example of how the business understanding steps \noutlined in this chapter can be applied to a particular proble", " nal models are built. \nCase Study: Recovering Lapsed Donors\nThis case study provides an example of how the business understanding steps \noutlined in this chapter can be applied to a particular problem. This particular \ncase study, and data from the case study, will be used throughout the book for \nexamples and illustrations.\n\n36 \nChapter 2 \u25a0 Setting Up the Problem\nOverview\nThe KDD-Cup is a data competition that became part of the annual Knowledge \nDiscovery and Data Mining (KDD) conference. In the competition, one or more \ndata sets, including record ID, inputs, and target variables, are made available to \nany who wish to participate. The 1998 KDD-Cup competition was a non-profi t \ndonation-modeling problem. \nBusiness Objectives\nThe business objective was to recover lapsed donors. When donors gave at \nleast one gift in the past year (0\u201312 months ago), they were considered active \ndonors. If they did not give a gift in the past year but did give 13\u201324 months ago, \nthey were considered ", "nors gave at \nleast one gift in the past year (0\u201312 months ago), they were considered active \ndonors. If they did not give a gift in the past year but did give 13\u201324 months ago, \nthey were considered lapsed donors. Of the lapsed donors, could you identify \ncharacteristics based on their historical patterns of behavior with the non-profi t \norganization? If these donors could be identifi ed, then these lapsed donors could \nbe solicited again and lapsed donors unlikely to give again could be ignored, \nincreasing the revenue to the organization.\nThe test mailing that formed the basis for the competition had already been \ncompleted so considerable information about the recovery of lapsed donors was \nalready known. The average donation amount for all lapsed donors who were \nmailed was $0.79 and the cost of the mail campaign was $0.68 per contact. So \nsoliciting everyone was still profi table, and the amount of profi t was approxi-\nmately $11,000 per 100,000 lapsed donors contacted.\nBut coul", " the cost of the mail campaign was $0.68 per contact. So \nsoliciting everyone was still profi table, and the amount of profi t was approxi-\nmately $11,000 per 100,000 lapsed donors contacted.\nBut could this be improved? If a predictive model could provide a score, where \na higher score means the donor is more likely to be a good donor, you could \nrank the donors and identify the best, most profi table subset. \nData for the Competition\nThe recovery concept was to mail to a random subset of lapsed donors \n(191,779 of them). Approximately 5.1 percent of them responded to the mail-\ning. The unit of analysis was a donor, so each record was a lapsed donor, and \nthe attributes associated with each donor included demographic information \nand historic patterns of giving with the non-profi t organization. Many derived \nvariables were included in the data, including measures that summarized their \ngiving behavior (recent, minimum, maximum, and average gift amounts), RFM \nsnapshots at the time of ", "ation. Many derived \nvariables were included in the data, including measures that summarized their \ngiving behavior (recent, minimum, maximum, and average gift amounts), RFM \nsnapshots at the time of each of the prior contacts with each donor, their socio-\neconomic status, and more.\nThe Target Variables\nTwo target variables were identifi ed for use in modeling: TARGET_B and \nTARGET_D. Responders to the recovery mailing were assigned a value of 1 for \n\n \nChapter 2 \u25a0 Setting Up the Problem \n37\nTARGET_B. If the lapsed donor did not respond to the mailing, he received a \nTARGET_B value of 0. TARGET_D was populated with the amount of the gift \nthat the lapsed donor gave as a result of the mailing. If he did not give a gift at all \n(i.e., TARGET_B = 0), the lapsed donor was assigned a value of 0 for TARGET_D. \nThus, there are at least two kinds of models that can be built. If TARGET_B is \nthe target variable, the model will be a binary classifi cation model to predict the \nlikelihood a lapse", " for TARGET_D. \nThus, there are at least two kinds of models that can be built. If TARGET_B is \nthe target variable, the model will be a binary classifi cation model to predict the \nlikelihood a lapsed donor can be recovered with a single mailing. If TARGET_D \nis the target variable, the model predicts the amount a lapsed donor gives from \na single recovery campaign.\nModeling Objectives\nThe next step in the Business Understanding process is translating the business \nobjective\u2014maximizing net revenue\u2014to a predictive modeling objective. This \nis a critical step in the process because, very commonly, the business objective \ndoes not translate easily or directly to a quantitative measure.\nWhat kinds of models can be built? There are two identifying measures that \nare candidate target variables. First, TARGET_B is the response indicator: 1 if the \ndonor responded to the mailing and 0 if the donor did not respond to the mail-\ning. Second, TARGET_D is the amount of the donation if the donor re", " First, TARGET_B is the response indicator: 1 if the \ndonor responded to the mailing and 0 if the donor did not respond to the mail-\ning. Second, TARGET_D is the amount of the donation if the donor responded. \nThe value of TARGET_D is $0 if the donor did not respond to the mailing.\nIf a TARGET_B model is built, the output of the prediction (the Score) is the \nprobability that a donor will respond. The problem with this number, however, \nis that it doesn\u2019t address directly the amount of a donation, so it won\u2019t address \ndirectly net revenue. In fact, it considers all donors equally (they all are coded \nwith a value of 1 regardless of how much they gave) and it is well known that \ndonation amounts are inversely correlated with likelihood to respond. These \nmodels therefore would likely favor low-dollar, lower net revenue donors.\nOn the other hand, a TARGET_D model appears to address the problem \nhead on: It predicts the amount of the donation and therefore would rank the \ndonors by their ", "low-dollar, lower net revenue donors.\nOn the other hand, a TARGET_D model appears to address the problem \nhead on: It predicts the amount of the donation and therefore would rank the \ndonors by their predicted donation amount and, by extension, their net revenue \n(all donors have the same fi xed cost). But TARGET_D models have a different \nproblem: You only know the amount given for those donors who gave. The vast \nmajority of donors did not respond and therefore have a value of TARGET_D \nequal to $0. This is a very diffi cult distribution to model: It has a spike at $0 \nand a much smaller, heavily skewed distribution for values greater than $0. \nTypically in these kinds of problems, you would create a model for only those \nwho donated, meaning those with TARGET_B = 1 or, equivalently, those with \nTARGET_D greater than 0.\nThis introduces a second problem: If you build a model for just the subset of \nthose who donated, and the model would be applied to the entire population \nof lapsed d", "se with \nTARGET_D greater than 0.\nThis introduces a second problem: If you build a model for just the subset of \nthose who donated, and the model would be applied to the entire population \nof lapsed donors, are you sure that the predicted values would apply well to \nthose who don\u2019t donate? The model built this way is only built to predict the \ndonation amount of donors who actually gave. \n\n38 \nChapter 2 \u25a0 Setting Up the Problem\nThis problem therefore is the classic censored regression problem where both \nthe selection stage (TARGET_B) and the amount stage (TARGET_D) need to be \nmodeled so that a complete prediction of an expected donation amount can be \ncreated.\nOne solution, though certainly not the only one, would be to build both \nTARGET_B and TARGET_D models, and then multiply their prediction values \nto estimate the expected donation amount. The TARGET_B prediction is the \nlikelihood that the donor will give, and the TARGET_D prediction is the amount \nthe donor will give. After mu", "ediction values \nto estimate the expected donation amount. The TARGET_B prediction is the \nlikelihood that the donor will give, and the TARGET_D prediction is the amount \nthe donor will give. After multiplying, if a donor is very likely to give and is \nlikely to give in a large amount, that donor will be at the top of the list. Donors \nwill be considered equally likely to give if, for example, their propensity to give \nis 0.1 (10 percent likelihood) and the predicted donation amount is $10 or their \npropensity is 0.05 (5 percent likelihood) and their predicted donation amount \nis $20. In both cases, the score is 1.0. \nScore = P(TARGET_B = 1) \u00d7 Estimated TARGET_D\nModel Selection and Evaluation Criteria\nAfter building models to predict TARGET_B or TARGET_D, how do you deter-\nmine which model is best? The answer is to use a metric that matches as closely \nas possible to the business objective. Recall that the business objective is to \nmaximize cumulative net revenue. If you build a model ", " is best? The answer is to use a metric that matches as closely \nas possible to the business objective. Recall that the business objective is to \nmaximize cumulative net revenue. If you build a model to predict TARGET_B, \ncomputing percent correct classifi cation is a measure of accuracy, but won\u2019t nec-\nessarily select the model that best matches the business objective. Computing \ngain or lift is closer, but still doesn\u2019t necessarily match the business objective. \nBut if you instead compute the cumulative net revenue, the business objective \nitself, and select the model that maximizes the cumulative net revenue, you will \nhave found the best model according to the business objectives.\nThe procedure would be this: \n \n1. Score all records by the multiplicative method, multiplying the TARGET_B \nprediction by the TARGET_D prediction.\n \n2.  Rank the scores from largest to smallest.\n \n3.  Compute the net revenue for each donor, which is the actual TARGET_D \nvalue minus $0.68.\n \n4. Sum the ne", "B \nprediction by the TARGET_D prediction.\n \n2.  Rank the scores from largest to smallest.\n \n3.  Compute the net revenue for each donor, which is the actual TARGET_D \nvalue minus $0.68.\n \n4. Sum the net revenue value as you go down the ranked list.\n \n5. When the net revenue is maximized, this is by defi nition the maximum \ncumulative net revenue.\n \n6. The model score associated with the record that generated the maximum \ncumulative net revenue is the score you should mail to in subsequent \ncampaigns.\n\n \nChapter 2 \u25a0 Setting Up the Problem \n39\nNote that once you compute a score for each record based on the model \npredictions, the scores themselves are not used at all in the computation of \ncumulative net revenue. \nModel Deployment\nIn subsequent mailings to identify lapsed donors to try to recover, you only \nneed to generate model scores and compare the scores to the threshold defi ned \nin the model evaluation criterion defi ned earlier.\nCase Study: Fraud Detection\nThis case study provides", "over, you only \nneed to generate model scores and compare the scores to the threshold defi ned \nin the model evaluation criterion defi ned earlier.\nCase Study: Fraud Detection\nThis case study provides an example of how the business understanding steps \noutlined in this chapter can be applied to invoice fraud detection. It was an \nactual project, but the name of the organization has been removed upon their \nrequest. Some details have been changed to further mask the organization.\nOverview\nAn organization assessed invoices for payment of services. Some of the invoices \nwere submitted fraudulently. The organization could afford to investigate a small \nnumber of invoices in detail. Predictive modeling was proposed to identify key \ninvoices to investigate.\nBusiness Objectives\nIn this invoice fraud example, the very defi nition of fraud is key to the model-\ning process. Two defi nitions are often considered in fraud detection. The fi rst \ndefi nition is the strict one, labeling an invoice as", "aud example, the very defi nition of fraud is key to the model-\ning process. Two defi nitions are often considered in fraud detection. The fi rst \ndefi nition is the strict one, labeling an invoice as fraudulent if and only if the \ncase has been prosecuted and the payee of the invoice has been convicted of \nfraud. The second defi nition is looser, labeling an invoice as fraudulent if the \ninvoice has been identifi ed as being worthy of further investigation by one or \nmore managers or agents. In the second defi nition, the invoice has failed the \n\u201csmell test,\u201d but there is no proof yet that the invoice is fraudulent. \nNote that there are advantages and disadvantages to each option. The primary \nadvantage of the fi rst defi nition is clarity; all of those invoices labeled as fraud \nhave been proven to be fraudulent. However, there are also several disadvan-\ntages. First, some invoices may have been fraudulent, but they did not meet \nthe standard for a successful prosecution. Some may ha", "n proven to be fraudulent. However, there are also several disadvan-\ntages. First, some invoices may have been fraudulent, but they did not meet \nthe standard for a successful prosecution. Some may have been dismissed on \ntechnicalities. Others may have had potential but were too complex to prosecute \neffi ciently and therefore were dropped. Still others may have shown potential, \nbut the agency did not have suffi cient resources to complete the investigation.\n\n40 \nChapter 2 \u25a0 Setting Up the Problem\nOn the other hand, if you use the second defi nition, many cases labeled as \nfraud may not be fraudulent after all, even if they appear suspicious upon fi rst \nglance. In other words, some \u201cfraudulent\u201d labels are created prematurely; if we \nhad waited long enough for the case to proceed, it would have been clear that \nthe invoice was not fraudulent after all. \nIn this project, the fi nal determination was made that the strict defi nition \nshould be used.\nData for the Project\nHundreds of tho", "ld have been clear that \nthe invoice was not fraudulent after all. \nIn this project, the fi nal determination was made that the strict defi nition \nshould be used.\nData for the Project\nHundreds of thousands of invoices were available for use in modeling, more \nthan enough for building predictive models. However, there were relatively \nfew labeled fraud cases because of the strict defi nition of fraud. Moreover, just \nbecause an invoice was not labeled as being fraudulent didn\u2019t mean that the \ninvoice was defi nitively not fraudulent. Undoubtedly, there were invoices labeled \nas \u201cnot fraud\u201d that were actually fraudulent. The problem is that the modeling \nalgorithms don\u2019t know this and believe the labels of fraud and non-fraud are \ncompletely accurate. \nLet\u2019s assume that the invoice fraud rate was 1 percent. This means that 1 of \nevery 100 invoices is fraudulent. Let\u2019s assume, too, that only half of the fraudu-\nlent invoices are identifi ed and prosecuted, leaving the other half unprosec", "te was 1 percent. This means that 1 of \nevery 100 invoices is fraudulent. Let\u2019s assume, too, that only half of the fraudu-\nlent invoices are identifi ed and prosecuted, leaving the other half unprosecuted. \nFor every 100,000 invoices, 500 are identifi ed as fraudulent and 500 are left in \nthe data with the label \u201c0\u201d even though they are fraudulent. These mislabeled \ncases can at best confuse models and at worst cause models to miss patterns \nassociated with fraud cases because so many invoices with the same pattern of \nbehavior are also associated with non-fraud invoices.\nModelers, during the Business Understanding stage of the project, determined \nthat the non-fraud population should be sampled so that the likelihood that a \nmislabeled invoice would be included in the data was greatly reduced.\nThe Target Variables\nThe most obvious target variable is the indicator that an invoice is fraudulent \n(1) or not fraudulent (0). The organization decided, however, to be more precise \nin their d", "uced.\nThe Target Variables\nThe most obvious target variable is the indicator that an invoice is fraudulent \n(1) or not fraudulent (0). The organization decided, however, to be more precise \nin their defi nition by identifying six different kinds of fraud, each of which \nwould be investigated differently. After an initial cluster analysis of the fraud \ntypes, it was determined that two of the six fraud types actually overlapped \nconsiderably with other fraud types, and therefore the number of fraud types \nused in modeling was reduced to four. The fi nal target variable therefore was \na fi ve-level categorical variable.\n\n \nChapter 2 \u25a0 Setting Up the Problem \n41\nModeling Objectives\nThe models for this project were to be multi-valued classifi cation models with \nfour fraud classes and one non-fraud class. Misclassifi cation costs were set up \nwhen possible\u2014not all algorithms support misclassifi cation costs\u2014so that \nmisclassifying non-fraud invoices as fraud received four times the weight ", "fraud class. Misclassifi cation costs were set up \nwhen possible\u2014not all algorithms support misclassifi cation costs\u2014so that \nmisclassifying non-fraud invoices as fraud received four times the weight as \nthe converse. Moreover, misclassifying one fraud type as another fraud type \ndid not generate any additional penalty.\nModel Selection and Evaluation Criteria\nModels were selected to identify the 100 riskiest invoices each month. One key \nconcern was that the workload generated by the model had to be productive for \nthe investigators, meaning that there were few false alarms that the investigators \nwould ultimately be wasting their time pursuing. Therefore, a customized cost \nfunction was used to trade off true alerts with false alarms. False alarms were \ngiven four times the weight of true alerts. The model that was chosen was the \none that performed best on the top 100 invoices it would have selected. Scores \nwere computed according to the following process:\n \n1. Compute a score for e", " alerts. The model that was chosen was the \none that performed best on the top 100 invoices it would have selected. Scores \nwere computed according to the following process:\n \n1. Compute a score for each invoice where the score is the maximum of the \nfour probabilities.\n \n2. Sort the scores from high to low.\n \n3. Select the top 100 scores.\n \n4. If the invoice in each record was fraudulent, give the record a score of +1. \nIf the record was not fraudulent, give the record a score of \u20134.\n \n5. Add the +1 or \u20134 values for each record. The highest score wins. \nWith a 4:1 ratio in scoring false alarms to true alerts, if 80 of the top 100 \ninvoices scored by the maximum probability were identifi ed as fraudulent, \nthe weighted score is 0. The actual score, however, does not indicate how well \nthe models predicted fraud, although you could look at the individual prob-\nabilities to report which fraud type the invoice was fl agged for and how large \nthe probability for the record was. \nModel Depl", "models predicted fraud, although you could look at the individual prob-\nabilities to report which fraud type the invoice was fl agged for and how large \nthe probability for the record was. \nModel Deployment\nDeployment of the model was simple: Each month, the model generated scores for \neach invoice. The 100 top-scoring invoices were fl agged and sent to investigators \n\n42 \nChapter 2 \u25a0 Setting Up the Problem\nto determine if they were potentially fraudulent, and if so, they were prosecuted \njudicially.\nSummary \nSetting up predictive modeling problems requires knowledge of the business \nobjectives, how to build the data, and how to match modeling objectives to the \nbusiness objectives so the best model is built. Modelers need to be a part of \nsetting up the problem to ensure that the data and model evaluation criteria \nare realistic. Finally, don\u2019t be afraid to revisit and modify business objectives \nor modeling objectives as the project unfolds; lessons learned during the mod-\neling proc", " model evaluation criteria \nare realistic. Finally, don\u2019t be afraid to revisit and modify business objectives \nor modeling objectives as the project unfolds; lessons learned during the mod-\neling process can sometimes reveal unexpected problems with data or new \nopportunities for improving decisions.\n\n43\nNow that data has been collected for a modeling project, it needs to be examined \nso the analyst knows what is there. In many situations, the analyst is the fi rst \nperson to even look at the data as compiled into the modeling table in-depth. \nThe analyst, therefore, will see all of the imperfections and problems in the data \nthat were previously unknown or ignored. Without Data Understanding, the \nanalyst doesn\u2019t know what problems may arise in modeling.\nData Understanding, as the fi rst analytical step in predictive modeling, has \nthe following purposes:\n \n\u25a0Examine key summary characteristics about the data to be used for model-\ning, including how many records are available, how many", "analytical step in predictive modeling, has \nthe following purposes:\n \n\u25a0Examine key summary characteristics about the data to be used for model-\ning, including how many records are available, how many variables are \navailable, and how many target variables are included in the data.\n \n\u25a0Begin to enumerate problems with the data, including inaccurate or invalid \nvalues, missing values, unexpected distributions, and outliers.\n \n\u25a0Visualize data to gain further insights into the characteristics of the data, \nespecially those masked by summary statistics.\n C H A P T E R \n3\nData Understanding\n\n44 \nChapter 3 \u25a0 Data Understanding\nWhat the Data Looks Like\nAfter data has been imported into the predictive analytics software tool, the data \ncan be summarized and assessed: variables in the columns and records in the \nrows. Variables can be numeric, strings, and dates. Before using data, you need \nto ensure that it is typed properly, especially if data is read into the predictive \nanalytics software a", "cords in the \nrows. Variables can be numeric, strings, and dates. Before using data, you need \nto ensure that it is typed properly, especially if data is read into the predictive \nanalytics software as a fl at fi le where the software must guess the data types \nfrom the data itself. For example, ZIP codes are numbers, but are not numeric; \nthey should be considered strings so that leading zeros are not stripped out. \nDates are particularly cumbersome in software because so many different \nformats can be used. Some software doesn\u2019t handle date operations particularly \nwell. Nevertheless, dates can provide tremendous amounts of information in \npredictive modeling so they should be handled with care and precision.\nContinuous variables are numbers whose values can, in principle, range from \nnegative infi nity to positive infi nity. These numbers can be integers or real val-\nues, sometimes called ints and doubles respectively. For example, Age, Income, \nProfi t/Loss, Home Value, Invoice Amo", "gative infi nity to positive infi nity. These numbers can be integers or real val-\nues, sometimes called ints and doubles respectively. For example, Age, Income, \nProfi t/Loss, Home Value, Invoice Amount, Visit Count, and DaysSinceLastVisit \nare all continuous variables. Some of the values of the variables may be restricted \nin some way. Some may be positive only, some positive or zero, some negative, \nsome limited in magnitude (at least practically). \nPredictive modelers and predictive analytics software usually think of \ncontinuous variables as one group of values, without differentiating subgroups \nof continuous variables, such as interval variables and ratio variables. \nCategorical variables have a limited number of values whose intent is to label \nthe variable rather than measure it. Examples of categorical variables include \nState, Title Code, SKU, and target variables such as Responder. Categorical \nvariables are sometimes called nominal variables or discrete variables. Flag \nva", "Examples of categorical variables include \nState, Title Code, SKU, and target variables such as Responder. Categorical \nvariables are sometimes called nominal variables or discrete variables. Flag \nvariables or binary variables are often used as names for categorical variables \nhaving only two values, like Gender (M,F), responses to questions (Yes and No), \nand dummy variables (1 and 0).\nSingle Variable Summaries\nAfter data has been loaded into any software tool for analysis, it is readily \napparent that for all but the smallest data sets, there is far too much data to be \nable to make sense of it through visual inspection of the values. \nAt the core of the Data Understanding stage is using summary statistics and \ndata visualization to gain insight into what data you have available for model-\ning. Is the data any good? Is it clean? Is it representative of what it is supposed \nto measure? Is it populated? Is it distributed as you expect? Will it be useful for \n\n \nChapter 3 \u25a0 Data Unders", "el-\ning. Is the data any good? Is it clean? Is it representative of what it is supposed \nto measure? Is it populated? Is it distributed as you expect? Will it be useful for \n\n \nChapter 3 \u25a0 Data Understanding \n45\nbuilding predictive models? These are all questions that should be answered \nbefore you begin to build predictive models.\nThe simplest way to gain insight into variables is to assess them one at a time \nthrough the calculation of summary statistics, including the mean, standard \ndeviation, skewness, and kurtosis. \nMean\nThe mean of a distribution is its average, simply the sum of all values for the \nvariable divided by the count of how many values the variable has. It is some-\ntimes represented by the Greek symbol mu, \u03bc. \nThe mean value is often understood to represent the middle of the distribu-\ntion or a typical value. This is true when variables match a normal or uniform \ndistribution, but often this is not the case. \nStandard Deviation\nThe Standard Deviation measures the spr", " the distribu-\ntion or a typical value. This is true when variables match a normal or uniform \ndistribution, but often this is not the case. \nStandard Deviation\nThe Standard Deviation measures the spread of the distribution; a larger stan-\ndard deviation means the distribution of values for the variable has a greater \nrange. Most often in predictive modeling, the standard deviation is considered \nin the context of normal distributions. The Greek symbol sigma, \u03c3, is often used \nto denote the standard deviation.\nThe Normal Distribution\nNormal distributions are fundamental to data analysis. The shape of the normal \ndistribution is shown in Figure 3-1 and because of the shape, the normal distri-\nbution is often called the bell curve. In some disciplines, it is called the Gaussian \ndistribution. Many algorithms assume normal distributions either explicitly or \nimplicitly. Considerable effort is expended by some analysts to fi nd transfor-\nmations that change the variables so that they becom", ". Many algorithms assume normal distributions either explicitly or \nimplicitly. Considerable effort is expended by some analysts to fi nd transfor-\nmations that change the variables so that they become normally distributed.\nNormal distributions have some additional properties that are worth noting:\n \n\u25a0The distribution is symmetric.\n \n\u25a0The mean value is the most likely value to occur in the distribution.\n \n\u25a0The mean, median, and mode are all the same value.\n \n\u25a0Approximately 68 percent of the data will fall between the mean and \n+/\u2013 1 standard deviation from the mean.\n \n\u25a0Approximately 95 percent of the data will fall between the mean and \n+/\u2013 2 standard deviations from the mean.\n\n46 \nChapter 3 \u25a0 Data Understanding\n \n\u25a0Approximately 99.7 percent of the data will fall between the mean and \n+/\u2013 3 standard deviations from the mean.\n \n\u25a0The distribution does not have a defi nitive, fi xed range. However, because \ndata is so unlikely to fall outside of three standard deviations from the \nmean, t", "3 standard deviations from the mean.\n \n\u25a0The distribution does not have a defi nitive, fi xed range. However, because \ndata is so unlikely to fall outside of three standard deviations from the \nmean, these values are often considered to be outliers. \n\u03bc \u2212 3\u03c3 \u03bc \u2212 2\u03c3 \u03bc \u2212 1\u03c3\n\u03bc + 1\u03c3 \u03bc + 2\u03c3 \u03bc + 3\u03c3\n\u03bc\n68.27%\n95.45%\n99.73%\nFigure 3-1: Standard deviation\nUniform Distribution\nThe uniform distribution assumes that a variable has a fi xed, fi nite range. \nFigure 3-2 shows a conceptualization of a uniform distribution. The mean value \nis in the center of the range of the variable. Standard deviations can be computed \nbut are rarely used in predictive modeling algorithms. \nTrue uniform distributions are rare in behavioral data, though some distribu-\ntions are close enough to be treated as uniform.\nIn uniform distributions, the mean can also be computed by simply comput-\ning one-half of the difference between the maximum minus the minimum of \nthe values of the variable.\nSome interesting properties of u", "orm distributions, the mean can also be computed by simply comput-\ning one-half of the difference between the maximum minus the minimum of \nthe values of the variable.\nSome interesting properties of uniform distributions include:\n \n\u25a0The distribution is symmetric about the mean.\n \n\u25a0The distribution is fi nite, with a maximum and minimum value.\n \n\u25a0The mean and midpoint of the distribution are the same value.\n \n\u25a0Random number generators create uniform random distributions, most \noften in the range 0 to 1 as their default.\nSometimes the distribution of a variable is primarily uniform but will have \ngaps between the denser part of the distribution and the maximum or mini-\nmum. These outliers can cause problems for some algorithms that are sensitive \nto outliers and therefore should be noted during Data Understanding.\n\n \nChapter 3 \u25a0 Data Understanding \n47\na\nCounts\nMin\nValue\nMax\nValue\nb\n\u03bc\nFigure 3-2: Uniform distribution\nApplying Simple Statistics in Data Understanding\nConsider computing simp", "ing Data Understanding.\n\n \nChapter 3 \u25a0 Data Understanding \n47\na\nCounts\nMin\nValue\nMax\nValue\nb\n\u03bc\nFigure 3-2: Uniform distribution\nApplying Simple Statistics in Data Understanding\nConsider computing simple statistics for some key variables in the KDD Cup \n1998 data shown in Table 3-1: As a part of data understanding, there are several \nmeasures to examine to understand the characteristics of the data. The purpose \nin examining these statistics is to verify the data is distributed as expected and \nto identify potential problems in the data that should be cleaned or corrected \nduring the Data Preparation stage.\nTable 3-1: Summary Statistics for a Subset of Continuous Variables for KDD Cup 1998 Data\nVARIABLE\nMINIMUM\nMAXIMUM\nMEAN\nSTANDARD \nDEVIATION\nNUMBER \nOF MISSING \nVALUES\nAGE\n1\n98\n       61.61\n  16.66\n23665\nRAMNTALL\n13\n9485\n    104.48\n118.58\n0\nNGIFTALL\n1\n237\n         9.60\n     8.55\n0\nLASTGIFT\n0\n1000\n      17.31\n   13.96\n0\nLASTDATE\n9503\n9702\n9548.10\n   49.24\n0\nFISTDATE\n0\n9603\n9135.65\n320.3", "1\n  16.66\n23665\nRAMNTALL\n13\n9485\n    104.48\n118.58\n0\nNGIFTALL\n1\n237\n         9.60\n     8.55\n0\nLASTGIFT\n0\n1000\n      17.31\n   13.96\n0\nLASTDATE\n9503\n9702\n9548.10\n   49.24\n0\nFISTDATE\n0\n9603\n9135.65\n320.39\n0\nNEXTDATE\n7211\n9702\n9151.02\n294.26\n9973\nTIMELAG\n0\n1088\n       8.09\n     8.21\n9973\nAVGGIFT\n1.2857\n1000\n13.35\n10.77\n0\nCONTROLN\n1\n191779\n95778.18\n55284.60\n0\nTARGET_B\n0\n1\n0.05\n0.22\n0\nTARGET_D\n0\n200\n0.79\n4.43\n0\n\n48 \nChapter 3 \u25a0 Data Understanding\nFirst, look at the minimum and maximum values. Do these make sense? Are \nthere any unexpected values? For most of the variables, the values look fi ne. \nRAMNTALL, the total donation amount given by a donor to the non-profi t orga-\nnization over his or her lifetime, ranges from $13 to $9,485. These are plausible. \nNext, consider AGE. The maximum value is 98 years old, which is high, but \nbelievable. The minimum value, however, is 1. Were there 1-year-old donors? \nDoes this make sense? Obviously, 1-year-olds did not write checks or submit \ncredit card", " 98 years old, which is high, but \nbelievable. The minimum value, however, is 1. Were there 1-year-old donors? \nDoes this make sense? Obviously, 1-year-olds did not write checks or submit \ncredit card donations to the organization. Could it mean that donations were \ngiven in the name of a 1-year-old? This could be, but will require further inves-\ntigations by someone to provide the necessary information. Note, however, that \nwe only know that there is at least one 1-year-old in the data; this summary \ndoes not indicate to us how many 1-year-olds there are.\nThird, consider FISTDATE, the date of the fi rst gift to the organization. The \nmaximum value is 9603, meaning that the most recent date of the fi rst gift in \nthis data is March 1996. Given the timeframe for the data, this makes sense. \nBut the minimum value is 0. Literally, this means that the fi rst donation date \nwas the year of Christ\u2019s birth. Obviously this is not the intent. The most likely \nexplanation is that the value was u", "\nBut the minimum value is 0. Literally, this means that the fi rst donation date \nwas the year of Christ\u2019s birth. Obviously this is not the intent. The most likely \nexplanation is that the value was unknown but at some time a value of 0 was \nused to replace the missing or null value. \nNext consider the standard deviations. For AGE, the standard deviation is \nabout 17 years compared to the mean of 61.61 years old. If AGE were normally \ndistributed, you would expect 68 percent of the donors to have AGE values \nbetween 44 years old and 78 years old, 95 percent of the donors to have AGE \nvalues between 27 and 95 years old, and 99.7 percent of the AGE values to range \nbetween 10 and 112 years old. It is obvious from these values, where the maxi-\nmum value is less than the value assumed by the normal distribution for three \nstandard deviations above the mean, that the data is not normally distributed and \ntherefore the standard deviation is not an accurate measure of the true spread \nof the ", "ormal distribution for three \nstandard deviations above the mean, that the data is not normally distributed and \ntherefore the standard deviation is not an accurate measure of the true spread \nof the data. This can be seen even more clearly with RAMNTALL where one \nstandard deviation below the mean is negative, less than the minimum value \n(and a value that makes no operational sense). \nThe target variables, TARGET_B and TARGET_D, are also shown in the table. \nTARGET_B, the indication whether a donor responded to the recapture mail-\ning (a value of 1) or didn\u2019t respond to the recapture mailing (a value of 0) is a \ndummy variable, but because it was coded with values 0 and 1, it can be treated \nas a numeric variable for the purposes of calculating summary statistics. The \nminimum and maximum values make sense and match what you expect. The \nmean value is 0.05, which is the response rate to the mailing. The standard \ndeviation doesn\u2019t apply here.\nFor TARGET_D, notice that the maximum gif", "m values make sense and match what you expect. The \nmean value is 0.05, which is the response rate to the mailing. The standard \ndeviation doesn\u2019t apply here.\nFor TARGET_D, notice that the maximum gift in response to the mailing was \n$200 and the average was $0.79 (79 cents). The minimum value is interesting, \nhowever. Were there individuals who gave $0 in response to the mailing? A \n\n \nChapter 3 \u25a0 Data Understanding \n49\nquick investigation would reveal the answer is \u201cno,\u201d but non-responders were \nassigned TARGET_D values of 0. This isn\u2019t exactly correct: There could be a dif-\nference between a lapsed donor not responding and a lapsed donor responding \nto the mailing but not giving any money at all (perhaps indicating their support \nfor the organization even while not being able to donate at this time). However, \nassigning the value 0 to these is not unreasonable and is done frequently in these \nsituations; it is a decision that has to be made during Business Understanding \nor Data Und", "at this time). However, \nassigning the value 0 to these is not unreasonable and is done frequently in these \nsituations; it is a decision that has to be made during Business Understanding \nor Data Understanding by analytics and domain experts so the appropriate \nvalues are assigned to TARGET_D.\nThe last column in Table 3-1 summarizes how many missing values occurred \nin the variable. A missing value could have been coded as NULL, an empty \ncell, or some other indicator of missing. How missing values are interpreted is \nsoftware dependent. Most predictive analytics software will handle null values \nand treat them as missing. But other codings of missing, such as 99 or 999, \u20131, \nan empty string, and others could cause deceptive values. You have already \nseen this in FISTDATE: The 0 for the minimum value, the recoding of missing \nto the value 0, distorts the mean value. AGE is likewise distorted because of \nthe 1-year-olds in the data. These are all values that need to be investigated and", "inimum value, the recoding of missing \nto the value 0, distorts the mean value. AGE is likewise distorted because of \nthe 1-year-olds in the data. These are all values that need to be investigated and \npossibly corrected before modeling begins.\nSkewness\nSkewness measures how balanced the distribution is. A normal distribution \nhas a skewness value of 0. But so does a uniform distribution or any balanced \ndistribution. Positive skew\u2014skew values greater than zero\u2014indicates that the \ndistribution has a tail to the right of the main body of the distribution. Negative \nskew indicates the converse: The distribution has a tail to the left of the main \nbody. Examples of the possible shape of distributions with positive and negative \nskew are shown in Figure 3-3.\nNegative Skew\nPositive Skew\nFigure 3-3: Positive and negative skew\nWhy does measuring skew matter? Some algorithms assume variables have \nnormal distributions, and signifi cant deviations from those assumptions affect \n\n50 \nChapter 3 \u25a0", "3-3: Positive and negative skew\nWhy does measuring skew matter? Some algorithms assume variables have \nnormal distributions, and signifi cant deviations from those assumptions affect \n\n50 \nChapter 3 \u25a0 Data Understanding\nthe model accuracy or at least the model interpretation. Signifi cant deviations \nfrom normal result in the mean and standard deviation having deceptive values. \nExamining how skewed a distribution is provides one way you can determine \nhow much the variable deviates from normal.\nHow much skew indicates signifi cant deviation from normal? There is no fi xed \nrule on what values of skewness are too high. However, several rules of thumb \nare often used. The fi rst indication of positive skew is when the mean is larger \nthan the median (to be defi ned in the next section). An additional indicator is \nwhen the mode is less than the median, which is also less than the mean. In \nTable 3-2, RAMNTALL has the class indicators of positive skew: The mode is \nless than the median, ", "additional indicator is \nwhen the mode is less than the median, which is also less than the mean. In \nTable 3-2, RAMNTALL has the class indicators of positive skew: The mode is \nless than the median, and the median is less than the mean. Note the skewness \nvalue is 13.9, a large positive value. \nSome software also provides a measure of the standard error for skewness. \nThe larger the sample, the smaller the standard error is, generally. \nTable 3-2: Skewness Values for Several Variables from KDD Cup 1998\nVARIABLE\nMODE\nMEDIAN\nMEAN\nSKEWNESS\nSKEWNESS \nSTANDARD ERROR\nRAMNTALL\n20\n78\n104.48\n13.91\n0.008\nMAXRAMNT\n15\n17\n  20.00\n99.79\n0.008\nLASTGIFT\n15\n15\n 17.31\n16.29\n0.008\nNUMPRM12\n13\n12\n 12.86\n 2.99\n0.008\nCARDPROM\n6\n18\n 18.44\n 0.15\n0.008\nNUMPROM\n13\n47\n 46.97\n 0.44\n0.008\nOften, values of skew greater than a magnitude of 2 or 3 are considered sig-\nnifi cant. Some even tie signifi cantly large skew to a value twice the skewness \nstandard error; of course, when the sample size is large, even skewne", "r than a magnitude of 2 or 3 are considered sig-\nnifi cant. Some even tie signifi cantly large skew to a value twice the skewness \nstandard error; of course, when the sample size is large, even skewness values \nconsidered small would show as signifi cant. In Table 3-2, the standard error is \nonly 0.008 due to the sample size (95,412), so any skew value greater than 0.016 \ncould be considered signifi cant.\nPositive skew is observed more often than negative skew, especially in appli-\ncations with monetary variables. Home values, income, credit card transaction \namounts, and loan values are typically positively skewed monetary variables. \nThe positive skew in these variables results in their mean values no longer rep-\nresenting a typical value for the variable. This is the reason that home values, \nfor instance, are reported using the median rather than the mean; the mean is \ninfl ated by the large values to the right of the distribution.\nBecause of the adverse effect skew has on how some", "lues, \nfor instance, are reported using the median rather than the mean; the mean is \ninfl ated by the large values to the right of the distribution.\nBecause of the adverse effect skew has on how some algorithms behave, many \nanalysts consider identifying large skew (positive or negative) so that during \nData Preparation the variable can be corrected to remove the skew.\n\n \nChapter 3 \u25a0 Data Understanding \n51\nKurtosis\nKurtosis measures how much thinner or fatter the distribution is compared to \nthe normal distributions. Figure 3-4 shows three distributions: On the left is a \nnormal distribution; the center is a skinnier-than-normal distribution (called lep-\ntokurtic); and on the right is a fatter-than-normal distribution (called platykurtic). \nA variable that is normally distributed will have a kurtosis value equal to 3, a \nleptokurtic distribution less than 3, and a platykurtic distribution greater than 3.\nFigure 3-4: Types of skewness\nKurtosis is always positive. However, kurtosis is s", "e a kurtosis value equal to 3, a \nleptokurtic distribution less than 3, and a platykurtic distribution greater than 3.\nFigure 3-4: Types of skewness\nKurtosis is always positive. However, kurtosis is sometimes reported in software \nas excess kurtosis even if the label of the statistic is still kurtosis. Excess kurtosis \nis the difference between the kurtosis value assumed for normal distributions \n(3) and the value computed. A value of excess kurtosis that is greater than 0 \nindicates a platykurtic distribution (fat), whereas an excess kurtosis value that \nis less than 0 indicates a leptokurtic distribution (skinny).\nIf a variable is uniformly distributed, it will appear to be leptokurtic because \nthere is not a sharp peak in the distribution. If a variable has outliers both posi-\ntive and negative, such as a profi t/loss variable, the distribution will have a high \npeak with big tails and therefore have high kurtosis.  \nKurtosis is rarely used in predictive analytics for anything more ", "negative, such as a profi t/loss variable, the distribution will have a high \npeak with big tails and therefore have high kurtosis.  \nKurtosis is rarely used in predictive analytics for anything more than provid-\ning insight into the shape of the distribution of a variable. However, there are \nsome situations where understanding kurtosis can be valuable. For instance, \nconsider a distribution that is skinny with long symmetric tails on both sides of \nthe distribution, like the shape of Figure 3-5. The skew will be 0 in this distribu-\ntion because the tails are symmetric and balance each other out. The kurtosis, \nhowever, will be very large (platykurtic) because of the large tails (in spite of \nthe tall spike in the center of the distribution). This is evidence therefore that \nthe variable\u2019s standard deviation will not be representative of the true spread \nin the data for algorithms that use standard deviation or variance in the model. \nThe modeler should consider transformations that w", "s standard deviation will not be representative of the true spread \nin the data for algorithms that use standard deviation or variance in the model. \nThe modeler should consider transformations that would correct this problem.\nThe kurtosis values for the same variables from Table 3-2 are shown in Table \n3-3. The fi rst three variables show a strong indication that not only is there posi-\ntive skew\u2014a tail in the distribution to the right\u2014but there is also high excess \nkurtosis, so the distribution appears to be much fatter than a normal distribution.\n\n52 \nChapter 3 \u25a0 Data Understanding\n\u221210\n\u22125\n0\n5\n10\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nFigure 3-5: Big-tailed distribution\nTable 3-3: Kurtosis Values for Several Variables from KDD Cup 1998\nVARIABLE\nMEAN\nSKEWNESS\nEXCESS KURTOSIS\nKURTOSIS \nSTANDARD ERRORR\nRAMNTALL\n104.48\n13.91\n     652.64\n0.016\nMAXRAMNT\n  20.00\n99.79\n17673.12\n0.016\nLASTGIFT\n  17.31\n16.29\n    728.44\n0.016\nNUMPRM12\n  12.86\n  2.99\n      13.14\n0.016\nCARDPROM\n  18.44\n  0.15\n      \u20130.93\n0.0", "\nRAMNTALL\n104.48\n13.91\n     652.64\n0.016\nMAXRAMNT\n  20.00\n99.79\n17673.12\n0.016\nLASTGIFT\n  17.31\n16.29\n    728.44\n0.016\nNUMPRM12\n  12.86\n  2.99\n      13.14\n0.016\nCARDPROM\n  18.44\n  0.15\n      \u20130.93\n0.016\nNUMPROM\n  46.97\n  0.44\n        0.02\n0.016\nRank-Ordered Statistics\nThe statistics described above assumed a normal distribution\u2014the mean, stan-\ndard deviation, skew, and kurtosis. Outliers can have a large infl uence on each of \nthese values, misleading an analyst about the true characteristics of the variables. \nHowever, there are other statistics that do not share this problem. If the vari-\nable is sorted from smallest to largest variable and each value is associated with \n\n \nChapter 3 \u25a0 Data Understanding \n53\nthe percentile of its rank-ordered value, insights can be gained regardless of the \nshape of the distribution, regardless of how many or how extreme the outliers \nare. The most common rank statistics are listed in Table 3-4.\nTable 3-4: Rank-Ordered Metrics\nPERCENTILE\nMETRIC\n0th\nM", "he \nshape of the distribution, regardless of how many or how extreme the outliers \nare. The most common rank statistics are listed in Table 3-4.\nTable 3-4: Rank-Ordered Metrics\nPERCENTILE\nMETRIC\n0th\nMinimum\n25th\n1st quartile\n50th\n2nd quartile; the median\n75th\n3rd quartile\n100th\nMaximum\nThese are considered robust statistics because small changes in the data do not \nproduce large changes in the statistics. This is not the case with the mean. The \nvariable LASTGIFT has a mean value of 17.3 (from Table 3-1). A single additional \nLASTGIFT value of $10,000 would change the mean from $17.3 to $19.3 (if there \nwere 5,000 records in the data). However, the median would not change at all \neven with this single large outlier, which is an additional reason to use median \nhome value as the measure of a typical home price rather than the mean. If a \nrich celebrity would build a new 30 million dollar home in Los Angeles county, \nthe median home value in the county would not change, though the mean w", "a typical home price rather than the mean. If a \nrich celebrity would build a new 30 million dollar home in Los Angeles county, \nthe median home value in the county would not change, though the mean will \ntick upward.\nThe median is defi ned as the value that is exactly 50 percent of the way from \nthe minimum to maximum value of the variable (or vice versa). If there are an \neven number of rows, the median is the average of the two middle numbers. \nIf, for example, you have the list of sorted numbers 1, 4, 8, 21, 34, 45, and 50, the \nmedian is 21, the fourth of seven numbers in the list. For the list 1, 4, 8, 21, 34, \nand 45, the median is the average of 8 and 21, or 14.5 mean\nThe median is by far the most well-known of the rank-ordered statistics and \nis commonly included in summary statistics in software, though often with a \ndisclaimer. Because computing the median, in particular, and quartiles, in general, \nrequires the data fi rst be sorted, computing the median can be computationa", "tics in software, though often with a \ndisclaimer. Because computing the median, in particular, and quartiles, in general, \nrequires the data fi rst be sorted, computing the median can be computationally \nexpensive when there are large numbers of rows. Additionally, this operation \nmust be repeated separately for each and every variable whose summary statistics \nare being computed. The mean, standard deviation, skewness, and kurtosis do \nnot require sorting the data; only summing operations are needed.\nQuartiles are a special case of the general rank-ordered measure called  quantiles. \nOther commonly used quantiles are listed in Table 3-5. The consensus of pre-\ndictive analytics software is to use quartile, quintile, decile, and percentile. \nHowever, there is no consensus for the name of the label for 20 bins; three of \nthe names that appear most often are shown in the table. Of course, if you know \n\n54 \nChapter 3 \u25a0 Data Understanding\nthe deciles, you also know the quintiles because qu", "he label for 20 bins; three of \nthe names that appear most often are shown in the table. Of course, if you know \n\n54 \nChapter 3 \u25a0 Data Understanding\nthe deciles, you also know the quintiles because quintiles can be computed by \nsumming pairs of deciles. If you know the percentiles, you also know all of the \nother quantiles.\nTable 3-5: Quantile Labels\nQUANTILE LABEL\nNUMBER OF BINS\nPERCENT OF POPULATION\nQuartile\n4\n25\nQuintile\n5\n20\nDecile\n10\n10\nDemi-decile\nVingtile\nTwentile\n20\n5\nPercentile\n100\n1\nThe analogous measure to standard deviation using quartiles is the \n Inter-Quartile Range (IQR): the difference between the 3rd quartile value and \nthe 1st quartile value, or in other words, the range between the 25th and 75th \npercentiles. This, too, is a robust statistic as individual outliers have little effect \non the range; each data point shifts the values of the 1st and 3rd quartiles (Q1 \nand Q3 respectively) by, at most, the difference in value of the next variable \nvalue in the sorted lis", "little effect \non the range; each data point shifts the values of the 1st and 3rd quartiles (Q1 \nand Q3 respectively) by, at most, the difference in value of the next variable \nvalue in the sorted list.\nThis doesn\u2019t mean that outliers cannot be labeled using the IQR. Some prac-\ntitioners use a rule of thumb that values more than 1.5 times the IQR from the \nupper or lower quartiles are considered outliers. Table 3-6 shows the same \nvariables shown in Tables 3-2 and 3-3, this time with quantile information. For \nexample, LASTGIFT, while having a huge 1000 maximum value, has a median of \n15, a 3rd quartile value of 20, a 1st quartile value of 10, and therefore an IQR (not \nin the table) equal to 10 (20\u201310). Any value of LASTGIFT greater than 35 may be \nconsidered an outlier because it exceeds 1.5 times the IQR plus the 3rd quartile \nvalue. As it turns out, that includes 3,497 records of the 95,412, or 3.7 percent of \nthe population. No records are 1.5 times the IQR below the 1st quartile ", " 1.5 times the IQR plus the 3rd quartile \nvalue. As it turns out, that includes 3,497 records of the 95,412, or 3.7 percent of \nthe population. No records are 1.5 times the IQR below the 1st quartile because \noutliers would have to be negative, less than the minimum value of LASTGIFT. \nOnly NUMPRM12 has lower outliers, those values below 8. \nThe fewest IQR outliers were for CARDPROM and NUMPROM, in large \npart because they both have small values of excess kurtosis (refer to Table 3-3). \n\n \nChapter 3 \u25a0 Data Understanding \n55\nTable 3-6: Quartile Measures for Several Variables from KDD Cup 1998\nVARIABLE\nMINIMUM\nQ1\nMEDIAN\nQ3\nMAXIMUM\n\uf6bb1.5*IQR \nOUTLIERS\n+1.5*IQR \nOUTLIERS\nRAMNTALL\n13\n40\n78\n131\n9485\n-96.5\n267.5\nMAXRAMNT\n5\n14\n17\n23\n5000\n  0.5\n36.5\nLASTGIFT\n0\n10\n15\n20\n1000\n\u20135\n  35\nNUMPRM12\n1\n11\n12\n13\n78\n8\n  16\nCARDPROM\n1\n11\n18\n25\n61\n\u201310\n  46\nNUMPROM\n4\n27\n47\n64\n195\n\u201328.5\n119.5\nCategorical Variable Assessment\nCategorical or nominal variables are assessed by counting the number of occur-\nrences fo", "13\n78\n8\n  16\nCARDPROM\n1\n11\n18\n25\n61\n\u201310\n  46\nNUMPROM\n4\n27\n47\n64\n195\n\u201328.5\n119.5\nCategorical Variable Assessment\nCategorical or nominal variables are assessed by counting the number of occur-\nrences for every level, informing the modeler which values occur frequently \nand infrequently in the data. \nThere are several issues typically addressed with frequency counts. \n \n\u25a0Do the values make sense? Are there values that are unexpected, errone-\nous, or indicators of missing or unknown values?\n \n\u25a0Are there missing values? How many? How are they coded?\n \n\u25a0How many levels are there? Is there only one value? Are there more than \n50 or 100 levels?\n \n\u25a0What is the mode of the levels?\nTable 3-7 shows the counts for variable RFA_2 with 14 levels in the 95,412 \nrows of data. You would expect that each of these categorical values would be \npopulated with approximately 6815 rows (95,412 \u00f7 14). While the values in the \nCount column are not equal, each value is populated with at least 900 records: \na good", "these categorical values would be \npopulated with approximately 6815 rows (95,412 \u00f7 14). While the values in the \nCount column are not equal, each value is populated with at least 900 records: \na good amount. \nTable 3-7: Frequency Counts for Variable RFA_2\nRFA_2\n% OF POPULATION \nCOUNT\nL1E\n  5.2\n4,911\nL1F\n31.8\n30,380\nContinues\n\n56 \nChapter 3 \u25a0 Data Understanding\nRFA_2\n% OF POPULATION \nCOUNT\nL1G\n13.0\n12,384\nL2E\n5.2\n4,989\nL2F\n11.5\n10,961\nL2G\n  4.8\n4,595\nL3D\n  2.6\n2,498\nL3E\n  8.1\n7,767\nL3F\n  3.7\n3,523\nL3G\n  1.6\n1,503\nL4D\n  5.2\n4,914\nL4E\n  4.2\n3,972\nL4F\n  2.2\n2,100\nL4G\n  1.0\n915\nOr consider a second example, with the variable STATE in Table 3-8. These \nare only the fi rst 15 of 58 values for STATE\u2014obviously the variable contains \nmore than states, but also contains U.S. territories. Part of the task of the ana-\nlyst in this stage is to confi rm that values AA, AE, AP, and AS are correct. Of \nadditional concern, however, are the low counts for values AA, AE, and CT, but \nespecially AS, DC, a", " task of the ana-\nlyst in this stage is to confi rm that values AA, AE, AP, and AS are correct. Of \nadditional concern, however, are the low counts for values AA, AE, and CT, but \nespecially AS, DC, and DE. The last three have so few examples (1 or 3) that no \nreasonable inference can be made about their relationship to a target variable. \nThis problem will need to be considered during data preparation.\nTable 3-8: Frequency Counts for Variable STATE\nSTATE\n% OF POPULATION\nCOUNT\nAA\n0.0\n18\nAE\n0.0\n15\nAK\n0.3\n282\nAL\n1.8\n1,705\nAP\n0.1\n81\nAR\n1.1\n1,020\nAS\n0.0\n1\nAZ\n2.5\n2,407\nTable 3-7 (continued)\n\n \nChapter 3 \u25a0 Data Understanding \n57\nCA\n18.2\n17,343\nCO\n2.1\n2,032\nCT\n0.0\n23\nDC\n0.0\n1\nDE\n0.0\n3\nFL\n8.8\n8,376\nGA\n3.6\n3,403\nA third example is shown in Table 3-9. This categorical variable, PEPSTRFL, \nhas one value that is populated, with the remaining missing. On the surface, \nit appears that this variable may be uninteresting with only one value, but can \nbe cleaned during Data Preparation.\nTable 3-9: Freq", "lue that is populated, with the remaining missing. On the surface, \nit appears that this variable may be uninteresting with only one value, but can \nbe cleaned during Data Preparation.\nTable 3-9: Frequency Count for Variable with One Level\nPEPSTRFL\n% OF POPULATION\nCOUNT\n52.6\n50,143\nX\n47.4\n45,269\nA convenient summary of categorical variables is shown in Table 3-10, includ-\ning most of the key measures that indicate data cleaning must occur with the \nvariable. These key measures are the following: existence of missing values, \nhigh-cardinality, and low counts for some levels. \nHigh cardinality presents two problems for predictive modeling. First, some \nsoftware will automatically convert categorical variables to numeric variables \nby creating a dummy variable for every level of the categorical variable. If the \nnumber of levels is large, the number of dummy variables is equally large and \ncan result in an overwhelming number of input variables. For example, in the \nKDD Cup 1998 data, the", "al variable. If the \nnumber of levels is large, the number of dummy variables is equally large and \ncan result in an overwhelming number of input variables. For example, in the \nKDD Cup 1998 data, there are 23 RFA variables: RFA_2, RFA_3, RFA_4, and so \non, through RFA_24. Twenty-two (22) of these (all except for RFA_2) have 70 to \nmore than 100 levels each. Therefore, if these are converted to dummy variables \nby some software, there are no longer just 23 variables but more than 2,000 \nvariables that become inputs to models, a huge number of variables to consider \nin modeling. Some data prep will likely be needed to help with this problem.\nSecond, the more levels in the categorical variable, the fewer the number of \nrecords that can populate each level. If there are too few (for example, fewer \nthan 30 records per level), inferences made by the modeling algorithms about \nthe relationship between that level and the target variable are noisy at best, and \ncan lead to model overfi t. Low", "fewer \nthan 30 records per level), inferences made by the modeling algorithms about \nthe relationship between that level and the target variable are noisy at best, and \ncan lead to model overfi t. Low counts, of course, can occur even with relatively \n\n58 \nChapter 3 \u25a0 Data Understanding\nfew levels, but the more levels you have in the variable, the more likely it is that \nseveral or even most levels will have low counts. Rules of thumb for handling \nhigh-cardinality variables are described in Chapter 4. \nTable 3-10: Frequency Count Summary Table for Several Variables \nVARIABLE\nLEVELS\nMODE\nPOPULATED\nMISSING\nMISSING \uf6ae%\uf6af\nSTATE\n57\nCA\n95,412\n0\n0.0\nGENDER\n 7\nF\n92,455\n2,957\n3.1\nPEPSTRFL\n 2\n45,269\n50,143\n52.6\nRFA_2\n14\nL1F\n95,412\n0\n0.0\nRFA_3\n71\nA1F\n93,462\n1,950\n2.0\nRFA_2A\n4\nF\n95,412\n0\n0.0\nData Visualization in One Dimension\nData visualization is a graphical representation of the data. Like statistical \nsummaries, visualization also summarizes data but does so in a way that \nconveys much more inf", "lization in One Dimension\nData visualization is a graphical representation of the data. Like statistical \nsummaries, visualization also summarizes data but does so in a way that \nconveys much more information and context of the data. Some ideas that are \ndiffi cult to convey in numbers from a table but can be seen easily in graphi-\ncal form are distances and density. Seeing distances between some data val-\nues and others reveals if data points or groups of data points are outliers or \nunexpectedly different. \nVisualizing density shows how and where data is distributed, revealing if the \ndata is normal, uniform, or neither. Visualization can be particularly effective \nin identifying high skew or excess kurtosis, answering questions such as, \u201cIs \nskew high because of a small group of outliers or due to a long tail?\u201d \nVisualization is done primarily in two stages of a modeling project. First, it can \nbe done effectively in the beginning of a project, during the Data Understanding \nstage t", "s or due to a long tail?\u201d \nVisualization is done primarily in two stages of a modeling project. First, it can \nbe done effectively in the beginning of a project, during the Data Understanding \nstage to help the modeler verify the characteristics of the data. Some one-dimen-\nsional data visualization methods can be done for all variables in the data as \na part of the data verifi cation step. Other methods\u2014two or more dimensional \nvisualization methods\u2014require manipulation of settings that render them \ndiffi cult to use in any automated manner. \nData visualization is also done after building the models to understand why \nthe most important variables in the models were effective in predicting the \ntarget variable. Pre-modeling, there may be dozens, hundreds, or even thou-\nsands of candidate input variables. Post-modeling, even if all the candidate \n\n \nChapter 3 \u25a0 Data Understanding \n59\ninput variables were used in the model, only a small subset are typically good \npredictors, reducing the", "nput variables. Post-modeling, even if all the candidate \n\n \nChapter 3 \u25a0 Data Understanding \n59\ninput variables were used in the model, only a small subset are typically good \npredictors, reducing the number of variables the analysts needs to visualize to \nunderstand the most important patterns in the data. \nHistograms\nHistograms operate on a single variable and show an estimate of the shape of \nthe distribution by creating bins of the data and counting how many records of \nthe variable fall within the bin boundaries. The x-axis typically contains equal-\nwidth bins of the variable, and the y-axis contains the count or percentage of \ncount of records in the bins. The analyst can usually specify the number of bins \nin the software, though the default tends to be 10. \nThe fi rst reason to examine a histogram is to examine the shape. Figure 3-6 \nshows a standard histogram with a shape that appears to be normal or close to \nit. Figure 3-7, on the other hand, shows a histogram that is bi-mod", "mine a histogram is to examine the shape. Figure 3-6 \nshows a standard histogram with a shape that appears to be normal or close to \nit. Figure 3-7, on the other hand, shows a histogram that is bi-modal; there are \ntwo peaks in the distribution. The mean is actually at the trough between the \npeaks and therefore is not a typical value at all. Moreover, the standard devia-\ntion in the summary statistics will be much larger than the spread in either of \nthe two subdistributions of the data.\n[0\u201322]\n(22\u201344]\n(44\u201366]\n(66\u201388]\n(88\u2013110]\n(110\u2013132]\n(132\u2013154]\n(154\u2013176]\n(176\u2013198]\n(198\u2013220]\n(220\u2013242]\n(242\u2013264]\n(264\u2013286]\n0\n109\n218\n327\n436\n545\n654\n763\n872\n981\n1090\n1207\nFigure 3-6:  Standard histogram\nThe third histogram, Figure 3-8, shows a spike in the data that dwarfs any \nother bin. This variable is clearly not normal or uniform, and is a good candi-\ndate for a variable transformation if you are building models using a numeric \nalgorithm such as regression or K-Means clustering. \n\n60 \nChapter 3 \u25a0 D", "early not normal or uniform, and is a good candi-\ndate for a variable transformation if you are building models using a numeric \nalgorithm such as regression or K-Means clustering. \n\n60 \nChapter 3 \u25a0 Data Understanding\nFigure 3.7:  Bi-modal distribution\nHistograms can be customized in many software packages, including these \nthree, which are the most common:\n \n\u25a0Change the number of bins. This can be particularly helpful in identify-\ning data shape that can be distorted by using a particular number of bins \nor when data clumps near bin boundaries. Some software allows you to \nclick through the number of bins as a part of the graphic, making this very \neasy to do and visually effective. Other software requires you to exit the \nhistogram and rebuild the graphic completely, which makes changing \nthe number of bins tedious at best.\n \n\u25a0Change the y-axis scale. Another common modifi cation is to make the \ncounts on the y-axis of the histogram log scale rather than linear scale \nwhen a few bins", " \nthe number of bins tedious at best.\n \n\u25a0Change the y-axis scale. Another common modifi cation is to make the \ncounts on the y-axis of the histogram log scale rather than linear scale \nwhen a few bins have large bin counts\u2014spikes in the histogram\u2014that \noverwhelm the size of counts in all other bins. Some software also includes \noptions to show the mean value of a second variable on the y-axes, such \nas the target variable.\n \n\u25a0Overlay a second categorical variable. Some software allows the histogram \nto be color coded by a categorical variable, such as the target variable. This \nis helpful to determine if there is an obvious, visible relationship between \nthe input variable and the target, as shown in Figure 3-9. However, when \nbin sizes vary considerably, judging the relative proportion of the values of \nthe target variable represented in each bar can be diffi cult. In these cases, \nif the software also allows you to change the scale of each bar so that each \nis full height, it is easy", " the values of \nthe target variable represented in each bar can be diffi cult. In these cases, \nif the software also allows you to change the scale of each bar so that each \nis full height, it is easy to see the relative proportions of the target in each \nbin. This is essentially a visual crosstab. \n\n \nChapter 3 \u25a0 Data Understanding \n61\n0\n0.000\n0.100\n0.200\n0.300\n0.400\n0.500\n0.600\n0.700\n0.800\n0.900\n50\n100\n150\n200\nFigure 3-8:  Spike in distribution\nCARDPROM\n[0\u20135]\n0\n41\n82\n123\n164\n205\n246\n287\n328\n369\n410\n459\n[5\u201310]\n[10\u201315]\n[15\u201320]\n[20\u201325]\n[25\u201330]\n[30\u201335]\n[35\u201340]\n[40\u201345]\n[45\u201350]\n1\n0\nFigure 3-9:  Histogram overlaid with Target variable\nThe box plot is a graphical representation of the quartile statistics of a vari-\nable and is an excellent method to gain quick insight into the characteristics of \nnumeric data. In the box plot, the \u201cbox\u201d is the IQR; the median is typically rep-\nresented by a line cutting the IQR box in half. Some software also shows whiskers \nby drawing lines between the IQR ", " \nnumeric data. In the box plot, the \u201cbox\u201d is the IQR; the median is typically rep-\nresented by a line cutting the IQR box in half. Some software also shows whiskers \nby drawing lines between the IQR box and some other value in the data. Most \ncommonly, this other value is an upper quartile outlier boundary (+1.5 * IQR \nfrom the 3rd quartile) and the lower quartile outlier boundary (\u20131.5*IQR from \n\n62 \nChapter 3 \u25a0 Data Understanding\nthe 1st quartile), though it can also represent the minimum and maximum of \nthe range in the data or even a small and large percentile in the data (1 percent \nand 99 percent for example). \nFigure 3-10 shows a box plot for NUMPROM with the whiskers representing \nthe minimum and maximum of the distribution. However, this form can obscure \nthe main density of the data if the maximum is an extreme outlier. This can be \nseen with the box plot of LASTGIFT in Figure 3-11. A different depiction of the \nquartiles in LASTGIFT is shown in Figure 3-12, though this time", "data if the maximum is an extreme outlier. This can be \nseen with the box plot of LASTGIFT in Figure 3-11. A different depiction of the \nquartiles in LASTGIFT is shown in Figure 3-12, though this time the whiskers \nshow outliers in the data, and all values exceeding the extent of the whiskers \nare identifi ed as outliers. This plot has had all values greater than 200 removed \nfrom the plot to make the IQR box easier to see.\nValues\n195.00\nmax\n\u201cNUMPROM\u201d\n64.00\n27.00\n4.00\n75%\nmedian\nmean\n25%\nmin\n47.00\n46.97 +\u2212 22.97\n95412 total values\n165 distinct values\nFigure 3-10:  Box plot of NUMPROM\nBox plots are sometimes represented horizontally in predictive analytics and \nstatistics software. Unfortunately, not all predictive modeling software contains \nbox plots, although it is becoming much more commonplace. \n\n \nChapter 3 \u25a0 Data Understanding \n63\n75%\nmean\n20.00\n1000.00\nValues\n17.31 +\u2212 13.95\n15.00\n10.00\n0.00\n95412 total values\n132 distinct values\nmedian\n25%\nmin\nmax\n\u201cLASTGIFT\u201d\nFigure 3-11:  Box pl", "monplace. \n\n \nChapter 3 \u25a0 Data Understanding \n63\n75%\nmean\n20.00\n1000.00\nValues\n17.31 +\u2212 13.95\n15.00\n10.00\n0.00\n95412 total values\n132 distinct values\nmedian\n25%\nmin\nmax\n\u201cLASTGIFT\u201d\nFigure 3-11:  Box plot with outliers squashing IQR\n+\n+\n+\n+\n+\n+++\n+++++++++++++++++++++\n150.00\n200.00\n140.00\n111.0\n99.0\n85.0\n72.0\n63.0\n54.0\n45.0\n35.0\n20.0\n10.0\n0.0\nLASTGIFT\n36.0\nFigure 3-12:  Box and whiskers plot showing outliers\n\n64 \nChapter 3 \u25a0 Data Understanding\nMultiple Variable Summaries\nExamining single variables provides valuable information to assess the quality \nand value of the variables for use in predictive modeling. However, the value \nof a variable in predicting a target variable is sometimes hidden and can only \nbe revealed through interactions with other variables. \nHidden Value in Variable Interactions: Simpson\u2019s Paradox\nOne of the most intriguing examples demonstrating the value of variable inter-\nactions can be seen in a phenomenon known as Simpson\u2019s Paradox where a trend \nis seen in indivi", "ctions: Simpson\u2019s Paradox\nOne of the most intriguing examples demonstrating the value of variable inter-\nactions can be seen in a phenomenon known as Simpson\u2019s Paradox where a trend \nis seen in individual variables but is reversed when variables are combined. \nConsider a patient who needs to go to a hospital for surgery. The patient con-\nsiders two hospitals, Hospital A and Hospital B, by examining their mortality \nrates. The data used to make the decision is shown in Table 3-11.\nTable 3-11: Simpson\u2019s Paradox Example, Aggregate Table\nRESULT\nHOSPITAL A\nHOSPITAL B\nTOTAL\nDied\n63\n16\n79\nSurvived\n2037\n784\n2821\nTotal\n2100\n800\n2900\nMortality Rate\n3.0%\n2.0%\n2.7%\nBased on the information, you would obviously choose Hospital B because \nits mortality rate is 33 percent lower than Hospital A.\nBut what if you know an additional piece of information before selecting the \nhospital, namely the condition of the patient prior to surgery? Table 3-12 shows \nthese values.\nTable 3-12: Simpson\u2019s Paradox Examp", "if you know an additional piece of information before selecting the \nhospital, namely the condition of the patient prior to surgery? Table 3-12 shows \nthese values.\nTable 3-12: Simpson\u2019s Paradox Example, The Interactions\nGOOD CONDITION\nBAD CONDITION\nCONDITION HOSPITAL A HOSPITAL B TOTAL HOSPITAL A\nHOSPITAL B TOTAL\nDied\n6\n8\n 14\n57\n8\n65\nSurvived\n594\n592\n1186\n1443\n192\n1635\nTotal\n600\n600\n1200\n1500\n200\n1700\nMortality \nRate\n1%\n1.3%\n1.6%\n3.8%\n4.0%\n3.8%\nNow Hospital A has the lower mortality rate for both subgroups: those in \ngood condition and those in bad condition. But how can this be when Hospital \nA was higher overall?\n\n \nChapter 3 \u25a0 Data Understanding \n65\nThe key is in the counts: Hospital A not only treats more patients (2100 vs. \n800, from Table 3-10), but it treats a higher percentage of patients in bad condi-\ntion, whereas Hospital B actually treats fewer patients in bad condition than in \ngood condition. Since the mortality rate for patients arriving in bad condition \nis three to fo", "patients in bad condi-\ntion, whereas Hospital B actually treats fewer patients in bad condition than in \ngood condition. Since the mortality rate for patients arriving in bad condition \nis three to four times worse, Hospital A\u2019s overall mortality rate is biased and \nappears worse. In other words, Hospital A appears to have a worse mortality \nrate because it treats the most diffi cult patients.\nSimpson\u2019s Paradox isn\u2019t just an academic oddity; it happens in many real data \nsets. One of the most famous is the case against the University of California at \nBerkeley, which was accused of gender bias in admissions to graduate schools. \nIn 1973, 44 percent of male applicants were accepted, whereas only 35 percent \nof female applicants were accepted, a statistically signifi cant difference very \nunlikely to occur by chance. It seemed like a clear case of discrimination.\nHowever after separating the admissions rates by department, it turned out that \nwomen were admitted at a higher rate than men", "likely to occur by chance. It seemed like a clear case of discrimination.\nHowever after separating the admissions rates by department, it turned out that \nwomen were admitted at a higher rate than men in most departments. How can \nthis be when the overall rates were so overwhelmingly in favor of men? Women \napplied to departments that were more competitive and therefore accepted lower \npercentages of applicants, suppressing their overall acceptance rates, whereas \nmen tended to apply to departments with higher overall acceptance rates. It was \nonly after considering the interactions that the trend appeared.\nThe Combinatorial Explosion of Interactions\nInteractions therefore sometimes provide critical information in assessing \nvariables and their value for predictive modeling. However, assessing all pos-\nsible combinations can be problematic because of the combinatorial explosion. \nTable 3-13 shows how many combinations must be considered if all possible \ninteractions are considered. \nTa", "sing all pos-\nsible combinations can be problematic because of the combinatorial explosion. \nTable 3-13 shows how many combinations must be considered if all possible \ninteractions are considered. \nTable 3-13: Number of Interactions as Number of Variables Increases\nNUMBER OF \nVARIABLES\nNUMBER OF \nPOSSIBLE \nTWO\uf6baWAY \nINTERACTIONS\nNUMBER OF \nPOSSIBLE THREE\uf6ba\nWAY INTERACTIONS\nNUMBER OF \nPOSSIBLE \nFOUR\uf6baWAY \nINTERACTIONS\n5\n10\n10\n5\n10\n45\n120\n210\n50\n1,225\n19,600\n230,300\n100\n4,950\n161,700\n3,921,225\n500\n124,750\n20,708,500\n2,573,031,125\n1000\n499,500\n166,167,000\n41,417,124,750\n\n66 \nChapter 3 \u25a0 Data Understanding\nFor only fi ve variables, there isn\u2019t a problem in considering all possible com-\nbinations. However, if you have even 50 input variables, not unusual in most \npredictive modeling data sets, you would have to consider 1,225 two-way, 19,600 \nthree-way, and 230,300 four-way interactions. With 1000 variables, there are over \n40 billion four-way interactions.\nAs a result, any analysis of interac", " would have to consider 1,225 two-way, 19,600 \nthree-way, and 230,300 four-way interactions. With 1000 variables, there are over \n40 billion four-way interactions.\nAs a result, any analysis of interactions that is not automatic must focus on \nparticular interactions expected to be interesting. This will be the case in consid-\nering crosstabs of variables and visualization of pairs of variables in scatterplots.\nCorrelations\nCorrelations measure the numerical relationship of one variable to another, a \nvery useful way to identify variables in a modeling data set that are related to \neach other. This doesn\u2019t mean one variable\u2019s meaning is related to another\u2019s. \nAs the well-known phrase goes, \u201cCorrelation does not imply causation.\u201d Or, in \nother words, just because two variables are numerically related to one another \ndoesn\u2019t mean they have any informational relationship. \nSpurious Correlations\nSpurious correlations are relationships that appear to be related but in fact \nhave no direct re", "y related to one another \ndoesn\u2019t mean they have any informational relationship. \nSpurious Correlations\nSpurious correlations are relationships that appear to be related but in fact \nhave no direct relationship whatsoever. For example, when the AFC wins the \nSuper Bowl, 80 percent of the time the next year will be a bull market. Or but-\nter production in Bangladesh produces a statistically signifi cant correlation to \nthe S&P 500 index. \nOne such rule is the so-called Redskin Rule. The rule is defi ned in this way. \nExamine the outcome of the last Washington Redskins home football game prior \nto a U.S. Presidential Election, and between 1940 and 2000, when the Redskins \nwon that game, the incumbent party won the electoral vote for the White House. \nOn the other hand, when the Redskins lost, the incumbent party lost the elec-\ntion. Interesting, it worked perfectly for 17 consecutive elections (1940 \u20132000). \nIf you were to fl ip a fair coin (a random event with odds 50/50), the likelihoo", ", the incumbent party lost the elec-\ntion. Interesting, it worked perfectly for 17 consecutive elections (1940 \u20132000). \nIf you were to fl ip a fair coin (a random event with odds 50/50), the likelihood \nof fl ipping 17 consecutive heads is 1 in 131,072, very unlikely. \nHowever the problem with spurious correlations is that we aren\u2019t asking the \nquestion, \u201cHow associated is the Redskins winning a home game prior to the \npresidential election to the election outcome\u201d any more than we are asking the \nsame question of any other NFL team. But instead, the pattern of Redskin home \nwins is found after the fact to be correlated as one of thousands of possible patterns \nto examine. If you examined enough other NFL outcomes, such as examining \nthe relationship between Presidential elections and all teams winning their \n\n \nChapter 3 \u25a0 Data Understanding \n67\nthird game of the year, or fourth game of the year, or fi rst pre-season game of \nthe year, and so on until you\u2019ve examined over 130,000 poss", "teams winning their \n\n \nChapter 3 \u25a0 Data Understanding \n67\nthird game of the year, or fourth game of the year, or fi rst pre-season game of \nthe year, and so on until you\u2019ve examined over 130,000 possible patterns, one \nof them is likely to be associated. \nThis problem of over-searching to fi nd patterns that match your target is a \nreal problem in predictive analytics and will be addressed during the discus-\nsion on sampling. For now, you can merely chuckle at the myriad of spurious \ncorrelations you fi nd in data. \nHowever, some spurious correlations are related, albeit not directly. For \nexample, consider a town where it is noticed that as ice cream cone sales increase, \nswimsuit sales also increase. The ice cream sales don\u2019t cause the increased sales \nin swimsuits, but they are related by an unobserved variable: the outdoor tem-\nperature. The correlation is spurious but not a random relationship in this case.\nBack to Correlations\nNevertheless, correlations are very helpful ways to ", " by an unobserved variable: the outdoor tem-\nperature. The correlation is spurious but not a random relationship in this case.\nBack to Correlations\nNevertheless, correlations are very helpful ways to assess numeric variables in \na pairwise analysis. Finding pairs of variables with high correlation indicates \nthere is redundancy in the data that should be considered for two main reasons. \nFirst, redundant variables don\u2019t provide new, additional information to help \nwith predicting a target variable. They therefore waste the time spent building \nmodels. Second, some algorithms can be harmed numerically by including \nvariables that are very highly correlated with one another. \nMost predictive analytics software provides a way to produce a correlation \nmatrix, one correlation for each pair of variables. If there are N numeric vari-\nables in the data, you can create a correlation matrix that is N \u00d7 N. This shows \nan advantage of the correlation matrix as a method of Data Understanding: All ", "variables. If there are N numeric vari-\nables in the data, you can create a correlation matrix that is N \u00d7 N. This shows \nan advantage of the correlation matrix as a method of Data Understanding: All \nnumeric variables can be assessed in a single report.\nTable 3-14 contains one such correlation matrix, from the same variables \nincluded in Tables 3-2, 3-3, and 3-5 except for NUMPRM12. The values above the \ndiagonal of the matrix are equivalent to those below the diagonal: The correlation \nof CARDPROM to NUMPROM is identical to the correlation of NUMPROM to \nCARDPROM. The values of correlation range from \u20131 to +1, where +1 indicates \nperfect positive correlation and \u20131 indicates perfect negative correlation. Perfect \ncorrelation doesn\u2019t necessarily mean the values are identical, only that they \nall fall along a line; the relationship between values of the two variables have \nconstant positive slope. However, it is obvious that every variable is perfectly \ncorrelated with itself, which is", "ey \nall fall along a line; the relationship between values of the two variables have \nconstant positive slope. However, it is obvious that every variable is perfectly \ncorrelated with itself, which is why the diagonal in the correlation matrix must \nall have the value 1.\nPerfect negative correlation indicates the slope is negative and constant. A \ncorrelation value of 0 indicates there is no relationship between the two variables; \nthey appear to be random with respect to one another.\n\n68 \nChapter 3 \u25a0 Data Understanding\nTable 3-14: Correlations for Six Variables from the KDD Cup 98 Data\nCORRELATIONS\nCARDPROM\nNUMPROM\nRAMNTALL\nMAXRAMNT\nLASTGIFT\nCARDPROM\n1.000\n0.949\n0.550\n0.023\n\u20130.059\nNUMPROM\n0.949\n1.000\n0.624\n0.066\n\u20130.024\nRAMNTALL\n0.550\n0.624\n1.000\n0.557\n0.324\nMAXRAMNT\n0.023\n0.066\n0.557\n1.000\n0.563\nLASTGIFT\n\u20130.059\n\u20130.024\n0.324\n0.563\n1.000\nAnother way to understand correlation is that the higher the magnitude of \ncorrelation, the more you can determine the value of one variable once you k", "0\n0.563\nLASTGIFT\n\u20130.059\n\u20130.024\n0.324\n0.563\n1.000\nAnother way to understand correlation is that the higher the magnitude of \ncorrelation, the more you can determine the value of one variable once you know \nthe value of the other. If two variables are highly correlated and one value is \nat the upper end of its range of values, the value of the second variable in the \nsame record must have its value at the upper end of its range of values as well. \nIf the values are highly correlated, greater than 0.95 as a rule of thumb, they are \nessentially identical, redundant variables.\nNote, too, that in Table 3-14, NUMPROM and CARDPROM have very high \ncorrelation (0.949), although CARDPROM is nearly unrelated numerically to \nLASTGIFT as measured by their correlation (\u20130.059). \nYou must beware of how correlations are interpreted, however, for several \nreasons:Correlations are linear measures. Just because two variables are not \nlinearly related to one another doesn\u2019t mean that they are not related a", "rrelations are interpreted, however, for several \nreasons:Correlations are linear measures. Just because two variables are not \nlinearly related to one another doesn\u2019t mean that they are not related at all to \none another. The correlation between x and x2 is 0 because when x is positive, \nx2 is positive, but when x is negative, x2 is still positive with the same magni-\ntude as when x was positive. Correlations are affected severely by outliers and \nskewness. Even a few outlier value pairs can change the correlation between \nthem from something small, like 0.2, to something large, like 0.9. Visualization \nis a good idea to verify that high correlation is real.\nCrosstabs\nCrosstabs, short for cross-tabulations, are counts of the intersection between \ntwo variables, typically categorical variables. As was already noted in the com-\nbinatorial explosions shown in Table 3-13, you can only specify so many of \nthese crosstabs to compute, so usually only those interactions suspected to be \ninter", "bles. As was already noted in the com-\nbinatorial explosions shown in Table 3-13, you can only specify so many of \nthese crosstabs to compute, so usually only those interactions suspected to be \ninteresting are considered during Data Understanding. \nConsider the crosstab of RFA_2F and RFA_2A in Table 3-15. As a reminder, \nRFA_2F indicates how many gifts were given in the past time period of interest \n(1, 2, 3, or 4 or more), and RFA_2A indicates the dollar amount given during this \ntime period of interest, D being the lowest and G the largest. \n\n \nChapter 3 \u25a0 Data Understanding \n69\nTable 3-15: Crosstab of RFA_2F vs. RFA_2A\nRFA_2A / \nRFA_2F\nD\nE\nF\nG\nTOTAL\nPERCENT OF \nTOTAL\n1\n0\n4,911\n30,380\n12,384\n47,675\n50.0%\n2\n0\n4,989\n10,961\n4,595\n20,545\n21.5%\n3\n2,498\n7,767\n3,523\n1,503\n15,291\n16.0%\n4\n4,914\n3,972\n2,100\n915\n11,901\n12.5%\nTotal\n7,412\n21,639\n46,964\n19,397\n95,412\n100.0%\nPercent of \nTotal\n7.8%\n22.7%\n49.2%\n20.3%\n100.0%\nInterestingly, the two cells in the upper left of the crosstab are 0: There ", "3,972\n2,100\n915\n11,901\n12.5%\nTotal\n7,412\n21,639\n46,964\n19,397\n95,412\n100.0%\nPercent of \nTotal\n7.8%\n22.7%\n49.2%\n20.3%\n100.0%\nInterestingly, the two cells in the upper left of the crosstab are 0: There are \nno donors who gave small gifts (RFA_2A=D) 1 or 2 times during the past year \n(RFA_2F = 1 or 2). \nIs this unexpected? You can compute the number of gift records expected \nto be given if the interactions were truly random by multiplying the Percent of \nTotal values for RFA_2A=D and RFA_2F = 1: 50.0% * 7.8% = 3.9%. You would \nexpect 3.9 percent of the donors to fall in this cross-tab bin, which, for 94,412 \ndonors is 3,721. However, none fall there. Could this be by chance? Very unlikely. \nUndoubtedly there is some other reason for these zeros. These cells describe \ndonors who give infrequently and in low amounts. Apparently, when donors \ngive less often, they give in larger amounts, or if they give in smaller amounts, \nthey give more frequently. But, apparently for this non-profi t, don", "tly and in low amounts. Apparently, when donors \ngive less often, they give in larger amounts, or if they give in smaller amounts, \nthey give more frequently. But, apparently for this non-profi t, donors never give \nboth infrequently and in low amounts together. \nData Visualization, Two or Higher Dimensions\nVisualizing data in two or more dimensions provides additional insights into \ndata, but because of the combinatorial explosion shown in Table 3-13, the number \nof plots can become unmanageable. \nIn this section, all plots will use the Nasadata data set because the value of \nhigher-dimensional visualization is seen more readily.  \nScatterplots\nScatterplots are the most commonly used two-dimensional visualization method. \nThey merely plot the location of data points for two variables. A sample scat-\nterplot between Band 1 and Band 2 is shown in Figure 3-13. Note that there is \n\n70 \nChapter 3 \u25a0 Data Understanding\na strong relationship between the two variables\u2014their correlation coeffi ", "ample scat-\nterplot between Band 1 and Band 2 is shown in Figure 3-13. Note that there is \n\n70 \nChapter 3 \u25a0 Data Understanding\na strong relationship between the two variables\u2014their correlation coeffi cient \nis +0.85. Scatterplots are effective ways to show visually how correlated pairs \nof variables are: The more cigar-shaped the distribution and the narrower that \ncigar is, the stronger the correlation. \n\u221213.100\n\u221213.6\n\u221211.6\n\u22129.6\n\u22127.6\n\u22125.6\n\u22123.6\n\u22121.6\n0.4\nBand 1\nBand 2\n2.4\n4.4\n6.4\n8.4\n10.4\n\u221211.100\n\u22129.100\n\u22127.100\n\u22125.100\n\u22123.100\n\u22121.100\n0.900\n2.900\n4.900\n6.900\n8.899\nFigure 3-13: Scatterplot of Band 2 vs. Band 1\nFor highly correlated NUMPROM and CARDPROM (0.949), the scatterplot \nlooks like Figure 3-14.\nCARDPROM\nNUMPROM\n1\n4\n4\n21\n38\n55\n72\n89\n106\n123\n140\n157\n174\n195\n7\n10\n13\n16\n19\n22\n25\n28\n31\n34\n37\n40\n43\n46\n49\n52\n55\n58\n61\nFigure 3-14: NUMPROM vs. CARDPROM scatterplot\n\n \nChapter 3 \u25a0 Data Understanding \n71\nThere are many practical limitations to the effective use of scatterplots. Some \nof the probl", "0\n43\n46\n49\n52\n55\n58\n61\nFigure 3-14: NUMPROM vs. CARDPROM scatterplot\n\n \nChapter 3 \u25a0 Data Understanding \n71\nThere are many practical limitations to the effective use of scatterplots. Some \nof the problems include the following:\n \n\u25a0Too many data points: If too many data points are plotted, you won\u2019t be \nable to even see them all on the computer screen or a piece of paper, even \nif each data point is represented as a single pixel. Moreover, when the \npoints lie on top of each other or too near each other for distinct points to \nbe seen, the plot contains indistinct blobs and ceases to be informative. \nMost predictive modeling software limits the number of data points that \ncan be used in a scatterplot to 2,000 or so. One way to present the data \npoints more effectively is, rather than using small squares or circles to \nrepresent data points, use open circles. When data points differ slightly, \nthe rings will still show themselves as distinct open circles. However, with \ntoo many data poin", "mall squares or circles to \nrepresent data points, use open circles. When data points differ slightly, \nthe rings will still show themselves as distinct open circles. However, with \ntoo many data points, this is still not enough.\nYet another solution some software vendors have for this problem is that \nrather than plotting individual data points, plot regions in the scatterplot \nspace and color code those regions with the relative density of counts \nwithin the region. It is, in effect, a two dimensional histogram of the data \nor a heat map plot.\n \n\u25a0Outliers or severe skew pushed data into a small area of the plot: When \nthere is severe skew of severe outliers in the data, the data is sparse in \nthe tails of the distribution. However, the plot is presented using a linear \nscale, which means that much, if not most, of the number line has little \ndata in it; most of the data ends up in a small area of the plot so that there \nis little information that can be gleaned from the scatterplot i", " that much, if not most, of the number line has little \ndata in it; most of the data ends up in a small area of the plot so that there \nis little information that can be gleaned from the scatterplot itself. Figure \n3-15 shows highly skewed data plotted on a linear scale.\nOne solution to the problem is to scale the data before plotting it so that \nthere aren\u2019t big outliers or large sparse ranges of input data. A second \nsolution is to change the scale of the plot axes. For positively skewed data, \nlog scales are effective in changing the data so that the analyst can see \nthe data resolution and patterns better. However, the analyst must keep \nin mind that the data is no longer in its natural units but is in log units.\nAnscombe\u2019s Quartet\nStatistics can be deceiving. If data is not normally distributed and statistics that \nassume normal distributions are used, the statistics may not represent the sum-\nmary you expect and can therefore be misleading. One famous example of this \nphenomenon ", "istributed and statistics that \nassume normal distributions are used, the statistics may not represent the sum-\nmary you expect and can therefore be misleading. One famous example of this \nphenomenon is captured in four scatterplots known as Anscombe\u2019s Quartet, \ncreated in 1973 by the statistician Francis Anscombe. The data plotted in the \nquartet of scatterplots is shown in Table 3-16.\n\n72 \nChapter 3 \u25a0 Data Understanding\n13\n874\n1735\n2596\n3457\n4318\n5179\n6040\n6901\n7762\n8623\n9485\n282\n559\n5\n836\n1113\n1390\n1667\n1944\n2221\n2498\n2775\nMAXRAMNT\nRAMNTALL\n3052\n3329\n3606\n3883\n4160\n4437\n4714\n5000\nFigure 3-15: Highly skewed data in scatterplot\nTable 3-16: Anscombe\u2019s Quartet Data\nSET 1\nSET 2\nSET 3\nSET 4\nX\nY\nX\nY\nX\nY\nX\nY\n10\n8.04\n10\n9.14\n10\n7.46\n8\n6.58\n8\n6.95\n8\n8.14\n8\n6.77\n8\n5.76\n13\n7.58\n13\n8.74\n13\n12.74\n8\n7.71\n9\n8.81\n9\n8.77\n9\n7.11\n8\n8.84\n11\n8.33\n11\n9.26\n11\n7.81\n8\n8.47\n14\n9.96\n14\n8.1\n14\n8.84\n8\n7.04\n6\n7.24\n6\n6.13\n6\n6.08\n8\n5.25\n4\n4.26\n4\n3.1\n4\n5.39\n19\n12.5\n12\n10.84\n12\n9.13\n12\n8.15\n8\n5.56\n7\n4.82\n7\n7.26\n7\n6.4", ".71\n9\n8.81\n9\n8.77\n9\n7.11\n8\n8.84\n11\n8.33\n11\n9.26\n11\n7.81\n8\n8.47\n14\n9.96\n14\n8.1\n14\n8.84\n8\n7.04\n6\n7.24\n6\n6.13\n6\n6.08\n8\n5.25\n4\n4.26\n4\n3.1\n4\n5.39\n19\n12.5\n12\n10.84\n12\n9.13\n12\n8.15\n8\n5.56\n7\n4.82\n7\n7.26\n7\n6.42\n8\n7.91\n5\n5.68\n5\n4.74\n5\n5.73\n8\n6.89\n\n \nChapter 3 \u25a0 Data Understanding \n73\nInterestingly, each of the four pairs of data points, has the properties shown \nin Table 3-17.\nTable 3-17: Anscombe\u2019s Quartet Statistical Measures\nMEASURE\nVALUE\nMean X\n9.0\nVariance X\n11.0\nMean Y\n7.50\nVariance Y\n4.12 to 4.13\nCorrelation X vs. Y\n0.816 to 0.817\nRegression Equation\nY = 3.0 + 0.500*X\nR2 for \ufb01 t\n0.666 to 0.667\nAfter seeing these measures and values for all four of the data sets, especially \nthe correlation, regression equation, and r-squared values, an image usually \nemerges in the minds of analysts. However, a different story appears after see-\ning the four scatterplots in Figure 3-16.\nThe fi rst plot is the one that probably was in the minds of most analysts. \nHowever, the remaining three plots fi t the", " a different story appears after see-\ning the four scatterplots in Figure 3-16.\nThe fi rst plot is the one that probably was in the minds of most analysts. \nHowever, the remaining three plots fi t the data just as well. The upper-right \nplot (data set 2) shows a quadratic structure to the data. If you built a regression \nmodel or trend line that included both linear and quadratic terms for x, the fi t \nwould be perfect. In the lower-left plot, data set 3 shows a perfect linear relation-\nship except for one outlier. If this outlier, the third data point with coordinates \n(13,12.74), was removed from the data, the trend line would be a nearly perfect \nlinear fi t, with the equation:\ny = 4.0056 + 0.3454x, R2 = 0.99999\nso the removal of this one data point changes the slope from 0.5 in Table 3-2 \nto 0.3454 without the outlier. \nThe fi nal plot (data set 4) is a pathological data set where every point has \nthe same x value except one, creating an extreme outlier. Extreme outliers can \nbecom", "-2 \nto 0.3454 without the outlier. \nThe fi nal plot (data set 4) is a pathological data set where every point has \nthe same x value except one, creating an extreme outlier. Extreme outliers can \nbecome leverage points, meaning that because they are so far from the general \ntrend of the data, the regression line will pass through or near the data point. \nFigure 3-17 shows the original value of Y, 12.5, three other values for Y, 2.5, 6.5, \nand\u2013-4, and the linear fi t of the data that results from these values. The original \nvalue of Y just so happens to produce the summary statistics that match the \nsummary statistics of the other three data sets.\n\n74 \nChapter 3 \u25a0 Data Understanding\n0\n0\n2\n4\n6\n8\n10\n12\n14\n2\n4\n6\n8 10\nX1\nY1\n12 14 16 18 20\n0\n0\n2\n4\n6\n8\n10\n12\n14\n2\n4\n6\n8 10\nX3\nY3\n12 14 16 18 20\n0\n0\n2\n4\n6\n8\n10\n12\n14\n2\n4\n6\n8 10\nX4\nY4\n12 14 16 18 20\n0\n0\n2\n4\n6\n8\n10\n12\n14\n2\n4\n6\n8 10\nX2\nY2\n12 14 16 18 20\nFigure 3-16: Anscombe\u2019s Quartet scatterplots\n0\n0\n2\n4\n6\n8\n10\n12\n14\n2\n4\n6\n8\n10\n12\nData Set 4 \nData S", "2\n4\n6\n8\n10\n12\n14\n2\n4\n6\n8 10\nX4\nY4\n12 14 16 18 20\n0\n0\n2\n4\n6\n8\n10\n12\n14\n2\n4\n6\n8 10\nX2\nY2\n12 14 16 18 20\nFigure 3-16: Anscombe\u2019s Quartet scatterplots\n0\n0\n2\n4\n6\n8\n10\n12\n14\n2\n4\n6\n8\n10\n12\nData Set 4 \nData Set 4 \nData Set 4 \nData Set 4 \n14\n16\n18\n20\n0\n0\n1\n3\n2\n5\n4\n6\n7\n8\n10\n9\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n0\n0\n2\n\u22122\n\u22124\n\u22126\n4\n6\n8\n10\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\ny = 0.4999x + 3.0017\nR2 = 0.66671\ny = \u22120.0455x + 7.3654\nR2 = 0.01633\ny = \u22120.4092x + 10.274\nR2 = 0.57268\ny = \u22121.0001x + 15.002\nR2 = 0.88896\nFigure 3-17: Data Set 4, four different leverage points\n\n \nChapter 3 \u25a0 Data Understanding \n75\nThe conclusion therefore is that summary statistics are useful but don\u2019t always \ntell the whole story. \nScatterplot Matrices\nJust like a correlation matrix shows all pairwise correlations in the data, the \nscatterplot matrix shows all pairwise scatterplots for variables included in the \nanalysis. They provide a good way to show several relationships between pairs \nof var", "e correlations in the data, the \nscatterplot matrix shows all pairwise scatterplots for variables included in the \nanalysis. They provide a good way to show several relationships between pairs \nof variables in one plot. \nHowever, scatterplots take signifi cant amounts of space to display, and sig-\nnifi cant resources to draw, so typically, the number of variables included in the \nscatterplot is kept to less than 10. In the scatterplot matrix of Figure 3-18, only \nsix variables are shown, the fi rst of which is the categorical target variable. Note \nthat you can see immediately which pairs of variables are highly correlated: \nBand 1 and Band 3, Band 3 and Band 6, and Band 9 and Band 11 (negatively). \nClass\nClass\nalfalfa\n\u221213.6\n10.4\n\u22123.6\n\u221215.800\n9.200\n\u22125.801\n\u22128.601\n7.399\n1.399\nalfalfa\nrye\n...\n...\n\u221224.800 22.200\n\u22124.801\n\u22121.6\n\u22123.301\n\u221215.1\n\u221268.1\n37.9\n\u221218.1\n\u22120.601\n\u22121.301\nBand1\nBand3\nBand6\nBand9\nBand11\nBand1\nBand3\nBand6\nBand9\nBand11\nFigure 3-18: Scatterplot matrix\nPlotting many variables in one", "\n...\n\u221224.800 22.200\n\u22124.801\n\u22121.6\n\u22123.301\n\u221215.1\n\u221268.1\n37.9\n\u221218.1\n\u22120.601\n\u22121.301\nBand1\nBand3\nBand6\nBand9\nBand11\nBand1\nBand3\nBand6\nBand9\nBand11\nFigure 3-18: Scatterplot matrix\nPlotting many variables in one plot is usually a problem. One solution to that is \nthe idea of creating plots of parallel coordinates. In this plot, shown in Figure 3-19, \neach variable is represented in a column with its range independently scaled to \nits minimum to maximum range. The points on the vertical axes are the values \nof that variable. The lines connect data points for a single record, so in principle, \nyou could trace an entire record from left to right in a parallel coordinate plot. \nThe advantage of the plot is that all, or at least many, variables can be shown \nin one diagram. However, there are many issues that make parallel coordinates \ncumbersome to use practically: When there are large numbers of records, the \n\n76 \nChapter 3 \u25a0 Data Understanding\nlines blur together making trends exceedingly diffi cul", "that make parallel coordinates \ncumbersome to use practically: When there are large numbers of records, the \n\n76 \nChapter 3 \u25a0 Data Understanding\nlines blur together making trends exceedingly diffi cult to see. Parallel coordi-\nnates work best when the number of records is low\u2014hundreds, not thousands \nor more. Some implementations overcome this problem by plotting density \nof records rather than individual records so that you can see darker colors for \nmore common patterns and lighter for less common patterns. Patterns may be \nclear between neighboring variables, but are diffi cult to see when variables \nare separated. Even with fewer records, patterns are diffi cult to see because \nthe order of the variables matters in the interpretation. One solution is to color \ncode lines by a key categorical variable, such as the target, making the pattern \nmore evident throughout the range. In Figure 3-19, the Class values wheat, soy, \nand rye are all in the upper range of Band11, whereas clover a", "orical variable, such as the target, making the pattern \nmore evident throughout the range. In Figure 3-19, the Class values wheat, soy, \nand rye are all in the upper range of Band11, whereas clover and alfalfa are at \nthe lower range of Band11. However, the plot shows that for Band9, soy is larger \nthan wheat and rye. Too many variables make the patterns diffi cult to see. You \nmust limit the number of variables to the number that can fi t legibly on a single \npage on the screen, PowerPoint presentation, or on a sheet of paper.\nwheat\n37.9\n22.2\n10.4\nsoy\nrye\nclover\noats\ncorn\nalfalfa\nClass\nBand11\nBand9\nBand1\n\u221213.6\n\u221268.1\n\u221224.8\nFigure 3-19: Parallel coordinates\nOverlaying the Target Variable in Summary \nOne simple but powerful extension to any data visualization method is a simple \noverlay of an addition variable, usually a categorical variable or the target variable. \nConsider a binary target variable as the overlay. For scatterplots and scatterplot \nmatrices, the data points are color co", "an addition variable, usually a categorical variable or the target variable. \nConsider a binary target variable as the overlay. For scatterplots and scatterplot \nmatrices, the data points are color coded by the value of the target variable \nshowing regions in the scatterplots that have stronger or weaker relationships \nto the target variable. In a histogram, the counts in the bar attributed to each \nvalue of the target variable are represented by a color. \n\n \nChapter 3 \u25a0 Data Understanding \n77\nAdding this additional dimension provides a fi rst look-ahead to the predic-\ntive power of the input variables in the chart. They can also provide critical \ninformation about target-variable leakage. If the relationship between an input \nvariable and the target is too strong, often the explanation is that the target \nvariable information is somehow held within the input variable. For example, if \nyou built a histogram of area codes overlaid by a 1/0 target variable, and if you \nfound that for all", " that the target \nvariable information is somehow held within the input variable. For example, if \nyou built a histogram of area codes overlaid by a 1/0 target variable, and if you \nfound that for all area codes except for \u201c000\u201d the target variable was always \u201c1,\u201d \nwhereas for \u201c000\u201d the target variable was always \u201c0,\u201d you would not conclude that \nan area code is a perfect predictor, but rather that the area code is only known \nor coded after the target variable value is known. For the area code, this could \nmean that telephone numbers are only known after the donor has responded \nand therefore it is not a good predictor of response.\nFigure 3-20 shows a histogram of Band 1 overlaid with the target variable: \nthe seven crop types. Note that you can see that wheat, clover, oats, and alfalfa \nare present only for high values of Band 1, whereas corn and soy are present \nonly for low values of Band 1.\n0\n(\u221214\u2013 \u221211.558)\n(\u221211.558\u2013 \u22129.116)\n(\u22129.116\u2013 \u22126.674)\n(\u22126.674\u2013 \u22124.232)\n(\u22124.232\u2013 \u22121.79)\n(\u22121.79", "are present only for high values of Band 1, whereas corn and soy are present \nonly for low values of Band 1.\n0\n(\u221214\u2013 \u221211.558)\n(\u221211.558\u2013 \u22129.116)\n(\u22129.116\u2013 \u22126.674)\n(\u22126.674\u2013 \u22124.232)\n(\u22124.232\u2013 \u22121.79)\n(\u22121.79\u20130.654)\n(0.654\u20133.098)\n(3.098\u20135.542)\nBand 1\n(5.542\u20137.986)\n(7.986\u201310.43)\n8\n16\n24\n32\n40\n48\n56\n64\n72\n80\n88\n98\nalfalfa\ncorn\noats\nclover\nrye\nsoy\nwheat\nFigure 3-20: Histogram color coded with target\nWith box plots, you can split the box plot into one for each value of the target \nvariable, as shown in Figures 3-21 and 3-22. From these plots, you can see even \nmore clearly the trends identifi ed in the histogram.  \n\n78 \nChapter 3 \u25a0 Data Understanding\n10.4\n5.4\n0.4\nBand1\n\u22124.6\n\u221213.6\nFigure 3-21: Box plot for Band 1\n10.4\n8.4\n6.4\n4.4\n2.4\n0.4\n\u22121.6\n\u22121.6\n\u22121.6\n\u22125.6\n\u22125.6\n\u22125.6\n\u22125.6\n\u22122.6\n\u22122.6\n\u22127.6\n\u22127.6\n\u22127.6\n\u221211.6\n\u221212.6\n\u221213.6\n\u22120.6\n\u22120.6\n\u22121.6\n\u22124.6\n\u22124.6\n\u22124.6\n\u22123.6\n\u22125.6\n\u22127.6\n\u22129.6\n\u221211.6\n\u221213.6\nalfalfa\nclover\ncorn\noats\nrye\nsoy\nwheat\n7.4\n7.4\n8.4\n9.4\n5.4\n3.4\n3.4\n0.4\n1.4\n4.4\n6.4\n10.4\n3.4\n5.4\n+\n4.4\n2.4\nFigure 3-22: Box pl", "\n\u221212.6\n\u221213.6\n\u22120.6\n\u22120.6\n\u22121.6\n\u22124.6\n\u22124.6\n\u22124.6\n\u22123.6\n\u22125.6\n\u22127.6\n\u22129.6\n\u221211.6\n\u221213.6\nalfalfa\nclover\ncorn\noats\nrye\nsoy\nwheat\n7.4\n7.4\n8.4\n9.4\n5.4\n3.4\n3.4\n0.4\n1.4\n4.4\n6.4\n10.4\n3.4\n5.4\n+\n4.4\n2.4\nFigure 3-22: Box plot for Band 1 split by a target variable\nScatterplots in More Than Two Dimensions\nScatterplots can be enhanced to represent more than two dimensions in two \nways. The fi rst method is through projection; higher dimensional space can \n\n \nChapter 3 \u25a0 Data Understanding \n79\nbe projected onto two dimensions for visualization. The projection space must \nalways be two dimensions because the medium onto which the data is projected \nis two dimensional, whether it be a computer screen, monitor, or sheet of paper. \nThe simplest of the higher dimensional projections is a scatterplot in three \ndimensions. However, any snapshot summary of the three-dimensional plot is \njust one of the possible 2-D projections of the three-dimensional data. The value \nof 3-D plots comes from rotating the plot and observ", " However, any snapshot summary of the three-dimensional plot is \njust one of the possible 2-D projections of the three-dimensional data. The value \nof 3-D plots comes from rotating the plot and observing the changes as you \nrotate it, which isn\u2019t easy to replicate on paper. As the dimensions increase to 4, \n5, and more, the projection is even more troublesome because the combinatorial \nexplosion makes picking the right projection ever more diffi cult.\nSome algorithms can help select the project so that the interesting informa-\ntion in the higher dimensional projection is retained. Kohonen Self-Organizing \nMaps, discussed in Chapter 6 as an unsupervised learning technique, is one \nalgorithm often used to project high-dimensional data onto two dimensions. \nOthers algorithms belonging to a class of multidimensional scaling (MDS) algo-\nrithms, such as the Sammon Projection, try to retain the relative distances of data \npoints in the 2-D projection as was found in the higher dimensional spa", "ass of multidimensional scaling (MDS) algo-\nrithms, such as the Sammon Projection, try to retain the relative distances of data \npoints in the 2-D projection as was found in the higher dimensional space. Of \ncourse, there are limitations; it is rare that analysts are able to effectively project \nmore than 20 or 30 dimensions down to 2 without the help of algorithms to fi nd \ninteresting projections. Few predictive modeling software packages implement \nMDS algorithms and the grand tour because of the computational problems \nwith them as the number of records and fi elds grows.\nA simpler way to visualize higher-dimensional data is to use other means \nto modify the individual data points to provide the information found in the \nhigher dimensions. Three of these are color, size, and shape. If each data point \nrepresents two dimensions spatially (the x and y axes), and also is represented \nby these three additional modifi ers, fi ve dimensions can now be visualized. \nFigure 3-23 shows the s", "each data point \nrepresents two dimensions spatially (the x and y axes), and also is represented \nby these three additional modifi ers, fi ve dimensions can now be visualized. \nFigure 3-23 shows the same two spatial dimensions as was found in Figure \n3-13, but this time color coded by Class (the target variable), shape also coded \nby Class, and icon size coded by variable Band 9. \nNote that the size shows the square shapes representing alfalfa have larger \nvalues of Band 9 (larger icon size), large values of Band 1, but small values of \nBand 12. The same is true for the reverse triangles representing clover. \nThis plot shows revealing patterns in the fi ve dimensions plotted (only four \nunique dimensions), but fi nding these relationships can be quite diffi cult and \ntake exploration on the part of the analyst to use these dimensions effectively. \nSometimes, these dimensions are only found after modeling has revealed which \nvariables are the most effective in predicting the target vari", "e part of the analyst to use these dimensions effectively. \nSometimes, these dimensions are only found after modeling has revealed which \nvariables are the most effective in predicting the target variable.\n\n80 \nChapter 3 \u25a0 Data Understanding\n\u221243.8\n\u221213.6\n\u221211.6\n\u22129.6\n\u22127.6\n\u22125.6\n\u22123.6\n\u22121.6\n0.4\nBand 1\nBand 12\n2.4\n4.4\n6.4\n8.4\n10.4\n\u221238.8\n\u221233.8\n\u221228.8\n\u221223.8\n\u221218.8\n\u221213.8\n\u22128.8\n\u22123.8\n1.2\n6.2\n11.2\n16.2\n21.2\n24.2\nRectangle\nCircle\nTriangle\nReverse\nTriangle\nDiamond\nAsterisk\nCross\nalfalfa\nValues of\nClass\nShapes\ncorn\noats\nclover\nrye\nsoy\nwheat\nFigure 3-23: Scatterplot with multidimensional overlay\nThe Value of Statistical Signi\ufb01 cance\nThe idea of the signifi cance of a measured effect is not commonly considered \nin predictive modeling software except for those tools that have a strong con-\nnection with the fi eld of Statistics. Describing and explaining these statistics \nis outside the scope of this book; any statistics textbook will describe them in \ndetail. However, on the most basic level, understanding t", "d of Statistics. Describing and explaining these statistics \nis outside the scope of this book; any statistics textbook will describe them in \ndetail. However, on the most basic level, understanding these statistics well \nenough to interpret them can provide a modeler with additional insight. A \nshort list of the signifi cance measures described so far in this chapter is shown \nin Table 3-18.\nThe sample size, N, has a large effect on the signifi cance of the statistical \nmeasures. Signifi cance was devised primarily for small data (small N) to aid in \nunderstanding the uncertainty associated with interpreting the statistics. For \ndata sets with tens of thousands of records or more, even when there is statistical \nsignifi cance, there may not be operational differences that matter. For example, \nin Table 3-2, the skewness value for CARDPROM is statistically signifi cant, but \nit does not indicate a problem with skew, only that there were so many records \nin the data that even a skew of ", "ple, \nin Table 3-2, the skewness value for CARDPROM is statistically signifi cant, but \nit does not indicate a problem with skew, only that there were so many records \nin the data that even a skew of 0.15 is statistically signifi cant.\n\n \nChapter 3 \u25a0 Data Understanding \n81\nTable 3-18: Measures of Signi\ufb01 cance in Data Understanding\nBASE STATISTIC\nSIGNIFICANCE MEASURE\nMean\nStandard Error of Mean\nSkewness\nStandard Error of Skew\nKurtosis\nStandard Error of Kurtosis\nCrosstabs\nChi-square Statistic and p value\nCorrelations\nPearson correlation coe\ufb03  cient and p value\nBinomial Test\nError at 95% Con\ufb01 dence Level\nAnova, Di\ufb00 erence of Means\nF-statistic\nPulling It All Together into a Data Audit\nPutting the Data Understanding steps together\u2014computing summary statistics, \nexamining trends in the data, identifying problems in the data, and visualizing \nthe data\u2014forms what you might call a data audit. You should take note of the \nfollowing items for correction during Data Preparation.\nA list of data aud", "ntifying problems in the data, and visualizing \nthe data\u2014forms what you might call a data audit. You should take note of the \nfollowing items for correction during Data Preparation.\nA list of data audit items might include the following:\n \n\u25a0How many missing values are there? Do any variables have mostly or \nall missing values?\n \n\u25a0Are there strange minimum or maximum values?\n \n\u25a0Are there strange mean values or large differences between mean and \nmedian?\n \n\u25a0Is there large skew or excess kurtosis? (This only matters for algorithms \nthat assume normal distributions in the data.)\n \n\u25a0Are there gaps in the distributions, such as bi-modal or multi-modal \ndistributions?\n \n\u25a0Are there any values in the categorical variables that don\u2019t match the \ndictionary of valid values?\n \n\u25a0Are there any high-cardinality categorical variables?\n \n\u25a0Are there any categorical variables with large percentages of records \nhaving a single value?\n\n82 \nChapter 3 \u25a0 Data Understanding\n \n\u25a0Are there any unusually strong rel", "nality categorical variables?\n \n\u25a0Are there any categorical variables with large percentages of records \nhaving a single value?\n\n82 \nChapter 3 \u25a0 Data Understanding\n \n\u25a0Are there any unusually strong relationships with the target variable, \npossibly indicating leakage of the target into a candidate input variable?\n \n\u25a0Are any variables highly correlated with each other, possibly indicating \nredundant variables?\n \n\u25a0Are there any crosstabs that show strong relationships between categori-\ncal variables, possibly indicating redundant variables?\nSummary\n Data Understanding provides the analytic foundation for a project by summariz-\ning and identifying potential problems in the data. Summaries of the data can \nconfi rm the data is distributed as expected, or reveal unexpected irregularities \nthat need to be addressed during Data Preparation. Problems in the data, such \nas missing values, outliers, spikes, and high-cardinality, should be identifi ed \nand quantifi ed so they can be fi xed during D", "ed to be addressed during Data Preparation. Problems in the data, such \nas missing values, outliers, spikes, and high-cardinality, should be identifi ed \nand quantifi ed so they can be fi xed during Data Preparation.\nData visualization provides the analyst insights that are diffi cult or even \nimpossible to discover through summary statistics. It must be done with care \nprior to modeling when the number of candidate modeling variables is high. \nVisualization should be revisited after modeling is fi nished to show graphically \nhow the most important variables relate to the target variable. \nThis stage in the predictive modeling process cannot be rushed; Problems \nmissed during Data Understanding will come back to haunt the analyst \nduring Modeling.\n\n83\nData Preparation is the third stage of the predictive modeling process, intended \nto convert data identifi ed for modeling into a form that is better for the predic-\ntive modeling algorithms. Each data set can provide different challenges", " the predictive modeling process, intended \nto convert data identifi ed for modeling into a form that is better for the predic-\ntive modeling algorithms. Each data set can provide different challenges to \ndata preparation, especially with data cleansing, rendering recipes for data \npreparation exceedingly diffi cult to create; deviations from any recipe will \ncertainly be the norm. The approach taken in this chapter is to provide a set \nof techniques one can use to prepare data and the principles to consider when \ndeciding which to use. \nThe key steps in data preparation related to the columns in the data are \nvariable cleaning, variable selection, and feature creation. Data preparation \nsteps related to the rows in the data are record selection, sampling, and feature \ncreation (again). \nMost practitioners describe the Data Preparation stage of predictive modeling \nas the most time-intensive step by far, with estimates of the time taken during \nthis stage ranging between 60 and 90 perc", "ost practitioners describe the Data Preparation stage of predictive modeling \nas the most time-intensive step by far, with estimates of the time taken during \nthis stage ranging between 60 and 90 percent. Conversations with recruiters \nwho know and understand predictive modeling reveal a similar story, even to \nthe point where if the candidate does not give this answer, the recruiter knows \nthat the candidate has not been actively engaged in predictive modeling. This \nalso differentiates \u201creal-world\u201d modeling data from the data used in many com-\npetitions and sample data sets made available in software: These data sets are \noften cleaned extensively fi rst because the time and domain expertise needed \nto clean data is beyond what can be accomplished in a short period of time. \nC H A P T E R \n4\nData Preparation\n\n84 \nChapter 4 \u25a0 Data Preparation\nA new predictive modeling hire, upon receiving the data for his fi rst modeling \nproject, was quoted as complaining, \u201cI can\u2019t use this data; it\u2019", "R \n4\nData Preparation\n\n84 \nChapter 4 \u25a0 Data Preparation\nA new predictive modeling hire, upon receiving the data for his fi rst modeling \nproject, was quoted as complaining, \u201cI can\u2019t use this data; it\u2019s all messed up!\u201d \nPrecisely. Data almost always has problems, and fi xing them is what a predic-\ntive modeler has to do. \nOne reason that data cleaning in particular takes so long is that the predictive \nmodeler is often the fi rst person who has ever examined the data in such detail. \nEven if the data has been scrubbed and cleaned from a database storage perspec-\ntive, it may never have been examined from a predictive modeling perspective; \ndatabase cleanliness is not the same as predictive modeling cleanliness. Some \nvalues may be \u201cclean\u201d in a technical sense because the values are populated \nand defi ned. However, they may not communicate the information effectively \nto modeling algorithms in their original form. Additionally, some values that \nto a human are completely understandable\u2014", "d \nand defi ned. However, they may not communicate the information effectively \nto modeling algorithms in their original form. Additionally, some values that \nto a human are completely understandable\u2014a value of \u20131 in a profi t variable \nthat should always be positive is known to mean the company report hasn\u2019t \ncome out yet\u2014are confounding and contradictory, and could be damaging to a \npredictive model; predictive modeling algorithms know only what is explicitly \ncontained in the data.\nThe 60\u201390 percent guideline is a good rule of thumb for planning the fi rst \npredictive modeling project using data that has never been used for modeling \nbefore. However, once the approaches for cleaning and transforming data have \nbeen defi ned, this number will drop to less than 10 percent of the overall time \nbecause the most diffi cult work has already been done. \nVariable Cleaning\nVariable cleaning refers to fi xing problems with values of variables themselves, \nincluding incorrect or miscoded value", "ime \nbecause the most diffi cult work has already been done. \nVariable Cleaning\nVariable cleaning refers to fi xing problems with values of variables themselves, \nincluding incorrect or miscoded values, outliers, and missing values. Fixing \nthese problems can be critical to building good predictive models, and mistakes \nin variable cleaning can destroy predictive power in the variables that were \nmodifi ed. \nThe variables requiring cleaning should have been identifi ed already dur-\ning Data Understanding. This section describes the most common solutions to \nthose data problems.\nIncorrect Values\nIncorrect values are problematic because predictive modeling algorithms assume \nthat every value in each column is completely correct. If any values are coded \nincorrectly, the only mechanism the algorithms have to overcome these errors \nis to overwhelm the errors with correctly coded values, thus making the incor-\nrect values insignifi cant. \n\n \nChapter 4 \u25a0 Data Preparation \n85\nHow do you fi nd", "gorithms have to overcome these errors \nis to overwhelm the errors with correctly coded values, thus making the incor-\nrect values insignifi cant. \n\n \nChapter 4 \u25a0 Data Preparation \n85\nHow do you fi nd incorrect values and fi x them? For categorical variables, \nthe fi rst step is to examine the frequency counts as described in Chapter 3 and \nidentify values of a variable that are unusual, occurring very infrequently. \nIf there are not too many values of a variable, every value can be examined and \nverifi ed. However, for some values that are incorrect, there is no easy way to \ndetermine what the actual value is. The analysis may require domain experts \nto interpret the values so the analyst better understands the intent of the values \nsaved in the data, or database experts to uncover how and why the incorrect \nvalues made their way into the data in the fi rst place.\nIf the values cannot be interpreted, they should be treated as missing if they \nare infrequent, but if they occur in signi", "d why the incorrect \nvalues made their way into the data in the fi rst place.\nIf the values cannot be interpreted, they should be treated as missing if they \nare infrequent, but if they occur in signifi cant numbers, they can be left as their \nincorrect value for the purposes of modeling, only to be interpreted later.\nFor continuous values, incorrectly coded values are most often detected \nas outliers or unusual values, or as spikes in the distribution, where a single \nrepeated value occurs far more often than you would otherwise expect. If the \nincorrectly coded values occur so infrequently that they cannot be detected \neasily, they will most likely be irrelevant to the modeling process.\nConsistency in Data Formats\nA second problem to be addressed with variable cleaning is inconsistency in \ndata. The format of the variable values within a single column must be consistent \nthroughout the column. Most often, mismatches in variable value types occur \nwhen data from multiple sources are c", "n \ndata. The format of the variable values within a single column must be consistent \nthroughout the column. Most often, mismatches in variable value types occur \nwhen data from multiple sources are combined into a single table. For example, if \nthe purchase date in a product table comes from several sources\u2014HTTP, mobile, \nand .mobi, for instance\u2014the date format must be consistent for predictive mod-\neling software to handle the dates properly. You cannot have some dates with \nthe format mm/dd/yyyy and others with the format mm/dd/yy in the same \ncolumn. Another common discrepancy in data types occurs when a categorical \nvariable (like ZIP code) is an integer in some data tables and a string in others. \nOutliers\nOutliers are unusual values that are separated from the main body of the distri-\nbution, typically as measured by standard deviations from the mean or by the \nIQR. Whether or not you remove or mitigate the infl uence of outliers is a critical \ndecision in the modeling process. ", "i-\nbution, typically as measured by standard deviations from the mean or by the \nIQR. Whether or not you remove or mitigate the infl uence of outliers is a critical \ndecision in the modeling process. Outliers can be unusual for different reasons. \nSometimes outliers are just data coded wrong; an age value equal to 141 could \nreally be an age equal to 41 that was coded incorrectly. These outliers should \nbe examined, verifi ed, and treated as incorrectly coded values.\nHowever, some outliers are not extremes at all. An outlier defi ned as an \nunusual value rather than a value that exceeds a fi xed number of standard \n\n86 \nChapter 4 \u25a0 Data Preparation\ndeviations from the mean or a multiple of the IQR can be anywhere in the range \nof the variable, provided there aren\u2019t many other values in that range of the \nvariable\u2019s values. For example, a profi t/loss statement for large organizations \ncould typically have large profi ts or large losses in the millions of dollars, but \nmagnitudes less t", "t range of the \nvariable\u2019s values. For example, a profi t/loss statement for large organizations \ncould typically have large profi ts or large losses in the millions of dollars, but \nmagnitudes less than $100 could be very rare. These outliers are not extremes \nat all; they are at a trough in the middle of a multi-modal distribution. Without \ndata visualization, the analyst may not even know these are unusual values at \nall because the summary statistics will not provide immediately obvious clues: \nThe mean may be close to zero, the standard deviation reasonable, and there \nis no skew! These outliers don\u2019t cause bias in numeric algorithms, but can still \neither cause the algorithms to behave in unexpected ways, or result in large \nmodeling errors because the algorithms essentially ignore the records. \nIf the outliers to be cleaned are examples of values that are correctly coded, \nthere are four typical approaches to handling them:\n \n\u25a0Remove the outliers from the modeling data. In this ", "he records. \nIf the outliers to be cleaned are examples of values that are correctly coded, \nthere are four typical approaches to handling them:\n \n\u25a0Remove the outliers from the modeling data. In this approach, the assump-\ntion is that the outliers will distort the models so much that they harm more \nthan help. This can be the case particularly with the numeric algorithms \n(linear regression, k-nearest neighbor, K-Means clustering, and Principal \nComponent Analysis). In these algorithms, outliers can dominate the solu-\ntion to the point that the algorithms essentially ignore most of the data \nbecause they are biased toward modeling the outliers well. \nRemoving the outliers can carry some risk as well. Because these values \nare not included in the modeling data, model deployment could be com-\npromised when outliers appear in the data; records with any outliers will \neither have to be removed prior to scoring, or the records will be scored \nbased on data never seen before by the modeling ", "-\npromised when outliers appear in the data; records with any outliers will \neither have to be removed prior to scoring, or the records will be scored \nbased on data never seen before by the modeling algorithms, perhaps \nproducing scores that are unreliable. This will have to be tested during \nmodel evaluation to evaluate the risk associated with scoring outliers. \nAnother problem with removing outliers occurs when the models are \nintended to be able to identify a subset of customer behavior that is par-\nticularly good or particularly bad. Fraud detection by its very nature is \nidentifying unusual patterns of behavior. The very best customer lifetime \nvalue represents the very best of unusual behavior. Removing records with \noutliers in the inputs could remove those cases that are associated with \nthe most egregious fraud or with the very best customer lifetime value. \n \n\u25a0Separate the outliers and create separate models just for outliers. This \napproach is an add-on to the fi rst appro", "ed with \nthe most egregious fraud or with the very best customer lifetime value. \n \n\u25a0Separate the outliers and create separate models just for outliers. This \napproach is an add-on to the fi rst approach. Records with outliers are pulled \nout into separate data sets for modeling, and additional models are built \njust on these data. Because outliers are relatively rare events, the models \nwill likely be simpler, but building models specifi cally for these outliers \nwill enable you to overcome the problem described in the fi rst approach, \n\n \nChapter 4 \u25a0 Data Preparation \n87\nnamely the problem of deploying models that did not have outliers in the \ntraining data. Some predictive modelers who build separate models for \nthe outliers will relax the defi nition of outliers to increase the number of \nrecords available for building these models One way to achieve this is by \nrelaxing the defi nition of an outlier from three standard deviations from \nthe mean down to two standard deviations from", "of \nrecords available for building these models One way to achieve this is by \nrelaxing the defi nition of an outlier from three standard deviations from \nthe mean down to two standard deviations from the mean.\nDecision trees can essentially incorporate this approach automatically. \nIf outliers are good predictors of a target variable, the trees will split the \ndata into the outlier/non-outlier data subsets naturally.\n \n\u25a0Transform the outliers so that they are no longer outliers. Another \napproach to mitigating the bias caused by outliers that are extreme values \nis to transform the data so the outliers are no longer outliers. The discussion \non skew will describe specifi c approaches that handle the extreme values \nsmoothly; these techniques all reduced the distance between the outliers \nand the main body of the distribution. For multi-modal distributions with \ntroughs that contain few records, even simple min-max transformations \nthat reduce the distance between data points can help ", "and the main body of the distribution. For multi-modal distributions with \ntroughs that contain few records, even simple min-max transformations \nthat reduce the distance between data points can help reduce the impact \nof outliers in the data.\n \n\u25a0Bin the data. Some outliers are too extreme for transformations to capture \nthem well; they are so far from the main density of the data that they \nremain outliers even after transformations have been applied. An alter-\nnative to numeric transformations is to convert the numeric variable to \ncategorical through binning; this approach communicates to the modeling \nalgorithms that the actual value of the outliers are not important, but it is \nthe mere fact that the values are outliers that is predictive. This approach \nis also a good one when the outliers are not extremes and go undetected \nby automatic outlier detection algorithms. Binning to identify outliers \nmay require some manual tuning to get the desired range of values for \nthe outliers ", "tliers are not extremes and go undetected \nby automatic outlier detection algorithms. Binning to identify outliers \nmay require some manual tuning to get the desired range of values for \nthe outliers into a single bin. These binned variables will, of course, have \nto be converted to dummy variables for use in numeric algorithms. \nAn alternative to binning the entire variable is to take a two-step approach \nto outlier mitigation. First, the modeler can create a single-column dummy \nvariable to indicate that the variable value is an outlier. Next, the modeler \ncan transform the outliers as described in the third bullet to mitigate the \nnumeric affect of the outlier on the numeric algorithms. \n \n\u25a0Leave the outliers in the data without modifi cation. The last approach \nto handling outliers is to leave them in the data as they appear naturally. \nIf this approach is taken, the modeler could decide to use only algorithms \nunaffected by outliers, such as decision trees. Or, in some cases, the ", "to leave them in the data as they appear naturally. \nIf this approach is taken, the modeler could decide to use only algorithms \nunaffected by outliers, such as decision trees. Or, in some cases, the mod-\neler could use algorithms that are affected by outliers to purposefully bias \nthe models in the direction of the outliers. For example, consider a model \n\n88 \nChapter 4 \u25a0 Data Preparation\nto predict customer lifetime value (CLV) for a retail chain. Assume that \nthere are large outliers at the upper end of CLV, indicative of the best \ncustomers for the retailer. The desire of the retailer is to build accurate \nCLV models, of course. But what kinds of errors are more acceptable \nthan others? If customers with CLV values in the $500\u2013$1,000 range are \npredicted accurately but at the expense of the $10,000 customers (who \nare predicted to have CLV values in the same range), is this acceptable?\nConsider an example with the variable MAXRAMNT. Table 4-1 shows sum-\nmary statistic values for MA", "f the $10,000 customers (who \nare predicted to have CLV values in the same range), is this acceptable?\nConsider an example with the variable MAXRAMNT. Table 4-1 shows sum-\nmary statistic values for MAXRAMNT, a variable with considerable positive \nskew: Note the mean is larger than the median and that the Maximum is many \ntimes larger than the upper bound of the IQR outlier threshold, and the values \ngreater than 36 are considered outliers by the IQR standard (there are 5,023 \n\u201coutliers\u201d in the 95,412 records). \nTable 4-1: Summary Statistics for MAXRAMNT\nSTATISTIC\nVALUE\nMinimum\n5\nLower IQR outlier threshold\n5\n1st quartile\n14\nMedian\n17\nMean\n19.999\n3rd quartile\n23\nUpper IQR outlier threshold\n36\nMaximum\n5000\nThe business objective will often dictate which of the fi ve approaches in the\n preceding bullets should be used to treat the outliers. Consider, however, \nthe fourth option: binning. Figure 4-1 shows the results from creating equal-\ncount bins. If only the extreme outliers are fl agge", "ing bullets should be used to treat the outliers. Consider, however, \nthe fourth option: binning. Figure 4-1 shows the results from creating equal-\ncount bins. If only the extreme outliers are fl agged, any value greater or equal to \n75 can be given the value 1 in a dummy variable, fl agging 893 records, just under \n1 percent of the population. The modeler could decide to lower the threshold \nunder 50, depending on how many records you want to include in the dummy \nvariable. \nYou may decide to deliberately bias the model toward the larger CLV values so \nthat, even if there are errors in predicting $500 customers, as long as the $10,000 \ncustomers are identifi ed well, the model is a success. In this case, leaving the \ndata alone and allowing the models to be biased by the large magnitude may be \n\n \nChapter 4 \u25a0 Data Preparation \n89\nexactly what is most desirable for the retailer. The decision to mitigate outliers, \ntherefore, depends on the business objectives of the model.\nMAXRAMNT Bin", "ay be \n\n \nChapter 4 \u25a0 Data Preparation \n89\nexactly what is most desirable for the retailer. The decision to mitigate outliers, \ntherefore, depends on the business objectives of the model.\nMAXRAMNT Binning\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\n18000\n20000\n0\n(5,6]\n[5,5]\n(6,7]\n(7,8]\n(8,10]\n(10,11]\n(11,12]\n(12,13]\n(13,14]\n(14,15]\n(15,16]\n(16,17]\n(17,18]\n(18,19]\n(19,20]\n(20,21]\n(21,22]\n(22,23]\n(23,25]\n(25,27]\n(27,30]\n(30,32]\n(32,35]\n(35,40]\n(40,45]\n(45,50]\n(50,75]\n(75,5,000]\nFigure 4-1:  Binning of MAXRAMNT\nMultidimensional Outliers\nNearly all outlier detection algorithms in modeling software refer to outliers in \nsingle variables. Outliers can also be multidimensional, but these are much more \ndiffi cult to identify. For example, donors to a non-profi t organization who have \ngiven lots of gifts historically are much more likely to give in small amounts, and\n those who donate infrequently are more likely to give in larger amounts. There \nmay be interesting outliers in the data repres", "of gifts historically are much more likely to give in small amounts, and\n those who donate infrequently are more likely to give in larger amounts. There \nmay be interesting outliers in the data representing large-dollar, frequent donors. \nNeither single-dimension may be an outlier, but the combination is. The problems \nwith these outliers are different than the single-variable outliers; the problem is \nnot with numeric algorithm bias, but rather with the lack of data in the region \nwith the outliers that makes good predictions diffi cult to achieve. \nPredictive modelers rarely try to identify multidimensional outliers before \nmodeling, but they sometimes uncover them after building and assessing the \nmodels. If you would like to identify potential multidimensional outliers, one \nway to do so is with unsupervised learning techniques such as clustering; clus-\nters that have relatively few values in them are indicative of unusual groups of \ndata, and therefore could represent multidimensi", "so is with unsupervised learning techniques such as clustering; clus-\nters that have relatively few values in them are indicative of unusual groups of \ndata, and therefore could represent multidimensional outliers. \nTo mitigate the effects of multidimensional outliers, the modeler could apply \nthe same approaches described for single-variable outliers. The only difference \nis that in the fourth item in the previous list, rather than creating a dummy vari-\nable for each variable separately, you could create a dummy variable indicating \nthe interaction is an outlier or indicating membership in a cluster that identifi es the \nmultidimensional outliers.\n\n90 \nChapter 4 \u25a0 Data Preparation\nMissing Values\nMissing values are arguably the most problematic of all data problems. Missing \nvalues are typically coded in data with a null value or as an empty cell, although \nmany more representations can exist in data. Table 4-2 shows typical missing \nvalues you may encounter in data.\nTable 4-2: Typica", "pically coded in data with a null value or as an empty cell, although \nmany more representations can exist in data. Table 4-2 shows typical missing \nvalues you may encounter in data.\nTable 4-2: Typical Missing Values Codes\nPOSSIBLE REPRESENTATION \nOF MISSING VALUES\nDESCRIPTION\nNull, empty string (\u201c\u201d)\nFor numeric or categorical variables\n0\nFor numeric variables that are never equal to zero\n\u20131\nFor numeric variables that are never negative\n99, 999, and so on\nFor numeric variables that have values less than 10, \n100, and so on\n\u201399, \u2013999, and so on\nFor numeric variables that can be negative\nU, UU\nFor categorical variables, especially 1- or 2-character \ncodes\n00000, 00000-0000, XXXXX\nFor ZIP Codes\n11/11/11\nFor dates\n000-000-0000, 0000000000\nFor phone numbers\nSometimes missing values are even coded with a value that looks like a \nlegitimate value. For example, customer income code may have values 1 through \n9, but the value 0 means the income code is missing. Or Gender of Head of \nHousehold c", "ed with a value that looks like a \nlegitimate value. For example, customer income code may have values 1 through \n9, but the value 0 means the income code is missing. Or Gender of Head of \nHousehold could be coded \u201cM\u201d for male, \u201cF\u201d for female, \u201cJ\u201d for joint (joint head \nof household, and \u201cD\u201d (did not respond). The \u201cD\u201d doesn\u2019t immediately look like \na missing value though in this case it is. \nOne problem with understanding missing values is that the mere fact that \nthe data is missing is only part of the story. Some missing values are due to \nsimple data entry errors. Some missing values mean nothing more than that the \nvalues are completely unknown. This can be because the data has been lost inad-\nvertently through data corruption or overwriting of database tables, or because \nit was never collected in the fi rst place because of data collection limitations \nor even forgetfulness. Some data is missing because the data was deliberately \nwithheld during data collection, such as with surv", " collected in the fi rst place because of data collection limitations \nor even forgetfulness. Some data is missing because the data was deliberately \nwithheld during data collection, such as with surveys and questionnaires. In \nthe case of deliberate omission of values, there may be predictive information \njust from the value being missing.\n\n \nChapter 4 \u25a0 Data Preparation \n91\nIn censored cases, data is not collected purposefully, often because orga-\nnizations do not have the resources to collect data on every individual or \nbusiness and have therefore decided to omit some from the data collection \nprocess. Marketing organizations do this routinely when they contact some \ncustomers but not others. Tax collection agencies do this when they audit some \nindividuals or businesses and not others. The audit outcome for those indi-\nviduals not audited, of course, is missing: We don\u2019t know what that outcome \nwould have been.\nMCAR, MAR, and MNAR\nThe abbreviations MCAR, MAR, and MNAR are used in ", ". The audit outcome for those indi-\nviduals not audited, of course, is missing: We don\u2019t know what that outcome \nwould have been.\nMCAR, MAR, and MNAR\nThe abbreviations MCAR, MAR, and MNAR are used in many cases by stat-\nisticians but rarely in predictive modeling software. However, the meaning of \nthese different types of missing values, when known by the modeler, can affect \nhow one chooses to impute the missing values. \nMCAR is the abbreviation for missing completely at random and means that there \nis no way to determine what the value should have been. Random imputation \nmethods work as well as any method can for MCAR. MAR is the most confusing \nof the abbreviations, and means missing at random. While this seems to be the \nsame as MCAR, MAR really implies a conditional relationship between the miss-\ning value and other variables. The missing value itself isn\u2019t known and cannot \nbe known, but it is missing because of another observed value. For example, if \nanswering \u201cyes\u201d for Questi", "een the miss-\ning value and other variables. The missing value itself isn\u2019t known and cannot \nbe known, but it is missing because of another observed value. For example, if \nanswering \u201cyes\u201d for Question 10 on a survey means one doesn\u2019t answer Question \n11, the reason Question 11 is missing is not random, but what the answer to \nQuestion 11 would have been had it been answered cannot be known.\nMNAR is the abbreviation for missing not at random (not a very clear way \nof describing it). In this case, the values of the missing value can be inferred \nin general by the mere fact that the value is missing. For example, a responder \nin a survey may not provide information about a criminal record if they have \none, whereas those without a criminal record would report \u201cno record.\u201d MNAR \ndata, if you know or suspect the data is MNAR, should not be imputed with \nconstants or at random if possible because these values will not refl ect well what \nthe values should have been.\nFixing Missing Data\nMis", " know or suspect the data is MNAR, should not be imputed with \nconstants or at random if possible because these values will not refl ect well what \nthe values should have been.\nFixing Missing Data\nMissing value correction is perhaps the most time-consuming of all the vari-\nable cleaning steps needed in data preparation. Whenever possible, imputing \nmissing values is the most desirable action. Missing value imputation means \nchanging values of missing data to a value that represents a plausible or expected \n\n92 \nChapter 4 \u25a0 Data Preparation\nvalue in the variable if it were actually known. These are the most commonly \nused methods for fi xing problems associated with missing values. \nListwise and Column Deletion\nThe simplest method of handling missing values is listwise deletion, meaning \none removes any record with any missing values, leaving only records with \nfully populated values for every variable to be used in the analysis. In some \ndomains, this makes perfect sense. For example, ", "e removes any record with any missing values, leaving only records with \nfully populated values for every variable to be used in the analysis. In some \ndomains, this makes perfect sense. For example, for radar data, if a radar return \nhas missing values, it most likely means there was a data collection problem \nwith the particular example, leaving that record suspect. If the occurrence of \nmissing values is rare, this will not harm the analysis.\nHowever, many data sets have missing values in most if not all records, \nespecially in domains related to the behavior of people, such as customer acquisi-\ntion or retention and survey analysis. One may begin with more than 1 million \nrecords, but after listwise deletion, only 10,000 remain. Clearly, this isn\u2019t desir-\nable and is rarely done in customer analytics. \nSometimes, you fi nd that predictive modeling software performs listwise \ndeletion by default without alerting the user that this is what is happening. \nYou only discover this defaul", "tomer analytics. \nSometimes, you fi nd that predictive modeling software performs listwise \ndeletion by default without alerting the user that this is what is happening. \nYou only discover this default when you examine reports on the models that \nhave been built and can see that very few records were actually used in build-\ning the models. \nAn alternative to listwise deletion is column deletion: removing any variable \nthat has any missing values at all, leaving only variables that are fully popu-\nlated. This approach solves the listwise deletion problem of removing too many \nrecords, but may still be too restrictive If only a few of the values are missing \nin a column, removing the entire column is a severe action. Nevertheless, both \nlistwise deletion and column deletion are practiced, especially when the num-\nber of missing values is particularly large or when the timeline to complete the \nmodeling is particularly short. \nImputation with a Constant\nThis option is almost always availa", "pecially when the num-\nber of missing values is particularly large or when the timeline to complete the \nmodeling is particularly short. \nImputation with a Constant\nThis option is almost always available in predictive analytics software. For \ncategorical variables, this can be as simple as fi lling missing values with a \u201cU\u201d \nor another appropriate string to indicate missing. For continuous variables, this \nis most often a 0. For some continuous variables, fi lling missing values with a 0 \nmakes perfect sense. Bank account balances for account types that an individual \ndoes not have can be recoded with a 0, conveying information about the account \nthat makes sense (no dollars on account). Sometimes, imputing with a 0 causes \nsignifi cant problems, however. Age, for example, could have a range of values \nbetween 18 and 90. Imputing missing values with a 0 clearly doesn\u2019t convey a \nvalue that is possible, let alone likely, for donors in a data set. \n\n \nChapter 4 \u25a0 Data Preparation \n93\nMea", "ge of values \nbetween 18 and 90. Imputing missing values with a 0 clearly doesn\u2019t convey a \nvalue that is possible, let alone likely, for donors in a data set. \n\n \nChapter 4 \u25a0 Data Preparation \n93\nMean and Median Imputation for Continuous Variables\nThe next level of sophistication is imputing with a constant value that is not \npredefi ned by the modeler. Mean imputation is perhaps the most commonly \nused method for several reasons. First, it\u2019s easy; mean values are easy to com-\npute and often available without extra computational steps. Second, the idea \nbehind mean imputation is that the value that is imputed should do the least \namount of harm possible; on average, one expects the values that are missing \nto approach the mean value.  \nHowever, mean imputation has problems as well. The more values that are \nmissing and imputed with the mean, the more values of the variable fall exactly \nat the mean, so a spike emerges in the distribution. The problem is primarily \nwith algorithms that", "re values that are \nmissing and imputed with the mean, the more values of the variable fall exactly \nat the mean, so a spike emerges in the distribution. The problem is primarily \nwith algorithms that compute summary statistics; the more values imputed with \nthe mean, the smaller the standard deviation becomes when you compare the \nstandard deviation of the variable before and after imputation. \nConsider the variable AGE, shown in Figure 4-2. The missing values, \n25 percent of the population, are shown as a separate bar at the right end of \nthe histogram. \n[0\u20137]\n(7\u201314]\n(14\u201321]\n(21\u201328]\n(28\u201335]\n(35\u201342]\n(42\u201349]\n(49\u201356]\n(56\u201363]\n(63\u201370]\n(70\u201377]\n(77\u201384]\n(84\u201391]\n(91\u201398]\nMissing Value\n0\n450\n225\n675\n900\n1125\n1350\n1575\n1800\n2025\n2250\n2478\nFigure 4-2:  Histogram for AGE, including missing values\nTable 4-3 shows the summary statistics for AGE: a mean of 61.64 and standard \ndeviation with 16.62. If you were to impute missing values for AGE when AGE \nhas 25 percent, 40 percent, and 62 percent missin", "e 4-3 shows the summary statistics for AGE: a mean of 61.64 and standard \ndeviation with 16.62. If you were to impute missing values for AGE when AGE \nhas 25 percent, 40 percent, and 62 percent missing, you would see the standard \ndeviation shrink from 16.6 to 14.4, 12.9, and 10.2 respectively. What causes the \nshrinking of standard deviation?\n\n94 \nChapter 4 \u25a0 Data Preparation\nTable 4-3: Shrinking Standard Deviation from Mean Imputation\nVARIABLE\nORIGINAL \nMEAN\nORIGINAL \nSTANDARD \nDEVIATION MISSING\nPERCENT \nMISSING\nMEAN AFTER \nIMPUTATION\nSTANDARD \nDEVIATION \nAFTER \nIMPUTATION\nAGE (25% \nmissing)\n61.6\n16.6\n23,665\n24.8\n61.64\n14.4\nAGE (40% \nmissing)\n61.6\n16.6\n37,984\n39.8\n61.63\n12.9\nAGE (62% \nmissing)\n61.6\n16.6\n59,635\n62.5\n61.67\n10.2\nFigure 4-3 shows the plots for AGE with 25 percent and 40 percent missing \nvalues imputed with the mean, 61.6. The shrinking of the standard deviation \noccurs because of the spike. The more missing values imputed with the mean, \nthe larger the spike relative to ", "ent missing \nvalues imputed with the mean, 61.6. The shrinking of the standard deviation \noccurs because of the spike. The more missing values imputed with the mean, \nthe larger the spike relative to the rest of the distribution. \nMean imputation is by far the most common method, but in some circum-\nstances, if the mean and median are different from one another, imputing \nwith the median may be better because the median will represent better the \nmost typical value of the variable. However, median imputation can be more \ncomputationally expensive, especially if the number of records in the data \nis large. \nImputing with Distributions\nWhen large percentages of values are missing, the summary statistics are affected \nby mean imputation. An alternative to this is, rather than imputing with a con-\nstant value, to impute randomly from a known distribution. For the variable \nAGE, if instead of imputing with the mean (61.6), you impute using a random \nnumber generated from a normal distributi", "con-\nstant value, to impute randomly from a known distribution. For the variable \nAGE, if instead of imputing with the mean (61.6), you impute using a random \nnumber generated from a normal distribution with mean 61.6 and standard \ndeviation 16.6 (see Table 4-4). These imputed values will retain the same shape \nas the original shape of AGE, though there will be small differences because \nAGE was not shaped exactly like a normal distribution originally. If the variable \nappears to be more uniformly distributed, you can use a random draw from a \nuniform distribution rather than a normal distribution for imputation.\nUnder most circumstances, imputing missing values from a distribution \nrather than from the mean is preferred, though sometimes the simplicity of \nmean imputation is preferred.\n\n \nChapter 4 \u25a0 Data Preparation \n95\n0\n2540\n5080\n7620\n10160\n12700\n15240\n17780\n20320\n22860\n25400\n27940\n0\n[18, 21]\n[22, 25]\n[26, 28]\n[29, 32]\n[33, 36]\n[37, 39]\n[40, 43]\n[44, 47]\n[48, 50]\n[51, 54]\n[55, 58]\n", "\nChapter 4 \u25a0 Data Preparation \n95\n0\n2540\n5080\n7620\n10160\n12700\n15240\n17780\n20320\n22860\n25400\n27940\n0\n[18, 21]\n[22, 25]\n[26, 28]\n[29, 32]\n[33, 36]\n[37, 39]\n[40, 43]\n[44, 47]\n[48, 50]\n[51, 54]\n[55, 58]\n[59, 61.632]\n[62, 65]\n[66, 68]\n[69, 72]\n[73, 76]\n[77, 79]\n[80, 83]\n[84, 87]\n[88, 90]\n[91, 94]\n[95, 98]\n[18, 21]\n[22, 25]\n[26, 28]\n[29, 32]\n[33, 36]\n[37, 39]\n[40, 43]\n[44, 47]\n[48, 50]\n[51, 54]\n[55, 58]\n[59, 61.632]\n[62, 65]\n[66, 68]\n[69, 72]\n[73, 76]\n[77, 79]\n[80, 83]\n[84, 87]\n[88, 90]\n[91, 94]\n[95, 98]\n3866\n7732\n11598\n15464\n19330\n23196\n27062\n30928\n34794\n38660\n42532\nFigure 4-3:  Spikes caused by mean imputation\nTable 4-4: Comparison of Standard Deviation for Mean and Random Imputation\nAGE\nMEAN\nSTANDARD \nDEVIATION\nSTANDARD DEVIATION \nAFTER MEAN \nIMPUTATION\nSTANDARD DEVIATION \nAFTER RANDOM \nIMPUTATION\n25% missing\n61.6\n16.6\n14.4\n16.6\n40% missing\n61.6\n16.6\n12.9\n16.6\n63% missing\n61.6\n16.6\n10.2\n16.6\n\n96 \nChapter 4 \u25a0 Data Preparation\nRandom Imputation from Own Distributions\nA similar procedure is", "TATION\n25% missing\n61.6\n16.6\n14.4\n16.6\n40% missing\n61.6\n16.6\n12.9\n16.6\n63% missing\n61.6\n16.6\n10.2\n16.6\n\n96 \nChapter 4 \u25a0 Data Preparation\nRandom Imputation from Own Distributions\nA similar procedure is called \u201cHot Deck\u201d imputation, where the \u201cdeck\u201d referred \noriginally to the days when the Census Bureau had cards for individuals. If \nsomeone did not respond, one could take another card from the deck at random \nand use that information as a substitute.\nIn predictive modeling terms, this is random imputation, but instead of using \na random number generator to pick a number, a random actual value of the \nvariable of the non-missing values is selected. Predictive modeling software \nrarely provides this option, but it is easy to do. One way to do it is as follows:\n1. Copy the variable to impute (variable \u201cx\u201d) to a new column.\n2. Remove missing values and randomly scramble that column.\n3.  Replicate values of the column so that there are as many values as there \nwere records in the original d", "(variable \u201cx\u201d) to a new column.\n2. Remove missing values and randomly scramble that column.\n3.  Replicate values of the column so that there are as many values as there \nwere records in the original data.\n4. Join this column to the original data.\n5. Impute missing values of variable X with the value in the scrambled column.\nThe advantage of random imputation from the same variable is that the dis-\ntribution of imputed values matches the populated data even when the shape \nof the data is not uniform or normal. \nImputing Missing Values from a Model\nImputing missing values as a constant (the mean or median) or with a random value \nis quick, easy, and often suffi cient to solve the problem of missing data. However, \nbetter imputation can be achieved for non-MCAR missing values by other means. \nThe model approach to missing value imputation begins with changing \nthe role of the input variable with missing values to now be a target variable. \nThe inputs to the new model are other input varia", "\nThe model approach to missing value imputation begins with changing \nthe role of the input variable with missing values to now be a target variable. \nThe inputs to the new model are other input variables that may predict this \nnew target variable well. The training data should be large enough, and all \nof the inputs must be populated; listwise deletion is an appropriate way to \nremove records with any missing values. Keep in mind that even a moderately \naccurate model can still produce good imputations; the alternatives are either \nrandom or constant imputation methods. \nModelers have two problems with using models to impute missing values. \nFirst, if the software doesn\u2019t have methods to impute values from models auto-\nmatically, this kind of imputation can take considerable time and effort; one \nmust build as many models as there are variables in the data. \nSecond, even if the imputation methods are done effi ciently and effectively, \nthey have to be done again when the models are de", "rt; one \nmust build as many models as there are variables in the data. \nSecond, even if the imputation methods are done effi ciently and effectively, \nthey have to be done again when the models are deployed; any missing values \nin data to be scored must fi rst be run through a model, adding complexity to \nany deployment process.\nNevertheless, the benefi ts may outweigh the problems. It seems that the \nmodeling algorithms most often included in predictive analytics software to \n\n \nChapter 4 \u25a0 Data Preparation \n97\nautomatically impute missing values are decision trees and k-nearest neighbor, \nboth of which can work well with imputing continuous and categorical values. \nDummy Variables Indicating Missing Values\nWhen missing data is MAR or MNAR, the mere fact that the data is miss-\ning can be informative in predictive modeling. Capturing the existence of \nmissing data can be done with a dummy variable coded as 1 when the variable \nis missing and 0 when it is populated. \nImputation for Cate", "informative in predictive modeling. Capturing the existence of \nmissing data can be done with a dummy variable coded as 1 when the variable \nis missing and 0 when it is populated. \nImputation for Categorical Variables\nWhen the data is categorical, imputation is not always necessary; the missing \nvalue can be imputed with a value that represents missing so that no cell con-\ntains a null any longer. This approach works well when the missing value is \nMCAR or when \u201cmissing\u201d itself is a good predictor. \nIf the data is MNAR, meaning that other variables could predict the missing \nvalue effectively, there may be advantages to building predictive models to \npredict the variable with missing values, just as was described for continuous \nvariables. The model can be as simple as a crosstab or as complex as the most \ncomplex predictive model, but in either case, the predicted categorical value \nshould be a better guess for what the missing value would have been than a \nrandom or constant imputed ", "plex as the most \ncomplex predictive model, but in either case, the predicted categorical value \nshould be a better guess for what the missing value would have been than a \nrandom or constant imputed value.\nHow Much Missing Data Is Too Much?\nWhen is it no longer worthwhile to try to impute missing values? If 50 percent \nof the values are missing, should the variable just be removed from the analysis? \nThe question is complex and there is no complete answer. If you are trying to \nbuild a classifi cation model with the target variable populated with 50 percent \n1s and 50 percent 0s, but one of the candidate variables is 95 percent missing, \nthat variable is very unlikely to be useful, and will usually be removed from \nthe analysis. But what if the target variable is populated with 99 percent 0s and \n1 percent 1s? The proportion of populated input values is fi ve times that of the \ntarget; it may still be a useful predictor.\nSoftware Dependence and Default Imputation\nSome fi nal recommend", " 0s and \n1 percent 1s? The proportion of populated input values is fi ve times that of the \ntarget; it may still be a useful predictor.\nSoftware Dependence and Default Imputation\nSome fi nal recommendations to consider with missing values include:\n \n\u25a0Know what your software does with missing values by default. Some \nsoftware provides for no special handling of missing values. Algorithms \nwill throw errors if there are any missing values at all in candidate inputs. \nSome software will perform listwise deletion automatically if there are any \n\n98 \nChapter 4 \u25a0 Data Preparation\nmissing values, and you will only discover this by looking at diagnostics \nor log fi les created by the software. Some software is more sophisticated \nand will automatically impute missing values, sometimes with the mean, \nsometimes with 0 (not a good default choice in general), sometimes even \nwith the midpoint (also not a good default unless the data has already \nbeen normalized). \n \n\u25a0Know the options in your soft", "n, \nsometimes with 0 (not a good default choice in general), sometimes even \nwith the midpoint (also not a good default unless the data has already \nbeen normalized). \n \n\u25a0Know the options in your software for imputing missing values. Most \npredictive analytics software provides options for imputing with the \nmean or a constant or your choice. Sometimes imputing with a model is \npossible but hidden behind advanced options. \n \n\u25a0Consider including dummy variables as missing value indicators. Dummy \nvariables can provide signifi cant information for predictive models in and \nof themselves. These dummy variables don\u2019t have to take the place of \nthe original variable or impute missing values for the variable; they can \nbe additional candidate inputs. A second reason for including dummy \nvariables to indicate missing values, especially for continuous variables, is \nthat for some modeling algorithms, such as decision trees, the story asso-\nciated with a missing value dummy can add insight into", "s to indicate missing values, especially for continuous variables, is \nthat for some modeling algorithms, such as decision trees, the story asso-\nciated with a missing value dummy can add insight into the predictions; \nsometimes the dummy variable can tell you more than an imputation.\nFeature Creation\nFeatures are new variables to add to data that are created from one or more exist-\ning variables already in the data. They are sometimes called derived variables or \nderived attributes. Creating features provides more value-added to the quality of \ndata than any other step. They are also the great equalizer between algorithms: \nGood features reduce the need for specialization and extensive experimentation \nwith algorithms because they can make the patterns that relate inputs to the \ntarget variable far more transparent. \nSimple Variable Transformations\nSkewness was defi ned in Chapter 3, and examples of positive and negative skew \nwere shown in Figure 3-3. Many practitioners want to corre", "riable far more transparent. \nSimple Variable Transformations\nSkewness was defi ned in Chapter 3, and examples of positive and negative skew \nwere shown in Figure 3-3. Many practitioners want to correct for skew because \nof the assumptions many algorithms make regarding the shape of the data, \nnamely normal distributions. For example, when data has positive skew, algo-\nrithms such as linear regression, k-nearest neighbor, K-Means the positive tail of \nthe distribution will produce models with bias, meaning that the coeffi cient and \ninfl uence of variables are more sensitive to the tails of the skewed distribution \nthan one would fi nd if the data were normally distributed. Some analysts will \ndismiss the models as not being truly representative of the patterns in the data.\n\n \nChapter 4 \u25a0 Data Preparation \n99\nFor example, consider a linear regression model that computes the square \nof the errors between the data points and the trend line. Figure 4-4 shows one \nexample. The larger the v", "Data Preparation \n99\nFor example, consider a linear regression model that computes the square \nof the errors between the data points and the trend line. Figure 4-4 shows one \nexample. The larger the values, the larger the error could be. In Figure 4-4, \nthe error for the x-axis value equal to 500 is 150 units, 100x more than the \nactual values at the left end of the x-axis. But the problem is even worse than it \nappears. The Regression model computes the square of the error, so 150 units \nis 1502 = 22,500 units, thousands of times greater than the errors at the left end \nof the plot. \nTherefore, to minimize the square of the errors, the regression model must \ntry to keep the line closer to the data point at the right extreme of the plot, \ngiving this data point a disproportionate infl uence of the slope of the line. Of \nthe 28 data points in the plot, the square of the error for the data point with an \nx-axis value equal to 500 is 21 percent of the total error of all the data points. \n", "e of the slope of the line. Of \nthe 28 data points in the plot, the square of the error for the data point with an \nx-axis value equal to 500 is 21 percent of the total error of all the data points. \nThe four data points with x-axis values 250 or above contribute more than 60 \npercent of the error (for 14 percent of the data). Clustering methods such as \nK-Means and Kohonen use the Euclidean Distance, which computes the square \nof the distances between data points, and therefore tails of distributions have \nthe same disproportionate effect. Other algorithms, such as decision trees, are \nunaffected by skew.\nThe power to raise the distribution is not always known beforehand and may \nrequire some experimentation. \n\u2212100\n0\n0\n100\n200\n300\n400\n500\n600\n100\n200\n300\n400\n500\n600\nDistance/Error\nDistance/Error\nFigure 4-4:  Effect of skew on regression models\nFixing Skew\nWhen a skewed distribution needs to be corrected, the skewed variables are \ntypically transformed by a function that has a dispropo", "e/Error\nFigure 4-4:  Effect of skew on regression models\nFixing Skew\nWhen a skewed distribution needs to be corrected, the skewed variables are \ntypically transformed by a function that has a disproportionate effect on the tails \nof the distribution. Ideally, for most modeling algorithms, the desired outcome \nof skew correction is a new version of the variable that is normally distributed. \n\n100 \nChapter 4 \u25a0 Data Preparation\nHowever, even if the outcome is a variable that has a uniform distribution, as \nlong as the distribution is balanced\u2014skew equals a value close to 0\u2014the algo-\nrithms will behave in a less biased way.\nFor positive skew, the most common corrections are the log transform, the \nmultiplicative inverse, and the square root transform. Table 4-5 shows the formu-\nlas for these three transformations. Note that in all three cases, they operate by \nreducing the larger values more and reducing (or even expanding) the smaller \nvalues less, or, in the case of the inverse, actually", "hree transformations. Note that in all three cases, they operate by \nreducing the larger values more and reducing (or even expanding) the smaller \nvalues less, or, in the case of the inverse, actually increasing the smaller values. \nTable 4-5: Common Transformations to Reduce Positive Skew\nTRANSFORM NAME\nTRANSFORM EQUATION\nLog transform\nlog(x), logn(x), log10(x)\nMultiplicative inverse\n1/x\nSquare root\nsqrt(x)\nOf these, the log transform is perhaps the most often used transformation to \ncorrect for positive skew. A log transform of any positive base will pull in the \ntail of a positively skewed variable, but the natural log (base \u201ce,\u201d 2.7182818) or \nthe log base 10 are usually included in software. The log base 10 pulls the tail in \nmore than the natural log and is preferred sometimes for this reason. Another \nreason to prefer the log base 10 is that translation from the original units to log \nunits is simpler (see Table 4-6). The log base 10 increments by 1 for every order \nof magnitude", "is reason. Another \nreason to prefer the log base 10 is that translation from the original units to log \nunits is simpler (see Table 4-6). The log base 10 increments by 1 for every order \nof magnitude increase, whereas the natural log has increments of 2.3.\nTable 4-6: Log Unit Conversion\nNATURAL UNITS\nLOG10 UNITS\nLOGN UNITS\n0\nUnde\ufb01 ned\nUnde\ufb01 ned\n1\n0\n0\n10\n1\n2.3 . . .\n100\n2\n4.6 . . .\n1,000\n3\n6.9 . . .\n10,000\n4\n9.2 . . .\nNatural units\nLog10 units\nLogn units\nThe effect of the transformation can be seen in Figure 4-5. The histogram \nat the left shows original units and the one to the right shows the distribution \nafter applying the log10 transform. The skew for the histogram on the left was \n3.4, and on the right only 0.2; the log transform removed nearly all of the skew. \n\n \nChapter 4 \u25a0 Data Preparation \n101\nThe square root and multiplicative inverse on this same data help remove skew \nbut not as effectively for this data as the log transform.\n0\n0\n20\n40\n60\n80\n100\n2,500\n5,000\nCount\n7,500\n10", "reparation \n101\nThe square root and multiplicative inverse on this same data help remove skew \nbut not as effectively for this data as the log transform.\n0\n0\n20\n40\n60\n80\n100\n2,500\n5,000\nCount\n7,500\n10,000\n12,500\n0\n0.0\n0.5\n1.0\n1.5\n2.0\n1,000\n2,000\nCount\n3,000\n4,000\n5,000\n6,000\nFigure 4-5:  Positively skewed distribution normalized with log transform\nBecause the log transform is undefi ned for the values less than or equal to \nzero, you must take care that there are no 0 or negative values in the data, or \nyou introduce undefi ned values into the transformed data, which are treated \nby most software as missing. Then you would have to follow the same process \nalready discussed for imputing missing values. \nHowever, a simple fi x to the 0 values for data that is non-negative is to add 1 \nto the value that is transformed: log10(x + 1). This has two helpful effects. First, \n0s, instead of becoming undefi ned, are now transformed to the value 0, exactly \nwhere they started, so 0s stay as 0s. S", " value that is transformed: log10(x + 1). This has two helpful effects. First, \n0s, instead of becoming undefi ned, are now transformed to the value 0, exactly \nwhere they started, so 0s stay as 0s. Second, because the log transform of a \nvalue between 0 and 1 is negative, adding 1 to the original value keeps the log \ntransform positive just like the original version of the variable. For this reason, \npractitioners often make adding 1 to the original variable a common practice. \nOn rare occasions, the original variable is positively skewed but contains \nnegative values as well. In these situations, you can add the absolute value of \nthe minimum value of the variable (the magnitude of the most negative value) \nplus 1 so that the log transform is never undefi ned:\nlog10(|min(x)| + 1 + x )\nNegative skew is less common than positive skew but has the same problems \nwith bias that positive skew has. Of course with negative values, the log trans-\nform cannot be used as it was with positive sk", "gative skew is less common than positive skew but has the same problems \nwith bias that positive skew has. Of course with negative values, the log trans-\nform cannot be used as it was with positive skew. One correction used often to \ntransform negatively skewed variables is a power transform: square, cube, or \nraise the variable to a higher power.\nIf the variable has a large magnitude, raising that value to a high power will \ncreate very large transformed values and even cause numeric overfl ow. It is \ntherefore advisable to scale the variable fi rst by its magnitude before raising it \nto a high power. \nConsider the four plots in Figure 4-6. The upper-left histogram is the negatively \nskewed original variable (skew = \u20133.4). The subsequent three histograms show \nthe distributions when the power transform is applied using power 2, 20, and 80. \n\n102 \nChapter 4 \u25a0 Data Preparation\nThe last of these fi nally achieves the desired effect of creating a version of the vari-\nable with zero skew. ", "ower transform is applied using power 2, 20, and 80. \n\n102 \nChapter 4 \u25a0 Data Preparation\nThe last of these fi nally achieves the desired effect of creating a version of the vari-\nable with zero skew. The power to raise the distribution to is not always known \nbeforehand and may require some experimentation.\n0\n0\n0.0\n0.2\n0.4\nx^20, Skew = \u22121.6\nx^2, Skew = \u22123.3\nx^80, Skew = 0\nOriginal variable, x: Skew = \u22123.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n2,000\n4,000\n6,000\n0\n1,000\n2,000\n3,000\n4,000\n\u2212100\n\u221280\n\u221260\n\u221240\n\u221220\n0\n2,500\n5,000\nCount\nCount\nCount\nCount\n7,500\n10,000\n12,500\n0\n0.80\n0.85\n0.90\n0.95\n1.00\n2,500\n5,000\n7,500\n10,000\n12,500\nFigure 4-6:  Negatively skewed distribution normalized with the power transform\nAn alternative approach is to take advantage of the same log transform \nalready described. However, because the log transform is undefi ned for nega-\ntive values, you must fi rst ensure the values are positive before applying the \nlog. Moreover, the log transform pulls in the tail of a positi", "wever, because the log transform is undefi ned for nega-\ntive values, you must fi rst ensure the values are positive before applying the \nlog. Moreover, the log transform pulls in the tail of a positively skewed dis-\ntribution. Therefore, you must fi rst fl ip the distribution before applying the \nlog transform and then restore the distribution to its original negative (now \ntransformed) values. The equation for transforming either a positively or \nnegatively skewed variable is as follows:\nTransformed_x = sgn(x) \u00d7 log10(1 + abs(x))\nInside the log transform, an absolute value is applied fi rst to make the values \npositive. One (1) is then added to shift the distribution away from zero. After \napplying the log transform, the sgn(x) function applies the original sign of the \nvariable back to the distribution. If the original values were negative, this restores \nthe negative sign. Note that if the original values are all positive, like the ones \nshown in Figure 4-5, the absolute value and ", "the distribution. If the original values were negative, this restores \nthe negative sign. Note that if the original values are all positive, like the ones \nshown in Figure 4-5, the absolute value and sgn function have no effect, so this \nformula can be used for either positively or negatively skewed data. \nIn fact, this transformation can be used in yet another distribution, one with \nlarge tails in both the positive and negative directions but peaks at 0. This occurs \nfor monetary fi elds such as profi t/loss variables. \nIn summary, Table 4-7 lists transformations that are effective in convert-\ning positively or negatively skewed distributions to distributions that are at \n\n \nChapter 4 \u25a0 Data Preparation \n103\nleast more balanced and often are more normally distributed. The particular \ntransformation that is used by a predictive modeler will vary depending on \npersonal preferences and the resulting distribution found after applying the \ntransformation.\nTable 4-7: Table of Corrective Ac", "nsformation that is used by a predictive modeler will vary depending on \npersonal preferences and the resulting distribution found after applying the \ntransformation.\nTable 4-7: Table of Corrective Actions for Positive or Negative Skew\nPROBLEM\nTRANSFORM\nPositive skew \nlog10(1 + x), 1/x, sqrt(x)\nNegative skew\nxn, \u2013log10(1 + abs(x))\nBig tails, both directions\nsgn(x) x log10(1 + abs(x))\nBinning Continuous Variables\nCorrecting skewed distributions is a straightforward process, but other more \ncomplex distributions require more interaction by the modeler. If the variable \nhas long tails and outliers, is multi-modal, has spikes, or has missing values, \na binned version of the variable may be a better representation for modeling \nrather than trying to convert the odd distribution to one that appears normally \nor uniformly distributed. \nFor example, consider the distribution in Figure 4-7. Note there are two main \npeaks to the distribution, and at the far right, there is also a small bin repre", "ars normally \nor uniformly distributed. \nFor example, consider the distribution in Figure 4-7. Note there are two main \npeaks to the distribution, and at the far right, there is also a small bin represent-\ning missing values. This population is diffi cult to represent as a continuous \nvariable for algorithms that assume normal distributions. A different represen-\ntation that may help modeling is one that captures the two groups of values, \nas shown in Figure 4-8.\n0\n[0, 4]\n(8\u201312]\n(12\u201316]\n(28\u201332]\n(32\u201336]\n(48\u201352]\n(52\u201356]\n(56\u201360]\nMissing\nvalues\n3\n6\n9\n12\n15\n18\n21\n24\n27\n30\n34\nFigure 4-7:  Multi-modal distribution\n\n104 \nChapter 4 \u25a0 Data Preparation\n0\n[0, 30]\n(30\u201360]\nMissing values\n4\n8\n12\n16\n20\n24\n28\n32\n36\n40\n48\n44\n53\nFigure 4-8:  Binned version of multi-modal variable\nNumeric Variable Scaling\nJust as positive and negative skew can bias models toward the tails of the dis-\ntribution, the magnitudes of variables can also bias the models created by \nsome algorithms, especially those that are buil", "t as positive and negative skew can bias models toward the tails of the dis-\ntribution, the magnitudes of variables can also bias the models created by \nsome algorithms, especially those that are built based on Euclidean Distances \nsuch as K-Means clustering and k-nearest neighbor. Magnitudes affect these \nkinds of algorithms for the same reason pictured in Figure 4-1: Larger mag-\nnitudes produce larger distances. Scaling variables so that all have the same \nmagnitudes removes this bias. \nLinear regression coeffi cients are infl uenced disproportionately by the large \nvalues of a skewed distribution, but not by the magnitudes of normally dis-\ntributed variables; the coeffi cients of a regression model scale the magnitudes \nof inputs appropriately. Most implementations of neural networks also scale the \ninputs before building the models. Scaling may need to be done even after trans-\nforming variables through the use of log transforms. Some software packages \nrefer to this kind of scalin", "so scale the \ninputs before building the models. Scaling may need to be done even after trans-\nforming variables through the use of log transforms. Some software packages \nrefer to this kind of scaling as normalization.\nA list of commonly used normalization methods appears in Table 4-8, where \nthe most common of these are min-max normalization and z-scores. The mag-\nnitude scaling method, just scaling the variable by its maximum magnitude, \nwill create a maximum value of -1 or +1 depending on whether the maximum \nmagnitude is negative or positive but is not guaranteed to fi ll the entire range \nfrom -1 to +1. The sigmoid, a transformation also used in neural networks and \nlogistic regression, sometimes requires some experimentation to fi nd a value \nfor the scaling factor, c, that makes the sigmoid shape good for the particular \nvariable.\n\n \nChapter 4 \u25a0 Data Preparation \n105\nTable 4-8: Commonly Used Scaling and Normalization Methods\nSCALING METHOD\nFORMULA\nRANGE\nMagnitude scaling\nx, = x", "e sigmoid shape good for the particular \nvariable.\n\n \nChapter 4 \u25a0 Data Preparation \n105\nTable 4-8: Commonly Used Scaling and Normalization Methods\nSCALING METHOD\nFORMULA\nRANGE\nMagnitude scaling\nx, = x / max|x|\n[-1,1]\nSigmoid\nx, = 1 / (1 + e(-x/c))\n[0,1]\nMin-max normalization\nx, = (x \u2013 xmin)/(xmax \u2013 xmin)\n[0,1]\nZ-score\nx, = (x \u2013 xmean)/xstd\nmostly [\u20133,3]\nRank binning\n100*rank order / # records\n[0,100]\nMin-max normalization merely changes the range of a variable to the range \nfrom 0 to 1, compressing variable magnitudes if the maximum is greater than 1. \nIf the maximum of a variable is less than 1, min-max scaling actually expands \nthe range of the variable. The min-max values therefore represent the percent-\nage of the value of the variable relative to its maximum. \nZ-scores are most appropriate when the data is normally distributed because \nthe mean and standard deviation used in the scaling assume normal distribu-\ntions. The interpretation of z-scores is straightforward: Each unit ref", "riate when the data is normally distributed because \nthe mean and standard deviation used in the scaling assume normal distribu-\ntions. The interpretation of z-scores is straightforward: Each unit refers to one \nunit of standard deviation from the mean. Consider Table 4-9, showing 10 values \nof AGE with the corresponding min-max and z-score normalized values. The \nAGE value equal to 81 years old is 78 percent of the distance from the minimum \nto the maximum, but represents just over one standard deviation from the mean, \nwhere standard deviations assume AGE follows a normal distribution. The \nmean appears to be near 62 years old as this value is nearly 0 in z-scored units. \nTable 4-9: Sample Z-Scored Data\nAGE\nAGE \uf6aeMIN\uf6baMAX\uf6af\nAGE \uf6aeZ\uf6baSCORE\uf6af\n81\n0.7821\n1.1852\n62\n0.5385\n\u20130.0208\n79\n0.7564\n1.0583\n66\n0.5897\n0.2331\n69\n0.6282\n0.4235\n73\n0.6795\n0.6774\n28\n0.1026\n\u20132.1790\n50\n0.3846\n\u20130.7825\n89\n0.8846\n1.6930\n58\n0.4872\n\u20130.2747\n\n106 \nChapter 4 \u25a0 Data Preparation\nNormalizing data can aid considerably in dat", "6\n0.5897\n0.2331\n69\n0.6282\n0.4235\n73\n0.6795\n0.6774\n28\n0.1026\n\u20132.1790\n50\n0.3846\n\u20130.7825\n89\n0.8846\n1.6930\n58\n0.4872\n\u20130.2747\n\n106 \nChapter 4 \u25a0 Data Preparation\nNormalizing data can aid considerably in data visualization as well. In Figure \n4-9, the variables AGE and MAXRAMNT are plotted. MAXRAMNT has a con-\nsiderable outlier, a value equal to $1,000. The scale on the x-axis is linear, as is \ntypical for scatterplots, and therefore the plot contains primarily white space, \nwhich provides very little visual information to the analyst. \nNormalization of the data can help utilize more the space on the printed page \navailable to view the data. If MAXRAMNT is log transformed to compress the \neffect of outliers and skew, and then is scaled to the range [0,1], and if the vari-\nable AGE is also scaled to the range [0,1], the plot in Figure 4-10 results. Now \nyou can see several patterns that were compressed into the very left side of the \nnatural units plot. First, there are stripes in the data ind", "to the range [0,1], the plot in Figure 4-10 results. Now \nyou can see several patterns that were compressed into the very left side of the \nnatural units plot. First, there are stripes in the data indicating typical gift sizes \nin the MAXRAMNT variable (people tend to give in $5 or $10 increments). Also, \nthe larger gifts tend to be given by older donors as evidenced by the fact that \nthe lower-right region of the plot has few data points.\nWhile structural information is gained about individual variables and their \ninteractions, scaling and transforming data obscures the original variable values, \nforcing the analyst to keep the translations in his or her head or superimpose \nthe original values of the variables on the plots. Some software provides the \ncapability to redraw scatterplots with different scales, such as a log scale, but for \nother scaling methods, the analyst is on his or her own. In Figure 4-10, you know \nthat the maximum age is 98 years old, and therefore the normalized", "th different scales, such as a log scale, but for \nother scaling methods, the analyst is on his or her own. In Figure 4-10, you know \nthat the maximum age is 98 years old, and therefore the normalized age value \n1.0 on the y-axis represents 98 years old. However, the log transform makes the \nother values more diffi cult to estimate. What age does the value 0.3 represent? \nIt turns out to be about 45 years old, but this isn\u2019t obvious.\n20\n5\n52\n99\n146\n193\n240\n287\n334\n381\n428\n475\n522\n569\n616\n663\n710\n757\n804\n851\n898\n945\n1000\n27\n34\n41\n48\n55\n62\n69\n76\n83\n90\n98\nFigure 4-9:  AGE versus MAXRAMNT in natural units\n\n \nChapter 4 \u25a0 Data Preparation \n107\n0\n0\n0.050\n0.100\n0.150\n0.200\n0.250\n0.300\n0.350\n0.399\n0.449\n0.499\n0.549\n0.600\n0.650\n0.700\n0.750\n0.800\n0.850\n0.900\n0.950\n1.000\n0.100\n0.200\n0.300\n0.400\n0.500\n0.600\n0.700\n0.799\n0.899\n1.000\nFigure 4-10:  AGE versus MAXRAMNT in scaled and normalized units\nNominal Variable Transformation\nNominal variables present problems for numeric algorithms because they us", "0.600\n0.700\n0.799\n0.899\n1.000\nFigure 4-10:  AGE versus MAXRAMNT in scaled and normalized units\nNominal Variable Transformation\nNominal variables present problems for numeric algorithms because they usu-\nally are not in numeric form, and even if they are numeric, they aren\u2019t properly \ninterpreted as numbers in the way modeling algorithms need. The most common \napproach to using nominal variables is to explode them into dummy variables. \nDummies are dichotomous variables (two values), usually coded with 0s and 1s, \nwhere the value 1 corresponds to \u201ctrue\u201d and 0 to \u201cfalse.\u201d Recoding of categorical \nvariables is accomplished by exploding a single column of categorical values \nhaving N values to N columns of dummy variables.\nFor example, in the simple example of Table 4-10, four state values are con-\nverted to four columns of dummy variables. With this representation, each \nrecord contains one and only one column of the individual states whose value \nequals 1. The actual numbers representing", " con-\nverted to four columns of dummy variables. With this representation, each \nrecord contains one and only one column of the individual states whose value \nequals 1. The actual numbers representing the dummy variables do not matter, \nthough the 1/0 representation has advantages, including the ability to count for \neach state how many records hit.\nTable 4-10: Exploding Categorical Variables to Dummy Variables\nSTATE\nCA\nTX\nFL\nNY\nCA\n1\n0\n0\n0\nTX\n0\n1\n0\n0\nFL\n0\n0\n1\n0\nNY\n0\n0\n0\n1\n\n108 \nChapter 4 \u25a0 Data Preparation\nYou must also beware of high cardinality\u2014that is, when there are many \nvalues in the original categorical variable, especially if it results in one or more \nof the columns of dummy variables having very few 1s. These columns could \nbe removed during a variable selection stage, however. Or, as an alternative, \nall columns of the same exploded categorical variable that have small counts \n(maybe less than 50) could be combined into a single bin. A simple \u201cor\u201d opera-\ntion for these varia", "r, as an alternative, \nall columns of the same exploded categorical variable that have small counts \n(maybe less than 50) could be combined into a single bin. A simple \u201cor\u201d opera-\ntion for these variables will result in the new binned column having value 1 \nwhen any of the columns have value 1, or 0 otherwise. \nOrdinal Variable Transformations\nOrdinal variables are problematic for predictive modeling and most algorithms \ndo not have specifi c ways to incorporate ordinal variables in the algorithms. \nAlgorithms that can use the information contained in an ordinal variable are \noften either not included in predictive modeling software or are not implemented \nin a way that takes advantage of ordinal variables. \nThe modeler must usually make a choice: Should the ordinal variable be \ntreated as continuous or categorical? If it is treated as continuous, the algorithm \nwill assume the variable is an interval or ratio variable and may be misled or \nconfused by the scale. For example, if the or", " continuous or categorical? If it is treated as continuous, the algorithm \nwill assume the variable is an interval or ratio variable and may be misled or \nconfused by the scale. For example, if the ordinal variable is education level and \nis coded as 1 for high school graduate, 2 for some college, 3 for associate\u2019s degree, \n4 for bachelor\u2019s degree, and so forth, a bachelor\u2019s degree will be considered \ntwice as big as some college, and four times as big as a high school graduate. \nOn the other hand, if the variable is considered to be categorical, a model could \ncombine high school, bachelor\u2019s degree, and PhD in one group, a grouping that \ndoesn\u2019t make intuitive sense. \nNevertheless, most often, ordinal variables are treated as categorical, but care \nshould be taken to ensure that non-intuitive grouping or splitting of categories \ndoesn\u2019t take place. If it is occurring in the model, consider making the ordinal \nvariable continuous.\nOne additional consideration for ordinal variables is h", "tive grouping or splitting of categories \ndoesn\u2019t take place. If it is occurring in the model, consider making the ordinal \nvariable continuous.\nOne additional consideration for ordinal variables is how to code them when \nthey are used in numeric modeling algorithms. The typical treatment of cat-\negorical variables is to explode them into their corresponding dummy variables, \na form that results in one and only one column having the value 1, and all oth-\ners having the value 0. \nOrdinal variables are different than nominal, however. For variables such \nas education, a thermometer scale may be more appropriate. Table 4-11 shows \nan example of the thermometer scale for education level (not all possible levels \nare shown to make the table easier to see). With this scale, modeling algorithms \nwill be able to incorporate education levels already achieved in the models. For \nexample, individuals with a master\u2019s degree or PhD also (presumably) have a \nbachelor\u2019s degree. If bachelor\u2019s degrees ", "will be able to incorporate education levels already achieved in the models. For \nexample, individuals with a master\u2019s degree or PhD also (presumably) have a \nbachelor\u2019s degree. If bachelor\u2019s degrees are predictive, including the individuals \n\n \nChapter 4 \u25a0 Data Preparation \n109\nwith master\u2019s degrees or PhDs will strengthen the pattern associated with \nbachelor\u2019s degrees.\nIf the typical explosion of a nominal variable were to be used to measure the \neffect of a bachelor\u2019s degree on the target variable, the model would be trying to \nfi nd individuals with bachelor\u2019s degrees who did not go on to higher degrees. \nEither one could be useful and predictive, but you don\u2019t know which is more \npredictive unless they are both constructed and tried in models.\nTable 4-11: Thermometer Scale\nEDUCATION \nLEVEL\nHIGH \nSCHOOL\nSOME \nCOLLEGE\nASSOCIATE\u2019S \nDEGREE\nBACHELOR\u2019S \nDEGREE\nMASTER\u2019S \nDEGREE\nPHD\nHigh school \ngraduate\n1\n0\n0\n0\n0\n0\nSome college\n1\n1\n0\n0\n0\n0\nAssociate\u2019s \ndegree\n1\n1\n1\n0\n0\n0\nBachelor\u2019s \ndeg", "\nLEVEL\nHIGH \nSCHOOL\nSOME \nCOLLEGE\nASSOCIATE\u2019S \nDEGREE\nBACHELOR\u2019S \nDEGREE\nMASTER\u2019S \nDEGREE\nPHD\nHigh school \ngraduate\n1\n0\n0\n0\n0\n0\nSome college\n1\n1\n0\n0\n0\n0\nAssociate\u2019s \ndegree\n1\n1\n1\n0\n0\n0\nBachelor\u2019s \ndegree\n1\n1\n1\n1\n0\n0\nMaster\u2019s \ndegree\n1\n1\n1\n1\n1\n0\nPhD\n1\n1\n1\n1\n1\n1\nDate and Time Variable Features\nDate and time variables are notoriously diffi cult to work with in predictive \nmodeling and nearly always require some feature creation to be used effectively. \nDate features will be described in this section, but the same principles apply \nto time features as well. \nThe fi rst problem with date variables is their representation. Dates can be coded \nin a myriad of ways: March 1, 2013, 1-March-2013, 1-Mar-99, 3/1/99, 3/1/2013, \n03/01/2013, 03012013, and many more. Note that all of these formats are absolute \ndates. Assume, for example, the date refers to the date an individual visited a \nwebsite. If the visit date is used in a predictive model to predict the likelihood for \nthe visitor to purchase a", "lute \ndates. Assume, for example, the date refers to the date an individual visited a \nwebsite. If the visit date is used in a predictive model to predict the likelihood for \nthe visitor to purchase a product, the interpretation may be, \u201cIf a visitor visited \nthe website on March 1, 2013, then the visitor is likely to purchase product X.\u201d \nIf the model is used week after week, the pattern in the model is still literally \nchecking for a visit on March 1, 2013, a pattern that is unlikely to be the intent \nof the visitor. \nA more desirable interpretation is this: \u201cIf a visitor visited the website \nwithin the last week, then the visitor is likely to purchase product X.\u201d In this \ncase, the feature of interest is a relative date, computed between the date in the \n\n110 \nChapter 4 \u25a0 Data Preparation\ndata and \u201ctoday.\u201d Therefore, features such as \u201cdays since,\u201d \u201cdays between,\u201d \u201cdays \nuntil,\u201d and so forth need to be computed from the fi xed dates in the data. In \naddition, because the date feature", "on\ndata and \u201ctoday.\u201d Therefore, features such as \u201cdays since,\u201d \u201cdays between,\u201d \u201cdays \nuntil,\u201d and so forth need to be computed from the fi xed dates in the data. In \naddition, because the date features are relative dates, the \u201ctoday\u201d date is critically \nimportant to get right, and may itself be relative to other dates in the data. The \nsame principles can apply to calculations related to months, quarters, and years.\nHowever, fi xed dates are sometimes more valuable than relative dates. Was the \nvisit on a holiday? Which one? Was the visit on Black Friday? These representa-\ntions of the dates are domain-specifi c and require signifi cant domain expertise \nto create in a way that captures visitor intent. One additional problem that \noccurs with dates and times but not with other numeric variables is their cyclical \nnature. An integer representation of the month of the year, a number between 1 \nand 12 (inclusive), can be an effective representation of month. However, every \nyear, the roll", "ables is their cyclical \nnature. An integer representation of the month of the year, a number between 1 \nand 12 (inclusive), can be an effective representation of month. However, every \nyear, the rollover from 12 (December) to 1 (January) appears to be a huge leap \nnumerically\u2014a difference of 11 months\u2014even though it is only a one-month \ndifference on the calendar. Computing relative months from a fi xed month can \nrelieve this problem as well. \nZIP Code Features\nZIP codes, which represent a geographical region, can be good predictors of \nbehavior. They can be represented as fi ve-digit numbers, though they should \nalways be stored as text so the predictive analytics software does not remove \nthe leading zeros for some ZIP codes. There are approximately 43,000 ZIP codes \nin the United States, far too many for typical predictive modeling algorithms to \nhandle effectively. Some software reduces the cardinality of ZIP codes by using \nonly the fi rst two or three digits to represent the re", "es, far too many for typical predictive modeling algorithms to \nhandle effectively. Some software reduces the cardinality of ZIP codes by using \nonly the fi rst two or three digits to represent the region.\nOther variables have the same characteristics as ZIP codes in that they may \nhave leading zeros, such as IDs (customer ID, address ID) and codes (title codes, \ndemographic codes, vendor codes). These should always be stored as text. \nHowever, it is rarely the actual ZIP code itself that is the causal reason for \nthe behavior being measured, but rather the characteristics about the peo-\nple who live in the ZIP code or the businesses that operate in that ZIP code. \nAn alternative, then, is to summarize characteristics of people or businesses in \nthe ZIP code as the feature to use as a candidate input in predictive models. This \nis exactly what demographic variable appends do: They summarize age, income, \neducation, race, and other characteristics of a ZIP code in a column of the data.\n", "idate input in predictive models. This \nis exactly what demographic variable appends do: They summarize age, income, \neducation, race, and other characteristics of a ZIP code in a column of the data.\nWhich Version of a Variable Is Best?\nComputing transformed variables is easy: the original version, a log trans-\nformed version, one or more binned versions, a z-scored version, and more. But \nwhich of these should be used as candidates for the actual modeling? After all, \n\n \nChapter 4 \u25a0 Data Preparation \n111\nthe original version of a variable and its related log10 transform will be very \nhighly correlated: Their rank-ordering is identical. \nTable 4-12 shows a list of histograms containing real-world data, along with \nthe rationale for selecting one of the normalization or scaling methods described \nin this section.\nOne solution to the problem of selecting which feature to keep is to use an \nempirical approach to select the best version of a variable for predictive mod-\neling. If you use e", "ribed \nin this section.\nOne solution to the problem of selecting which feature to keep is to use an \nempirical approach to select the best version of a variable for predictive mod-\neling. If you use each version as a candidate input variable by itself to predict \nthe target variable, you can score each version of the variable and pick the one \nthat is the best predictor. This process can be extremely labor intensive if this \nvariable feature search is not automated in the software, or is able to be scripted \nby the modeler. \nTable 4-12: Distributions and Possible Corrective Action\nDISTRIBUTION\nPOSSIBLE CORRECTIVE ACTIONS\nMaybe none; close enough to uniform.\nPrimarily positive skew; consider log10 transform.\nConsider binning to capture four or \ufb01 ve regions centered \non spikes.\nConsider binning into two bins (for peaks) or four bins (two \npeak and two trough regions).\nNegative skew; consider power transform or \ufb02 ip transform \nand use log10.\nContinues\n\n112 \nChapter 4 \u25a0 Data Preparation\nCo", "ing into two bins (for peaks) or four bins (two \npeak and two trough regions).\nNegative skew; consider power transform or \ufb02 ip transform \nand use log10.\nContinues\n\n112 \nChapter 4 \u25a0 Data Preparation\nConsider dummy variables for spikes on left and right.\nPositive skew; consider a log transform.\nSpike in distribution, consider dummy indicator of the spike \nvs. non-spike. If spike generated through mean imputation, \nconsider imputing with a distribution.\nClassic positive skew. Log10 transform.\nMultidimensional Features\nCreating features from several variables usually provides more improvement \nin accuracy than any single-variable transformation because of the information \ngain associated with these interactions. However, it is often very time-consuming \nto hypothesize good multidimensional features and then to build them. Several \napproaches to building multidimensional features are described in this section, \nincluding using domain experts, Principal Component Analysis (PCA), clustering, ", "tures and then to build them. Several \napproaches to building multidimensional features are described in this section, \nincluding using domain experts, Principal Component Analysis (PCA), clustering, \nand supervised learning algorithms. The description of how the algorithms are \nused to create features is included here; a description of algorithms themselves \nwill be described in Chapters 6 and 8.\nDomain Experts\nSo far, the features described have all entailed converting a single variable from \none form to a different, more convenient or predictive form. The most power-\nful features, however, are usually multidimensional features. Two common \nexamples of multidimensional features are interactions (created by multiplying \ntwo variables together) and ratios (created by dividing one variable by another). \nAdditive variables (created by adding or subtracting two variables) are not as \nTable 4-12 (continued)\n\n \nChapter 4 \u25a0 Data Preparation \n113\nuseful because most algorithms can compute the", "by another). \nAdditive variables (created by adding or subtracting two variables) are not as \nTable 4-12 (continued)\n\n \nChapter 4 \u25a0 Data Preparation \n113\nuseful because most algorithms can compute these features as a part of their \nalgorithms.\nRatios are particularly good features to create for several reasons. First, they \ncan provide a normalized version of a variable that interprets the value of a vari-\nable within a record. A simple normalized version of a variable is a percentage; \na feature could compute the percentage of visits by a visitor to a website that \nresulted in a purchase. The denominator, the total number of visits, varies from \nvisitor to visitor, and thus, two purchases can take on a very different meaning \ndepending on how many visits the visitor had. Ratios can also incorporate more \ncomplex ideas. The price-to-earnings ratio is a powerful representation of the \npotential earnings growth: Neither price nor earnings on their own provide \nthis information. \nRatios a", "ncorporate more \ncomplex ideas. The price-to-earnings ratio is a powerful representation of the \npotential earnings growth: Neither price nor earnings on their own provide \nthis information. \nRatios are important as features because they are diffi cult for predictive \nmodeling algorithms to uncover: Division is a diffi cult operator to estimate well. \nCreating ratios helps the algorithms fi nd patterns more clearly and simply than \nif they had to approximate the ratio through a series of multiplies and adds. \nWhere do these powerful ratios come from? Nearly always, ratio features come \nfrom domain experts who understand which ratios are likely to be predictive, \nthough sometimes modelers can use their own commonsense to hypothesize \nnew ratio features that are worth examining. During the Business Understanding \nstage of the modeling project, interviews with domain experts can uncover \ninteresting interaction and ratio features. \nModelers should be prepared to have to convert relatively", "usiness Understanding \nstage of the modeling project, interviews with domain experts can uncover \ninteresting interaction and ratio features. \nModelers should be prepared to have to convert relatively vague descrip-\ntions of features into numeric form. The domain expert might say, \u201cWhen the \nprice is relatively high compared to earnings, the stock has good potential.\u201d \nWhenever the words \u201ccompared to\u201d occur, the modeler can infer that a ratio \nis being described, price-to-earnings ratio in this case. Whenever the domain \nexpert describes two variables with an \u201cand\u201d between them, multiplication can \nbe inferred: for example, \u201cIf donors give frequently and in large amounts per \ngift, they are excellent donors.\u201d The feature in this case is AVGGIFT \u00d7 NGIFTALL \nor AVGGIFT \u00d7 RFA_2F. \nPrincipal Component Analysis Features\nPrincipal Component Analysis (PCA) fi nds linear projections of numeric data \nthat maximize the spread or variables of the projections. Qualitatively speaking, \nPCA tries to", "omponent Analysis Features\nPrincipal Component Analysis (PCA) fi nds linear projections of numeric data \nthat maximize the spread or variables of the projections. Qualitatively speaking, \nPCA tries to fi nd correlated variables, variables that have signifi cant overlap \nwith each other, and describe them as Principle Components (PCs). These PCs \ntherefore can be understood as representing a broader idea in the data that sev-\neral of the variables are representing. PCA models can identify how many real \nideas are represented in the data rather than the modeler relying on the variables \nto indicate how many ideas there are in the data. A PCA model may fi nd that \n\n114 \nChapter 4 \u25a0 Data Preparation\n5 PCs describe most of the data even though there are actually 20 variables in the \ndata. This could happen if the 20 variables are highly correlated with each other. \nAfter building a PCA model, rather than using all 20 variables as inputs to \na predictive model, you could use the 5 PCs instea", "could happen if the 20 variables are highly correlated with each other. \nAfter building a PCA model, rather than using all 20 variables as inputs to \na predictive model, you could use the 5 PCs instead and know that most of \nthe information in the 20 variables is still present in the 5 PCs, but has been \ncompressed into a more parsimonious form. The PCs therefore can be viewed \nas features of the original data, providing a different representation of the data \nfor the models to use. \nPCA can be very useful as a feature extraction algorithm, but also has draw-\nbacks. First, it is only a linear projection of the data. If patterns in the data are \nnonlinear, the PCA algorithm could even destroy patterns that exist in the data, \nproducing worse models than the original data would itself. \nClustering Features\nClustering can also be used to create one or more features of the original data. \nThe algorithms for building clustering models are described in Chapter 6, but \nbriefl y, these algorit", "tering Features\nClustering can also be used to create one or more features of the original data. \nThe algorithms for building clustering models are described in Chapter 6, but \nbriefl y, these algorithms fi nd records that fall into groups from the variable \nvalues and assign a label for each record indicating which group the record \nhas been assigned to. Therefore, the clustering algorithm has reduced the data \nfrom the number of variables included in the clustering model down to one \ndimension, the cluster label.\nThe most straightforward feature to create from a clustering model is the \ncluster label itself. This label can be converted into dummy variables if neces-\nsary. A second set of features you can create from the clustering model is a \nset of distances from each cluster. For each record, you can create a distance \nbetween the record and each of the cluster centers in the model. If there are \n10 clusters, there will be 10 distances to compute. These distances can then be \nused ", "record, you can create a distance \nbetween the record and each of the cluster centers in the model. If there are \n10 clusters, there will be 10 distances to compute. These distances can then be \nused as new features in predictive models. In fact, this is exactly what some \npredictive modeling algorithms do, like Radial Basis Function Networks.\nOther Modeling Algorithms\nNeural networks, decision trees, and other predictive modeling algorithms can \nbe used to create features as well. Decision trees are perhaps the most commonly \nused algorithm for this purpose because they produce rules that are easy to \nunderstand and easy to implement. Neural networks can be used, especially \nif the modeler has a way to expose the outputs of hidden layer neurons in the \nnetwork; these neurons are essentially feature extractors. \nInsights into how to use algorithms for feature creation is deferred to Chapter \n8 where the algorithms are described.\n\n \nChapter 4 \u25a0 Data Preparation \n115\nTying Records Togeth", "ly feature extractors. \nInsights into how to use algorithms for feature creation is deferred to Chapter \n8 where the algorithms are described.\n\n \nChapter 4 \u25a0 Data Preparation \n115\nTying Records Together\nPredictive modeling algorithms assume each record in the data is an independent \nobservation, unrelated to the other records. Because the algorithms assume this \nindependence, they don\u2019t try to tie together records into higher-level features \nbased on multiple records in the data. \nIn many cases, this is a good assumption. If the data represents customers \nof a retail chain of stores and their historic purchasing behavior, and if there is \none and only one record per customer, the independence of records is a good \nassumption; in general, customers are not colluding with each other each time \nthey visit the store.\nBut what if rather than having one record per customer, the modeling data \ninstead represents each visit to any store of that retailer. Now the modeling \ndata may contain mult", "e \nthey visit the store.\nBut what if rather than having one record per customer, the modeling data \ninstead represents each visit to any store of that retailer. Now the modeling \ndata may contain multiple entries for each customer, and the records are no longer \ncompletely independent. The records are not independent, but the algorithms \ndon\u2019t know this and have no mechanism to tie records together and uncover \npatterns that are only revealed when one examines the sequence of visits. \nThis kind of modeling data set occurs frequently, identifi able when a key \nrecord label, such as Customer ID, Business ID, or transaction code, is not unique \nin the modeling data. Frequently in these cases, there is also a date or time fi eld, \nand the unique record ID is the customer ID and the date fi eld. \nThe modeler, in these cases, should create the temporal relationship between \nrecords and code it explicitly in the data. For example, a government agency \nwanted to identify invoices sent to the f", ". \nThe modeler, in these cases, should create the temporal relationship between \nrecords and code it explicitly in the data. For example, a government agency \nwanted to identify invoices sent to the federal government that might be improper \nor even fraudulent. Each record was an invoice, and the intent was to identify \nsuspicious invoices and then connect them to the payee, and then investigate \nthe payee. Each payee could be in the modeling data many times, depending \non how many invoices he or she sent to the federal government. \nOne of the features that a domain expert used himself in identifying improper \ninvoices was this: Label invoices that were mailed to the same address, and fl ag \nthose that had different payees on the invoices. The feature name became \u201cmul-\ntiple_payees_to_same_address.\u201d For example, if there were fi ve invoices paid \nto a particular address, and three of the fi ve payees were John Smith but two \nof the fi ve payees were John Smithers, the multiple_payees_t", "_address.\u201d For example, if there were fi ve invoices paid \nto a particular address, and three of the fi ve payees were John Smith but two \nof the fi ve payees were John Smithers, the multiple_payees_to_same_address \nvariable would be populated with a \u201c1\u201d indicating there was more than one \npayee for that address. This feature was a record-based feature, tying together \ninformation about several records, not just tying together values from multiple \nfi elds in the same records.\nHow does one create such a feature? First, the entire database of invoices had \nto be scanned, and all addresses that were identical were fl agged. Then within \neach group of invoices that went to the same address, a second scan of the data \n\n116 \nChapter 4 \u25a0 Data Preparation\ndetermined through a fuzzy match how many payees there were on invoices \ngoing to the address. If there was more than one payee going to the address, \nthe invoice was fl agged as having this multi-record characteristic. \nOne additional consi", " payees there were on invoices \ngoing to the address. If there was more than one payee going to the address, \nthe invoice was fl agged as having this multi-record characteristic. \nOne additional consideration when creating record-based features on tempo-\nral data is to ensure that in each record, no information for records with dates \nfuture to that record are included.  \nTime Series Features\nPredictive modeling data is inherently two-dimensional, formatted into rows \nand columns because modeling algorithms work in two dimensions. Introducing \na time dimension into modeling data increases the layout of data from two to \nthree dimensions, requiring transformation of data into two-dimensional layouts. \nFor example, the KDD Cup 1998 donor data consists of rows of donors and \ncolumns of attributes. However, these attributes were not always organized \nin neat columns. Consider donor history tables. Each record is a transactional \nsummary of a donor giving a gift. The unique identifi er of t", "utes. However, these attributes were not always organized \nin neat columns. Consider donor history tables. Each record is a transactional \nsummary of a donor giving a gift. The unique identifi er of these records is \ndonor-date pairs, as shown in Figure 4-11, where CID is the donor ID, the Date \nis the date of the gift, and Amount is the amount of the gift. \n13013413 $ \n581\n$ \n980\n$ \n383\n$ \n189\n$ \n193\n$ \n163\n$ \n72.63\n$ \n98.00\n$ \n95.75\n23191348\n31348313\nCID\nAmount_Sum Amount_Mean Amount_Max\n13013413\n13013413\n13013413\n13013413\n13013413\n13013413\n13013413\n13013413\n23191348\n23191348\n23191348\n23191348\n23191348\n23191348\n23191348\n23191348\n23191348\n23191348\n31348313\n31348313\n31348313\n31348313\n2/13/2005\n5/9/2005\n1/16/2006\n3/16/2006\n3/28/2007\n6/6/2007\n7/12/2007\n9/19/2007\n8/25/2005\n11/2/2005\n3/14/2006\n5/25/2006\n11/16/2006\n12/16/2006\n1/6/2007\n4/11/2007\n6/5/2007\n6/8/2007\n1/31/2005\n2/16/2006\n5/16/2006\n10/21/2006\n80\nCID\nDate\nAmount\n59\n22\n59\n49\n189\n36\n87\n23\n123\n38\n26\n61\n193\n88\n157\n181\n90\n61\n110\n19\n163\n", "\n5/25/2006\n11/16/2006\n12/16/2006\n1/6/2007\n4/11/2007\n6/5/2007\n6/8/2007\n1/31/2005\n2/16/2006\n5/16/2006\n10/21/2006\n80\nCID\nDate\nAmount\n59\n22\n59\n49\n189\n36\n87\n23\n123\n38\n26\n61\n193\n88\n157\n181\n90\n61\n110\n19\n163\nFigure 4-11:  Creating features from time series data\n\n \nChapter 4 \u25a0 Data Preparation \n117\nIn order to use this data in predictive modeling with each record representing \na donor, the data must fi rst be rolled up to the donor level (a groupby opera-\ntion). Capturing any date or gift amount information can only be accomplished \nthrough feature creation. Three simple features are shown in Figure 4-11, but \nmany more are usually created. \nSome commonly computed features for donations are already in the KDD \nCup 1998 data, including the mean donation amount (AVGGIFT), the most recent \ndonation amount (LASTGIFT), the minimum donation amount (MINRAMNT), \nthe maximum donation amount (MAXRAMNT), and the sum of all donation \namounts (RAMNTALL). Common date variable rollups include the earliest \ndo", "ount (LASTGIFT), the minimum donation amount (MINRAMNT), \nthe maximum donation amount (MAXRAMNT), and the sum of all donation \namounts (RAMNTALL). Common date variable rollups include the earliest \ndonation date (FISTDATE) and the most recent donation date (LASTDATE). \nIndividual donation amounts are captured through a process often referred to \nas \u201cpushing the data to the right,\u201d meaning new columns are created to capture \ndata originally appearing in rows of the data. Time lags can be represented in \nabsolute or relative form. In absolute form, separate columns are created for the \nfi rst gift, second gift, third gift, and so on. In relative form, the new columns \nrepresent the last gift, the next to last gift, and so on. \nThe constraint on this process, however, is that the data must be rectangu-\nlar. If donors give differing numbers of gifts, the modeler must fi nd a way to \nrepresent these donors in a consistent manner. If, for example, one donor gives \n4 gifts and another 20 gift", " rectangu-\nlar. If donors give differing numbers of gifts, the modeler must fi nd a way to \nrepresent these donors in a consistent manner. If, for example, one donor gives \n4 gifts and another 20 gifts, the data cannot have 4 columns for the fi rst donor \nand 20 for the second; the data must be coded consistently for both. This could \nbe that 20 columns are created for both, but the 4-gift donor has only 4 of them \npopulated, or that the modeling data contains only 4 columns so that the 20-gift \ndonor has only 4 of the gifts shown in the record. \nThere is no theory to tell the modeler to build features based on fi xed or rela-\ntive time, how many lags to include in the historic data, and which summary \nstatistics to create (min, max, mean, etc.). Domain expertise helps considerably \nin this process, but oftentimes modelers will create as many different kinds of \nfeatures as can be handled by the modeling software. \nVariable Selection Prior to Modeling\nNow that data has been cleaned and", "s process, but oftentimes modelers will create as many different kinds of \nfeatures as can be handled by the modeling software. \nVariable Selection Prior to Modeling\nNow that data has been cleaned and new features have been created, one may \nthink that it\u2019s now time to begin modeling. However, I recommend that an \nadditional process be added: Reduce variables through variable selection. The \ngoal of variable selection is to remove irrelevant variables, redundant variables, \nand variables that are unlikely to improve model accuracy.\nNote that the techniques described here differ from variable selection in the con-\ntext of predictive modeling. Forward and backward variable selection algorithms \nare often added to modeling algorithms to select variables in an effi cient, albeit \ngreedy, manner. Decision trees perform variable selection as a natural part of the \nlearning algorithm. Stepwise Regression is a common variable selection technique \n\n118 \nChapter 4 \u25a0 Data Preparation\nthat has bee", "r. Decision trees perform variable selection as a natural part of the \nlearning algorithm. Stepwise Regression is a common variable selection technique \n\n118 \nChapter 4 \u25a0 Data Preparation\nthat has been integrated into linear regression. More sophisticated techniques such \nas using guided random search (genetic algorithms or simulated annealing) have \nbeen added to neural networks to improve variable selection. \nThese techniques are described in the context of the algorithms themselves, \nand while many are very time consuming, they have the advantage that the \nvariable selection is done in the context of the modeling. When variable selec-\ntion is wrapped around building neural networks, for example, the variables \nthat are retained are those that have worked best for the neural network, not \nvariables that appear to be good predictors using a different metric, such as the \nchi-square test or the F test. If there is suffi cient time available for incorporating \nvariable selection with th", " \nvariables that appear to be good predictors using a different metric, such as the \nchi-square test or the F test. If there is suffi cient time available for incorporating \nvariable selection with the modeling algorithm, it is always the preferred method.\nRemoving Irrelevant Variables\nThe fi rst variable selection step merely removes irrelevant variables. These are \nthe variables that have either no relationship with the target variable or \nare improper to include in the models. The latter steps are usually based on regu-\nlatory concerns, such as removing race or ZIP code from the analysis. They can \nalso include removing variables that cannot be replicated or are too expensive \nto collect on an ongoing basis; a company may have done a study that involved \nextensive interviews with their customers, but unless this can be done for all cus-\ntomers in the future, the variables cannot be used as inputs to a predictive model. \nOther irrelevant variables include labels, such as customer ID.", "customers, but unless this can be done for all cus-\ntomers in the future, the variables cannot be used as inputs to a predictive model. \nOther irrelevant variables include labels, such as customer ID. While there \nmay be some predictive information possible in an ID\u2014the order of assignment \nof IDs may indicate how long the customer has been a customer\u2014there are usu-\nally more direct ways to represent any information they convey, and therefore \nthey are routinely excluded. Finally, other variables that could be collected and \nused as candidate inputs have no known relationship with the target and are \nnormally excluded. The phase of the moon is always known and could be included \nin the modeling data, but there is no known causal relationship between phase \nof the moon and someone\u2019s propensity to respond to a marketing campaign. \nRemoving variables with no possible connection to the target prevents algo-\nrithms from discovering happenstance relationships in the data\u2014spurious \ncorrelatio", "ity to respond to a marketing campaign. \nRemoving variables with no possible connection to the target prevents algo-\nrithms from discovering happenstance relationships in the data\u2014spurious \ncorrelations\u2014as described in Chapter 3. Spurious correlations are largely a \nsmall-data phenomenon, but not completely. Even with larger data, if you search \nthrough enough variables, you may eventually happen upon a variable that \npredicts the outcome even though it is unrelated, sometimes described as over-\nsearching. Over-searching may be one reason many published studies cannot \nbe replicated even if they are shown to be statistically signifi cant by seasoned \nmodelers using good practices. The particularly dangerous part of over-searching \nfor predictive modelers is that you often include variables in your analyses that \nyou don\u2019t suspect will be good predictors, but \u201cwhy not give it a shot?\u201d Thus \nyou should make sure that all input variables could have a real relationship \nwith the target var", " in your analyses that \nyou don\u2019t suspect will be good predictors, but \u201cwhy not give it a shot?\u201d Thus \nyou should make sure that all input variables could have a real relationship \nwith the target variable.\n\n \nChapter 4 \u25a0 Data Preparation \n119\nRemoving Redundant Variables\nRedundant variables, at a minimum, waste time in the modeling process, and \nat worst can cause model instability. They are variables whose information is \nconveyed by one or more other variables, and can occur with both numeric and \ncategorical variables. \nCorrelation matrices (introduced in Chapter 3) identify highly correlated \nvariables that are good candidates for removal. When a pair of variables have \na high correlation, higher than 0.9 or smaller than \u20130.9, they are largely redun-\ndant and only one of the two is needed for use in the predictive models. The 0.9 \nthreshold is a rule of thumb and should not be considered an absolute standard; \ndepending on how aggressively the modeler would like to remove variable", "eded for use in the predictive models. The 0.9 \nthreshold is a rule of thumb and should not be considered an absolute standard; \ndepending on how aggressively the modeler would like to remove variables, \nthe threshold could be reduced to a magnitude of 0.8 or even 0.7. \nIn Table 4-13, if the threshold magnitude of 0.9 is used, CARDPROM and \nNUMPROM are considered redundant predictors and only one or the other is \nneeded: you can remove either CARDPROM or NUMPROM without removing \ninformation in the data. The analysis of correlation matrices is a manual process, \nthough it is possible to automate it, keeping the fi rst in your list of variables with \ncorrelations exceeding a threshold. Some software packages include functions \nor nodes specifi cally for this purpose.\nTable 4-13: Correlations for Six Variables from the KDD Cup 98 Data\nCORRELATIONS\nCARDPROM\nNUMPROM\nRAMNTALL MAXRAMNT\nLASTGIFT\nCARDPROM\n1.000\n0.949\n0.550\n0.023\n\u20130.059\nNUMPROM\n0.949\n1.000\n0.624\n0.066\n\u20130.024\nRAMNTALL\n0.550\n0.62", "s for Six Variables from the KDD Cup 98 Data\nCORRELATIONS\nCARDPROM\nNUMPROM\nRAMNTALL MAXRAMNT\nLASTGIFT\nCARDPROM\n1.000\n0.949\n0.550\n0.023\n\u20130.059\nNUMPROM\n0.949\n1.000\n0.624\n0.066\n\u20130.024\nRAMNTALL\n0.550\n0.624\n1.000\n0.557\n0.324\nMAXRAMNT\n0.023\n0.066\n0.557\n1.000\n0.563\nLASTGIFT\n\u20130.059\n\u20130.024\n0.324\n0.563\n1.000\nSelecting Variables When There Are Too Many\nMany predictive modeling projects include hundreds of candidate input variables \nas a part of the analysis, including original variables and new features created \nto improve the predictive models. The inclusion of hundreds of variables as \ncandidates for predictive models can cause problems, however:\n \n\u25a0Some algorithms cannot reliably use hundreds or thousands of input \nvariables. \n \n\u25a0Algorithms that can reliably incorporate hundreds or thousands of \nvariables as candidate inputs or actual inputs to models may take consider-\nable time to train, slowing the iterative process of building and selecting \ngood models.\n\n120 \nChapter 4 \u25a0 Data Preparation\n", "ariables as candidate inputs or actual inputs to models may take consider-\nable time to train, slowing the iterative process of building and selecting \ngood models.\n\n120 \nChapter 4 \u25a0 Data Preparation\n \n\u25a0Hundreds or thousands of input variables in a model could cause \na problem related to the curse of dimensionality, described later in this chapter.\n \n\u25a0The size of the data with so many inputs may exceed the memory \ncapacity of the computer.\n \n\u25a0Some models must be built very quickly, leaving little time for extensive \nvariable selection and variable assessment. \nFor these reasons, predictive modelers often perform variable selection prior \nto modeling to make the modeling process more effi cient with, one hopes, \nminimal loss in model accuracy. \nThe simplest way to select variables for predictive modeling when there are \ntoo many is to fi rst run an assessment to score the predictive power for each \nvariable in predicting the target variable, one at a time. Single-variable mod-\nels or st", "ctive modeling when there are \ntoo many is to fi rst run an assessment to score the predictive power for each \nvariable in predicting the target variable, one at a time. Single-variable mod-\nels or statistical tests are fast and simple to do, and usually it is the case that \nvariables that are poor predictors on their own are not good predictors even in \ncombination with other inputs. This is not the case with Simpson\u2019s Paradox, as \ndescribed in Chapter 3, so single-variable selection comes with risks.\nNevertheless, sometimes there is little else one can do but reduce the number \nof candidate inputs. Table 4-14 shows a list of techniques that can be used for \nvariable selection. This is not an exhaustive list, and you can easily adapt the \nprinciple of assessing single variables to a wide variety of statistical tests and \nmodeling algorithms. \nTable 4-14: A Short List of Single-Variable Selection Techniques\nTECHNIQUE\nCOMPARISON TYPE\nINCLUSION METRIC\nChi-square test\nCategorical input vs", "variety of statistical tests and \nmodeling algorithms. \nTable 4-14: A Short List of Single-Variable Selection Techniques\nTECHNIQUE\nCOMPARISON TYPE\nINCLUSION METRIC\nChi-square test\nCategorical input vs. categorical \ntarget\np value or top \nN variables\nCHAID tree stump using \nChi-square test\nContinuous or categorical input \nvs. continuous or categorical \ntarget\np value or top \nN variables\nAssociation Rules con\ufb01 -\ndence, 1 antecedent\nCategorical input vs. categorical \ntarget\nCon\ufb01 dence, Support\nANOVA\nContinuous input vs. categorical \ntarget\np value, top N variables\nKolmogorov-Smirnov (K-S) \nDistance, two sample test\nContinuous input vs. continuous \ntarget\nK-S test critical value, \ntop N variables\nLinear regression forward \nselection (1 step)\nContinuous input (or dummy) \nvs. continuous target\np value, AIC, MDL, top \nN variables\nPrinciple Component \nAnalysis (select top loader \nfor each PC)\nContinuous input vs. continuous \ntarget\nTop N variables\n\n \nChapter 4 \u25a0 Data Preparation \n121\nEach tech", " value, AIC, MDL, top \nN variables\nPrinciple Component \nAnalysis (select top loader \nfor each PC)\nContinuous input vs. continuous \ntarget\nTop N variables\n\n \nChapter 4 \u25a0 Data Preparation \n121\nEach technique works for different comparison types, which, for predictive \nmodeling, means the tests work for different combinations of continuous or \ncategorical inputs and targets. There are also advantages to using several of \nthese techniques and examining which input variables hove strong relation-\nships to the target variables. You can include input variables that pass all of the \ntests or input variables that pass any one of the tests, depending on how many \ninputs pass the tests.  \nConsider the KDD Cup 1998 data, with nearly 500 inputs in the modeling \ndata. After applying an ANOVA with TARGET_B as the grouping variable, \nTable 4-15 shows the top 15 variables in the list, sorted by the F-statistic. The \nobvious winner is TARGET_D, though this is related directly to TARGET_B. \nIf TARGET_D =", "_B as the grouping variable, \nTable 4-15 shows the top 15 variables in the list, sorted by the F-statistic. The \nobvious winner is TARGET_D, though this is related directly to TARGET_B. \nIf TARGET_D = 0 then TARGET_B = 0 as well. If there is a variable that has a \nsurprisingly large F-statistic, you should investigate the variable to make sure \nthe target variable is not leaking into the defi nition of the input. \nTable 4-15: Variable Selection Test Using the F-statistic \nVARIABLE\nF STATISTIC\nP VALUE\nMEAN \nTARGET_B = 0\nMEAN \nTARGET_B = 1\nTARGET_D\n142,779\n0\n0.00\n15.62\nRFA_2F\n501.52\n0\n1.89\n2.25\nCARDGIFT\n279.31\n0\n5.00\n6.12\nNGIFTALL\n247.79\n0\n9.50\n11.48\nLASTDATE\n161.89\n0\n9547.63\n9556.86\nLASTGIFT\n120.57\n0\n17.43\n15.17\nNUMPROM\n105.04\n0\n46.80\n50.27\nFISTDATE\n102.13\n0\n9138.07\n9090.34\nCARDPROM\n100.68\n0\n18.37\n19.64\nAVGGIFT\n100.53\n0\n13.43\n11.84\nMINRAMNT\n91.82\n0\n8.00\n6.76\nRAMNT_16\n72.26\n0\n14.17\n11.73\nMAXRDATE\n66.85\n3.33E-16\n9442.92\n9422.08\nHV2\n62.61\n2.55E-15\n1127.42\n1237.94\nHV1\n59.72\n1.10E-14\n1056.39", "0\n18.37\n19.64\nAVGGIFT\n100.53\n0\n13.43\n11.84\nMINRAMNT\n91.82\n0\n8.00\n6.76\nRAMNT_16\n72.26\n0\n14.17\n11.73\nMAXRDATE\n66.85\n3.33E-16\n9442.92\n9422.08\nHV2\n62.61\n2.55E-15\n1127.42\n1237.94\nHV1\n59.72\n1.10E-14\n1056.39\n1163.70\nOf the 404 variables included in the full ANOVA table, 198 of them have \na p value less than 0.05. There is no magic to this cutoff; frequently, you may use \na cutoff related to selection of the top 50 variables rather than using a specifi c \np value of F statistic. \n\n122 \nChapter 4 \u25a0 Data Preparation\nSelecting Variable Interactions\nSingle variable selection is fast, easy, and usually effective at retaining the best \npredictors. There are situations where the selection techniques can be fooled, \nresulting in the removal of variables that, while not predicting well on their \nown, predict very well when interacting with one or more other variables. This \nis precisely why variable selection before building predictive models is to be \ndone only when necessary for time or memory effi c", "ct very well when interacting with one or more other variables. This \nis precisely why variable selection before building predictive models is to be \ndone only when necessary for time or memory effi ciency reason. \nYou can accomplish a second stage of variable selection by identifying highly \npredictive interactions. The number of interactions to test can become quite \nlarge, however, as shown in Table 3-12 and reproduced in part in Table 4-16. \nIt is not unusual to have more than 500 candidate input variables, particularly \nafter building features. A typical approach to searching for good interactions \nis through exhaustive searching in a factorial design, sometimes available in \nsoftware. But trying 124,750 combinations may be impractical and the software \noften returns only the best interaction of these rather than listing all interactions.\nTable 4-16: Number of Two-Way Interaction Combinations\nNUMBER OF VARIABLES\nNUMBER OF POSSIBLE TWO\uf6baWAY INTERACTIONS\n   5\n     10\n  10\n     45\n  5", "eraction of these rather than listing all interactions.\nTable 4-16: Number of Two-Way Interaction Combinations\nNUMBER OF VARIABLES\nNUMBER OF POSSIBLE TWO\uf6baWAY INTERACTIONS\n   5\n     10\n  10\n     45\n  50\n  1,225\n 100\n  4,950\n 500\n 124,750\n1000\n499,500\nThe only quick, automated way to assess all interactions effi ciently and report \nthe predictive accuracy of all of the interactions in most predictive analytics \nsoftware available today is through the use of association rules. They work only \nwith categorical variables, so any continuous variables must fi rst be binned, but \nassociation rules are fast, effi cient ways to fi nd interesting interactions. \nIn summary, variable creation and selection follow these steps:\n \n1. Clean single variables from problems such as incorrect values, missing \nvalues, and outliers.\n \n2. Create new features from existing variables.\n \n3. Reduce the number of candidate inputs by removing redundant and \nirrelevant variables and features.\n\n \nChapter 4 \u25a0 Data Pre", "values, and outliers.\n \n2. Create new features from existing variables.\n \n3. Reduce the number of candidate inputs by removing redundant and \nirrelevant variables and features.\n\n \nChapter 4 \u25a0 Data Preparation \n123\nSampling\nThe biggest danger in predictive modeling stability and accuracy when models \nare deployed is overfi tting the models. Overfi t refers to models that are accurate \non the data used to train the models, but are inherently too complex and there-\nfore perform much worse on data not in the modeling set. \nFigure 4-12 shows conceptually what happens with models that lead to over-\nfi t. Two target variable values are plotted in the fi gure represented with fi lled \ntriangles and fi lled circles. On testing data, data not used in defi ning the model \nand decision boundary, the two target values are represented as unfi lled shapes. \nIn the left-hand plot, the model is a simple linear model so the decision bound-\nary is also linear. On training data, only one point is misclass", "wo target values are represented as unfi lled shapes. \nIn the left-hand plot, the model is a simple linear model so the decision bound-\nary is also linear. On training data, only one point is misclassifi ed: the fi lled \ntriangle to the right of the line. Testing data overlaid on top of the plot, two \ntriangles and two circles, are shown as well, and all four are classifi ed correctly. \nSimple Model\nDecision\nBoundary\nComplex Model\nDecision\nBoundary\nOutlier that\nin\ufb02uences decision\nboundary\nTraining Data\nTesting Data\nFigure 4-12:  Example of overfitting data\nNext, consider the plot on the right. This model is non-linear, with additional \ncomplexity introduced so that the fi lled triangle, misclassifi ed in the linear \nmodel, is now classifi ed correctly in the training data. The evidence that the \nalgorithm has introduced additional complexity is the curved decision bound-\nary that enables the model to achieve higher classifi cation accuracy on the \ntraining data. However, the testing da", "t the \nalgorithm has introduced additional complexity is the curved decision bound-\nary that enables the model to achieve higher classifi cation accuracy on the \ntraining data. However, the testing data tells a different story; both unfi lled \ncircles in the testing data are now misclassifi ed in the class represented by the \n\n124 \nChapter 4 \u25a0 Data Preparation\ntriangles, to the left of the decision boundary. What happened? There was not \nsuffi cient evidence in the data to justify the additional complexity in the model, \nand sometimes it is diffi cult to determine whether or not the model complexity \nis too high without considering additional data. This is the role of sampling in \nbuilding predictive models.\nPartitioning Data\nThe most common sampling strategy is splitting the data into training and test-\ning subsets. Some describe the testing subset as validation data or evaluation \ndata, but the principle is the same: One builds the model on the training data \nand then assesses the mo", "ining and test-\ning subsets. Some describe the testing subset as validation data or evaluation \ndata, but the principle is the same: One builds the model on the training data \nand then assesses the model on the held-out, testing subset.\nTraining and testing subsets are usually created through a random selection \nof records without replacement; each record belongs to one and only one subset. \nOne way to think of the roles of sampling into training and testing subsets \nis to consider what you do when you are preparing to take a standardized test \nsuch as the GRE. To prepare for the exam, assume that you go to the store and \nbuy a test book containing sample questions and an answer key in the back. \nAfter taking the practice exam, you grade yourself with the answers in the \nback of the book. Assume that after the fi rst attempt, you have only 65 percent \ncorrect, indicating that you need to study a lot more to do better. So you study \nthe material covered in the test harder and try again.", "Assume that after the fi rst attempt, you have only 65 percent \ncorrect, indicating that you need to study a lot more to do better. So you study \nthe material covered in the test harder and try again. This time, you get 82 per-\ncent correct: good, but not good enough. So you go back and study the material \neven harder. After several passes through testing and studying, you fi nally have \na grade of 98 percent. After feeling quite good about yourself and how well \nyou did on the test, a nagging feeling comes over you: Do you really know the \nmaterial, or did you just memorize the answers to these particular questions?\nOne way to answer that question is to go to the store and buy a second book, \nthe test or validation book. You haven\u2019t seen these questions before, so if you get \n98 percent correct on this book, you can have a much higher degree of confi dence \nthat you really know the material. However, if you get only 65 percent correct \non this book after getting 98 percent correct on ", "orrect on this book, you can have a much higher degree of confi dence \nthat you really know the material. However, if you get only 65 percent correct \non this book after getting 98 percent correct on the fi rst book\u2019s test, you know \nthat you simply memorized the answers to the questions in that book, and don\u2019t \nreally know or understand the material.\nSampling is a lot like this. Predictive models can do very well on the data \nthat was used to build the model. But the model could be too specifi c to those \nparticular records, especially if there is noise in the data that may cause small \ndifferences in values of some inputs that the model can pick up and use to \ndifferentiate values of the target variable. This effect is sometimes even called \nmemorizing the data. \n\n \nChapter 4 \u25a0 Data Preparation \n125\nMeasuring model performance on data the algorithm didn\u2019t use in build-\ning the model is the best way to identify overfi t. If the performance on the \ntesting data is about the same as the", "aration \n125\nMeasuring model performance on data the algorithm didn\u2019t use in build-\ning the model is the best way to identify overfi t. If the performance on the \ntesting data is about the same as the performance on the training data, you have \nstrong evidence that the model is not overfi t. The typical process for building \nmodels therefore is this:\n \n1. Build a model on training data.\n \n2. Assess the model on testing data. \n \n3. Change settings, parameters, or inputs to the model.\n \n4. Re-build the model on training data.\n \n5. Assess this new model on testing data.\n \n6. Change settings, and repeat the process.\nThis iterative process could be done manually by a practitioner or automati-\ncally by the modeling software. Either way, you could iterate dozens, hundreds, \nor even thousands of times through the process. \nSplitting data into these two data sets, training and testing, has a long history \nin statistical modeling. However, an additional nagging question should emerge. \nBecause t", "imes through the process. \nSplitting data into these two data sets, training and testing, has a long history \nin statistical modeling. However, an additional nagging question should emerge. \nBecause the testing data is informing the modeler or software on how to adjust \nthe modeling algorithm parameters so that model performance on testing data is \nimproved, is the testing data truly an independent measure of model accuracy? \nThe answer is a defi nitive \u201cmaybe.\u201d It is certainly possible that the testing data \nbecomes a driver for how the algorithm builds the model, leading to overfi tting \nnot based on the training data, but indirectly based on the testing data.\nOne solution to this potential problem is to have a third data set available, \nheld out until the very end of the model assessment stage. This data, called \nvalidation data (as a part of the training, testing, validation labels for the three \nmodeling data sets), is used to provide a fi nal estimate of the predicted model \nperf", "t stage. This data, called \nvalidation data (as a part of the training, testing, validation labels for the three \nmodeling data sets), is used to provide a fi nal estimate of the predicted model \nperformance or accuracy once it is deployed. \nEach of these three sampled subsets of the entire data set must be large enough \nto be representative samples, but are separate from one another: Each record \nbelongs to one and only one of the three subsets. Table 4-17 shows 20 records \nfrom a data set with each record labeled with the sampling subset to which it \nbelongs: training, testing, or validation.\nThe idea of partitioning data into three subsets has been widely adopted by \nthe software vendors; most predictive analytics software includes functions \nto split data into all three of these subsets. For those software packages that \nstill only allow for partitioning into training and testing, you can include an \nadditional split of the testing subset to create the testing and validation subset", "s. For those software packages that \nstill only allow for partitioning into training and testing, you can include an \nadditional split of the testing subset to create the testing and validation subsets. \n\n126 \nChapter 4 \u25a0 Data Preparation\nTable 4-17: Sample Split into Sample Subsets\nSAMPLE SUBSET\nCONTROLN\nSTATE\nAGE\nGENDER\nLASTGIFT\nTraining\n24610\nSC\n81\nM\n3\nTesting\n118673\nTX\nF\n10\nTesting\n82943\nMN\n62\nF\n5\nValidation\n73371\nMI\n79\n \n10\nTraining\n190313\nTX\nM\n7\nTraining\n76585\nIA\n66\nF\n5\nValidation\n156378\nCA\n69\nF\n10\nTesting\n25641\nGA\n73\nM\n12\nTraining\n123822\nTN\nF\n9\nTraining\n31911\nVA\nF\n5\nThe Curse of Dimensionality\nDistance between data points is a key concept in building many modeling \nalgorithms, including k-nearest neighbor and K-Means clustering models. \nEven if distances are not explicitly computed in the modeling algorithms, at its \nroot, predictive modeling tries to fi nd commonalities between values of vari-\nables, and in the case of supervised learning, the algorithms relate these values \nto", "in the modeling algorithms, at its \nroot, predictive modeling tries to fi nd commonalities between values of vari-\nables, and in the case of supervised learning, the algorithms relate these values \nto a target variable. To fi nd these patterns in the data, there must be enough of \nthem to fi nd commonalities and differences.\nOne way to appreciate potential problems with distances is to consider what \nhappens to distances as the number of variables in a clustering model increase. \nWhat if you have 10 volleyballs in a room that is 20 feet by 20 feet and they were \nspread randomly throughout the room. How far apart are the balls? On aver-\nage, they may be about 10 feet from any other ball. (Keep this measurement in \nmind.) But from a clustering standpoint, the more apt question is this: How far \naway is the nearest ball to each of the balls? It turns out that with 10 volleyballs \nin the room, it will be about 3 feet. \nWhat happens if instead of the 10 volleyballs being thrown and landing ", "ar \naway is the nearest ball to each of the balls? It turns out that with 10 volleyballs \nin the room, it will be about 3 feet. \nWhat happens if instead of the 10 volleyballs being thrown and landing on \nthe ground in the two-dimensional area of the fl oor, you could suspend the \nlaws of physics and allow the same 10 volleyballs to end up anywhere in the \nroom, including in the air all the way up to the ceiling. Furthermore, assume \nthat the ceiling is 20 feet high (the same size as the other two dimensions). \nHow far apart is the smallest distance from each ball to its nearest neighbor \nnow? It turns out that the average minimum distance may now double to more \nthan 6 feet, depending on the random location of the balls. \n\n \nChapter 4 \u25a0 Data Preparation \n127\nThese two spatial dimensions can be thought of as two inputs to a model, \nand the fl oor of the room like the canvas of a scatterplot (see Figure 4-13). \nIn this plot, the ten randomly distributed data points are on average 10 unit", "n be thought of as two inputs to a model, \nand the fl oor of the room like the canvas of a scatterplot (see Figure 4-13). \nIn this plot, the ten randomly distributed data points are on average 10 units apart. \n0\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\nFigure 4-13:  Random spread of data in 2D\nIf we could possibly imagine higher dimensional space, say four or fi ve dimen-\nsions rather than just the three spatial dimensions, it would be a different story; \nthe volleyballs will be further apart because of the exponentially increasing \nspace to fi ll due to the higher dimensions. Or in a predictive analytics frame-\nwork, four or fi ve inputs rather than just two or three means a lot more empty \nspace in the scatterplot. \nNow consider Figure 4-14. With only two inputs and 10 data points (the \npoint at the bottom left of the figure), the average minimum distance to \nthe nearest data point is just under three units away. As you increase the number \nof inputs to fi ve, this numbe", "points (the \npoint at the bottom left of the figure), the average minimum distance to \nthe nearest data point is just under three units away. As you increase the number \nof inputs to fi ve, this number rises to more than eleven units. Or think of the \ndata in a different way: How many data points would you need to maintain \nthe same minimum distance to the nearest point as you increase the number \nof inputs of the data? It turns out that to achieve the same distance between \ndata points (about three) as the data in Figure 4-13 when you have three inputs, \nyou need between 100 data points for three inputs and 500 data points in four \ninputs. With fi ve inputs, even 1000 data points is not enough. In other words, \nas the number of inputs increases, the number of data points needed to fi ll \nthe space comparably increases exponentially. This, in a nutshell, is the curse \nof dimensionality. \n\n128 \nChapter 4 \u25a0 Data Preparation\n0.0\n0\n100\n200\n300\n400\n500\nNumber of Data Points (Records)\nAverag", "the space comparably increases exponentially. This, in a nutshell, is the curse \nof dimensionality. \n\n128 \nChapter 4 \u25a0 Data Preparation\n0.0\n0\n100\n200\n300\n400\n500\nNumber of Data Points (Records)\nAverage Minimum Distance\n600\n700\n800\n900\n1000\n2.0\n4.0\n6.0\n8.0\n10.0\n12.0\n5 Inputs\n4 Inputs\n3 Inputs\n2 Inputs\nFigure 4-14:  Increasing number of data points needed to populate data\nIf a suffi cient number of records are not included in the data set, the data space \nis not populated densely enough and individual data points become further and \nfurther away from each other, appearing more like outliers than patterns of data. \nIn reality, the problem isn\u2019t as bad as described here; the data used for this \nexperiment was randomly generated, and presumably, the patterns that exist \nin the data are not random and will clump in different regions of the data \nspace. However, the curse of dimensionality provides a principle to keep in \nmind when determining how many records are needed for effective modelin", "om and will clump in different regions of the data \nspace. However, the curse of dimensionality provides a principle to keep in \nmind when determining how many records are needed for effective modeling: \nAs you increase the number of candidate inputs for modeling, the number of \nrecords needed to capture the higher-dimensional patterns must increase, and \ncertainly much more than linearly.\nRules of Thumb for Determining Data Size\nAll practitioners know that one needs enough records in the data to build reli-\nable models. The actual number of records, however, is not easy to determine \nprecisely and depends on patterns found in the data itself, not just how many \ninputs may be considered in the models. For example, if the patterns in the \ndata are linear, and there is little noise in the data, you need only one or two \nrecords per input in the model, so merely dozens of records would be needed \nfor modeling.\nUnfortunately, few problems have these characteristics: linear and low-noise. \n", "ta, you need only one or two \nrecords per input in the model, so merely dozens of records would be needed \nfor modeling.\nUnfortunately, few problems have these characteristics: linear and low-noise. \nThe more nonlinear the patterns in the data, the more data needed to characterize \n\n \nChapter 4 \u25a0 Data Preparation \n129\nthe patterns. And the more noise in the data, the more data needed to overcome the \nnoise. Noise in this context means unobserved relationships in the data, not \ncaptured by the inputs. It appears as random fl uctuations that models never \ncan capture (unless they are overfi t!).\nThere are two primary considerations for determining sample size. \nFirst, you must consider the inherent dimensionality of the data and the effect \nof the curse of dimensionality on the data. For smaller problems when you \nare expecting to consider only a few variables, 10\u201320 records per input may \nbe suffi cient. For more complex problems, at least 100 records per input may be \nneeded to capture", "ller problems when you \nare expecting to consider only a few variables, 10\u201320 records per input may \nbe suffi cient. For more complex problems, at least 100 records per input may be \nneeded to capture the multidimensional patterns in the data. Therefore, for \nmodels with 100 candidate inputs, as a general rule, you would want to have \na minimum of 10,000 records in the data for each subset (training, testing, \nand validation). \nThe second consideration is of particular concern with classifi cation problems. \nNot only do you need to have suffi ciently populated inputs, but you need to have \nthe target variable suffi ciently populated. For each level in the target variable, as \na rule of thumb, you would want to have at least 1,000 examples for each of the \ntraining, testing, and validation data sets. This is typically only a problem for \nthe most interesting values: responders for customer acquisition models, fraud \ncases for fraud detection, churners for customer attrition models, and ", "data sets. This is typically only a problem for \nthe most interesting values: responders for customer acquisition models, fraud \ncases for fraud detection, churners for customer attrition models, and so forth. \nThe under-represented class is the one that is in the most danger of having too \nfew records for building stable models.\nThese rules of thumb can be used in two ways. First, they can help determine \nif there is enough data available for modeling. If there are not enough records \nto have at least training and testing subsets in suffi cient numbers, you may \nhave to consider other resampling techniques to ensure that stable models are \nbeing built. \nIf you have, from a practical standpoint, limitless data, these rules of thumb \ncan also be used to reduce the size of the data in building models. From a \npractical standpoint, data size is often a limiting factor for the modeler for most \nalgorithms; the time it takes to build models increases more than linearly with \nthe number of r", "ng models. From a \npractical standpoint, data size is often a limiting factor for the modeler for most \nalgorithms; the time it takes to build models increases more than linearly with \nthe number of records and the number of inputs. Iterating over training and \ntesting of models to fi nd good parameters and inputs for the models is a critical \npart of the modeling building process, so using smaller data helps the modeler \nbuild models more effi ciently. If you have 10 million records, but the rule of \nthumb indicates that you need only 50,000 records, most of these records are \nnot needed in the training set. If you need to reduce the time to build the mod-\nels, most of these records would not be needed to include in the training and \ntesting subsets. Of course if the hardware and software handles the 10 million \nrecords effi ciently and without problems, there is no harm in using all the data.\n\n130 \nChapter 4 \u25a0 Data Preparation\nPartitioning Data: How Much Data to Include in the Subset", " handles the 10 million \nrecords effi ciently and without problems, there is no harm in using all the data.\n\n130 \nChapter 4 \u25a0 Data Preparation\nPartitioning Data: How Much Data to Include in the Subsets\nOnce the sample size is determined, you must also determine how much data \nshould be included in each of the training, testing, and validation subsets. Most \nsoftware packages follow the approach taken historically as their defaults, either \n50 percent of the data used for training and 50 percent for testing, or on some \noccasions, two-thirds of the data used for training and one-third for testing. \nHowever these rules of thumb do not capture the key issue. Statistical signifi -\ncance and stability are determined not by percentages but by N: the number of \nrecords. If the number of records is just right, the percentages are fi ne to use. \nBut if the there are too few records, no partitioning will be good: Each subset \nwill have too few records. When there is more data than is needed for ", "s just right, the percentages are fi ne to use. \nBut if the there are too few records, no partitioning will be good: Each subset \nwill have too few records. When there is more data than is needed for model-\ning, splitting by percentage will only include more data than is necessary for \nbuilding good models in each subset.  \nConsider a data set with 1,000,000 records, and assume you have determined \nthat 100,000 records are needed to build an effective model. Table 4-18 shows \na summary of sample sizes based on the percentage defaults compared with a \nrecord-based default. If 50 percent of the records were selected for the training \nset, 500,000 records would be used compared with 100,000 for the record-based \nselection method. If 100,000 records are needed in the training data to build \nstable models, you should have 100,000 in the testing data and a minimum of \n100,000 in the validation data. If only 100,000 are needed for training and test-\ning, however, the remaining 800,000 can be ", "able models, you should have 100,000 in the testing data and a minimum of \n100,000 in the validation data. If only 100,000 are needed for training and test-\ning, however, the remaining 800,000 can be used for validation to provide the \nfi nal estimate of model performance once it is deployed. \nTable 4-18: Sample Size of Partitioned Data Based on Rules of Thumb\nPARTITION\nRULE OF THUMB \nBASED ON \nPERCENTAGE OF \nRECORDS, PERCENT\nSAMPLE SIZE \nFOR RULE OF \nTHUMB BASED ON \nPERCENTAGE OF \nRECORDS\nSAMPLE SIZE \nFOR RULE OF \nTHUMB BASED \nON SUFFICIENT \nNUMBER OF \nRECORDS\nTraining\n50\n500,000\n100,000\nTesting\n25\n250,000\n100,000\nValidation\n25\n250,000\n800,000\nCross-Validation\nCross-validation is a sampling technique used primarily for small data sets, when \ndata is too small to partition into training and testing subsets. The advantage \nof cross-validation is that all of the data is used in building the models, so all \nof the patterns represented in the training data help form the models. \n\n \nChapter", "d testing subsets. The advantage \nof cross-validation is that all of the data is used in building the models, so all \nof the patterns represented in the training data help form the models. \n\n \nChapter 4 \u25a0 Data Preparation \n131\nCross-validation is usually described as k-fold cross-validation, where the \nk refers to how many subsets are used for modeling. The procedure for k-fold \ncross-validation is as follows:\n \n1. Create k distinct data subsets through random sampling.\n \n2. Assign a role to each subset. k\u20131 of them will be used for training, 1 for \ntesting. Begin by assigning subset 1 to testing, and subsets 2 through \nk for training.\n \n3. Rotate roles so each subset is used for testing one time and training k\u20131 \ntimes.\nFigure 4-15 shows a visualization of cross-validation. \nTarget\nInput 1\nInput 2\nInput 3\nInput 4\nInput 5\nData Subset 1\nData Subset 3\nData Subset 4\nData Subset 6\nData Subset 5\nrecords\nData Subset 7\nData Subset 8\nData Subset 9\nData Subset 10\nData Subset 2--Testing\nFigure 4", "1\nInput 2\nInput 3\nInput 4\nInput 5\nData Subset 1\nData Subset 3\nData Subset 4\nData Subset 6\nData Subset 5\nrecords\nData Subset 7\nData Subset 8\nData Subset 9\nData Subset 10\nData Subset 2--Testing\nFigure 4-15:  Cross validation sampling\nA type value for k is 10, yielding 10-fold cross-validation. However, any number \nof folds can be used. The more folds you use, however, the smaller the held-\nout testing subset is, and the more error variance you will observe for the held-out \ndata. As you increase the number of folds, the size of each fold becomes smaller \nand smaller until you have only one record in each fold. This is a special case \nof cross-validation sampling sometimes called \u201chold-one-out\u201d sampling. \nThe error for each fold isn\u2019t interesting, but rather the average error over each \nfold is what you compute to assess the models.\nStatisticians use cross-validation to assess model stability. A model is built \nfrom each of the folds\u2014a total of k models. The hold-out subset is used to tes", "is what you compute to assess the models.\nStatisticians use cross-validation to assess model stability. A model is built \nfrom each of the folds\u2014a total of k models. The hold-out subset is used to test \nthe model, and the errors are averaged over the subsets. The model accuracy \non the k testing subsets can be used to assess how stable the models are: If \nthe accuracy is similar for all folds, the modeling procedure is viewed as being \nstable and not overfi t. This average error can be used as an estimate of how \naccurate a model will be on new data.\n\n132 \nChapter 4 \u25a0 Data Preparation\nFigure 4-16 shows the average error from a linear regression model training \nusing 10-fold cross-validation. Most folds demonstrate consistent behavior, which \nsuggests that stable models have been built. Fold 7, and to a lesser degree Fold 5, \nhas a much larger average error. Table 4-19 shows why Folds 5 and 7 have large \naverage errors. This table shows the largest absolute errors in the testing data, \n", "d 7, and to a lesser degree Fold 5, \nhas a much larger average error. Table 4-19 shows why Folds 5 and 7 have large \naverage errors. This table shows the largest absolute errors in the testing data, \nand clearly there are outliers in the predictions that fall into these two folds; \nthe evidence shows that the large cross-validation errors were due to outliers \nin the data rather than unstable models.\nIf the k models are all stable, a fi nal model of comparable complexity to the \nmodels built from the folds can be built using all the data, and the modeler \nexpects that the model will exhibit the problems due to overfi tting. \n25.000\n0\nfold 0\nfold 1\nfold 2\nfold 3\nfold 4\nfold 5\nfold 6\nfold 7\nfold 8\nfold 9\n50.000\n75.000\n100.000\n125.000\n150.000\n175.000\n200.000\n225.000\n250.000\n275.000\n294.977\nFigure 4-16:  Errors from cross-validation out-of-sample data\nTable 4-19: Largest Errors in 10-Fold Cross-Validation Models\nFOLD\nACTUAL\nPREDICTED\nABSOLUTE ERROR\n5\n20\n329.1\n309.1\n7\n200\n 20.0\n180.0\n1\n200\n", "gure 4-16:  Errors from cross-validation out-of-sample data\nTable 4-19: Largest Errors in 10-Fold Cross-Validation Models\nFOLD\nACTUAL\nPREDICTED\nABSOLUTE ERROR\n5\n20\n329.1\n309.1\n7\n200\n 20.0\n180.0\n1\n200\n 37.1\n162.9\n7\n150\n 21.9\n128.1\n5\n100\n  9.8\n 90.2\n9\n100\n 19.8\n 80.2\n6\n100\n 20.3\n 79.7\n5\n200\n121.5\n 78.5\n\n \nChapter 4 \u25a0 Data Preparation \n133\nBootstrap Sampling\nAnother approach to building robust models on small data is the use of bootstrap \nsampling. The bootstrap sampling method samples the data with replacement in \ncontrast to the random sampling methods described so far. For an illustration of \nthe concept, consider a big ping pong ball hopper for picking lottery numbers. \nEach ping pong ball has a number on it representing a record number in the \ndata to be sampled from. Assume there are 1,000 records in the data set, and \nthe fi rst ping pong ball that is selected is #143. Record 143 is then included in \nthe bootstrap data set. Then that ping pong ball is returned to the hopper, or, \ni", "000 records in the data set, and \nthe fi rst ping pong ball that is selected is #143. Record 143 is then included in \nthe bootstrap data set. Then that ping pong ball is returned to the hopper, or, \nin other words, the ping pong #143 is replaced into the hopper and is therefore \neligible to be selected again. Now assume ball #879 is selected. Record 879 is \nnow put into the bootstrap data set, and the ball is returned to the hopper so \nit is eligible to be selected again. Next, assume ball #143 emerged again. That \nrecord would be placed in the bootstrap data set for a second time. This pro-\ncess continues until, typically, the same number of records are selected for the \nbootstrap data set as were in the original data set. \nTable 4-20 shows a small sample of 10 records and 3 bootstrap samples from \nthe same 10-record data. In each case, only 6 or 7 of the original 10 records were \nincluded in the bootstrap sample. Moreover, some records were included mul-\ntiple times (records 4, 7, an", "ples from \nthe same 10-record data. In each case, only 6 or 7 of the original 10 records were \nincluded in the bootstrap sample. Moreover, some records were included mul-\ntiple times (records 4, 7, and 10 in bootstrap sample 1), and some records not at \nall (records 1, 3, and 5 in bootstrap sample 1).\nTable 4-20: Example of the Bootstrap Sampling Method.\nROW \nNUMBER\nBOOTSTRAP SAMPLE 1 BOOTSTRAP SAMPLE 2\nBOOTSTRAP SAMPLE 3\n1\n8\n1\n7\n2\n7\n6\n7\n3\n4\n4\n5\n4\n7\n2\n8\n5\n4\n3\n8\n6\n2\n3\n1\n7\n6\n6\n3\n8\n10\n3\n9\n9\n10\n9\n1\n10\n9\n10\n1\nSummary \nNotes\nSeven of ten records in \nsample\nNo samples from 1, 3, 5\nTwo samples from 4, 7, 10\nSeven of ten records in \nsample\nNo samples from 5, 7, 8\nTwo samples from 6\nThree samples from 3\nSix of ten records in \nsample\nNo samples from \n2, 4, 6, 10\nTwo samples from 7, 8\nThree samples from 1\n\n134 \nChapter 4 \u25a0 Data Preparation\nYou may ask at this point, \u201cWhy bootstrap sample at all? There are too many \nof some records and not enough of others?\u201d The answer is that you don\u2019t sample \nonl", "om 1\n\n134 \nChapter 4 \u25a0 Data Preparation\nYou may ask at this point, \u201cWhy bootstrap sample at all? There are too many \nof some records and not enough of others?\u201d The answer is that you don\u2019t sample \nonly once or twice, but dozens or hundreds of times. Each sample is drawn \nfrom the original data but shifts the distribution randomly by some amount, \nproviding a different emphasis in the data. \nOn average 1\u00f7e (or about 63 percent) of the records will be selected for inclu-\nsion in the bootstrap sample. This leaves a 37 percent hold-out sample for each \nbootstrap sample. One advantage bootstrapping has over cross-validation is that \nno matter how many samples one makes, there is always a 37 percent hold-out \nsample available to assess the models.\nAfter creating the bootstrap samples, the procedure is the same as was done \nfor cross-validation. One model is built for each sample and is assessed to ensure \nstability, consistency, and robustness.\nTemporal Considerations in Sampling\nSo far, sam", "ure is the same as was done \nfor cross-validation. One model is built for each sample and is assessed to ensure \nstability, consistency, and robustness.\nTemporal Considerations in Sampling\nSo far, sampling descriptions have focused on creating unbiased data sets of \nsuffi cient size to avoid model overfi t. The implicit assumption is that the time \nperiod of the historic data is irrelevant: Modelers assume future relationships \nbetween observed behavior, and the target variable will be consistent with the \nhistorical patterns stored in the data. For example, if you are predicting whether \na donor to a non-profi t will donate again, patterns such as how many donations \nthe donor made (at any time in the past) and the amount the donor donated \n(at any time in the past) are all you need to predict future donation behavior. \nPut another way, the data does not expose the sequence of historic giving in \norder to predict if the donor will donate again. \nHowever, some modeling problems have ad", "edict future donation behavior. \nPut another way, the data does not expose the sequence of historic giving in \norder to predict if the donor will donate again. \nHowever, some modeling problems have additional constraints that should \nbe considered, such as data that is non-stationary. Non-stationary data changes \nbehavior with time and therefore should be refl ected in the modeling data \nand sampling strategies. For this kind of data, simple random sampling is not \nappropriate: It will infl ate the expected accuracy of the model. \nThis phenomenon is similar to the difference between interpolation and \nextrapolation. Interpolation refers to estimating values that are bracketed by \nother values; predictive modeling usually involves interpolation, such as the \nprediction shown in Figure 4-17. The model provides the connection between \nhistoric data. Predictions for unseen data are assumed to be consistent with the \nmodel, falling in between data points observed historically in the data.\nE", " The model provides the connection between \nhistoric data. Predictions for unseen data are assumed to be consistent with the \nmodel, falling in between data points observed historically in the data.\nExtrapolation, on the other hand (see Figure 4-18), extends beyond what has \nbeen seen historically. These predictions are usually more diffi cult, and often \nless accurate, because the data available for predictions is limited to one side \nof the curve fi t. \n\n \nChapter 4 \u25a0 Data Preparation \n135\nTraining data\nPredicted value:\ninterpolation\nFigure 4-17:  Interpolation of data points\nTraining data\nPredicted value:\nextrapolation\nFigure 4-18:  Extrapolation of data points\nWhen temporal sequencing is important in predictions, some modifi cations \nin sampling need to be incorporated. Consider predicting data from a time \nseries, such as stock market future value prediction. Training and validation \ndata used for predictive modeling should be set up in chunks: training on one \nset of data and val", "ting data from a time \nseries, such as stock market future value prediction. Training and validation \ndata used for predictive modeling should be set up in chunks: training on one \nset of data and validating on data future to the data used in training. It is inher-\nently an extrapolation problem. Figure 4-19 shows a representation of the idea. \nThis fi gure is an extension of Figure 2-2 with changes made to the left of \u201cnow.\u201d \nIf data were split into training and validation subsets randomly from any point \nin time, the validation data would measure the interpolation capabilities of the \nmodels in a way that is not representative of how the model will be used. It will \nbe overly optimistic in estimates of accuracy. \nNotice that testing data falls in the same temporal group as training data. \nTesting data in this context is used to avoid overfi tting, whereas validation \ndata is used to assess expected accuracy of the models when deployed. This is \n\n136 \nChapter 4 \u25a0 Data Preparation\nthe ", " \nTesting data in this context is used to avoid overfi tting, whereas validation \ndata is used to assess expected accuracy of the models when deployed. This is \n\n136 \nChapter 4 \u25a0 Data Preparation\nthe case with many algorithms: Temporal separation of training and testing \ndata is diffi cult to achieve. \nAttributes\ncomputed\nbased on\nthis date\nAttributes for\nvalidation\ndata\nAttributes\ncomputed\nbased on\nthis date\nDate the\npredictive\nmodels are\nbuilt\nTarget\nvariable\ncomputed\nbased on\nthis data\n\u201cNow\u201d\nTime\nValidation\nHistoric Data\nFuture Data;\nwhen model will\nbe deployed\nTraining and\nTesting\nTarget\nvariable\ncomputed\nbased on\nthis date\nFigure 4-19:  Sampling for temporal data\nThe key principle at work is to sample in a way that helps measure the accu-\nracy of predictive models in a manner consistent with how the models will be \ndeployed. \nStrati\ufb01 ed Sampling\nStratifi ed sampling balances counts based on one or more variables. The pur-\npose is to remove the bias that may result from a sample ha", "nt with how the models will be \ndeployed. \nStrati\ufb01 ed Sampling\nStratifi ed sampling balances counts based on one or more variables. The pur-\npose is to remove the bias that may result from a sample having more records \nof one value of a variable of interest. For example, you may want to stratify a \ncustomer behavior data set to ensure males or females are represented equally, \nor the income quantiles of the customers so each quantile is represented \nequally. If historic data is representative of the expected distributions of vari-\nables that will be included in models, stratifi cation of inputs is unnecessary. For \nsample sizes that are large enough, random sampling is unlikely to produce \nbiased samples where key variables are distributed in unnatural ways. \nThe most common reason predictive modelers stratify samples is related to the \ntarget variable rather than input variables. The reasoning usually follows this \nline of thinking: Suppose a modeling group is building a classifi cati", "ive modelers stratify samples is related to the \ntarget variable rather than input variables. The reasoning usually follows this \nline of thinking: Suppose a modeling group is building a classifi cation model \nto predict whether or not customers will respond to a mailing. Furthermore, \nsuppose the response rate of the historic data to be used in building the model \nis 5 percent. The modeling team built a model and assessed it, fi nding that it was \n\n \nChapter 4 \u25a0 Data Preparation \n137\n95 percent accurate. Upon further examination, they found that the reason that \nmodel was 95 percent accurate was that it predicted every customer would not \nrespond to the mailing. So the good news is that the model has high accuracy \n(95 percent). The bad news is that the model tells the business not to mail to \nanyone because this is the best way to maximize accuracy.\nAs a result of this analysis, the modeling team decided the reason the \nmodels predicted every customer would not respond to the mailing", "l to \nanyone because this is the best way to maximize accuracy.\nAs a result of this analysis, the modeling team decided the reason the \nmodels predicted every customer would not respond to the mailing was that \nthe sample was biased. Because most of the records by far were non-responders, \nthe algorithms focused on the non-responders, learning those patterns better. \nThe solution was to break up this bias by stratifying the sample so that the \nmodeling population is 50 percent 1s and 50 percent 0s. \nThis approach is perfectly fi ne statistically and can work well. But was it \nnecessary? The answer is usually \u201cno,\u201d and in addition, stratifying can lead to \nmodels that are worse than those built from the natural population. Often, these \nmodels that appear to be predicting every customer to be a non-responder are \nactually working very well; it is only the interpretation of the model that causes \nproblems for the modeler. \nConsider the example again with the 5 percent historic response r", "be a non-responder are \nactually working very well; it is only the interpretation of the model that causes \nproblems for the modeler. \nConsider the example again with the 5 percent historic response rate. The model \nassessment stated that all of the customers were predicted as non-responders, \nbut then the modelers decided to look at a ROC curve as well. The ROC curve \nfor two models is shown in Figure 4-20. Examining the ROC curve tells a dif-\nferent story than the classifi cation accuracy: Nothing appears wrong in the \nROC curve at all. \n0\n0\n0.050\n0.100\n0.150\n0.200\n0.250\n0.300\n0.350\n0.399\n0.449\n0.499\n0.549\n0.600\n0.650\n0.700\n0.750\n0.800\n0.850\nP(TARGET_B=1) (0.6108)\n0.900\n0.950\n1.000\n0.100\n0.200\n0.300\n0.400\n0.500\n0.600\n0.700\n0.799\n0.899\n1.000\nFigure 4-20:  ROC curve for non-stratified sample\n\n138 \nChapter 4 \u25a0 Data Preparation\nThe question therefore is this: \u201cHow can a ROC curve look fi ne, but the clas-\nsifi cation accuracy of a model be so bad?\u201d\nFirst, classification algorithms do not", "le\n\n138 \nChapter 4 \u25a0 Data Preparation\nThe question therefore is this: \u201cHow can a ROC curve look fi ne, but the clas-\nsifi cation accuracy of a model be so bad?\u201d\nFirst, classification algorithms do not generate classification decisions. \nIn binary classifi cation problems, if the two class values of the target variable are \n0 and 1, the classifi cation model does not predict a 0 or a 1. The model predicts \nthe likelihood that the customer belongs to the class 0 or the class 1. The actual \ndistribution of the predicted probabilities is shown in Figure 4-21. The peak of \nthe histogram is near the value 0.05, which is expected since the percentage \nof target variables equal to 1 is 5 percent (0.05). There is clearly a range of pre-\ndictive values, which is why the ROC curve looked good. The model is not \ndegenerate, predicting 0s for all records.\nNote that the largest value of the predicted probabilities is only 0.35, less than \nthe value 0.5. This is important because every predictive mod", "del is not \ndegenerate, predicting 0s for all records.\nNote that the largest value of the predicted probabilities is only 0.35, less than \nthe value 0.5. This is important because every predictive modeling software \npackage thresholds the classifi er predictions by 0.5: If the predicted probability \nexceeds 0.5, the record is given the predicted label equal to 1; otherwise, it is \ngiven a value 0. Because no record exceeds the 0.5 threshold, all of the records \nare given a label of 0.\n[0\u20130.015]\n(0.015\u20130.03]\n0\n92\n184\n276\n368\n460\n552\n644\n736\n828\n920\n1016\n(0.03\u20130.045]\n(0.045\u20130.06]\n(0.06\u20130.075]\n(0.075\u20130.09]\n(0.09\u20130.105]\n(0.105\u20130.12]\n(0.12\u20130.135]\n(0.135\u20130.15]\nFigure 4-21:  Distribution of predicted probabilities\nThis is exactly what is happening in the confusion matrix of Table 4-21: \nAll records are given a predicted label of 0 because no record exceeded the \n0.5 threshold. But if a threshold of 0.05 is used instead (the prior probability), \n\n \nChapter 4 \u25a0 Data Preparation \n139\nthe confusi", "ds are given a predicted label of 0 because no record exceeded the \n0.5 threshold. But if a threshold of 0.05 is used instead (the prior probability), \n\n \nChapter 4 \u25a0 Data Preparation \n139\nthe confusion matrix looks much different, as shown in Table 4-22. This confu-\nsion matrix looks far more interesting and is evidence that the model is working.\nTable 4-21: Confusion Matrix from 5 Percent Target Rate, Threshold 0.5\nTARGET\n1\n0\nTOTAL\n1\n0\n2,399\n2,399\n0\n0\n45,307\n45,307\nTotal\n0\n47,706\n47,706\nTable 4-22: Confusion Matrix from 5 Percent Target Rate, Threshold 0.05\nTARGET\n1\n0\nTOTAL\n1\n1,312\n1,087\n2,399\n0\n18,115\n27,192\n45,307\nTotal\n19,427\n28,279\n47,706\nIf you stratifi ed the sample, you would get a confusion matrix very similar \nto the one shown in Table 4-22. What are the dangers (if any) of just stratifying \nthe sample based on the target variable? First, with only 2,399 1s, stratifying \nby removing records with target variable = 0 would remove 43,000 records, \nleaving 4798 total records to ", " stratifying \nthe sample based on the target variable? First, with only 2,399 1s, stratifying \nby removing records with target variable = 0 would remove 43,000 records, \nleaving 4798 total records to form decision boundaries from (instead of 47,706). \nRemoving records when data sizes are already small can lead to less precise \ndecision boundaries and more false alarms.\nIf instead you replicate records to achieve equal sized populations of the \ntarget variable, you will be building models from 95KB records, nearly half \nof which are replicates of the 2,399 1s (they would be replicated, on average, \n19 times each in the data). \nExample: Why Normalization Matters for K-Means Clustering\nK-Means clustering is an unsupervised learning method that is susceptible to \nbias when the input data is non-normal or at least non-uniform. An example of \nthe kinds of problems you can encounter is described here.\nLet\u2019s begin by using the KDD Cup data from 1998, but only four of the \nvariables: AGE, RFA_2", "n-normal or at least non-uniform. An example of \nthe kinds of problems you can encounter is described here.\nLet\u2019s begin by using the KDD Cup data from 1998, but only four of the \nvariables: AGE, RFA_2F, LASTGIFT, and MAXRAMNT. All missing values \n\n140 \nChapter 4 \u25a0 Data Preparation\nhave been removed to simplify the analysis, and all AGE values less than 19 \nhave been removed. Furthermore, only 3,684 examples are used. Table 4-23 shows \nthe summary statistics for the four variables, and Figure 4-22 shows the box plots.\nTable 4-23: Summary Statistics for Four Inputs to K-Means Model\nVARIABLE\nMINIMUM\nMAXIMUM\nMEAN\nSTD. DEVIATION\nROW COUNT\nAGE\n20\n98\n62.33\n15.75\n3,684\nMAXRAMNT\n5\n1000\n18.29\n21.03\n3,684\nLASTGIFT\n0\n250\n15.21\n11.23\n3,684\nRFA_2F\n1\n4\n2.20\n1.15\n3,684\n20.0\nAGE\nMAXRAMNT\nLASTGIFT\nRFA_2F\n50.0\n63.5\n75.0\n98.0\n1000.0\n250.0\n200.0\n133.0\n80.0\n250.0\n4.0\n3.0\n2.0\n1.0\n200.0\n133.0\n80.0\n34.0\n36.0\n33.0\n35.0\nFigure 4-22:  Box plots for four inputs to cluster model\nA few characteristics of the data ar", ".0\n63.5\n75.0\n98.0\n1000.0\n250.0\n200.0\n133.0\n80.0\n250.0\n4.0\n3.0\n2.0\n1.0\n200.0\n133.0\n80.0\n34.0\n36.0\n33.0\n35.0\nFigure 4-22:  Box plots for four inputs to cluster model\nA few characteristics of the data are worth noting. First, LASTGIFT and \nMAXRAMNT have severe positive skew. Second, LASTGIFT and MAXRAMNT \nhave magnitudes that are much larger than RFA_2F and, to a lesser degree, AGE. \nHowever, AGE and RFA_2F are distributed without signifi cant skew; even with-\nout showing the value of skew, you can see that the mean values from Table 4-23 \nare nearly the same as the median values shown in the box plots of Figure 4-22.\nFive clusters were created from the data, with characteristics summarized \nin Table 4-24. The table has been sorted by the mean value of LASTGIFT and \nMAXRAMNT to show more clearly how the clustering algorithm determined \ncluster membership. Cluster_4 is a small cluster, having only 19 records, but with \nlarge values of LASTGIFT and MAXRAMNT; these are the outliers in the da", "rly how the clustering algorithm determined \ncluster membership. Cluster_4 is a small cluster, having only 19 records, but with \nlarge values of LASTGIFT and MAXRAMNT; these are the outliers in the data \n\n \nChapter 4 \u25a0 Data Preparation \n141\nas shown in the box plot. In fact, the mean values for these two variables are 8 \nto 13 times larger than the mean values in cluster_1. Cluster_2 is also a relatively \nsmall cluster with relatively large values of LASTGIFT and MAXRAMNT. \nTable 4-24: Cluster Description by Mean Values of Inputs\nCLUSTER\nCOUNT MEAN AGE MEAN RFA_2F MEAN LASTGIFT MEAN MAXRAMNT\ncluster_1\n1,044\n62.00\n2.37\n12.03\n14.4\ncluster_3\n1,200\n79.10\n2.37\n12.19\n14.6\ncluster_0\n1,096\n43.42\n2.03\n15.14\n17.1\ncluster_2\n325\n64.77\n1.64\n31.68\n38.8\ncluster_4\n19\n69.63\n1.84\n102.89\n184.2\nThe spread of mean values for these two variables is clearly much larger than \nthe spread for AGE and RFA_2F, indicating that these values are determining \ncluster membership at the expense of the other two fi elds", "mean values for these two variables is clearly much larger than \nthe spread for AGE and RFA_2F, indicating that these values are determining \ncluster membership at the expense of the other two fi elds.\nFollowing the steps outlined in this chapter, taking the log transform of \nLASTGIFT and MAXRAMNT will reduce their positive skew. In addition, \nnormalizing all of the variables so they have the same scale will reduce the \nbias introduced by the magnitude of the variables so the clustering algorithm \nfocuses on the relative values of the variables. \nTable 4-25 shows the summary statistics after normalization, and Figure 4-23 \nthe box plot. First, note that the minimum and maximum values of each value \nare now 0 and 1 respectively, due to the min-max normalization. The mean \nvalues of three of the variables are between 0.4 and 0.6, roughly in the center of \nthe distributions though not exactly so. MAXRAMNT_log10 has a mean value \nof 0.2, indicating that the log transform did not remove all", " the variables are between 0.4 and 0.6, roughly in the center of \nthe distributions though not exactly so. MAXRAMNT_log10 has a mean value \nof 0.2, indicating that the log transform did not remove all of the skew from \nthe distribution. If you don\u2019t like this lingering skew, you may want to remove \noutliers to remove this skew by removing records with MAXRAMNT greater \nthan 1.5 times the IQR (33 for this data). You can see the remaining positive \noutliers more clearly in the box plot.\nTable 4-25: Summary Statistics of Four Variables after Normalization\nVARIABLE\nMINIMUM MAXIMUM MEAN STANDARD DEVIATION ROW COUNT\nAGE\n0\n1\n0.543\n0.202\n3,684\nRFA_2F\n0\n1\n0.401\n0.382\n3,684\nMAXRAMNT_log10\n0\n1\n0.201\n0.093\n3,684\nLASTGIFT_log10\n0\n1\n0.476\n0.100\n3,684\n\n142 \nChapter 4 \u25a0 Data Preparation\n+\n+\n+\n+\n+\n++\n++\n0.0\n0.38\n0.56\n0.71\n1.0\n1.0\n0.0\n0.14\n0.19\n0.24\n0.4\n0.41\n0.45\n0.5\n0.55\n0.61\n0.66\n0.73\n0.0\n0.13\n0.2\n0.25\n0.29\n0.43\n0.5\n0.55\n0.72\n0.74\n0.8\n0.85\n0.96\n0.0\n0.33\n0.67\n1.0\nAGE\n0\n0.100\n0.200\n0.300\n0.400\n0.500\n0.6", "0\n0.38\n0.56\n0.71\n1.0\n1.0\n0.0\n0.14\n0.19\n0.24\n0.4\n0.41\n0.45\n0.5\n0.55\n0.61\n0.66\n0.73\n0.0\n0.13\n0.2\n0.25\n0.29\n0.43\n0.5\n0.55\n0.72\n0.74\n0.8\n0.85\n0.96\n0.0\n0.33\n0.67\n1.0\nAGE\n0\n0.100\n0.200\n0.300\n0.400\n0.500\n0.600\n0.700\n0.799\n0.899\n1.000\nMAXRAMNT_log10\nLASTGIFT_log10\nRFA_2F\nFigure 4-23:  Box plots of normalized cluster inputs\nAfter building the five clusters, the summary statistics are shown in \nTable 4-26. Now the ratio of the maximum to minimum mean value of LASTGIFT \nand MAXRAMNT is closer to 2 rather than the 8 to 13 in the un-normalized \ndata. RFA_2F now takes a larger role in determining the clusters. \nTable 4-26: Cluster Summaries for Normalized Inputs\nCLUSTER COUNT MEAN AGE MEAN RFA_2F MEAN LASTGIFT MEAN MAXRAMNT\ncluster_1\n490\n0.670\n1\n0.036\n 0.0093\ncluster_3\n490\n0.683\n0.666\n0.042\n 0.0081\ncluster_0\n467\n0.314\n0.827\n0.044\n 0.0094\ncluster_2\n1,149\n0.698\n0.133\n0.073\n0.016\ncluster_4\n1,088\n0.356\n0.113\n0.075\n0.016\nThe change in the characteristics of the clusters can be seen even more clearly \nif ", "ter_0\n467\n0.314\n0.827\n0.044\n 0.0094\ncluster_2\n1,149\n0.698\n0.133\n0.073\n0.016\ncluster_4\n1,088\n0.356\n0.113\n0.075\n0.016\nThe change in the characteristics of the clusters can be seen even more clearly \nif the clusters are summarized by the variables in their original units, shown \nin Table 4-27. \n\n \nChapter 4 \u25a0 Data Preparation \n143\nTable 4-27: Summary of Clusters Built on Normalized Data with Inputs in Natural Units\nCLUSTER COUNT MEAN AGE\nMEAN RFA_2F\nMEAN LASTGIFT MEAN MAXRAMNT\ncluster_1\n490\n72.30\n4.00\n9.06\n14.27\ncluster_3\n490\n73.27\n3.00\n10.44\n13.10\ncluster_0\n467\n44.48\n3.48\n11.06\n14.39\ncluster_2\n1,149\n74.45\n1.40\n18.23\n21.28\ncluster_4\n1,088\n47.77\n1.34\n18.72\n20.95\nWhich cluster model is better: the one built on original variables or the \none built from normalized variables? That is not an easy question to answer. \nThe normalized model has less bias from LASTGIFT and MAXRAMNT. Clusters \n2 and 4 tell an interesting story: One cluster is younger, big-dollar donors (clus-\nter_4), and another clu", "uestion to answer. \nThe normalized model has less bias from LASTGIFT and MAXRAMNT. Clusters \n2 and 4 tell an interesting story: One cluster is younger, big-dollar donors (clus-\nter_4), and another cluster is older, big-dollar donors (cluster_2). Clusters 0, 1, \nand 3 all have similar LASTGIFT and MAXRAMNT values, but are differenti-\nated by AGE and RFA_2F. \nSummary\nData preparation clearly takes considerable thought and effort to ensure the data \nis presented to the algorithms in a way they can be used effectively. Predictive \nmodelers need to understand how the algorithms interpret the dat a so they \ncan prepare the data appropriately for the algorithm. The extensive discussion \nin this Chapter about how to correct for skewed distributions is not needed for \ndecision trees, for example, but can be necessary for linear regression. \nDo not consider Data Preparation a process that concludes after the fi rst pass. \nThis stage is often revisited once problems or defi ciencies are discovere", "ut can be necessary for linear regression. \nDo not consider Data Preparation a process that concludes after the fi rst pass. \nThis stage is often revisited once problems or defi ciencies are discovered while \nbuilding models. Feature creating, in particular, is iterative as you discover \nwhich kinds of features work well for the data. \nOverfi tting the data is perhaps the biggest reason predictive models fail when \ndeployed. You should always take care to construct the sampling strategy well \nso that overfi tting can be identifi ed and models adjusted appropriately.\n\n\n145\nThis chapter describes how to build and use association rules. Association \nrules have not always been in the mainstream of predictive analytics and aren\u2019t \nalways included in predictive analytics software packages even to this day. The \nname given to association rules in modeling software is not standardized; the \nalgorithms will sometimes be identifi ed as association rules, sometimes as item \nset mining or frequent", "o this day. The \nname given to association rules in modeling software is not standardized; the \nalgorithms will sometimes be identifi ed as association rules, sometimes as item \nset mining or frequent item set mining, sometimes even as market basket analysis \nafter a common application of association rules. \nThe purpose of association rules is to enumerate interesting interactions of \nvariables. While this sounds straightforward, the combinatorial explosion pre-\nvents one from fi nding all of the combinations using a brute-force approach. The \nalgorithms used to build association rules, such as the most popular algorithms \nApriori, Eclat, and FP-growth, use intelligent search to limit the full range of \nrules that could be found by eliminating branches of the search that no longer \napply. The primary difference between these algorithms is the effi ciency of the \nalgorithms on a particular data set.\nSuppose a grocery store chain wants to understand its customers\u2019 purchase \nbehavior by e", "The primary difference between these algorithms is the effi ciency of the \nalgorithms on a particular data set.\nSuppose a grocery store chain wants to understand its customers\u2019 purchase \nbehavior by examining which items are purchased together in a single visit. Ignore \nfor the time being multiple visits in the same day. There is no target variable in \nthis model because the decision makers are not interested in specifi c products \npurchased, but any product that is purchased and the relationships with the \npurchase of any product with the co-occurring purchases of any other product. \nC H A P T E R \n5\nItemsets and Association Rules\n\n146 \nChapter 5 \u25a0 Itemsets and Association Rules\nCustomers purchase a wide variety of items together. For example, the model \nshows that a high percentage of customers purchase bread and milk in the same \ntransaction, so if a customer purchases bread they are highly likely to purchase \nmilk, and vice versa. Milk, however, is purchased so frequently that many", "stomers purchase bread and milk in the same \ntransaction, so if a customer purchases bread they are highly likely to purchase \nmilk, and vice versa. Milk, however, is purchased so frequently that many prod-\nucts exhibit the same kind of behavior: Whether a customer purchases cereal, \nchicken, or Limburger cheese, there is still a high likelihood that milk is in the \nsame basket.\nHowever, the converse is not true: If one purchases milk, only rarely is \nLimburger cheese in that basket, in large part because very few customers \npurchase Limburger cheese.  Customers who purchases high-end, specialty \ncrackers, however, purchase Limburger cheese much more frequently than \nthe rate of purchasing Limburger cheese alone, which is low for this store. In \naddition, the purchasers of Limburger cheese spent more on average than the \naverage customer, a statistic that was computed after the model was computed \nand wasn\u2019t a part of the association rules model. As a result of the analysis, the \ndecis", " spent more on average than the \naverage customer, a statistic that was computed after the model was computed \nand wasn\u2019t a part of the association rules model. As a result of the analysis, the \ndecision-makers kept the high-end cracker on the shelf even though its sales \nwere not strong because of the strong association with Limburger cheese and \nthe type of customers who purchase Limburger cheese. \nThe most famous association rule is the fabled \u201cbeer and diaper\u201d association, \na story that is apparently true but has been retold in a variety of forms over \nthe decades. According to the story, a drug store was analyzing their data and \ndiscovered that between 5 p.m. and 7 p.m. on Friday there was an unusually \nhigh association between beer purchases and diaper purchases. Aside from the \namusing aspect of the story, it is a particularly good example of an algorithm \nfi nding unexpected patterns in the data that no analysts would have hypoth-\nesized as a potential. \nIt is precisely these ", "sing aspect of the story, it is a particularly good example of an algorithm \nfi nding unexpected patterns in the data that no analysts would have hypoth-\nesized as a potential. \nIt is precisely these kinds of interactions\u2014beer and diapers, or specifi c crack-\ners with high-valued cheese\u2014that association rules are particularly adept at \nfi nding. They can fi nd not only infrequent but unusual patterns, but also the \nmost frequent patterns in the data. \nTerminology\nTo help with the terminology of association rules, let\u2019s consider a simple super-\nmarket example, called market basket analysis. \nConsider the simple data set in Table 5-1. Each record represents a transac-\ntion. Only 12 of the 50 transactions are shown: the transactions with bread or \nmilk purchased. A purchase is indicated by the number 1 in the cell and no \npurchase of the product by an empty cell. The other products purchased are \nnot shown in the table. All the remaining transactions, 13 through 50, did not \ninclude milk ", "e number 1 in the cell and no \npurchase of the product by an empty cell. The other products purchased are \nnot shown in the table. All the remaining transactions, 13 through 50, did not \ninclude milk or bread in checkout.\n\n \nChapter 5 \u25a0 Itemsets and Association Rules \n147\nTable 5-1: Sample Supermarket Data\nTRANSACTION\nBREAD\nMILK\n1\n1\n2\n1\n3\n1\n4\n1\n5\n1\n1\n6\n1\n1\n7\n1\n1\n8\n1\n1\n9\n1\n1\n10\n1\n1\n11\n1\n12\n1\n. . .\n50\nIn each transaction, two products are listed: bread and milk. If either was \npurchased, the column representing the project in the transaction is labeled as \n1 if it was purchased and is empty if it wasn\u2019t.\nCondition\nA condition, or conditional statement, is a logical construct that evaluates to a true \nor false result. Conditions in association rules are built from nominal (categorical) \ninput variables, unlike decision trees that can build conditions for both nominal \nand continuous variables. The following are examples of conditions: \n \n\u25a0Product = \u201cBeer_brand_x\u201d\n \n\u25a0Product_Beer_brand_x =", " variables, unlike decision trees that can build conditions for both nominal \nand continuous variables. The following are examples of conditions: \n \n\u25a0Product = \u201cBeer_brand_x\u201d\n \n\u25a0Product_Beer_brand_x = 1\n \n\u25a0SKU = BU4C3D5R2Z6\n \n\u25a0Age in range 45 to 65\n \n\u25a0Bread = 1\n \n\u25a0Milk = \u201c\u201d\nThe fi rst three bullets are different ways to represent the same idea: The fi rst \nis Product as a multi-valued categorical variable, the second as an exploded \n\n148 \nChapter 5 \u25a0 Itemsets and Association Rules\nversion of the multi-valued categorical variable so that each product is a 1/0 \ndummy variable, and the third is the actual SKU for that product. The next \nbullet is a condition for the variable AGE. Note that continuous variables can \nbe used if they are binned prior to building the rules. The fi nal two conditions \nrefer to conditions related to products from Table 5-1.\nLeft-Hand-Side, Antecedent(s)\nAntecedents are the \u201cIf\u201d part of the rule. An association rule can be expressed \nby \u201cif-then\u201d or symbolically", "ons \nrefer to conditions related to products from Table 5-1.\nLeft-Hand-Side, Antecedent(s)\nAntecedents are the \u201cIf\u201d part of the rule. An association rule can be expressed \nby \u201cif-then\u201d or symbolically as left-hand-side (LHS) and right-hand-side (RHS) \nparts. An example rule containing milk and bread can be expressed in any of \nthe four ways:\nIf ( milk = 1 ) then ( bread = 1 )\nIf milk then bread\nmilk = 1 \u2192 bread = 1\nmilk \u2192 bread\nThe second and fourth examples assume implicitly that each of the two \nproducts were purchased. The antecedent in the rule is \u201cmilk = 1.\u201d There can \nbe any number of antecedents in the rule, where the combination of rules is \nalways an and function, such as:\nIf milk and peanut butter and jam then bread \nRight-Hand-Side, Consequent, Output, Conclusion\nThe right-hand-side of the rule is the \u201cthen\u201d part of the rule, bread in the examples \nfor the LHS rules. As with the LHS rules, there can be multiple conditions on \nthe RHS, although most implementations of associa", "side of the rule is the \u201cthen\u201d part of the rule, bread in the examples \nfor the LHS rules. As with the LHS rules, there can be multiple conditions on \nthe RHS, although most implementations of association rules limit the conclu-\nsion to a single condition. \nEvery rule must have a consequent. However, a rule does not have to have \nany antecedents. When no antecedents are in the rule, the rule is merely the \nconsequent itself, and match the records where the consequences is true. \nRule (Item Set)\nThe rule, or item set, is the full expression of the antecedents and consequent \ntogether, such as the rule \u201cif milk then bread.\u201d In rules with no antecedents, the \nrule is merely the consequent alone.\n\n \nChapter 5 \u25a0 Itemsets and Association Rules \n149\nSupport\nSupport, also called rule support, is defi ned as the number of times a rule occurs \nin the data divided by the number of transactions in the data. If the data is \narranged so that each record is one transaction, the number of transactions", "efi ned as the number of times a rule occurs \nin the data divided by the number of transactions in the data. If the data is \narranged so that each record is one transaction, the number of transactions is \nthe same as the number of records, and the calculation is the number of times \nthe rule is true divided by the number of records. \nConsider two rules built from Table 5-1:\nbread \u2192 milk\nmilk \u2192 bread\nThe support for both rules in Table 5-1 is 12 percent:\nRule 1: If bread then milk\nSupport\ntransactions withbread\nandmilk\ntotal transaction\n= #\n=\n=\n#\n1\n1\ns\nSupport =\n=\n%\n6\n50\n12\nRule 2: If milk then bread\nSupport\ntransactions withmilk\nandbread\ntotal transaction\n= #\n=\n=\n#\n1\n1\ns\nSupport =\n=\n%\n6\n50\n12\nWhile most often support is represented as a percentage of transactions, \nsometimes it is expressed as the actual number of transactions matching the rule.\nAntecedent Support\nSometimes the antecedent support is calculated and reported separately. \nAntecedent support is the percentage of transactio", "d as the actual number of transactions matching the rule.\nAntecedent Support\nSometimes the antecedent support is calculated and reported separately. \nAntecedent support is the percentage of transactions when the antecedent is \ntrue, regardless of whether the consequent is true.\nFor the data in Table 5-1, the antecedent support for bread is 20 percent (10 \npurchases in 50 transactions), and the antecedent support for milk is 16 per-\ncent (8 purchases in 50 transactions). If there were more products listed in \nTable 5-1, and the antecedents were \u201cbread and milk,\u201d the antecedent support \nof this two-condition antecedent would be 12 percent (6 occurrences of both \nmilk and bread divided by 50 transactions).\n\n150 \nChapter 5 \u25a0 Itemsets and Association Rules\nCon\ufb01 dence, Accuracy\nThe confi dence or accuracy of a rule is the percentage of transactions when the \nantecedents are true and the consequent is true divided by the number of rules \nwhere the antecedents are true, regardless of the value", "r accuracy of a rule is the percentage of transactions when the \nantecedents are true and the consequent is true divided by the number of rules \nwhere the antecedents are true, regardless of the value of the consequent\u2014or, \nin other words, the support divided by the antecedent support. \nFor the rule \u201cbread \u2192 milk,\u201d the support is 12 percent and the antecedent \nsupport is 20 percent, so the confi dence is 12 \u00f7 20, or 60 percent. \nLift\nThe lift of a rule is a measure of how many times more likely the consequent \nwill occur when the antecedent is true compared to how often the consequent \noccurs on it\u2019s own. Expressed differently, it is the confi dence of the rule divided \nby the baseline confi dence of the consequent alone. The baseline confi dence is \nthe percentage of records with the consequent true. \nTo summarize, Table 5-2 lists several measures of association for two condi-\ntions shown in Table 5-1: bread and milk purchased at checkout. The fi rst rule, \nwhere Bread is the antecede", "equent true. \nTo summarize, Table 5-2 lists several measures of association for two condi-\ntions shown in Table 5-1: bread and milk purchased at checkout. The fi rst rule, \nwhere Bread is the antecedent and Milk is the conseqeuent, can be written \u201cif \nBread then Milk.\u201d It matches 12 percent of the records, Milk on its own matches \n16 percent of the records, and the rule has a confi dence of 60 percent, meaning \nthat if someone purchased Bread (the antecedent), that person purchased Milk \nin 60 percent of transactions. The lift, 3.75, indicates that if the person purchased \nbread, the person is 3.75 times more likely to purchase Milk as well compared \nto all transactions. \nInterestingly, the lift is the same for the reverse of the rule, \u201cif Milk then Bread,\u201d \nthough the rule confi dence is higher (75 percent compared with 60 percent). Even \nthough the rule confi dence is higher, the lift is the same because the baseline \nconfi dence is also higher.\nTable 5-2: Key Measures of Association", "higher (75 percent compared with 60 percent). Even \nthough the rule confi dence is higher, the lift is the same because the baseline \nconfi dence is also higher.\nTable 5-2: Key Measures of Association Rules for Simple Market Basket Data \nCONSEQUENT\nANTECEDENT\nRULE \nSUPPORT \n%\nBASELINE \nCONFIDENCE \n%\nRULE \nCONFIDENCE \n%\nLIFT\nMilk\nBread\n12\n16\n60\n3.75\nBread\nMilk\n12\n20\n75\n3.75\nNote that while the support for both of the rules is the same, the confi dence \nand lift are different; the direction of the rule matters. \n\n \nChapter 5 \u25a0 Itemsets and Association Rules \n151\nParameter Settings\nThe primary purposes for parameter settings in association rules are to reduce \nthe number of rules identifi ed by the algorithms and therefore speed up the \nprocess of fi nding the rules. This is a concern because of the combinatory explo-\nsion that occurs when the item set length increases. If there are 50 products to \nconsider, there are 1,225 two-way combinations, 19,600 three-way combinations, \nand 230,300", " of the combinatory explo-\nsion that occurs when the item set length increases. If there are 50 products to \nconsider, there are 1,225 two-way combinations, 19,600 three-way combinations, \nand 230,300 four-way combinations. Clearly, with the vast number of rules \ngenerated from four and more antecedents , there is a need to reduce the set of \nrules that should be examined by the modeler.\nCommon parameter settings include:  \n \n\u25a0Minimum support\n \n\u25a0Minimum confi dence\n \n\u25a0Maximum number of antecedents or itemset length (# antecedents + 1 \nfor the consequent)\n \n\u25a0Maximum number of rules\nThere is no theory to tell the modeler what the minimum or maximum values \nset by these parameters should be. If minimum support is set too high, no rules \nwill be reported. If they are set too low, no rules are fi ltered and there may still \nbe large numbers of rules. Sometimes the modeler will have intuition about \nthe values of support. Sometimes a minimum confi dence will be required in \nthe business obje", "fi ltered and there may still \nbe large numbers of rules. Sometimes the modeler will have intuition about \nthe values of support. Sometimes a minimum confi dence will be required in \nthe business objectives.\nOne effective practice is to build rules with only one antecedent, then two \nantecedents, and continue to increase antecedents only after adjusting other \nparameters, such as minimum support or confi dence, based on what is observed \nwith a few antecedents.\nHow the Data Is Organized\nPredictive modeling data consists of rows and columns, where rows describe a \nunit of analysis and columns a description of that unit of analysis. For customer \nanalytics, a row may be a customer, and the columns, the set of descriptors of \nthat customer.\nStandard Predictive Modeling Data Format\nAssociation rule data can be built from data where each record is a transaction \nand each column a product. For problems other than market basket analysis, \n\n152 \nChapter 5 \u25a0 Itemsets and Association Rules\nthe s", " rule data can be built from data where each record is a transaction \nand each column a product. For problems other than market basket analysis, \n\n152 \nChapter 5 \u25a0 Itemsets and Association Rules\nthe same principles apply. For an invoice fraud detection problem, each row \ncan represent an invoice and the columns describe the characteristics of the \ninvoice. Association rules can be built from all of the categorical variables that \ndescribe the invoice. \nIn this layout, the columns used to build the association rules are either \nantecedents or consequents; each record has a unique record ID (a transaction \nin Table 5-1). The modeler must defi ne which columns represent antecedents \nand which column is the consequent.\nThe problem with this format is that the width of the data can be extraordi-\nnarily large. Consider an example with a supermarket building association rules \nto identify products purchased at checkout (market basket analysis). If there \nare 20,000 SKUs to be tracked and eval", "-\nnarily large. Consider an example with a supermarket building association rules \nto identify products purchased at checkout (market basket analysis). If there \nare 20,000 SKUs to be tracked and evaluated individually, the data has to have \n20,000 columns in the data. This is a challenge for many predictive analytics \nsoftware packages. In addition, the representation of the data will be sparse. \nIf, on average, only 5 SKUs are in any basket, 5 of the 20,000 columns in each \nrecord (transaction) will have a 1, indicating it is included in the transaction, \nand 19,995 will be 0.\nTransactional Format\nIn transactional format, there are only a few columns but many more rows. \nEach row represents a single item to be associated with others, often a product \nname or product code. The only columns are Transaction ID and Item. (One \nmay also have the item value or other ancillary pieces of information to aid in \ninterpretation, such as customer ID and store ID, but these are not necessary \nfor", "s are Transaction ID and Item. (One \nmay also have the item value or other ancillary pieces of information to aid in \ninterpretation, such as customer ID and store ID, but these are not necessary \nfor building the association rules.)\nIn this format, the association rules fi nd relationships between rows having \nthe same transaction ID rather than columns with values that co-occur in the \nsame record. If ten products were purchased at checkout, that transaction would \nbe represented by ten rows of data. Table 5-3 contains the same information \nas Table 5-1, but with each record representing a product in the shopping cart \nrather than containing all the products in the shopping cart. Transaction ID 5, \nfor example, is represented in two records in Table 5-3 but only one record in \nTable 5-1.\nTable 5-3: Sample Supermarket Data in Transactional Format\nTRANSACTION\nPRODUCT PURCHASED\n1\nBread\n2\nBread\n\n \nChapter 5 \u25a0 Itemsets and Association Rules \n153\n3\nBread\n4\nBread\n5\nBread\n5\nMilk\n6\nBread\n6\nMi", "\nTable 5-3: Sample Supermarket Data in Transactional Format\nTRANSACTION\nPRODUCT PURCHASED\n1\nBread\n2\nBread\n\n \nChapter 5 \u25a0 Itemsets and Association Rules \n153\n3\nBread\n4\nBread\n5\nBread\n5\nMilk\n6\nBread\n6\nMilk\n7\nBread\n7\nMilk\n8\nBread\n8\nMilk\n9\nBread\n9\nMilk\n10\nBread\n10\nMilk\n11\nMilk\n12\nMilk\n. . .\n50\nThe advantage of the transactional format is that the width of the data no \nlonger becomes a problem. In the example with 20,000 SKUs, rather than having \n20,000 columns in the data, you have only one column of SKUs, though many \nmore rows. In addition to transactional format being narrower, it is also a more \neffi cient representation of the data. In the format with one record per transaction, \na cell exists for every SKU in a row of the data, meaning that the vast majority \nof cell values will be 0, indicating the SKU was not a part of the transaction. If \nthe average basket size of a transaction is 5 SKUs, there are only fi ve records \nper transaction in the transactional format. \nConsider a data s", "icating the SKU was not a part of the transaction. If \nthe average basket size of a transaction is 5 SKUs, there are only fi ve records \nper transaction in the transactional format. \nConsider a data set with 1,000,000 transactions, 20,000 SKUs, and the same \naverage of 5 SKUs per transaction. In the standard predictive analytics format, \nyou have 1,000,000 rows and 20,000 columns, or 20 billion cells in the data con-\ntaining SKUs with 1 or 0 values indicating they were included in the transaction. \nOf the 20 billion cells, only 1 billion are populated. \nBy way of contrast, the transactional format will have 1,000,000 transactions \ntimes 5 SKUs per transaction only, or 5,000,000 records and two columns.\n\n154 \nChapter 5 \u25a0 Itemsets and Association Rules\nMeasures of Interesting Rules\nNow that dozens, hundreds, or even thousands of rules have been identifi ed \nin the data, what next? How does one identify which rules are interesting or \ngood? This depends on the purpose of the model. \nFor s", "dozens, hundreds, or even thousands of rules have been identifi ed \nin the data, what next? How does one identify which rules are interesting or \ngood? This depends on the purpose of the model. \nFor some applications, support is the key: You are trying to discover which \nrules occur most often. For other applications, high confi dence is key: You are \ntrying to fi nd rules whose consequent occurs more often in subsets of the data \ndefi ned by the antecedents. Or perhaps you would like to identify the highest \nlift rules. In still other applications, confi dence is important, but only if the sup-\nport is high enough. Interpretation of these rules is subjective, depending on \nwhat the analyst and decision-maker believes is most interesting.\nConsider association rules built on the KDD Cup 1998 data on a subset of \nthe data including only donors (TARGET_D > 0). The data was set up in the \nstandard predictive modeling format where each record was a donor. Several \nof the continuous variable", "8 data on a subset of \nthe data including only donors (TARGET_D > 0). The data was set up in the \nstandard predictive modeling format where each record was a donor. Several \nof the continuous variables were binned into quintiles, including LASTGIFT, \nAVGGIFT, NGIFTALL, and TARGET_D. The consequent was set as the top \nquintile of giving: TARGET_D between $20 and $200. The rules were limited \nto a maximum of two antecedents, a minimum support of 5 percent, and a \nminimum confi dence of 1 percent. More than 1,400 rules were found using the \nApriori algorithm.\nAfter sorting by confi dence, the top rules are shown in Table 5-4. As is typi-\ncal when sorting by confi dence, many of the rules are just above the minimum \nsupport threshold, although the top rule is considerably above this threshold. \nNote, too, that the fi fth and sixth ranked rules both have support of 5.492 per-\ncent and confi dence of 87.218, strong evidence that these two rules match the \nidentical records. \nTable 5-4: List ", " \nNote, too, that the fi fth and sixth ranked rules both have support of 5.492 per-\ncent and confi dence of 87.218, strong evidence that these two rules match the \nidentical records. \nTable 5-4: List of Association Rules Sorted by Con\ufb01 dence\nCONSEQUENT\nANTECEDENT\nSUPPORT \n%\nCONFIDENCE \n%\nLIFT\nTARGET_D = [ 20, 200 ]\nRFA_2A = G and \nAVGGIFT = [ 15, 450 ]\n11.5\n88.9\n2.82\nTARGET_D = [ 20, 200 ]\nRFA_2 = L1G and \nAVGGIFT = [ 15, 450 ]\n6.5\n88.2\n2.80\nTARGET_D = [ 20, 200 ]\nRFA_3 = A1G and \nLASTGIFT = [ 20, 450 ]\n5.5\n87.7\n2.78\nTARGET_D = [ 20, 200 ]\nRFA_4 = A1G and \nLASTGIFT = [ 20, 450 ]\n5.4\n87.5\n2.77\n\n \nChapter 5 \u25a0 Itemsets and Association Rules \n155\nTARGET_D = [ 20, 200 ]\nRFA_3 = A1G and \nRFA_2 = L1G\n5.45\n87.2\n2.77\nTARGET_D = [ 20, 200 ]\nRFA_3 = A1G and \nRFA_2F = 1.0\n5.5\n87.2\n2.77\nTARGET_D = [ 20, 200 ]\nRFA_5 = A1G and \nLASTGIFT = [ 20, 450 ]\n5.1\n87.1\n2.76\nTARGET_D = [ 20, 200 ]\nRFA_4 = A1G and \nRFA_3 = A1G\n5.6\n87.0\n2.76\nTARGET_D = [ 20, 200 ]\nRFA_2 = L1G and \nLASTGIFT = [ 20, 450 ]\n7.6\n87.0\n", ", 200 ]\nRFA_5 = A1G and \nLASTGIFT = [ 20, 450 ]\n5.1\n87.1\n2.76\nTARGET_D = [ 20, 200 ]\nRFA_4 = A1G and \nRFA_3 = A1G\n5.6\n87.0\n2.76\nTARGET_D = [ 20, 200 ]\nRFA_2 = L1G and \nLASTGIFT = [ 20, 450 ]\n7.6\n87.0\n2.76\nTARGET_D = [ 20, 200 ]\nRFA_4 = A1G and \nRFA_2 = L1G\n5.4\n87.0\n2.76\nTARGET_D = [ 20, 200 ]\nRFA_4 = A1G and \nRFA_2F = 1.0\n5.4\n87.0\n2.76\nTARGET_D = [ 20, 200 ]\nRFA_3 = A1G\n5.7\n87.0\n2.76\nTARGET_D = [ 20, 200 ]\nRFA_3 = A1G and \nRFA_2A = G\n5.7\n87.0\n2.76\nTARGET_D = [ 20, 200 ]\nRFA_2A = G and \nLASTGIFT = [ 20, 450 ]\n13.2\n86.9\n2.76\nTARGET_D = [ 20, 200 ]\nAVGGIFT = [ 15, 450 ] \nand LASTGIFT = [ 20, \n450 ]\n18.2\n86.8\n2.75\nTARGET_D = [ 20, 200 ]\nRFA_4 = A1G\n5.6\n86.7\n2.75\nTARGET_D = [ 20, 200 ]\nRFA_4 = A1G and \nRFA_2A = G\n5.6\n86.7\n2.75\nTARGET_D = [ 20, 200 ]\nRFA_5 = A1G and \nRFA_3 = A1G\n5.1\n86.7\n2.75\nA second list is shown in Table 5-5, this time sorting the rules by support. \nThe top rule matches nearly 60 percent of the population, although it has a lift \nbelow 1.0, meaning that when PEPSTRFL is t", "second list is shown in Table 5-5, this time sorting the rules by support. \nThe top rule matches nearly 60 percent of the population, although it has a lift \nbelow 1.0, meaning that when PEPSTRFL is true, the likelihood that a donor is \nin the top giving group is smaller than average. One interesting rule is the fi fth, \nwith the antecedent LASTGIFT in the range 20 to 450. This matches more than \n27 percent of the population and still has confi dence nearly as high as the top \nconfi dence rules. Just below this rule are four rules that apparently match the \nsame records as evidenced by their identical support and confi dence values: \nThese are redundant interactions.\n\n156 \nChapter 5 \u25a0 Itemsets and Association Rules\nTable 5-5: List of Association Rules Sorted by Support\nCONSEQUENT\nANTECEDENT \nSUPPORT %\nCONFIDENCE \n%\nLIFT\nTARGET_D = [ 20, 200 ]\nPEPSTRFL\n59.6\n21.1\n0.67\nTARGET_D = [ 20, 200 ]\nRFA_2A = F\n42.5\n42.9\n1.36\nTARGET_D = [ 20, 200 ]\nRFA_2F = 1.0\n37.0\n53.3\n1.69\nTARGET_D = [ 20, 200 ", "T \nSUPPORT %\nCONFIDENCE \n%\nLIFT\nTARGET_D = [ 20, 200 ]\nPEPSTRFL\n59.6\n21.1\n0.67\nTARGET_D = [ 20, 200 ]\nRFA_2A = F\n42.5\n42.9\n1.36\nTARGET_D = [ 20, 200 ]\nRFA_2F = 1.0\n37.0\n53.3\n1.69\nTARGET_D = [ 20, 200 ]\nRFA_2A = E\n28.9\n4.1\n0.13\nTARGET_D = [ 20, 200 ]\nLASTGIFT = [ 20, \n450 ]\n27.6\n79.6\n2.52\nTARGET_D = [ 20, 200 ]\nRFA_2 = L1F\n24.2\n52.3\n1.66\nTARGET_D = [ 20, 200 ]\nRFA_2 = L1F and \nRFA_2F = 1.0\n24.2\n52.3\n1.66\nTARGET_D = [ 20, 200 ]\nRFA_2 = L1F and \nRFA_2A = F\n24.2\n52.3\n1.66\nTARGET_D = [ 20, 200 ]\nRFA_2F = 1.0 \nand RFA_2A = F\n24.2\n52.3\n1.66\nTARGET_D = [ 20, 200 ]\nAVGGIFT = [ 15, \n450 ]\n24.0\n75.5\n2.40\nTARGET_D = [ 20, 200 ]\nNGIFTALL_\nbin_3 [ 7, 11 ]\n23.0\n30.9\n0.98\nTARGET_D = [ 20, 200 ]\nLASTGIFT_bin_4 \n[ 15, 19 ]\n22.8\n31.0\n0.98\nTARGET_D = [ 20, 200 ]\nRFA_2A = E and \nPEPSTRFL\n22.5\n3.8\n0.12\nTARGET_D = [ 20, 200 ]\nLASTGIFT_bin_4 \n[ 15, 19 ] and \nRFA_2A = F\n22.2\n30.3\n0.96\nDeploying Association Rules\nIn many applications, association rules are built solely to understand the data \nbetter. However, t", " = [ 20, 200 ]\nLASTGIFT_bin_4 \n[ 15, 19 ] and \nRFA_2A = F\n22.2\n30.3\n0.96\nDeploying Association Rules\nIn many applications, association rules are built solely to understand the data \nbetter. However, there are other applications where deployment of the rules \nis desirable. For example, you can build rules for cross-sell or up-sell of retail \nproducts. You may fi nd a rule that identifi es that if a customer buys a particular \npair of black pants, he also tends to buy a black belt. The model can be used to \n\n \nChapter 5 \u25a0 Itemsets and Association Rules \n157\nidentify customers who have purchased the black pants, but do not have the \nblack belt in their basket: These customers are good candidates for additional \ntreatment or incentives to encourage the purchase that many others have made.\nWhen deploying the rules, they are implemented in the order specifi ed by \nthe analyst: by confi dence, support, antecedent support, lift, or some other cri-\nterion of interest. However, for applications ", " deploying the rules, they are implemented in the order specifi ed by \nthe analyst: by confi dence, support, antecedent support, lift, or some other cri-\nterion of interest. However, for applications such as product recommendations \nand couponing, you should check not only if the antecedent products are in \nthe basket, but also if the consequent product is in the basket. Some products \nare purchased only once (the consequent) and should not be encouraged to be \npurchased again. Other products can be purchased in greater number; they can \nbe recommended even if already purchased.\nRedundant rules never fi re. In Table 5-5, if the antecedent RFA_2 = L1F is in the \nrecord, the consequent, TARGET_D = [20, 200 ], is expected to be true. However, \nthe next three rules in the list will never fi re because no matches remain after \nthe RFA_2 = L1F rule fi res. \nVariable Selection\nOne variable selection strategy is to identify redundant variables and keep one \nof them for modeling, removing all t", " because no matches remain after \nthe RFA_2 = L1F rule fi res. \nVariable Selection\nOne variable selection strategy is to identify redundant variables and keep one \nof them for modeling, removing all the others. This is often done by examining \nthe correlations of candidate input variables. However, this works for continu-\nous variables only. \nAssociation rules can be used to compare all categories in much the same \nway. After building the association rules, antecedents with identical support \nand confi dence are identifi ed. Only the variables in antecedents from one of the \nredundant rules are kept; the other variables are discarded. This works most \neasily with simple rules (two antecedents), but can be extended to more.\nInteraction Variable Creation\nSome algorithms are \u201cmain effect\u201d models and are not able to identify interac-\ntions directly. Linear and logistic regression, k-nearest neighbor, and K Means \nclusters are all examples. However, interactions are often useful and sometim", "els and are not able to identify interac-\ntions directly. Linear and logistic regression, k-nearest neighbor, and K Means \nclusters are all examples. However, interactions are often useful and sometimes \ncritical to building accurate predictive models. \nCreating interactions is most often either a manual process, where each inter-\naction is hypothesized and then built one at a time by the analyst, or an exhaus-\ntive process whereby algorithms include every interaction. If one begins with \n50 input variables, there are 1,225 two-way interactions either to test manually, \nor all are included in the model. Association rules can be used to fi nd the best \n\n158 \nChapter 5 \u25a0 Itemsets and Association Rules\ninteractions by searching exhaustively through all possible combinations \nof interactions and listing them by their association to the target variable.\nThe three-step process for fi nding interactions using association rules is as \nfollows:\n \n1. Set up the association rules to have one cons", "nd listing them by their association to the target variable.\nThe three-step process for fi nding interactions using association rules is as \nfollows:\n \n1. Set up the association rules to have one consequent: the predictive model \ntarget variable. \n \n2. Set the parameters to include only two or three antecedents depending \non whether one is trying to fi nd two-way or three-way interactions. If the \nsoftware limits by the itemset length rather than antecedents, choose an \nitemset length of 3 for two-way interactions or 4 for three-way interactions. \n \n3. After building the rules, sort them by confi dence. Include the interactions \nwith the highest confi dence if they match different records (don\u2019t select \ntwo interactions with identical confi dence and support). \nOnce the top rules have been identifi ed, these defi nitions can be used to build \nnew columns as new dummy variables that indicate this combination. \nHowever, the three-step process could result in far too many rules for practi", " identifi ed, these defi nitions can be used to build \nnew columns as new dummy variables that indicate this combination. \nHowever, the three-step process could result in far too many rules for practi-\ncal use. If too many interactions remain after applying Step 3, the approaches \nfor reducing described in the section \u201cVariable Selection Prior to Modeling,\u201d \nin Chapter 4. can be used, including applying statistical tests and computing \ncorrelations. Including only rules that domain experts recognize as interesting \ninteractions can also help remove rules. \nProblems with Association Rules\nThere are still many challenges to building, understanding, and deploying \nassociation rules. \nRedundant Rules\nYou have already seen examples of redundant rules in previous sections. If \nthe software supports the removal of these rules easily through a rule editor, \nthe redundant rules can be deselected. Otherwise, the analyst must remove \nthe variables before building the association rules. Removing r", "rts the removal of these rules easily through a rule editor, \nthe redundant rules can be deselected. Otherwise, the analyst must remove \nthe variables before building the association rules. Removing redundant rules \ndoes not affect how the association rules are deployed, only the effi ciency of \nthe rule deployment. However, the time savings can be signifi cant.\nToo Many Rules\nSometimes so many rules are found that navigating the rules to fi nd what \nis interesting becomes very diffi cult. In these cases, removing redundant \n\n \nChapter 5 \u25a0 Itemsets and Association Rules \n159\nrules, increasing thresholds, or reducing the number of input variables (if pos-\nsible) can help focus the analysis. If the number of antecedents or the item set \nlength is large, the size can quickly exceed what is expedient to compute.\nToo Few Rules\nIf the minimum support or confi dence thresholds are too high, no rules may \nbe found. Lowering these thresholds can help, although knowing specifi cally \nwhat thresh", "pedient to compute.\nToo Few Rules\nIf the minimum support or confi dence thresholds are too high, no rules may \nbe found. Lowering these thresholds can help, although knowing specifi cally \nwhat threshold is low enough is not always obvious.\nBuilding Classi\ufb01 cation Rules from Association Rules\nWhen the consequent of the association rules is limited to a single variable, asso-\nciation rules appear very similar to classifi cation models. They have inputs and \nan output. They generate rules, very much like decision trees. One can measure \nthe accuracy of the rule much like classifi cation accuracy or lift. \nHowever, there are also important differences, such as: \n \n\u25a0Association rules can have multiple consequents, one consequent condi-\ntion per rule. \n \n\u25a0Association rules, in some implementations, can even have multiple \nconditions as a part of the consequent. \n \n\u25a0Classifi cation fi nds the best way to associate inputs with the output. \nAssociation rules fi nd all associations between the ", "ons, can even have multiple \nconditions as a part of the consequent. \n \n\u25a0Classifi cation fi nds the best way to associate inputs with the output. \nAssociation rules fi nd all associations between the inputs and the outputs. \n \n\u25a0Most classifi cation algorithms operate on both continuous and categorical \ndata. Association rules operate only on categorical data. \n \n\u25a0Most classifi cation algorithms guard against overfi t, whether directly or \nindirectly. Association rules don\u2019t consider overfi t or underfi t, although \none can increase support to decrease the likelihood of fi nding happen-\nstance rules.\nAssociation rules can be converted into a viable, supervised learning clas-\nsifi cation algorithm by applying a few additional constraints. Some commercial \ntools support building classifi cation rules, greatly simplifying the task of the \nanalyst to produce them. \nTwo steps are often applied when building classifi cation rules:\n \n\u25a0Allow only one consequent, the target variable. Some implem", "on rules, greatly simplifying the task of the \nanalyst to produce them. \nTwo steps are often applied when building classifi cation rules:\n \n\u25a0Allow only one consequent, the target variable. Some implementations \nof association rules do not allow this limitation, and therefore rules will \nhave to be fi ltered after they are built to eliminate all consequents except \nfor the target variable.\n\n160 \nChapter 5 \u25a0 Itemsets and Association Rules\n \n\u25a0Include rules that, after applying the rules in sequence (such as by con-\nfi dence), match a suffi cient number of records to produce a statistically \nsignifi cant prediction. Signifi cance can be measured by tests such as the \nchi-square test.\nTable 5-6 shows classifi cation rules for a fraud detection application. The con-\nsequent is the fraud variable, a 1/0 outcome, and the rules in the table measure \nthe confi dence that the transaction was not fraud. Top rules where classifi cation \nrules were applied to a new data set are included in the table", ", a 1/0 outcome, and the rules in the table measure \nthe confi dence that the transaction was not fraud. Top rules where classifi cation \nrules were applied to a new data set are included in the table from the thousands \nof rules found. \nThe fi rst rule, 278, will match the same records when applying the rule as it \ndid on the training data. However, once this rule matches, those 11,809 records \nhave already been classifi ed. The second and third rules (2255 and 1693) have \nslightly smaller confi dence in selecting non-fraudulent transactions because they \nmatched transactions already classifi ed by the higher-accuracy Rule 278 that \npreceded them. This is even more evident with Rules 2258 and 1337 where only \n16 and 20 transactions, respectively, match. On the training data, not shown in \nthe table, these matched more than 4,000 transactions each.\nTable 5-6: Applying Rules in Sequence\nRULE ID\n# RECORDS \nHIT BY RULE IN \nSEQUENCE\nCONFIDENCE \uf6aeFROM \nMODEL\uf6af %\nCONFIDENCE \nAPPLYING RULE IN \n", "\nthe table, these matched more than 4,000 transactions each.\nTable 5-6: Applying Rules in Sequence\nRULE ID\n# RECORDS \nHIT BY RULE IN \nSEQUENCE\nCONFIDENCE \uf6aeFROM \nMODEL\uf6af %\nCONFIDENCE \nAPPLYING RULE IN \nSEQUENCE %\n278\n11,809\n99.53\n99.53\n2255\n7,215\n99.50\n99.21\n1693\n1,927\n99.49\n98.91\n2258\n16\n99.49\n93.75\n1337\n20\n99.48\n95.00\n993\n2,727\n99.48\n98.90\n977\n2\n99.47\n100.0\n2134\n2,516\n99.46\n98.77\n1783\n4,670\n99.46\n99.07\n984\n6\n99.46\n100.0\nTable 5-6 requires pruning to eliminate rules that, when applied in the sequence, \nno longer provide a stable prediction of the target variable.\n\n \nChapter 5 \u25a0 Itemsets and Association Rules \n161\nSummary\n Association rules provide a fast and fl exible way to identify patterns in categori-\ncal variables. Their speed allows them to be applied to large data sets. Their \nfl exibility allows them to be applied in a variety of ways, including building \nsupervised or unsupervised models and fi nding good interaction effects for \nuse with other modeling algorithms. \n\n\n163\nDescr", "bility allows them to be applied in a variety of ways, including building \nsupervised or unsupervised models and fi nding good interaction effects for \nuse with other modeling algorithms. \n\n\n163\nDescriptive modeling algorithms are also called unsupervised learning \nmethods, meaning that the algorithms try to fi nd relationships between inputs \nrather than to a specifi c target variable. It is an appropriate technique for model-\ning problems when a target variable cannot be quantifi ed or when the purpose \nof modeling is only to fi nd relationships between inputs.\nConsider a company seeking to design marketing programs that are effective \nfor their customers. A single program for all customers may be effective, but \nusually a company will identify customer segments or subgroups with simi-\nlar characteristics to each other, but different from the other segments. These \nsegments can form the basis of separate, more specifi c marketing campaigns \nbased on attributes such as age, sex, incom", "ar characteristics to each other, but different from the other segments. These \nsegments can form the basis of separate, more specifi c marketing campaigns \nbased on attributes such as age, sex, income, socio-economic status, geographic \nregion, and more. \nUnsupervised learning methods discover the best way to segment the data; \nthe algorithms fi nd which attributes drive the formation of the segments, how \nlarge these segments are naturally in the data, and what the statistical charac-\nteristics of these segments are. For example, a company may discover from its \ndata that 10 percent of its customers are female homeowners in their thirties \nliving in the northeast and any of several levels of education. This segment \nsays nothing about any target variables, such as customer lifetime value (CLV), \nunless the target variable is also treated as an input to the model. \nC H A P T E R \n6\nDescriptive Modeling\n\n164 \nChapter 6 \u25a0 Descriptive Modeling\nOr consider another example. A company would", " (CLV), \nunless the target variable is also treated as an input to the model. \nC H A P T E R \n6\nDescriptive Modeling\n\n164 \nChapter 6 \u25a0 Descriptive Modeling\nOr consider another example. A company would like to understand the risk \nassociated with making a loan to an applicant so they can apply additional \nscrutiny to potentially high-risk loan applicants. The company collected years \nof data including measures of which customers successfully paid their loans in \nfull and which customers defaulted on their loan, so on the surface, this appears \nto be a good candidate for a supervised learning approach to predictive model-\ning: predict the likelihood a customer will default on a loan given the known \nattributes of that customer at the time the decision is made to issue the loan or \ndeny it. The company has created a label with value 1 for those customers who \ndefaulted, and a label with value 0 for those who did not default.\nHowever, there can be problems with this approach. First, it can", "he company has created a label with value 1 for those customers who \ndefaulted, and a label with value 0 for those who did not default.\nHowever, there can be problems with this approach. First, it can take years to \nprosecute a customer for default on a loan. The more time that elapses between \nthe decision to issue the loan and the default, the more diffi cult it is to use the \ndefault action as a target variable because the economic circumstances of loan \napplicants changes with time; fi ve years after a loan is issued, circumstances \nthat led to an individual\u2019s default may no longer exist. Second, a customer may \nnot legally default, but may still represent high risk. The loan may be written \noff rather than being labeled a \u201cdefault.\u201d Or the loan may have been delinquent \nfor many months and a fi nal settlement was reached with the customer to avoid \nlegal battles. These loans may be labeled \u201cnot default,\u201d which is technically true, \nbut the label doesn\u2019t accurately refl ect the tru", "s and a fi nal settlement was reached with the customer to avoid \nlegal battles. These loans may be labeled \u201cnot default,\u201d which is technically true, \nbut the label doesn\u2019t accurately refl ect the true risk of the loan.\nBecause of these kinds of complications with supervised learning techniques \napplied to identifying risk, some companies opt for building an unsupervised \nlearning model to identify unusual or anomalous customers that may be asso-\nciated with higher risk. The thinking goes like this: The vast majority of cus-\ntomers will not default, and their attributes should look \u201cnormal.\u201d However, \nsome customers will have unusual fi nancial characteristics that should lead to \nincreased scrutiny of their application. These anomalous customers could be \nfl agged for additional screening prior to loan approval. \nThe unsupervised modeling algorithms described here are the ones most \ncommonly implemented in predictive software: K-Means clustering, Kohonen, \nSelf-Organizing Maps (SOM), ", " prior to loan approval. \nThe unsupervised modeling algorithms described here are the ones most \ncommonly implemented in predictive software: K-Means clustering, Kohonen, \nSelf-Organizing Maps (SOM), and Principal Component Analysis (PCA). \nData Preparation Issues with Descriptive Modeling\nAll three of these algorithms have several requirements for data preparation. \nDetails of how to prepare data for clustering are provided in Chapter 4. First, the \ninputs must be numeric. If one would like a categorical variable to be included \nin the model, it must be converted to a number. Most commonly, this is done \nthrough exploding the categorical variable, with N levels or values, into N \ndummy variables. \nSecond, all the data must be populated; there can be no missing values. Any \nmissing values need to be imputed with a numeric value, or records with any \n\n \nChapter 6 \u25a0 Descriptive Modeling \n165\nmissing values should be removed (listwise deletion). Third, usually, practitioners \ntry to trans", "need to be imputed with a numeric value, or records with any \n\n \nChapter 6 \u25a0 Descriptive Modeling \n165\nmissing values should be removed (listwise deletion). Third, usually, practitioners \ntry to transform inputs so that they are more nearly normally distributed. This \nincludes removing severe outliers and reducing skew (positive and negative). \nThis chapter explains key ideas for descriptive modeling. The algorithms \nincluded in the chapter are the three that are found most commonly in the \nleading predictive analytics software packages and are used most often when \nanalyzing large data.\nPrincipal Component Analysis\nPrincipal Component Analysis (PCA) is an old and well-known algorithm, \nand one that is available in nearly every predictive analytics software package. \nHowever, it isn\u2019t always described in predictive analytics and data mining text-\nbooks, especially those books that present algorithms from a machine learning \nperspective. \nPCA is often understood as a dimensionality redu", "ys described in predictive analytics and data mining text-\nbooks, especially those books that present algorithms from a machine learning \nperspective. \nPCA is often understood as a dimensionality reduction method: a technique \nto reduce the number of inputs to predictive models by combining original vari-\nables that are correlated with each other into a smaller number of independent, \ninformation-rich variables. But PCA can also be used to describe interesting \ncharacteristics of data, identifying variables that are correlated with each other.\nThe PCA Algorithm\nWe begin with two variables from the nasadata data set shown in the scatterplot \nin Figure 6-1, Band1 and Band2. The narrow band of values in the cigar-shaped \ncloud of data indicates that these two variables are strongly correlated with each \nother. In fact, their correlation coeffi cient is +0.855\u2014very high. \nFrom a predictive modeling perspective, if you include both of these variables \nas inputs to the models, are you gettin", " with each \nother. In fact, their correlation coeffi cient is +0.855\u2014very high. \nFrom a predictive modeling perspective, if you include both of these variables \nas inputs to the models, are you getting two ideas? Or, mathematically speak-\ning, how many independent ideas are represented as inputs? Independence \nis an assumption in many modeling algorithms, not least of which is linear \nregression. When two variables are correlated, they have shared variance, or \nco-variance. Coeffi cients in a linear regression model therefore do not convey \nthe full story in the predictive model of how much infl uence a variable really \nhas in predicting the outcome. \nThe principal components (PCs) in a PCA model identify the direction where \nthe spread of the data is maximized. In two dimensions, this corresponds to the \nimage in Figure 6-2: The fi rst principal component (PC1) is the direction where \nthe variance is largest, the arrow pointing to the upper right, and by extension, \nextending also dow", "responds to the \nimage in Figure 6-2: The fi rst principal component (PC1) is the direction where \nthe variance is largest, the arrow pointing to the upper right, and by extension, \nextending also downward and to the left. The second PC is by defi nition of the \nalgorithm orthogonal to the fi rst, where orthogonal means it is perpendicular \nbut in multidimensional space. Because these variables are orthogonal, their \ncorrelation is exactly 0, or in other words, they are mathematically independent \n\n166 \nChapter 6 \u25a0 Descriptive Modeling\nfrom one another. There is no shared variance, or stated equivalently: One \ncannot predict one projection from the other. This is ideal for linear regression \nmodels, for example, because the regression algorithm assumes independence \nof the input variables. It can also be benefi cial for other numeric algorithms such \nas neural networks, k-nearest neighbor, and support vector machines.\nWith more than two inputs, the process repeats with each subsequent ", " It can also be benefi cial for other numeric algorithms such \nas neural networks, k-nearest neighbor, and support vector machines.\nWith more than two inputs, the process repeats with each subsequent compo-\nnent still orthogonal to all the previous PCs, but representing the direction of the \nlargest variance remaining in the data. This is purely a conceptual characteriza-\ntion; the PCA algorithm is not iterative, but fi nds all of the PCs simultaneously \nthrough matrix multiplication (matrix decomposition) and is computationally \nfast even for a large number of inputs. The actual PCs are linear projections of the \noriginal inputs. PCA continues until there are no more directions left in the data. \nIf there are N inputs to the PCA model, this occurs after N PCs are computed.\nThe projection of these two variables, Band1 and Band2, onto the PC space, \nPC1 and PC2, is shown in Figure 6-3. The axes were scaled in the scatterplot to \nthe same minimum and maximum values, revealing how much mo", "of these two variables, Band1 and Band2, onto the PC space, \nPC1 and PC2, is shown in Figure 6-3. The axes were scaled in the scatterplot to \nthe same minimum and maximum values, revealing how much more spread \nthere is in PC1 compared to PC2. The correlation between the two variables \nis zero and can be inferred by a simple test: Can one pick a value of PC1 and \npredict from that the value of PC2? The answer is no: For any value of PC1, PC2 \ncan be any number between \u20135 and 5. \n\u22122.582\n\u22122.374\n\u22122.174\n\u22121.974\n\u22121.774\n\u22121.574\n\u22121.374\n\u22121.174\n\u22120.974\n\u22120.774\n\u22120.574\n\u22120.374\n\u22120.174\n0.026\n0.226\n0.426\n0.626\n0.826\n1.026\n1.226\n1.426\n1.626\n1.802\n\u22122.332\n\u22122.082\n\u22121.832\n\u22121.582\n\u22121.332\n\u22121.082\n\u22120.832\n\u22120.582\n\u22120.332\n\u22120.082\n0.168\n0.418\n0.668\n0.918\n1.168\n1.418\n1.747\nFigure 6-1:  Nasadata Band1 vs. Band2\n\n \nChapter 6 \u25a0 Descriptive Modeling \n167\n\u22122.582\n\u22122.374\n\u22122.174\n\u22121.974\n\u22121.774\n\u22121.574\n\u22121.374\n\u22121.174\n\u22120.974\n\u22120.774\n\u22120.574\n\u22120.374\n\u22120.174\n0.026\nBand1\nBand2\n0.226\n0.426\nPC 1\nPC 2\n0.626\n0.826\n1.026\n1.226\n1.426\n1.626\n1.802\n\u2212", "Descriptive Modeling \n167\n\u22122.582\n\u22122.374\n\u22122.174\n\u22121.974\n\u22121.774\n\u22121.574\n\u22121.374\n\u22121.174\n\u22120.974\n\u22120.774\n\u22120.574\n\u22120.374\n\u22120.174\n0.026\nBand1\nBand2\n0.226\n0.426\nPC 1\nPC 2\n0.626\n0.826\n1.026\n1.226\n1.426\n1.626\n1.802\n\u22122.332\n\u22122.082\n\u22121.832\n\u22121.582\n\u22121.332\n\u22121.082\n\u22120.832\n\u22120.582\n\u22120.332\n\u22120.082\n0.168\n0.418\n0.668\n0.918\n1.168\n1.418\n1.747\nFigure 6-2:  Nasadata Band1 vs. Band2 with principle component directions\n\u221220.0\n\u221220\n\u221218\n\u221216\n\u221214\n\u221212\n\u221210\n\u22128\n\u22126\n\u22124\n\u22122\n0\nPC1\nPC2\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n\u221217.5\n\u221215.0\n\u221212.5\n\u221210.0\n\u22127.5\n\u22125.0\n\u22122.5\n0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nFigure 6-3:  Scatterplot of Principal Component 2 vs. Principal Component 1\n\n168 \nChapter 6 \u25a0 Descriptive Modeling\nThe matrix decomposition of the PCA algorithm converts the original data \ninto two other matrices, a diagonal matrix containing eigenvalues, and a second \nmatrix containing eigenvectors. The precise defi nition of these matrices isn\u2019t \nthe concern of this book, but these two terms will be used in some predictive \nanalytics software. There are", " a second \nmatrix containing eigenvectors. The precise defi nition of these matrices isn\u2019t \nthe concern of this book, but these two terms will be used in some predictive \nanalytics software. There are as many eigenvalues as there are original inputs \nused in computing the PCs. Likewise, there are as many PCs as there are inputs \nto the model. The eigenvalues represent the proportion of variance explained \nby each PC. For the simple example, a PCA model with only Band1 and Band2 \nas inputs, the eigenvalues are 54.66 and 4.19, as shown in Table 6-1. The percent \nvariance explained by the fi rst PC is 92.9 percent [54.66 \u00f7 (54.66+4.19)], mean-\ning that nearly all of the variance in the scatterplot shown in Figure 6-1 can be \nexplained by a single component, PC1. \nTable 6-1: Eigenvalues and percent Variance Explained for PCA Computed from Band1 and Band2\nPCA\uf6bcBAND1 & \nBAND2\nEIGENVALUE\n% VARIANCE \nEXPLAINED\nCUMULATIVE % VARIANCE \nEXPLAINED\nPC1\n54.66\n92.9\n92.9\nPC2\n4.19\n7.1\n100\nTable 6-2: Eige", "rcent Variance Explained for PCA Computed from Band1 and Band2\nPCA\uf6bcBAND1 & \nBAND2\nEIGENVALUE\n% VARIANCE \nEXPLAINED\nCUMULATIVE % VARIANCE \nEXPLAINED\nPC1\n54.66\n92.9\n92.9\nPC2\n4.19\n7.1\n100\nTable 6-2: Eigenvectors for PCA Computed from Band1 and Band2\nEIGENVECTORS\nBAND1\nBAND2\nPC1\n0.756\n0.655\nPC2\n0.655\n\u20130.756\nThe eigenvectors, shown in Table 6-2, represent the contributions of each \ninput variable onto each PC: PC1 = 0.756 \u00d7 Band1 + 0.655 \u00d7 Band2, and PC2 = \n0.655 \u00d7 Band1 \u2013 0.756 \u00d7 Band2. These values are sometimes called loadings of \nthe variables on the PCs, although this term is more often used in the related \nfi eld of Factor Analysis. The loadings of the variables onto the fi rst PC are nearly \nequal, understandable given that the two variables are linearly correlated. In \nthe second component, they have opposite signs of one another, although once \nagain similar magnitudes, indicating that the direction of the second component \nis perpendicular to the fi rst. \nIn two dimensions, there ", "ent, they have opposite signs of one another, although once \nagain similar magnitudes, indicating that the direction of the second component \nis perpendicular to the fi rst. \nIn two dimensions, there isn\u2019t much need for PCA. However, in larger dimen-\nsional space, the value can be considerable. The nasadata data set has 12 candidate \ninputs. The PCA model inputs were normalized using z-scores so they are all \non the same scale. When PCA is extended to all of these dimensions, you get \nthe eigenvalues shown in Table 6-3.\n\n \nChapter 6 \u25a0 Descriptive Modeling \n169\nTable 6-3: Twelve PCs for Nasadata\nPCA\nEIGENVALUE\n% VARIANCE EXPLAINED\nCUMULATIVE % VARIANCE \nEXPLAINED\nPC1\n8.622\n71.8\n71.8\nPC2\n2.064\n17.2\n89.0\nPC3\n0.840\n7.0\n96.0\nPC4\n0.113\n0.9\n97.0\nPC5\n0.106\n0.9\n97.9\nPC6\n0.065\n0.5\n98.4\nPC7\n0.056\n0.5\n98.9\nPC8\n0.050\n0.4\n99.3\nPC9\n0.036\n0.3\n99.6\nPC10\n0.022\n0.2\n99.8\nPC11\n0.014\n0.1\n99.9\nPC12\n0.012\n0.1\n100.0\nThe fi rst three PCs explain 96 percent of the variance in the data; this data \nhas a high degr", "\n98.9\nPC8\n0.050\n0.4\n99.3\nPC9\n0.036\n0.3\n99.6\nPC10\n0.022\n0.2\n99.8\nPC11\n0.014\n0.1\n99.9\nPC12\n0.012\n0.1\n100.0\nThe fi rst three PCs explain 96 percent of the variance in the data; this data \nhas a high degree of cross-correlation in the inputs. Even though there are 12 \ncandidate inputs for use in predictive models, there are really only perhaps \nthree true dimensions to the data; one can use the top three PCs without losing \nmuch of the information (variance) in the data. \nThe Cumulative % Variance Explained column is often plotted in a scree plot, \nlike the one shown in Figure 6-4. Sometimes practitioners identify a knee in \nthe plot to determine when one has reached a point of diminishing returns in \nadding PCs to the model. This is an inexact science to be sure, but is used often \nnevertheless. In this fi gure, the knee appears at the third PC, which is consistent \nwith the percent variance explained method of selecting PCs. \nApplying PCA to New Data\nOnce the principal components have be", "ss. In this fi gure, the knee appears at the third PC, which is consistent \nwith the percent variance explained method of selecting PCs. \nApplying PCA to New Data\nOnce the principal components have been computed, the model itself is the \nset of eigevector; these are saved by the software and applied to new data. The \nmodeler typically selects how many PCs to keep and the software computes \nthe projections for the appropriate eigenvectors. These new variables, the PCs, \nare added to the data fl ow as additional columns in the data. For example, if one \ndecides to keep three PCs from the nasadata, three new created PCs from the \n\n170 \nChapter 6 \u25a0 Descriptive Modeling\ntwelve inputs (Band1, Band2, . . . , Band12) will be added. If one computed the \nvariance of each of these three PCs on the training data used to create the PCA \nmodel, one will see that the variance matches the eigenvalues exactly; even in \nthis sample shown in Table 6-4, it is clear that PC1 has larger magnitudes than \nPC2", "aining data used to create the PCA \nmodel, one will see that the variance matches the eigenvalues exactly; even in \nthis sample shown in Table 6-4, it is clear that PC1 has larger magnitudes than \nPC2 and PC3. If the input data was z-scored or scaled to have zero mean, the \nPCs will also have zero mean.\n0.0%\n0\n2\n4\n6\nPC Number\nCumulative % Variance Explained\n% Variance Explained\n8\n10\n12\n14\n10.0%\n20.0%\n30.0%\n40.0%\n50.0%\n60.0%\n70.0%\n80.0%\n90.0%\n100.0%\nFigure 6-4:  Scree plot\nTable 6-4: Top Three Principal Component Values for Ten Data Points\nCLASS\nPC1\nPC2\nPC3\nalfalfa\n\u20132.678\n\u20130.342\n\u20130.485\nwheat\n1.121\n2.642\n1.195\nwheat\n0.920\n2.442\n0.972\nrye\n3.064\n0.888\n-0.556\nalfalfa\n\u20132.451\n\u20130.262\n\u20130.840\noats\n1.726\n0.404\n\u20130.491\nrye\n4.602\n0.504\n\u20131.146\nsoy\n0.878\n\u20131.234\n1.286\noats\n0.781\n0.278\n\u20130.568\nsoy\n0.060\n\u20130.869\n1.399\n\n \nChapter 6 \u25a0 Descriptive Modeling \n171\nPCA for Data Interpretation\nSo far, PCA has been described as a way to reduce the number of inputs for \npredictive models. Another important use of PC", "69\n1.399\n\n \nChapter 6 \u25a0 Descriptive Modeling \n171\nPCA for Data Interpretation\nSo far, PCA has been described as a way to reduce the number of inputs for \npredictive models. Another important use of PCA models is for data interpreta-\ntion and variable selection. \nInterpretation of PCA models is subjective. Most often, practitioners look at \nthe loadings for each PC and pick the top few values. These variables are then \ndeemed to be the ones that are most important in the formation of that PC. The \ncutoff for which variables are the top contributors is not always obvious and \nusually not determined in a precise way. \nFor example, in Table 6-5, the top six PCs are shown along with the loadings \n(eigenvector values) for each PC. The highest loading variables on each PC are \nshown in boldface. In the fi rst PC, all of the inputs, Band1 through Band12, \ncontribute to the PC, although Band4 and Band5 contribute slightly more than \nthe others. One might label PC1 as the \u201cmain effect\u201d component", "In the fi rst PC, all of the inputs, Band1 through Band12, \ncontribute to the PC, although Band4 and Band5 contribute slightly more than \nthe others. One might label PC1 as the \u201cmain effect\u201d component because of the \nbroad-based contribution from all inputs. PC2, however, has as its top loaders \nBand1 and Band10. So PC2 is the Band1+Band10 component. PC3 is dominated \nby Band6, Band11, and Band12, and therefore is the Band6+Band11+Band12 \ncomponent. PC4 is the Band3+Band6+Band10 component, and so forth.\nTable 6-5: Six Principal Components of Nasadata\nNASADATA\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nBand1\n\u20130.188\n0.559\n\u20130.088\n0.249\n0.090\n\u20130.694\nBand2\n\u20130.273\n0.372\n0.018\n0.264\n\u20130.662\n0.389\nBand3\n\u20130.311\n0.201\n0.038\n0.417\n0.645\n0.442\nBand4\n\u20130.330\n0.072\n0.023\n\u20130.003\n\u20130.127\n\u20130.240\nBand5\n\u20130.329\n0.082\n0.133\n\u20130.110\n\u20130.034\n0.217\nBand6\n\u20130.283\n0.138\n0.519\n\u20130.506\n\u20130.065\n\u20130.015\nBand7\n\u20130.313\n\u20130.118\n0.320\n\u20130.194\n0.265\n\u20130.121\nBand8\n\u20130.307\n\u20130.292\n0.022\n\u20130.009\n\u20130.005\n\u20130.055\nBand9\n\u20130.291\n\u20130.346\n\u20130.093\n0.175\n\u20130.128\n\u20130.091\nC", "\u20130.034\n0.217\nBand6\n\u20130.283\n0.138\n0.519\n\u20130.506\n\u20130.065\n\u20130.015\nBand7\n\u20130.313\n\u20130.118\n0.320\n\u20130.194\n0.265\n\u20130.121\nBand8\n\u20130.307\n\u20130.292\n0.022\n\u20130.009\n\u20130.005\n\u20130.055\nBand9\n\u20130.291\n\u20130.346\n\u20130.093\n0.175\n\u20130.128\n\u20130.091\nContinues\n\n172 \nChapter 6 \u25a0 Descriptive Modeling\nNASADATA\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nBand10\n\u20130.221\n\u20130.504\n0.150\n0.456\n\u20130.143\n\u20130.191\nBand11\n0.289\n0.067\n0.545\n0.320\n0.015\n\u20130.031\nBand12\n0.295\n0.003\n0.522\n0.218\n\u20130.090\n\u20130.045\nThe magnitude of eigenvectors determines the infl uence of the variable on \nthe PC, but the sign indicates the direction of infl uence. In PC2 from Table 6-4, \nBand1 and Band10 are the top loaders, but they do so in the opposite direction \nas indicated by the opposite signs. Interestingly, they are only mildly negatively \ncorrelated on all the data (\u20130.22), but in the fi rst component PC1, their loadings \nare in the same direction (both negative). The PCA model is saying that as \nindicated by PC2, there are records where the two variables are strongly but \ninversely correlated", "nent PC1, their loadings \nare in the same direction (both negative). The PCA model is saying that as \nindicated by PC2, there are records where the two variables are strongly but \ninversely correlated with each other. \nSometimes, rather than using the PCs themselves as the inputs to predictive \nmodels, you can use the top loading variable for each of the components as the \nrepresentative of that component. From Table 6-4, instead of using the top three \nPCs as inputs to a predictive model, you could use the top magnitude repre-\nsentatives from these PCs: Band4 from PC1, Band1 from PC2, and Band11 from \nPC2. In fact, in several applications of PCA analysis for dimensionality reduc-\ntions, this approach of using the representative variable has actually improved \npredictive accuracy over using the actual principal components.\nAdditional Considerations before Using PCA\nPCA, however, has some issues to keep in mind before using it for variable \nselection with predictive modeling. First, jus", "g the actual principal components.\nAdditional Considerations before Using PCA\nPCA, however, has some issues to keep in mind before using it for variable \nselection with predictive modeling. First, just because the dimensionality and a \nfew principal components can describe the vast majority of the variance in the \ndata doesn\u2019t mean that classifi cation or regression accuracy will be improved. \nConsider Figure 6-5. The data is bi-modal as evidenced by the two ellipses; one \nellipse for the dark gray class and a second for the light gray class. The upper \narrow shows the direction of the fi rst PC computed from the light gray data \nonly and the lower arrow shows the direction of the fi rst PC computed from the \nTable 6-5 (continued)\n\n \nChapter 6 \u25a0 Descriptive Modeling \n173\ndark gray data only. The fi rst PC when computed from all the data, both dark \ngray and light gray, is in the same direction as these two. \n\u22122.582\n\u22123.200\n\u22122.950\n\u22122.700\n\u22122.450\n\u22122.200\n\u22121.950\n\u22121.700\n\u22121.450\n\u22121.200\n\u22120.950\n\u2212", "y. The fi rst PC when computed from all the data, both dark \ngray and light gray, is in the same direction as these two. \n\u22122.582\n\u22123.200\n\u22122.950\n\u22122.700\n\u22122.450\n\u22122.200\n\u22121.950\n\u22121.700\n\u22121.450\n\u22121.200\n\u22120.950\n\u22120.700\n\u22120.450\n\u22120.200\n0.050\n0.300\n0.550\n0.800\n1.050\n1.300\n1.550\n1.802\n\u22122.082\n\u22121.582\n\u22121.082\n\u22120.582\n\u22120.082\n0.418\n0.918\n1.418\n1.918\n2.418\n2.918\n3.550\nFigure 6-5:  Principal components for bi-modal distribution\nAfter performing PCA, the histogram of the fi rst PC in Figure 6-6 shows \noverlap between the two classes on this fi rst PC; just because a PC describes \nlarge amounts of variance in the data doesn\u2019t mean that it will discriminate \nbetween target classes.\nSecond, even if some of the components are good predictors, one doesn\u2019t nec-\nessarily know which ones are the best predictors from the variance explained. \nIt may be that PC6 is the best predictor even though it explains relatively little \nvariance overall in the data. Third, PCA is a linear method that creates linear \nprojections of the", "ance explained. \nIt may be that PC6 is the best predictor even though it explains relatively little \nvariance overall in the data. Third, PCA is a linear method that creates linear \nprojections of the inputs onto each of the PCs. If the predictive information in \nthe inputs is nonlinearly related to the target variable, or if the key interactions \nbetween variables are nonlinear, the PCA transformation itself can destroy any \npredictive information the inputs had, making predictive models less accurate \nthan models built from the original inputs.\n\n174 \nChapter 6 \u25a0 Descriptive Modeling\n0\n7\n14\n21\n28\n35\n42\n49\n56\n63\n70\n77\n87\n[\u22124 to \u22123.347]\n(\u22123.347 to \u22122.694]\n(\u22122.694 to \u22122.041]\n(\u22122.041 to \u22121.388]\n(\u22121.388 to \u22120.735]\n(\u22120.735 to \u22120.082]\n(\u22120.082 to 0.573]\n(0.573 to 1.228]\n(1.228 to 1.883]\n(1.883 to 2.538]\nFigure 6-6:  Histogram of first principal component of bi-modal data\nFourth, while PCA can reduce dimensionality, such as reducing the nasadata \nfrom twelve variables down to the top three PCs", " 2.538]\nFigure 6-6:  Histogram of first principal component of bi-modal data\nFourth, while PCA can reduce dimensionality, such as reducing the nasadata \nfrom twelve variables down to the top three PCs, the reduction affects only the \nmodels, not the original data needed to build and score the PCs. Whenever you \napply the PCA model, you still need all twelve of the original inputs to create \nthe three PCs. From a deployment perspective, this is not a simplifi cation of \ndata size at all. \nPCA works best when the data is numeric and distributed either Normally or \nUniformly rather than having multiple modes, clumps, large outliers, or skewed \ndistributions. Moreover, PCA works best when the data has primarily linear \nrelationships between inputs and with the target variable.\nThe E\ufb00 ect of Variable Magnitude on PCA Models\nComputing PCs unbiased by input variable magnitude requires normalization \nof the data. At a minimum,  scaling all the inputs to have comparable maximum \nmagnitude. This", "riable Magnitude on PCA Models\nComputing PCs unbiased by input variable magnitude requires normalization \nof the data. At a minimum,  scaling all the inputs to have comparable maximum \nmagnitude. This problem manifests itself in two ways. First, larger magnitude \ninput variables will load higher in the earlier PCs. Table 6-6 contains the minimum \nand maximum values for the twelve nasadata input variables in their natural \nunits, that is, as they occur in the data before any normalization or transforma-\ntion. Note that the largest magnitudes for the minimum and maximum values \nin the table are for Band11 and Band12. Then, in Table 6-7, you can see that \nin this PCA model, Band11 and Band12 have the largest magnitude and also \ndominate the fi rst PC. The highest loading variables on each factor are shown \n\n \nChapter 6 \u25a0 Descriptive Modeling \n175\nin boldface. After normalizing the data (using z-scoring), the fi rst PC is distrib-\nuted throughout all of the bands in the PCA model summarize", "tor are shown \n\n \nChapter 6 \u25a0 Descriptive Modeling \n175\nin boldface. After normalizing the data (using z-scoring), the fi rst PC is distrib-\nuted throughout all of the bands in the PCA model summarized in Table 6-5.\nTable 6-6: Summary Statistics for Nasadata Input Variables\nNASADATA\nMINIMUM\nMAXIMUM\nRANGE\nMAX MAGNITUDE\nBand1\n\u201313.6\n10.4\n24\n13.6\nBand2\n\u201313.1\n8.9\n22\n13.1\nBand3\n\u20138.6\n7.4\n16\n8.6\nBand4\n\u201310.9\n7.1\n18\n10.9\nBand5\n\u201318.2\n14.8\n33\n18.2\nBand6\n\u201315.8\n9.2\n25\n15.8\nBand7\n\u201314.5\n8.5\n23\n14.5\nBand8\n\u201326.7\n21.3\n48\n26.7\nBand9\n\u201324.8\n22.2\n47\n24.8\nBand10\n\u201324.5\n18.5\n43\n24.5\nBand11\n\u201368.1\n37.9\n106\n68.1\nBand12\n\u201343.8\n24.2\n68\n43.8\nTable 6-7: Eigenvectors of First Six PCs from Nasadata with Natural Units\nNASADATA\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nBand1\n\u20130.063\n\u20130.266\n0.423\n0.054\n\u20130.500\n\u20130.586\nBand2\n\u20130.090\n\u20130.123\n0.372\n\u20130.029\n\u20130.419\n0.547\nBand3\n\u20130.078\n\u20130.031\n0.228\n0.053\n\u20130.080\n\u20130.214\nBand4\n\u20130.097\n0.006\n0.205\n\u20130.006\n\u20130.060\n0.062\nBand5\n\u20130.169\n0.036\n0.446\n\u20130.078\n0.177\n0.285\nBand6\n\u20130.084\n0.070\n0.444\n\u20130.234\n0.352\n0.039\nBand7", "29\n\u20130.419\n0.547\nBand3\n\u20130.078\n\u20130.031\n0.228\n0.053\n\u20130.080\n\u20130.214\nBand4\n\u20130.097\n0.006\n0.205\n\u20130.006\n\u20130.060\n0.062\nBand5\n\u20130.169\n0.036\n0.446\n\u20130.078\n0.177\n0.285\nBand6\n\u20130.084\n0.070\n0.444\n\u20130.234\n0.352\n0.039\nBand7\n\u20130.097\n0.127\n0.225\n\u20130.074\n0.219\n\u20130.169\nBand8\n\u20130.304\n0.398\n0.144\n\u20130.042\n0.322\n\u20130.283\nBand9\n\u20130.316\n0.417\n\u20130.051\n0.103\n0.283\n\u20130.294\nContinues\n\n176 \nChapter 6 \u25a0 Descriptive Modeling\nNASADATA\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nBand10\n\u20130.198\n0.602\n\u20130.108\n0.105\n\u20130.351\n\u20130.165\nBand11\n0.702\n0.341\n0.328\n0.519\n0.058\n0.041\nBand12\n0.451\n0.283\n0.019\n\u20130.797\n\u20130.226\n\u20130.041\nSecond, the magnitudes of the eigenvalues are also affected by the magnitudes \nof the inputs. One rule-of-thumb many modelers use to select the signifi cant \nprincipal components is to keep those whose eigenvalues are greater than 1. In \nTable 6-8, ten of the twelve fi t this criterion even though the fi rst four compo-\nnents already describe 99 percent of the variance. After normalizing the data, \nonly two PCs had eigenvalues greater than 1 (see T", " of the twelve fi t this criterion even though the fi rst four compo-\nnents already describe 99 percent of the variance. After normalizing the data, \nonly two PCs had eigenvalues greater than 1 (see Table 6-3). \nNote, however, that simple scaling affects the eigenvalues but not the percent \nof the variance explained. If the z-score normalized inputs are scaled upward \nby a factor of 7 by simple multiplication, all of the eigenvalues end up being \ngreater than 1. However, the relative contribution of the PCs does not change \nwith a simple scaling of the inputs; the percent variance explained is the same \nfor both the z-scored inputs and the z-scored inputs that were scaled upward \nby the factor of 7.\nTable 6-8: Principal Component Eigenvalues for Nasadata with Natural Units\nNASADATA\nEIGENVALUE\n% VARIANCE \nEXPLAINED\nCUMULATIVE % VARIANCE \nEXPLAINED\nPC1\n1317.3\n83.0\n83.0\nPC2\n169.5\n10.7\n93.6\nPC3\n75.34\n4.7\n98.4\nPC4\n9.412\n0.6\n99.0\nPC5\n5.093\n0.3\n99.3\nPC6\n2.471\n0.2\n99.4\nPC7\n2.123\n0.1\n99.6\nPC8\n1", "LUE\n% VARIANCE \nEXPLAINED\nCUMULATIVE % VARIANCE \nEXPLAINED\nPC1\n1317.3\n83.0\n83.0\nPC2\n169.5\n10.7\n93.6\nPC3\n75.34\n4.7\n98.4\nPC4\n9.412\n0.6\n99.0\nPC5\n5.093\n0.3\n99.3\nPC6\n2.471\n0.2\n99.4\nPC7\n2.123\n0.1\n99.6\nPC8\n1.929\n0.1\n99.7\nPC9\n1.672\n0.1\n99.8\nPC10\n1.302\n0.1\n99.9\nTable 6-7 (continued)\n\n \nChapter 6 \u25a0 Descriptive Modeling \n177\nNASADATA\nEIGENVALUE\n% VARIANCE \nEXPLAINED\nCUMULATIVE % VARIANCE \nEXPLAINED\nPC11\n0.967\n0.1\n99.9\nPC12\n0.845\n0.1\n100.0\nClustering Algorithms\nCluster models fi nd groups of data points that are relatively close to one another. \nInherent in the defi nition of the word \u201cclose\u201d is the concept of distance between \ndata points. \nConsider a simple scatterplot from the KDD Cup 1998 data in Figure 6-7. It \nis obvious that point P1 and point P2 are closer to one another than points P1 \nand P3. The most common way this is measured in clustering algorithms is \nby the Euclidean distance, the distance we all learned in high-school algebra: \n\u201cThe shortest distance between two points is a strai", ". The most common way this is measured in clustering algorithms is \nby the Euclidean distance, the distance we all learned in high-school algebra: \n\u201cThe shortest distance between two points is a straight line.\u201d Distance plays a \nprominent role in both clustering algorithms described in this chapter: K-Means \nand Kohonen Self-Organizing Maps.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n4\n8\n12\n16\n20\n24\n28\n32\n36\n40\n44\n50\nP1\nP2\nP3\nFigure 6-7:  Distances between points in Scatterplot\n\n178 \nChapter 6 \u25a0 Descriptive Modeling\nThe K-Means Algorithm\nK-Means belongs to the class of clustering algorithms called hard partitioning \nalgorithms because every data point falls into one partition (cluster) and one \nonly. It is a simple algorithm logically and computationally, but is amazingly \neffective in creating data groups. It is also an iterative algorithm; it isn\u2019t guar-\nanteed to converge, but nearly always does so. A practitioner will typically only \nhave to specify the inputs to the model and how many clusters ", "ups. It is also an iterative algorithm; it isn\u2019t guar-\nanteed to converge, but nearly always does so. A practitioner will typically only \nhave to specify the inputs to the model and how many clusters the algorithm \nshould fi nd; the algorithm takes care of the rest. The following steps describe \nonly what the algorithm does, with the corresponding scatterplots illustrating \nthe steps in Figure 6-8:\nInitialization: Select the number of clusters to fi nd. More sophisticated \nimplementations of the algorithm will attempt a range in the number of \nclusters specifi ed by the user and retain the single number of clusters that \nis considered best. Either way, the user must have an idea of the number \nof clusters that exist in the data or that are desired to be discovered in \nthe data. \nStep 1: Assign one cluster center per cluster. Most software implementa-\ntions identify a random data point in the training data for each cluster \nand assign this point as a cluster center. \nStep 2: Compute the", "sign one cluster center per cluster. Most software implementa-\ntions identify a random data point in the training data for each cluster \nand assign this point as a cluster center. \nStep 2: Compute the distance between each cluster center and every data \npoint in the training data. \nStep 3: Assign a label to each data point indicating its nearest cluster center. \nStep 4: Compute the mean value for every input in each cluster based. \nThis is essentially like performing a SQL \u201cgroup by\u201d for each cluster and \ncomputing the mean for each input. This is the new cluster center.\nStep 5: Repeat Steps 2 through 4 until cluster membership does not change.\nMeasuring Cluster Goodness of Fit\nHow can one determine if a cluster model is a good one? This question goes to \nthe heart of why unsupervised learning is generally a more diffi cult problem \nthan supervised learning. With unsupervised models, there is no counterpart \nto metrics commonly used in supervised learning, such as average error or clas", "ning is generally a more diffi cult problem \nthan supervised learning. With unsupervised models, there is no counterpart \nto metrics commonly used in supervised learning, such as average error or clas-\nsifi cation accuracy. The metrics most commonly used in unsupervised learning \nare related to two areas: distances and interpretation. The latter is discussed in \nmore detail in Chapter 7.\nIdeally, a good cluster model will have relatively small intra-cluster distances \nand relatively large inter-cluster distances. Or stated another way, the clusters \nwill be compact and far from each other. Compactness is usually computed \nusing the same distance measures as were used in building the clusters. In \nthe case of Figure 6-9, the data shows distinct clusters easy to identify visually. \n\n \nChapter 6 \u25a0 Descriptive Modeling \n179\nThe clustering algorithm, however, doesn\u2019t know how many clusters there are, \nbut does the best it can to fi nd the number of clusters specifi ed by the user. \n0\n0\n1\nx2", "r 6 \u25a0 Descriptive Modeling \n179\nThe clustering algorithm, however, doesn\u2019t know how many clusters there are, \nbut does the best it can to fi nd the number of clusters specifi ed by the user. \n0\n0\n1\nx2\nx6\nx9\nx5\nx4\nx3\nx1\nx7\n2\n3\n4\nX-Axis\nY-Axis\n5\n6\n7\n8\n1\n2\n3\n4\n5\n6\n7\n8\n9\nStep 1: 3 Cluster centers (\u201c+\u201d) selected at\nrondom (x4, x5, and x6) \n0\n0\n1\n2\n3\n4\nX-Axis\nshortest\nY-Axis\n5\n6\n7\n8\n1\n2\n3\n4\n5\n6\n7\n8\n9\nx8\nx10\nx2\nx6\nx9\nx5\nx4\nx3\nx1\nx7\nx8\nx10\nx2\nx6\nx9\nx5\nx4\nx3\nx1\nx7\nx8\nx10\nx2\nx6\nx9\nx5\nx4\nx3\nx1\nx7\nx8\nx10\n0\n0\n1\n2\n3\n4\nX-Axis\nY-Axis\n5\n6\n7\n8\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\n0\n1\n2\n3\n4\nX-Axis\nY-Axis\n5\n6\n7\n8\n1\n2\n3\n4\n5\n6\n7\n8\n9\nStep 2: Compute distance from each point\nto each cluster center. Illustration:\ndistance from point x10 to each of the\nthree cluster centers\nStep 3: Assign Cluster Labels Based on\nNearest Cluster Center\nStep 4: Compute New Cluster Center for\neach Cluster\nx2\nx6\nx9\nx5\nx4\nx3\nx1\nx7\nx8\nx10\n0\n0\n1\n2\n3\n4\nX-Axis\nY-Axis\n5\n6\n7\n8\n1\n2\n3\n4\n5\n6\n7\n8\n9\nStep 5: Repeat Process Until Cluster\nMembership Does Not Chan", "ter\nStep 4: Compute New Cluster Center for\neach Cluster\nx2\nx6\nx9\nx5\nx4\nx3\nx1\nx7\nx8\nx10\n0\n0\n1\n2\n3\n4\nX-Axis\nY-Axis\n5\n6\n7\n8\n1\n2\n3\n4\n5\n6\n7\n8\n9\nStep 5: Repeat Process Until Cluster\nMembership Does Not Change\nFigure 6-8:  K-Means algorithm steps\n\n180 \nChapter 6 \u25a0 Descriptive Modeling\n0.015\n0.252\n0.277\n0.302\n0.327\n0.352\n0.377\n0.402\n0.427\n0.452\n0.477\n0.502\n0.527\n0.552\n0.577\n0.602\n0.627\n0.652\n0.677\n0.702\n0.727\n0.752\n0.777\n0.802\n0.827\n0.852\n0.877\n0.902\n0.927\n0.958\n0.065\n0.115\n0.165\n0.215\n0.265\n0.315\n0.365\n0.415\n0.465\n0.515\n0.591\nFigure 6-9:  Scatterplot of data with two obvious groups\nSometimes the data doesn\u2019t support clean clustering of the data with the \nnumber of clusters specifi ed by the user. Consider the data in Figure 6-10. A \ntwo-cluster model was created and the labels for the clusters is shown in Figure \n6-11. Clearly, the cluster boundaries are imposed and not natural. \n0.000\n0.057\n0.107\n0.157\n0.207\n0.257\n0.307\n0.357\n0.407\n0.457\n0.507\n0.557\n0.607\n0.657\n0.707\n0.757\n0.807\n0.857\n0.907\n", " shown in Figure \n6-11. Clearly, the cluster boundaries are imposed and not natural. \n0.000\n0.057\n0.107\n0.157\n0.207\n0.257\n0.307\n0.357\n0.407\n0.457\n0.507\n0.557\n0.607\n0.657\n0.707\n0.757\n0.807\n0.857\n0.907\n0.957\n0.997\n0.100\n0.200\n0.300\n0.400\n0.500\n0.600\n0.700\n0.800\n0.921\nFigure 6-10:  Scatterplot of data with one group\n\n \nChapter 6 \u25a0 Descriptive Modeling \n181\n0.057\n0.000\n0.100\n0.200\n0.300\n0.400\n0.500\n0.600\n0.700\n0.800\n0.921\n0.107\n0.157\n0.207\n0.257\n0.307\n0.357\n0.407\n0.457\n0.507\n0.557\n0.607\n0.657\n0.707\n0.757\n0.807\n0.857\n0.907\n0.957\n0.997\nCircle\nX-Shape\n0\n1\nFigure 6-11:  Cluster labels for forced two-cluster model\nOne of the most commonly used measures of cluster \u201cgoodness\u201d is the Sum \nof Squared Error (SSE) metric, which is the sum of the differences in distance \nbetween each data point in a cluster and the cluster center (the mean). The SSE \nvalue for the 1,000 data points in Figure 6-11 is 40.9, whereas SSE for the 1,000 \ndata points in Figure 6-9, clearly better suited for a K-Means cluster", "er and the cluster center (the mean). The SSE \nvalue for the 1,000 data points in Figure 6-11 is 40.9, whereas SSE for the 1,000 \ndata points in Figure 6-9, clearly better suited for a K-Means clustering model, \nis 16.5, less than half the value.\nSome software implementations also provide a measure of the distance between \nthe clusters, usually a measure of how many standard deviations apart the \ncluster centers are, typically an average of the standard deviations of the inputs \nmaking up the clusters. In the case of Figure 6-9, the clusters are approximately \n5 standard deviations apart, whereas for Figure 6-11, it is only 1.6. With values \nless than 2, you know there is considerable overlap not of the clusters themselves, \nbut of the spherical shape the cluster model assumes. Figure 6-12 shows the 1 \nstandard deviation ring around the cluster centers for the two clusters. Note \ntoo that, as is always the case with quadratic error functions (squared error), \nthe decision boundary betw", " shows the 1 \nstandard deviation ring around the cluster centers for the two clusters. Note \ntoo that, as is always the case with quadratic error functions (squared error), \nthe decision boundary between clusters is linear.\nSelecting Inputs\nThe examples shown so far are simple, with only two inputs to make visualiza-\ntion easier. However, the reality is that most often, clustering models are created \nfrom dozens to hundreds of candidate inputs. Ideally, you only include inputs \nthat are helpful in creating clean clusters and no more. \n\n182 \nChapter 6 \u25a0 Descriptive Modeling\n0.057\n0.000\n0.100\n0.200\n0.300\n0.400\n0.500\n0.600\n0.700\n0.800\n0.921\n0.107\n0.157\n0.207\n0.257\n0.307\n0.357\n0.407\n0.457\n0.507\n0.557\n0.607\n0.657\n0.707\n0.757\n0.807\n0.857\n0.907\n0.957\n0.997\nCircle\nX-Shape\n0\n1\nFigure 6-12:  One standard deviation ring for each of two clusters\nThe description of PCA models emphasized their ability to fi nd groups of \nvariables with high correlation. K-Means cluster models are similar in several ", "ndard deviation ring for each of two clusters\nThe description of PCA models emphasized their ability to fi nd groups of \nvariables with high correlation. K-Means cluster models are similar in several \nrespects. First, both algorithms are based on variance and squared error met-\nrics. Second, from a practical standpoint, both algorithms identify correlated \nvariables. A good K-Means model identifi es two or more variables whose cor-\nrelation is high enough that the data clumps together. Examine Figure 6-9 (the \ngood clustering model) again. When two variables are correlated, knowing one \nvariable provides a strong indication of the value of the second variable. In this \nfi gure, when the x-axis value is 0.4, you have a strong indication that the y-axis \nis more than 0.4. \nThe difference between correlation analysis and K-Means models is that the \nK-Means cluster label creates a subset of data points that are correlated. In Figure \n6-9, if the cluster in the upper left of the plot was in", "ween correlation analysis and K-Means models is that the \nK-Means cluster label creates a subset of data points that are correlated. In Figure \n6-9, if the cluster in the upper left of the plot was instead centered on an x-value \nof 0.625 rather than 0.4, it would still be easy to cluster, but the conditioned \ncorrelation, the correlation between the x-axis and y-axis values in that cluster, \nis very high even though the global correlation between the x-axis and y-axis \nwould not be high. In this sense, K-Means clustering models are multidimen-\nsional correlation detectors.\nK-Means does not provide a mechanism for variable selection in the algorithm \nitself; the user must make the selection before building the model. Therefore, \ncare must be taken to select good variables. But there is no simple way to fi nd \nwhat these variables are. Chapter 7 describes some cluster interpretation meth-\nods to uncover which variables are strong infl uencers in creating the clusters. \n\n \nChapter 6 \u25a0 De", "simple way to fi nd \nwhat these variables are. Chapter 7 describes some cluster interpretation meth-\nods to uncover which variables are strong infl uencers in creating the clusters. \n\n \nChapter 6 \u25a0 Descriptive Modeling \n183\nVariable selection can be done through a combination of domain knowledge \nand assessment of the infl uence of variables in the clusters. \nThe curse of dimensionality problem has already been described, and it is cer-\ntainly an issue with K-Means models; the more inputs included in the model, \nthe greater the number of data points that are needed to fi ll the space. This is a \nkey reason why most practitioners limit the number of inputs to dozens at most. \nData Preparation for K-Means \nCareful attention to data preparation steps should be taken to build good K-Means \nmodels. Many of the data preparation steps follow the best practices described \nin Chapter 4. Others are specifi c to this kind of algorithm. If data preparation \nsteps are not done properly, the cluster", "els. Many of the data preparation steps follow the best practices described \nin Chapter 4. Others are specifi c to this kind of algorithm. If data preparation \nsteps are not done properly, the clusters may be biased inadvertently toward \njust a few of the complete set input variables or may not be well formed.\nData Distributions\nData preparation for K-Means models is the same as has been described previ-\nously for numeric algorithms. The key corrective actions include:\n \n1. Fill in missing data or remove records with missing values.\n \n2. Remove or mitigate outliers through data transformations.\n \n3. Reduce skew.\n \n4. Explode categorical variables into dummy variables. Include categorical \nvariables in this way only if necessary.\n \n5. Scale all inputs so they are on the same scale through min-max normaliza-\ntion or z-scores.\nThese are all standard steps in building K-Means models. However, sometimes \nthere are advantages to breaking the rules. First, as was described with PCA \nmodels, m", "in-max normaliza-\ntion or z-scores.\nThese are all standard steps in building K-Means models. However, sometimes \nthere are advantages to breaking the rules. First, as was described with PCA \nmodels, magnitudes can affect the models signifi cantly, biasing the formation \nof clusters. Sometimes, however, this may be exactly what you want; if for some \nreason, you want some of the variables to drive cluster formation, one way to \nencourage this is to have their magnitudes larger than the other variables. We\u2019ve \ndone this on many occasions.\nSecond, categorical variables are problematic in computing distances. A vari-\nable encoded as 0 or 1 has only two distance values compared to another record: \nthe minimum distance (0) or the maximum distance (1). Because of this curse \nof extremes, categorical variables, especially when there are large numbers of \nvalues for the categorical variable, can have more infl uence in the formation of \nclusters than any of the continuous variables. To mitigate", "cal variables, especially when there are large numbers of \nvalues for the categorical variable, can have more infl uence in the formation of \nclusters than any of the continuous variables. To mitigate this effect, fi rst you \ncan either ensure all variables are scaled to the range 0 to 1, or if the infl uence of \n\n184 \nChapter 6 \u25a0 Descriptive Modeling\ncategorical variables is to be reduced further, you can z-score the numeric data \nso the range of their values is nearly always \u20133 to 3, a much larger magnitude \nand range of values than the dummy variables. A second way to reduce the \ninfl uence is to recode the dummy variables from 0 and 1 to new values, such \nas 0.2 and 0.8. \nSometimes, to avoid the inherent confl ict between clustering continuous and \ncategorical data, one can bin the continuous data and create dummy variables \nfrom those bins so all the inputs are dummy variables. There are circumstances \nwhere this approach can be helpful, but because K-Means is more naturally a \nnu", "tinuous data and create dummy variables \nfrom those bins so all the inputs are dummy variables. There are circumstances \nwhere this approach can be helpful, but because K-Means is more naturally a \nnumeric algorithm, it should be treated as a secondary option or last resort.\nIrrelevant Variables\nIrrelevant variables, variables that do not contribute in any way to the formation \nof the clusters, create a noise fl oor in the distance calculation for every cluster. \nThis may not have an effect in the actual cluster formation, but certainly will \naffect the assessment of how good the clusters are. If you are using the sum of \nsquare distances to measure cluster compactness, the more irrelevant variables \nthat are included in the model, the larger the sum of square distance will be and \ntherefore the worse the cluster will appear to be. These issues are addressed \nin Chapter 7. \nIn Chapter 4, two classes of input variables were identifi ed that should be \nremoved before beginning analysis: ", "re the worse the cluster will appear to be. These issues are addressed \nin Chapter 7. \nIn Chapter 4, two classes of input variables were identifi ed that should be \nremoved before beginning analysis: redundant variables and irrelevant vari-\nables. Clustering algorithms are particularly sensitive to both of these kinds of \nvariables and therefore need to be considered prior to modeling. \nWhen Not to Correct for Distribution Problems\nK-Means clustering, as an algorithm that uses the Euclidean Distance, builds \nspherical cluster shapes. More precisely, K-Means builds hyper-spherical clusters \nbecause clustering is done in multidimensional space. Finding spherical clus-\nters is more natural if the data itself is spherical, which translates to Normal \ndistributions. Most classical treatments of K-Means clustering recommends that \ndata be transformed so that it is normally distributed or at least nearly so. At a \nminimum, I recommend that the data be uniformly distributed.\nHow much deviation", " K-Means clustering recommends that \ndata be transformed so that it is normally distributed or at least nearly so. At a \nminimum, I recommend that the data be uniformly distributed.\nHow much deviation from Normal or Uniform is too much? There is no rule \nto tell us how much is too much. Modelers should examine clusters to make \nsure the most important variables to the formation of clusters are not selected \nprimarily because they have strange distributions or severe outliers.\n\n \nChapter 6 \u25a0 Descriptive Modeling \n185\nSelecting the Number of Clusters\nDetermining the number of clusters, the value of \u201ck,\u201d is an inexact science. The \nselection of k is required to build the model and as has already been described, \nsome software implementations try several values of k and make a judgment \nto determine the best number of clusters from the models that were created. \nThere are general guidelines you can use to determine the value of k. First \nand foremost, k is often determined by business cons", "ermine the best number of clusters from the models that were created. \nThere are general guidelines you can use to determine the value of k. First \nand foremost, k is often determined by business considerations. If the market-\ning department is interested in building customer segments for the purpose \nof creating customized marketing literature for a new up-sell campaign, they \nmay determine that four and only four customer segments are needed. A four-\ncluster model would therefore be created, identifying the best way to segment \nthe customers into four groups. When cluster interpretation is highly important, \nthe number of clusters in the model is usually kept small, perhaps 10 or fewer.\nOn the other hand, clustering can be used to identify anomalies in the data as \nwell. Outliers and anomalies can be identifi ed in two different ways in clustering. \nFirst, if one builds relatively small numbers of clusters, the cluster centers will \nrepresent the big picture: Where are the centers of", "malies can be identifi ed in two different ways in clustering. \nFirst, if one builds relatively small numbers of clusters, the cluster centers will \nrepresent the big picture: Where are the centers of the main group in the data? \nHowever, because every record must be included in one and only one cluster, \nsome data points, the outliers, will be relatively far from the cluster center to \nwhich they have been assigned. This kind of multidimensional outlier detec-\ntion is straightforward: Flag records more than 3 or more standard deviations \nfrom the cluster center.\nA second way to identify multidimensional outliers, especially when one \nwants to fi nd patterns of anomalies, is to build lots of clusters, perhaps hundreds. \nSome of the clusters will have one or two records in them; this merely indicates \na true outlier that is unlike anything else in the data. The more interesting clus-\nters may be those that have 0.1 to 1 percent of the total number of records. In \na data set with 10,000 ", " indicates \na true outlier that is unlike anything else in the data. The more interesting clus-\nters may be those that have 0.1 to 1 percent of the total number of records. In \na data set with 10,000 records, these will be the clusters with 10 to 100 records. \nWhat is particularly interesting about these outliers is that they are unusual, \nbut unusual in a group; a pattern of anomalous behavior is represented in the \ncluster. In fraud detection problems, for example, these may be interesting cases \nto investigate, perhaps indicating a new trend of fraud not previously identifi ed.\nThese methods for determining the number of clusters are based on business \nconcerns. There is no optimal number of clusters one can identify numerically. \nHowever, there are empirical ways to identify how many clusters occur natu-\nrally in the data compared to other numbers of clusters using SSE. Consider \nTable 6-9, which shows SSE for eight different K-Means clustering models, rang-\ning from 3 to 250 clust", "lusters occur natu-\nrally in the data compared to other numbers of clusters using SSE. Consider \nTable 6-9, which shows SSE for eight different K-Means clustering models, rang-\ning from 3 to 250 clusters. \n\n186 \nChapter 6 \u25a0 Descriptive Modeling\nTable 6-9: Sum of Squared Errors vs. Number of Clusters\nNUMBER OF CLUSTERS\nSSE\n% REDUCTION\nSSE * # CLUSTERS\n3\n80,224\n0.0\n240,673\n9\n49,641\n\u201338.1\n446,768\n15\n35,251\n\u201356.1\n528,764\n31\n17,722\n\u201377.9\n549,368\n51\n10,881\n\u201386.4\n554,934\n71\n7,678\n\u201390.4\n545,122\n101\n5,691\n\u201392.9\n574,782\n250\n2,694\n\u201396.6\n673,384\nMeasuring SSE is sometimes used to select the number of clusters in much \nthe same way the Scree Plot is used for PCA: You can fi nd a knee in the plot to \ndetermine the best number of clusters. The data in Table 6-9 is plotted in Figure \n6-13, and the knee appears to be somewhere between 70 to 100 clusters, once \nthe reduction in SSE is greater than 90 percent. \n10,000\n20,000\n30,000\n40,000\n50,000\n60,000\n70,000\n80,000\n90,000\n0\n0\n50\n100\n150\n# Clusters\nSum o", "pears to be somewhere between 70 to 100 clusters, once \nthe reduction in SSE is greater than 90 percent. \n10,000\n20,000\n30,000\n40,000\n50,000\n60,000\n70,000\n80,000\n90,000\n0\n0\n50\n100\n150\n# Clusters\nSum of Squared Error (SSE)\n200\n250\n300\nFigure 6-13:  SSE vs. # clusters to find the \u201cknee\u201d\nAs the number of clusters increases, the reduction in SSE is expected; the more \nclusters in the model, the fewer data points on average will be in the clusters \nand therefore the more compact the clusters will be. One way to compensate for \nthe natural decrease in the size of SSE is to multiply it by the number of clusters. \nWhen one does this, this new measure fl attens out by 31 clusters, although it \nincreases again at 250. Again, this is not a theoretical result, just another empiri-\ncal way to try to identify the best number of clusters.\n\n \nChapter 6 \u25a0 Descriptive Modeling \n187\nThe Kinds of Clusters K-Means Finds\nK-Means clustering typically fi nds spherical clusters when it uses Euclidean \ndistance", "entify the best number of clusters.\n\n \nChapter 6 \u25a0 Descriptive Modeling \n187\nThe Kinds of Clusters K-Means Finds\nK-Means clustering typically fi nds spherical clusters when it uses Euclidean \ndistance to measure distances between data points. Euclidean distance pro-\nduces linear boundaries between clusters when there is overlap between them. \nFigure 6-14 shows seven clusters found from only Band1 and Band9 of the \nnasadata data set. Each color represents a cluster label. Note that the transition \nbetween the cluster regions is always linear and that each region is a contigu-\nous group; no cluster region is broken up by another, inside another, or has a \nserpentine shape.\n\u22122.174\n\u22122.374\n\u22121.939\n\u22121.689\n\u22121.439\n\u22121.189\n\u22120.939\n\u22120.689\n\u22120.436\n\u22120.189\n0.061\n0.311\n0.561\n0.811\n1.061\n1.311\n1.561\n1.741\n\u22121.974\n\u22121.574\n\u22121.174\n\u22120.774\n\u22120.374\n0.026\n0.426\n0.826\n1.226\n1.626\n\u22121.774\n\u22121.374\n\u22120.974\n\u22120.574\n\u22120.174\n0.226\n0.626\n1.026\n1.426\n1.801\nHorizontal Line\ncluster_6\ncluster_0\ncluster_5\ncluster_3\ncluster_4\ncluste", ".974\n\u22121.574\n\u22121.174\n\u22120.774\n\u22120.374\n0.026\n0.426\n0.826\n1.226\n1.626\n\u22121.774\n\u22121.374\n\u22120.974\n\u22120.574\n\u22120.174\n0.226\n0.626\n1.026\n1.426\n1.801\nHorizontal Line\ncluster_6\ncluster_0\ncluster_5\ncluster_3\ncluster_4\ncluster_2\ncluster_1\nReverse Triangle\nTriangle\nDiamond\nVertical Line\nCross\nBand9\nBand1\nAsterisk\nFigure 6-14:  Cluster regions from Euclidean distance\nOn the other hand, data that is not spherical is diffi cult for K-Means, with \nthe algorithm imposing clusters that don\u2019t make sense visually. Consider a \nfunction called the sombrero function. The part of the sombrero function with a \ntarget variable equal to 1 is shown in Figure 6-15.\nThe K-Means clustering algorithm cannot fi nd two clusters for this data set \nthat make sense. Figure 6-16 shows models for 2, 5, and 8 clusters. The 2-cluster \nmodel just splits the data down the middle\u2014not very helpful. The 5-cluster \nmodel begins to divide the inner data clump from the outer ring, but still keeps \nportions of the inner circle mixed with the outer ", "st splits the data down the middle\u2014not very helpful. The 5-cluster \nmodel begins to divide the inner data clump from the outer ring, but still keeps \nportions of the inner circle mixed with the outer ring; it appears to segment \nthe data by wedges. The 8-cluster model is fi nally able to split the inner circle \nfrom the outer ring, although to do so, it needs two clusters to represent the \ninner circle and six clusters to represent the outer ring. A post-processing step \nwould be needed to group the clusters if the true underlying data structure is \nto be learned from the data.\n\n188 \nChapter 6 \u25a0 Descriptive Modeling\nCircle\nX-Shape\n0\n1\n\u221210.000\n\u22129.000\n\u22129.999\n\u22127.999\n\u22125.999\n\u22123.999\n\u22121.999\n0.001\n2.001\n4.001\n6.001\n8.001\n9.994\n\u22128.000\n\u22127.000\n\u22126.000\n\u22125.000\n\u22124.000\n\u22123.000\n\u22122.000\n\u22121.000\n0.000\n1.000\n2.000\n3.000\n4.000\n5.000\n6.000\n7.000\n8.000\n9.000\n9.993\nFigure 6-15:  Sombrero function\nThis is not to say that K-Means clustering cannot be used for data that isn\u2019t \nspherical. Even though the clusters fo", "00\n3.000\n4.000\n5.000\n6.000\n7.000\n8.000\n9.000\n9.993\nFigure 6-15:  Sombrero function\nThis is not to say that K-Means clustering cannot be used for data that isn\u2019t \nspherical. Even though the clusters found in Figure 6-16 are not spherical and \ndo not form spherical groups, they nevertheless form interesting groups that \nstill provide insight into the structure of the data. \nConsider again the sombrero data. After applying the cluster models shown \nin Figure 6-16 to randomly generated data, the cluster regions are clearer, as can \nbe seen in Figure 6-17. The 2-cluster regions in Figure 6-17 reveal nothing new, \nbut the 5- and 8-cluster models reveal interesting groupings. The 5-cluster model \nhas fi ve equal-sized regions. However, the 8-cluster model separates the center \nregion of the sombrero function into its own cluster, and divides the outer ring \ninto seven equal-sized wedges. These groupings, while not natural groupings, \nare nevertheless interesting groupings. As always with K-Me", "ero function into its own cluster, and divides the outer ring \ninto seven equal-sized wedges. These groupings, while not natural groupings, \nare nevertheless interesting groupings. As always with K-Means clusters using \nEuclidean distance, the boundary between pairs of clusters are linear.\nOther Distance Metrics\nEuclidean distance is by far the most commonly used distance metric used in \nbuilding the K-Means clustering model. Another metric that could be used \nand is sometimes available is the Mahalanobis distance, which normalizes the \ndistance by the covariance matrix, thus eliminating the need to scale the inputs \nto a common range. Another way to describe the Mahalanobis distance is that \nit transforms the inputs so that in a scatter plot of every pair of variables, the \ndata is centered on 0 in both directions, have roughly the same range, and will \nbe shaped like a circle. Using the Mahalanobis eliminates the need to correct \nfor variable scales in the data.\n\n \nChapter 6 \u25a0 Descri", "ed on 0 in both directions, have roughly the same range, and will \nbe shaped like a circle. Using the Mahalanobis eliminates the need to correct \nfor variable scales in the data.\n\n \nChapter 6 \u25a0 Descriptive Modeling \n189\n\u221210.000\n\u22129.000\n\u22128.000\n\u22129.999\n\u22127.999\n\u22125.999\n\u22123.999\n\u22121.999\n0.001\n2.001\n4.001\n6.001\n8.001\n9.994\n\u22127.000\n\u22126.000\n\u22125.000\n\u22124.000\n\u22123.000\n\u22122.000\n\u22121.000\n0.000\n2 clusters\n5 clusters\n8 clusters\n1.000\n2.000\n3.000\n4.000\n5.000\n6.000\n7.000\n8.000\n9.000\n9.993\n\u221210.000\n\u22129.000\n\u22128.000\n\u22129.999\n\u22127.999\n\u22125.999\n\u22123.999\n\u22121.999\n0.001\n2.001\n4.001\n6.001\n8.001\n9.994\n\u22127.000\n\u22126.000\n\u22125.000\n\u22124.000\n\u22123.000\n\u22122.000\n\u22121.000\n0.000\n1.000\n2.000\n3.000\n4.000\n5.000\n6.000\n7.000\n8.000\n9.000\n9.993\n\u221210.000\n\u22129.000\n\u22128.000\n\u22129.999\n\u22127.999\n\u22125.999\n\u22123.999\n\u22121.999\n0.001\n2.001\n4.001\n6.001\n8.001\n9.994\n\u22127.000\n\u22126.000\n\u22125.000\n\u22124.000\n\u22123.000\n\u22122.000\n\u22121.000\n0.000\n1.000\n2.000\n3.000\n4.000\n5.000\n6.000\n7.000\n8.000\n9.000\n9.993\nX-Shape\n1\nSquare\n0\nX-Shape\nCircle\nTriangle\nDiamond\nVertical Line\n1\n0\n4\n2\n3\nX-Shape\nCircle\nTriangle\nAsterisk\nReverse Triangl", "000\n\u22121.000\n0.000\n1.000\n2.000\n3.000\n4.000\n5.000\n6.000\n7.000\n8.000\n9.000\n9.993\nX-Shape\n1\nSquare\n0\nX-Shape\nCircle\nTriangle\nDiamond\nVertical Line\n1\n0\n4\n2\n3\nX-Shape\nCircle\nTriangle\nAsterisk\nReverse Triangle\nDiamond\nVertical Line\nHorizontal Line\n1\n0\n4\n6\n7\n2\n3\n5\nFigure 6-16:  2-, 5-, and 8-cluster models for the sombrero function\n\n190 \nChapter 6 \u25a0 Descriptive Modeling\n9.001\n7.001\n5.001\n3.001\n1.001\n\u22120.999\n\u22122.999\n\u22124.999\n\u22126.999\n\u22128.999\n\u22129.999 \u22127.999 \u22125.999 \u22123.999 \u22121.999\n0.001\n2.001\n4.001\n6.001\n8.001\n9.993\n9.001\n7.001\n5.001\n3.001\n1.001\n\u22120.999\n\u22122.999\n\u22124.999\n\u22126.999\n\u22128.999\n\u22129.991\n\u22127.991\n\u22125.991\n\u22123.991\n\u22121.991\n0.009\n2.009\n4.009\n6.009\n8.009\n9.999\n\u22129.999 \u22127.999 \u22125.999 \u22123.999 \u22121.999\n0.001\n2.001\n4.001\n6.001\n8.001\n9.993\n5 clusters\n9.001\n7.001\n5.001\n3.001\n1.001\n\u22120.999\n\u22122.999\n\u22124.999\n\u22126.999\n\u22128.999\n\u22129.999 \u22127.999 \u22125.999 \u22123.999 \u22121.999\n0.001\n2.001\n4.001\n6.001\n8.001\n9.993\n\u22129.991\n\u22127.991\n\u22125.991\n\u22123.991\n\u22121.991\n0.009\n2.009\n4.009\n6.009\n8.009\n9.999\n2 clusters\n\u22129.991\n\u22127.991\n\u22125.991\n\u22123.991\n\u22121.991\n0.009\n2.009\n4.009\n6.009\n8.009", "999 \u22123.999 \u22121.999\n0.001\n2.001\n4.001\n6.001\n8.001\n9.993\n\u22129.991\n\u22127.991\n\u22125.991\n\u22123.991\n\u22121.991\n0.009\n2.009\n4.009\n6.009\n8.009\n9.999\n2 clusters\n\u22129.991\n\u22127.991\n\u22125.991\n\u22123.991\n\u22121.991\n0.009\n2.009\n4.009\n6.009\n8.009\n9.999\n8 clusters\nFigure 6-17:  Application of 2-, 5-, and 8-cluster models to random data\nAnother distance metric sometimes included in software is the Manhattan \ndistance, also known as the \u201ccity block\u201d distance. Mathematically, this metric \nmeasures the absolute value of the difference between points rather than the \nsquare of the difference. It is rarely used in practice, but could be an interesting \nalternative for some applications.\nSome software also includes the Minkowski distance, which isn\u2019t a single \ndistance metric but rather is a generalization of Euclidean distance, parameter-\nized by r. When r is equal to 2, the Minkowski distance is exactly the Euclidean \ndistance which squares the distances between data points. When r is equal to 1, \nthe Minkowski distance is the Manhattan", "ed by r. When r is equal to 2, the Minkowski distance is exactly the Euclidean \ndistance which squares the distances between data points. When r is equal to 1, \nthe Minkowski distance is the Manhattan distance. However you aren\u2019t limited \nto setting r equal to 1 or 2. The larger the value of r, the more the Minkowski \ndistance becomes simply the value of the maximum absolute difference between \n\n \nChapter 6 \u25a0 Descriptive Modeling \n191\ndata points in any single dimension. This is called the L-infi nity norm or the \nChebyshev distance and can be used to build clusters that minimize the worst-\ncase distance. \nKohonen SOMs\nThe Kohonen Self-Organizing Map (SOM) algorithm is the most well-known \nunsupervised learning neural network algorithm, and perhaps the second in \npopularity only to the multilayered perceptron. They are known by a variety \nof names, such as SOMs, Kohonen Maps, Kohonen Networks, Feature Maps, \nand more. Predictive analytics software places Kohonen Maps in a variety of \nc", "tilayered perceptron. They are known by a variety \nof names, such as SOMs, Kohonen Maps, Kohonen Networks, Feature Maps, \nand more. Predictive analytics software places Kohonen Maps in a variety of \ncategories: Some put them in the grouping of clustering algorithms, others in \nthe neural network algorithms, and some even put them in a category unto \nthemselves.\nThe connection of Kohonen Maps to neural networks is due to a few algorith-\nmic similarities with other neural networks. First, the algorithm updates weights \nduring training from a single record at a time. Second, the visual representa-\ntion of the network looks like other kinds of neural networks; it has nodes and \nconnections between nodes. Third, conceptually, a Kohonen Map is considered \nto be a learning algorithm. It can be updated without retraining completely by \nstarting from the weights of a current model rather than from new, randomly \ngenerated weights.\nKohonen Maps are often described as projections of inputs onto a", "dated without retraining completely by \nstarting from the weights of a current model rather than from new, randomly \ngenerated weights.\nKohonen Maps are often described as projections of inputs onto a two-dimen-\nsional space, like the one shown in Figure 6-18, a 4 \u00d7 4 lattice of the inputs. The \nterms lattice and map are used interchangeable with Kohonen networks. The \nnumber of nodes in the SOM corresponds to the number of clusters the map is \nrepresenting in the model. For the 4 \u00d7 4 map, there are 16 nodes, which there-\nfore provide a 16-cluster representation of the data. If the number of inputs is \nsmall, the SOM can identify multi-modal patterns in the data by exploding the \noriginal inputs into additional dimensions. On the other hand, if the number \nof inputs is large, much greater than 16 for the 4 \u00d7 4 map, the SOM creates a \nmultidimensional scaling of the inputs down to two dimensions.\nFigure 6-19 shows a 6 \u00d7 1 map. Note that while the map is still two-dimensional, \nthe conne", "than 16 for the 4 \u00d7 4 map, the SOM creates a \nmultidimensional scaling of the inputs down to two dimensions.\nFigure 6-19 shows a 6 \u00d7 1 map. Note that while the map is still two-dimensional, \nthe connections occur in only one dimension. In general, while a 6 \u00d7 1 map and a \n3 \u00d7 2 map have the same number of nodes (6), they will produce different models \nbecause of the nature of the connections. The largest neighborhood distance \nfrom one end to the other of the 6 \u00d7 1 map is 5 units away (from the left end \nto the right end), whereas in a 3 \u00d7 2 map, the largest neighborhood distance is \nonly 3: up one and over 2 from the lower-left corner. \n\n192 \nChapter 6 \u25a0 Descriptive Modeling\n4 \u00d7 4\nlattice\nFigure 6-18:  Kohonen Self-Organizing Map, 4 \u00d7 4\n6 \u00d7 1 lattice\nFigure 6-19:  Kohonen Self-Organizing Map, 6 \u00d7 1 Map\nThe Kohonen SOM algorithm doesn\u2019t have to be represented as a two-dimen-\nsional map; the algorithm supports any representation, including 1, 2, 3, or more \ndimensions in the map. Most o", "Map, 6 \u00d7 1 Map\nThe Kohonen SOM algorithm doesn\u2019t have to be represented as a two-dimen-\nsional map; the algorithm supports any representation, including 1, 2, 3, or more \ndimensions in the map. Most often, undoubtedly because paper and screens are \ntwo-dimensional, the map is also shown in two dimensions. \nThe maps in Figures 6-18 and 6-19 are rectangular: The nodes are connected \nin a rectangular grid so each node has at most four other nodes connected to \nit. Some implementations of the algorithm choose a hexagonal grid, where six \nnodes are connected to each node.\nThe Kohonen SOM Algorithm\nSOMs are iterative algorithms like other neural networks, operating on one \nrecord at a time and then updating weights based on that single record. The \nspecifi c algorithm steps are as follows:\nInitialization Step 1: Select the SOM architecture, typically the length \nand width of the map. \nInitialization Step 2: Scramble the order of the records in the training set \nso that the order of presentat", "alization Step 1: Select the SOM architecture, typically the length \nand width of the map. \nInitialization Step 2: Scramble the order of the records in the training set \nso that the order of presentation of the records is random.\nInitialization Step 3: Initialize all weights in the map to random values.\nStep 1: Pass one record through the network, computing the difference \nbetween the inputs in the record and the weight values. The distance is \nthe Euclidean distance between the record and the weights.\nStep 2: Identify the winning node in the SOM (the node closest to the \ndata point). Adjust the weights of that node proportionally to the distance \nbetween the weights and the record.\n\n \nChapter 6 \u25a0 Descriptive Modeling \n193\nStep 3: Identify the neighbors to the winning node. Adjust the weights \nof the neighboring nodes proportionally to the distance to the weights of \nthe record, but less than for the winning node.\nStep 4: Repeat Steps 1\u20133 for all the records in the training set. The si", "hts \nof the neighboring nodes proportionally to the distance to the weights of \nthe record, but less than for the winning node.\nStep 4: Repeat Steps 1\u20133 for all the records in the training set. The size \nof the neighborhood infl uenced by each record shrinks as the algorithm \nproceeds until eventually only the winning node is adjusted. \nStep 5: Continue training until a stop condition applies, such as a time or \na maximum number of passes through the data.\nSome implementations of SOMs update the weights of the neighboring nodes \nby a fi xed amount depending on how many units away the node is from the \nwinning node, essentially based on the Manhattan distance to the winning node. \nOther implementations update the weights of the neighboring nodes propor-\ntional to the distance from the winning node; the closer the neighbor is to the \nwinning node, the more it is infl uenced to update in the direction of the record. \nThe neighborhood idea is key to the success of the algorithm. Without an", "ng node; the closer the neighbor is to the \nwinning node, the more it is infl uenced to update in the direction of the record. \nThe neighborhood idea is key to the success of the algorithm. Without any \nneighborhood infl uence, the SOM would be essentially the same algorithm as \nthe K-Means algorithm. By allowing for communication between node locations, \nthe SOM can often position the centers of the nodes in better locations that refl ect \nthe relative density of data in each region of the input space. \nSometimes Kohonen Maps will create node memberships that alternate between \nlarge number of records in the node and nearly no records in a neighboring \nnode: dead nodes. This can continue to the point where the map essentially uses \nonly half the nodes specifi ed because the others have 0 or nearly 0 records. For \nexample, using the 4 \u00d7 4 map of Figure 6-18, Figure 6-20 indicates signifi cant \npopulations of records in a node by a darker box, and nearly no records in a \nnode by a white", "or nearly 0 records. For \nexample, using the 4 \u00d7 4 map of Figure 6-18, Figure 6-20 indicates signifi cant \npopulations of records in a node by a darker box, and nearly no records in a \nnode by a white box. \n4 \u00d7 4\nlattice\nFigure 6-20:  Kohonen Map with dead nodes\n While this effect is not an indication of a model that hasn\u2019t converged properly, it \ncan be disconcerting to the modeler. If one truly would like to have 16 clusters in the \nmap, the size of the map may have to be increased, even doubled. Algorithmically, \nthere is a fi x that can be found in software on rare occasions, sometimes called a \n\u201cconscience\u201d mechanism, which penalizes a node for getting too large and therefore \nmaking nearby nodes more likely to pick up additional records. \n\n194 \nChapter 6 \u25a0 Descriptive Modeling\nKohonen Map Parameters\nThe Kohonen Map has a few parameters that can be modifi ed to affect the model \nit trains. Typical parameter settings include:\n \n\u25a0# Nodes in X Direction\n \n\u25a0# Nodes in Y Direction\n \n\u25a0M", "n Map Parameters\nThe Kohonen Map has a few parameters that can be modifi ed to affect the model \nit trains. Typical parameter settings include:\n \n\u25a0# Nodes in X Direction\n \n\u25a0# Nodes in Y Direction\n \n\u25a0Max # Iterations\n \n\u25a0Learning Rate\n \n\u25a0Learning Decay Rate\n \n\u25a0Random Seed for Weight Initialization\n \n\u25a0Type of Neighborhood (rectangular or hexagonal)\n \n\u25a0Neighborhood Infl uence (# hops)\nAt a minimum, implementations in software will allow selection of the num-\nber of nodes in the model (x and y directions, or sometimes listed as rows and \ncolumns) and maximum iterations (or maximum time). \nVisualizing Kohonen Maps\nKohonen Maps are often described as aids for data visualization. If there are \nN inputs to the Kohonen Maps, and the map itself is expressed in two dimen-\nsions, such as a 3 \u00d7 3 map, then the model can be interpreted as projecting the \nN dimensional space onto a two-dimensional plane. \nFor example, consider a 3 \u00d7 3 Kohonen Map trained from the nasadata data \nset using all twelve in", "hen the model can be interpreted as projecting the \nN dimensional space onto a two-dimensional plane. \nFor example, consider a 3 \u00d7 3 Kohonen Map trained from the nasadata data \nset using all twelve inputs, Band1 through Band12. A 3 \u00d7 3 map contains there-\nfore nine clusters that can be arranged in a two-dimensional grid. Figure 6-21 \nshows this 3 \u00d7 3 grid with the mean values for four different inputs to the model \noverlaid on top of the grid: Band1, Band5, Band10, and Band12. Along with the \nmean values, a color code has been applied to each node: green for larger positive \nmean value and red for larger negative mean values. Because all inputs to this \nmodel have been z-scored, the mean in each node is a measure of the number \nof standard deviations from the input\u2019s mean.\nThe grid labels are 1, 2, and 3 in each direction (x and y). The order of these \nlabels matters because of the connectedness of the Kohonen Map. This map is \na rectangular grid.\nOften, the most interesting aspect of ", "are 1, 2, and 3 in each direction (x and y). The order of these \nlabels matters because of the connectedness of the Kohonen Map. This map is \na rectangular grid.\nOften, the most interesting aspect of these visualizations is the gradient of \nthe mean values. Each of the four variables has relatively smooth and mono-\ntonic slopes of mean values through the map. Band1 becomes larger and more \n\n \nChapter 6 \u25a0 Descriptive Modeling \n195\npositive for smaller values of Kohonen Y, whereas Band12 is more positive for \nlarger values of Kohonen Y.\nA second way to look at the maps is to compare the relative values of the inputs \nin the same node. Kohonen Map coordinate (1,1) is positive for Band1, Band5, \nand Band10 but negative for Band12. Coordinate (3,1) is positive for Band1 and \nBand12, negative for Band10, but neutral for Band5.\nThese four maps also indicate that each of the four inputs is used in the \nformation of the node locations. If one of the inputs has a mean value close to \n0 in all ni", "r Band10, but neutral for Band5.\nThese four maps also indicate that each of the four inputs is used in the \nformation of the node locations. If one of the inputs has a mean value close to \n0 in all nine nodes, the input therefore has no infl uence in the formation of the \nnodes and can be safely discarded from the model.\nAnother method of visualizing the Kohonen nodes is to plot the 3 \u00d7 3 grid \nwith the relative distance between nodes represented by the length of the line \nconnecting the nodes. Some more sophisticated implementations of the Kohonen \nMap algorithm will identify when a map has \u201cwrapped on top of itself,\u201d as \nshown in Figure 6-22. In this situation, nodes in the Kohonen Map have con-\nnections that cross each other, indicating that initial nodes that were origi-\nnally neighboring each other have moved past one another during training. \nSome practitioners consider these maps to be inferior models and will discard \nthem. A new random initialization of weights is sometimes al", "ing each other have moved past one another during training. \nSome practitioners consider these maps to be inferior models and will discard \nthem. A new random initialization of weights is sometimes all that is needed \nfor the map to be rebuilt and converge properly.\nBAND 1\nKohonen\nY\nKohonen X\n1\n1\n0.840\n0.922\n1.102\n2\n2\n3\n3\n\u22120.322\n\u22121.285\n\u22121.463\n\u22120.275\n\u22120.146\n0.370\nBand 10\nKohonen\nY\nKohonen X\n1\n0.746\n0.033\n\u22121.309\n2\n3\n0.992\n0.992\n0.063\n\u22121.233\n\u22120.213\n\u22121.396\nBand 5\nKohonen\nY\nKohonen X\n1\n1.215\n0.869\n0.079\n2\n3\n0.424\n\u22120.366\n\u22120.908\n\u22121.508\n\u22120.025\n\u22120.453\nBAND 12\nKohonen\nY\nKohonen X\n1\n\u22121.513\n\u22120.125\n0.810\n2\n3\n\u22120.299\n0.609\n0.885\n0.685\n0.112\n0.637\n1\n2\n3\n1\n2\n3\n1\n2\n3\nFigure 6-21:  Overlaying mean values of variables on top of the Kohonen Map\n\n196 \nChapter 6 \u25a0 Descriptive Modeling\n3 \u00d7 3\nlattice,\noverlapped\nFigure 6-22:  3 \u00d7 3 Kohonen Map overlapped on itself\nSimilarities with K-Means\nKohonen Maps have many similarities to K-Means clustering models, many of \nwhich are described in Table 6-10. Data prepara", "igure 6-22:  3 \u00d7 3 Kohonen Map overlapped on itself\nSimilarities with K-Means\nKohonen Maps have many similarities to K-Means clustering models, many of \nwhich are described in Table 6-10. Data preparation steps for Kohonen Maps \nare very similar to those done for K-Means, including missing data handling, \nuse of numeric data, and the need for input magnitude normalization. \nThe similarities break down, however, with parameter updating. The training \nalgorithm for Kohonen Maps proceeds record-by-record, but more importantly, \nit isn\u2019t \u201cwinner take all\u201d; nearby nodes to the node closest to a record are also \nadjusted. In K-Means clustering, the statistics of each cluster are defi ned solely \non cluster members (the records nearest to its cluster center); nearby clusters \nare not affected. \nTable 6-10: Comparison of Kohonen SOMs and K-Means\nCHARACTERISTIC\nKOHONEN MAP\nK\uf6baMEANS\n# Nodes/Clusters\nNumber of nodes pre-deter-\nmined by length and width \nof map\nNumber of clusters \npre-determined.\n#", "able 6-10: Comparison of Kohonen SOMs and K-Means\nCHARACTERISTIC\nKOHONEN MAP\nK\uf6baMEANS\n# Nodes/Clusters\nNumber of nodes pre-deter-\nmined by length and width \nof map\nNumber of clusters \npre-determined.\n# Inputs in Model\nNumber of inputs \ufb01 xed at \nbeginning of training, no vari-\nable selection\nNumber of inputs \ufb01 xed at \nbeginning of training, no vari-\nable selection\n# Model Parameters\nNodes de\ufb01 ned by weights, \none per input\nClusters de\ufb01 ned by mean \nvalue, one per input\nModel Initialization\nRandom initialization of all \nweights\nRandom selection records to \nbe cluster centers\nCluster Membership \nand Distance Metric\nNode membership de\ufb01 ned by \nminimum Euclidean distance \nfrom record to node weights\nCluster membership de\ufb01 ned \nby minimum Euclidean dis-\ntance from record to cluster \nmean\n\n \nChapter 6 \u25a0 Descriptive Modeling \n197\nParameter Updating\nWeights changed for node \nclosest to one record at a time \nand repeated for all records;  \nnearby nodes also modi\ufb01 ed\nMean values for each cluster \n", "er 6 \u25a0 Descriptive Modeling \n197\nParameter Updating\nWeights changed for node \nclosest to one record at a time \nand repeated for all records;  \nnearby nodes also modi\ufb01 ed\nMean values for each cluster \nupdated based on all data at \nonce, independent of other \nclusters\nPositioning of Clusters/\nNodes\nInter-Node distances are direct \nresult of training\u2014indicate \ndata distances\nInter-cluster distances are \ndetermined indirectly\nHow similar Kohonen Maps appear compared to K-Means models varies \nwidely; sometimes for the same number of clusters and nodes, the models are \nnearly identical, while in other situations they bear little resemblance to one \nanother. Consider Kohonen Maps as a separate but related algorithm to K-Means.\nA key metric that measures how well K-Means models fi t the data is the SSE \nmeasure, which computes the average distance between the records in a cluster \nand the center of a cluster. Kohonen Maps could take advantage of this same \nmetric to help in determining the bes", "e SSE \nmeasure, which computes the average distance between the records in a cluster \nand the center of a cluster. Kohonen Maps could take advantage of this same \nmetric to help in determining the best number of nodes to include in the map. \nIn Kohonen Map terminology, SSE is computed by calculating the distance \nbetween the records in a Kohonen node and the weights of that node. However, \nthis measure is rarely if ever computed.\nSummary\n Every predictive modeler should become adept at building descriptive models, \nwhether the models are deployed or used primarily to understand the data bet-\nter. Most predictive modeling software includes Principal Component Analytics \nand K-Means algorithms; analysts should learn these algorithms to include \nin their repertoire. Some tools also contain Kohonen Self-Organizing Maps, \nwhich are built and assessed similarly to K-Means. Pay particular attention to \nthe input data to ensure the clusters are formed well; data preparation for these \nthree al", "honen Self-Organizing Maps, \nwhich are built and assessed similarly to K-Means. Pay particular attention to \nthe input data to ensure the clusters are formed well; data preparation for these \nthree algorithms is very important. \n\n\n199\nOnce a descriptive model has been built, the next task of predictive modelers \nis to interpret the meaning of the model. Most software provides summaries \nof the clusters in tabular or graphic format. The typical summaries provide \nhelpful information to interpret the meaning of the clusters, most typically \nthe cluster center, but fall short of explaining why the clustering algorithms \nformed the clusters.\nThis chapter describes approaches you can take to gain insights from clus-\ntering models. We focus on three aspects of interpretation: describing clusters, \ndescribing key differences between clusters, and translating variable values \nfrom normalized values back to their original units. The example in the chapter \nis based on a K-Means model, but the t", ", \ndescribing key differences between clusters, and translating variable values \nfrom normalized values back to their original units. The example in the chapter \nis based on a K-Means model, but the techniques apply to Kohonen SOMs just \nas well.\nStandard Cluster Model Interpretation\nModel summaries for K-Means clusters always contain counts of records in the \ntraining data that fell into each cluster and the mean value of every variable for \neach cluster. Sometimes the standard deviation of each variable in each cluster \nis also included. For Kohonen SOMs, sometimes only the count of records that \nfell into each node is included in the reports. \nC H A P T E R \n7\nInterpreting Descriptive Models\n\n200 \nChapter 7 \u25a0 Interpreting Descriptive Models\nEven if the software does not provide summaries, generating summary \nstatistics is straightforward: First group by cluster ID, and then for each cluster \ncompute the statistics of interest, most often counts of records in each cluster, \nthe minim", "s, generating summary \nstatistics is straightforward: First group by cluster ID, and then for each cluster \ncompute the statistics of interest, most often counts of records in each cluster, \nthe minimum, maximum, mean, and standard deviation for each variable in \nthe cluster. The mean value for each variable in each cluster is, by defi nition, \nthe cluster center. If you also include the standard deviation of each variable in \neach cluster, evaluating the spread of each variable furthers your understand-\ning of the characteristics of each cluster. If the standard deviation is relatively \nsmall, the variable has a compact range in the cluster.\nFor example, consider a K-Means clustering model containing 11 input variables \nand their data types, listed in Table 7-1. The dummy variables, originally having \nvalues 0 and 1, have been scaled to values 0.3 and 0.7 to reduce their infl uence \nin the clustering model. This practice is not usually done in software, but can \nbe helpful to reduce t", "nally having \nvalues 0 and 1, have been scaled to values 0.3 and 0.7 to reduce their infl uence \nin the clustering model. This practice is not usually done in software, but can \nbe helpful to reduce the bias that sometimes occurs with dummy variables. The \nordinal, interval, and ratio variables have all been scaled to the range from 0 to 1.\nTable 7-1: Variables Included in a 3-Cluster Model\nVARIABLE\nVARIABLE TYPE\nD_RFA_2A\nDummy\nE_RFA_2A\nDummy\nF_RFA_2A\nDummy\nRFA_2F\nOrdinal\nDOMAIN1\nDummy\nDOMAIN2\nDummy\nDOMAIN3\nDummy\nFISTDATE\nInterval\nLASTDATE\nInterval\nLASTGIFT_log10\nRatio\nNGIFTALL_log10\nRatio\nAfter building a 3-cluster model with the K-Means algorithm, Table 7-2 shows \nthe mean values of these 11 variables in the clusters: These are the cluster centers. \nThe relative values of each of the cluster centers can be compared to the global, \noverall statistics. Clusters 1 and 2 have a higher than average number of gifts \ngiven (NGIFTALL), higher than average number of gifts given in the past ye", "ster centers can be compared to the global, \noverall statistics. Clusters 1 and 2 have a higher than average number of gifts \ngiven (NGIFTALL), higher than average number of gifts given in the past year \n(RFA_2F), higher than average most recent gift given (LASTDATE), but lower \nthan average size of the last gift (LASTGIFT). In other words, Clusters 1 and 2 \n\n \nChapter 7 \u25a0 Interpreting Descriptive Models \n201\ncontain donors who give more often but in lower amounts than average. Cluster \n3 is the reverse: These donors give larger amounts but less frequently.\nTable 7-2: Cluster Centers for K-Means 3-Cluster Model\nVARIABLE\nCLUSTER 1\nCLUSTER 2\nCLUSTER 3\nOVERALL\n# Records in Cluster\n8,538\n8,511\n30,656\n47,705\nLASTDATE\n0.319\n0.304\n0.179\n0.226\nFISTDATE\n0.886\n0.885\n0.908\n0.900\nRFA_2F\n0.711\n0.716\n0.074\n0.303\nD_RFA_2A\n0.382\n0.390\n0.300\n0.331\nE_RFA_2A\n0.499\n0.500\n0.331\n0.391\nF_RFA_2A\n0.369\n0.366\n0.568\n0.496\nDOMAIN3\n0.449\n0.300\n0.368\n0.370\nDOMAIN2\n0.300\n0.700\n0.489\n0.493\nDOMAIN1\n0.515\n0.300\n0.427\n0", "074\n0.303\nD_RFA_2A\n0.382\n0.390\n0.300\n0.331\nE_RFA_2A\n0.499\n0.500\n0.331\n0.391\nF_RFA_2A\n0.369\n0.366\n0.568\n0.496\nDOMAIN3\n0.449\n0.300\n0.368\n0.370\nDOMAIN2\n0.300\n0.700\n0.489\n0.493\nDOMAIN1\n0.515\n0.300\n0.427\n0.420\nNGIFTALL_log10\n0.384\n0.385\n0.233\n0.287\nLASTGIFT_log10\n0.348\n0.343\n0.430\n0.400\nMany predictive analytics software packages provide reports in the form of \nhistograms of each variable by cluster. For example, Figure 7-1 shows a histo-\ngram of RFA_2F. It is clear from this fi gure that Cluster 3 is different from the \nother two, and that RFA_2F contributes to this difference.\ncluster_1\ncluster_2\ncluster_3\n[0\u20130.102]\n(0.305\u20130.406]\n(0.608\u20130.709]\n(0.911\u20131.013]\n0\n109\n218\n327\n436\n545\n654\n763\n872\n981\n1090\n1208\nFigure 7-1:  RFA_2F stacked histogram to interpret clusters\n\n202 \nChapter 7 \u25a0 Interpreting Descriptive Models\nProblems with Interpretation Methods\nThere are signifi cant problems with using the procedure described so far as a \nway to interpret cluster models. First, if you have normalized", "g Descriptive Models\nProblems with Interpretation Methods\nThere are signifi cant problems with using the procedure described so far as a \nway to interpret cluster models. First, if you have normalized the data prior to \nclustering or building the models, all of the summaries are in the normalized \nunits and may not be readily understood. Second, the summary statistics tell \nyou what each cluster is like, but not how the clusters differ, and therefore why \nthe clusters were formed as they were.\nNormalized Data\nChapter 6 described the common reasons for normalizing data prior to clustering. \nTable 7-2 shows the mean values of the normalized data. For instance, RFA_2F \nhas mean values of 0.711, 0.716, and 0.074 for Clusters 1, 2, and 3 respectively. \nRFA_2F had four values prior to normalization: 1, 2, 3 and 4. After normalizing \nto a range between 0 and 1, these four values become 0, 0.333, 0.666, and 1.0. \nTherefore, if the values of a normalized RFA_2F in Cluster 3 are close to 0, this", "ion: 1, 2, 3 and 4. After normalizing \nto a range between 0 and 1, these four values become 0, 0.333, 0.666, and 1.0. \nTherefore, if the values of a normalized RFA_2F in Cluster 3 are close to 0, this \ncorresponds to values that are predominantly 1 in the unnormalized units. On \noccasion in Cluster 3, RFA_2F values are equal to 0.333, which corresponds to \nthe value 2 in unnormalized units. \nContinuing with RFA_2F, you can see that Clusters 1 and 2 have values \nof RFA_2F primarily equal to 3 and 4, the more frequent donors. The same \nargument can be made for dummy variables such as DOMAIN1, DOMAIN2, \nand DOMAIN3. These have been scaled to values 0.3 and 0.7, and therefore \nCluster 2, DOMAIN1, and DOMAIN3 are always equal to 0.3 in normalized \nunits, which corresponds to the value 0 in unnormalized units; Cluster 2 has \nno members with DOMAIN3 or DOMAIN1 populated. Dummy variables and \nsimple ordinal variables are therefore easy to convert from normalized units \nback to unnormalized uni", "rmalized units; Cluster 2 has \nno members with DOMAIN3 or DOMAIN1 populated. Dummy variables and \nsimple ordinal variables are therefore easy to convert from normalized units \nback to unnormalized units.\nHowever, more complex ordinal variables, interval variables, and ratio variables \nbecome more problematic to interpret. This becomes even more complicated \nwhen additional transformations are applied prior to the scaling step; in this data, \nNGIFTALL and LASTGIFT are transformed by the log base 10 operation prior \nto scaling to the range 0 to 1. (LASTGIFT is the donation amount given the last \ntime the donor gave.) From Table 7-2, we know that the mean LASTGIFT_log10 \nvalue in Cluster 3 is 0.430. So how much money on average was donated in the \nlast gift for donors in Cluster 3? You cannot fi gure this out without knowing the \nscaling factor that transformed LASTGIFT_log10 to a number between 0 and 1. \nAfter doing the math to convert the normalized units to unnormalized units, \nthe mea", " fi gure this out without knowing the \nscaling factor that transformed LASTGIFT_log10 to a number between 0 and 1. \nAfter doing the math to convert the normalized units to unnormalized units, \nthe mean LASTGIFT_log10 value in Cluster 3 is $20.40. Interpreting clusters \nbased on the normalized units is therefore problematic, requiring additional \nwork to identify the actual values of the cluster centers. \n\n \nChapter 7 \u25a0 Interpreting Descriptive Models \n203\nWithin-Cluster Descriptions \nDescribing the mean of a cluster tells us what that cluster looks like, but tells \nus nothing about why that cluster was formed. Consider Clusters 1 and 2 from \nTable 7-2. The mean values describe each of the clusters, but a closer examination \nshows that nearly all the variables have means that are similar to one another \n(we will see how signifi cant these differences are after computing ANOVAs). \nThe only three variables that, after visual inspection, contain differences are \nDOMAIN1, DOMAIN2, and DOMAI", " one another \n(we will see how signifi cant these differences are after computing ANOVAs). \nThe only three variables that, after visual inspection, contain differences are \nDOMAIN1, DOMAIN2, and DOMAIN3. The differences are key: If the purpose \nof the cluster model is to fi nd distinct sub-populations in the data, it is critical \nto not only describe each cluster, but also to describe how they differ from one \nanother.\nExamining the differences between cluster characteristics provides addi-\ntional insight into why the clusters were formed. However, determining how \nthe clusters differ can be quite challenging from reports such as the one shown \nin Table 7-2. Good visualization of the clusters can help. But whether you use \ntables or graphs, identifying differences requires scanning every variable and \nevery cluster. If there are 20 inputs and 10 clusters, there are 200 histograms or \nsummary s tatistics to examine. \nIdentifying Key Variables in Forming Cluster Models\nThe question \u201chow ", "ariable and \nevery cluster. If there are 20 inputs and 10 clusters, there are 200 histograms or \nsummary s tatistics to examine. \nIdentifying Key Variables in Forming Cluster Models\nThe question \u201chow do the clusters differ from one another?\u201d can be  posed as \na supervised learning problem. After a cluster model is built, each record is \nplaced in one and only one cluster. Now each record has an additional column \nin the data: the cluster label column. If the cluster label is identifi ed as the tar-\nget variable and the same inputs used to build the cluster model are inputs to \nthe supervised learning model, you can determine which inputs are the best \npredictors of the cluster label from the supervised model.\nThere are several supervised learning methods that can help differentiate \nthe clusters, but the purpose of the model is interpretation not accuracy per se; \nwe want to use methods that emphasize interpretation. Three techniques for \ncluster model interpretation are described here", "usters, but the purpose of the model is interpretation not accuracy per se; \nwe want to use methods that emphasize interpretation. Three techniques for \ncluster model interpretation are described here: ANOVA, hierarchical cluster-\ning, and decision trees. These come from different disciplines and can describe \ndifferent characteristics of the cluster models. They should therefore be viewed \nas complementary rather than replacements for one another.\nThere is one additional issue to consider when building supervised models \nfor cluster model interpretation. The single column containing the cluster label \nhas as many levels as there are clusters. Table 7-2 shows the result of a model \nwith only three clusters. However, there can be dozens or even hundreds of \nclusters in the model, and therefore there can be dozens or hundreds of levels \nin the cluster label column. A single model intended to differentiate dozens to \n\n204 \nChapter 7 \u25a0 Interpreting Descriptive Models\nhundreds of variables ", "ore there can be dozens or hundreds of levels \nin the cluster label column. A single model intended to differentiate dozens to \n\n204 \nChapter 7 \u25a0 Interpreting Descriptive Models\nhundreds of variables can be problematic for supervised learning methods, \nparticularly for the ANOVA approach.\nOne approach to overcoming this problem is to explode the multi-level target \nvariable into dummy variables, one per cluster, and then build a separate model \nfor differentiating each cluster from all the others. This adds to the complexity \nof the analysis, but can yield insights that are hidden in models that try to dif-\nferentiate all levels simultaneously.\nANOVA\nRunning an ANOVA is a simple and effective way to determine which variables \nhave the most signifi cant differences in mean values between the clusters. The \nANOVA grouping variable is set to the cluster label, and the inputs to the cluster \nmodel are used as the test columns. Fortunately, the typical data preparation \nneeded prior to comp", "e clusters. The \nANOVA grouping variable is set to the cluster label, and the inputs to the cluster \nmodel are used as the test columns. Fortunately, the typical data preparation \nneeded prior to computing an ANOVA is the same preparation already done \nfor clustering, so no new data preparation is typically needed. The ANOVA \ncomputes an F-statistic that measures the differences in mean values between \nthe clusters and a p-value indicating the signifi cance of the differences in mean \nvalues. \nThe ANOVA method works best when there are relatively few clusters. The \nmore clusters in the model, the less likely that inputs will have signifi cantly \ndifferent values for all clusters; often, a few clusters will have values of a vari-\nable that differ from the other clusters but in most clusters there won\u2019t be much \nvariation at all. This effect will suppress the F-statistic and could obscure the \nsignifi cant role of a variable in the cluster models if the mean value of a variable \nis the s", " there won\u2019t be much \nvariation at all. This effect will suppress the F-statistic and could obscure the \nsignifi cant role of a variable in the cluster models if the mean value of a variable \nis the same for most of the clusters and only varies for a few. \nFor example, consider the 3-cluster model described in Table 7-2. Table 7-3 \nshows the F-statistic and p-value for each of the variables, sorted by signifi cance. \nRFA_2F has the largest F-statistic by far, and therefore is considered the most \nimportant variable in forming the clusters. The least signifi cant variables are \nthe DOMAIN1, 2, and 3 dummy variables. One note on signifi cance should be \nemphasized. The fact that the p-values for all of the variables is 0 or near 0 can \nbe indicative of the size of the data set as well as the differences in mean values. \nTable 7-3: ANOVA Interpretation of Cluster Variable Signi\ufb01 cance\nVARIABLE\nF\uf6baSTATISTIC\nP\uf6baVALUE\nRFA_2F\n111630.5\n0\nD_RFA_2A\n21187.9\n0\nLASTGIFT_log10\n10435.7\n0\nE_RFA_2A\n8499.", " differences in mean values. \nTable 7-3: ANOVA Interpretation of Cluster Variable Signi\ufb01 cance\nVARIABLE\nF\uf6baSTATISTIC\nP\uf6baVALUE\nRFA_2F\n111630.5\n0\nD_RFA_2A\n21187.9\n0\nLASTGIFT_log10\n10435.7\n0\nE_RFA_2A\n8499.3\n0\n\n \nChapter 7 \u25a0 Interpreting Descriptive Models \n205\nNGIFTALL_log10\n7664.8\n0\nF_RFA_2A\n6813.5\n0\nLASTDATE\n3313.3\n0\nFISTDATE\n1073.4\n0\nDOMAIN1\n103.0\n0\nDOMAIN2\n36.6\n1.11E\u221216\nDOMAIN3\n21.7\n3.84E\u221210\nFurther insights into the key variables driving cluster formation are then \nobtained through visualization of these variables through histograms and scat-\nterplots. One example is the histogram of RFA_2F in Figure 7-1; it is easy to see \nwhy it was identifi ed by the ANOVA as defi ning the clusters.\nConsider two fi nal notes on the ANOVA approach. First, if some of the vari-\nables did not have statistically signifi cant mean values, they would be good \ncandidates for pruning from the cluster model entirely. Second, the ANOVA \napproach can show the effect of different normalization and scaling approa", "ally signifi cant mean values, they would be good \ncandidates for pruning from the cluster model entirely. Second, the ANOVA \napproach can show the effect of different normalization and scaling approaches \nin preparing the data for clustering. When clusters were built without scaling \non this data set, the top four variables as ranked by F-statistic were DOMAIN2, \nDOMAIN3, E_RFA_2A, and F_RFA_2A: all dummy variables. These dummy \nvariables therefore drove the formation of the clusters when their natural 0 and \n1 values were used. After they were scaled to 0.3 and 0.7, they fell in importance. \nNeither scaling method is right or wrong; they only provide different emphases \nfor the K-Means and Kohonen SOMs to use in building the clusters.\nHierarchical Clustering\nA second technique that you can use to interpret cluster models is hierarchical \nclustering. Why build a hierarchical cluster model to interpret K-Means clusters? \nHierarchical clustering has the very appealing quality that the m", "an use to interpret cluster models is hierarchical \nclustering. Why build a hierarchical cluster model to interpret K-Means clusters? \nHierarchical clustering has the very appealing quality that the model shows all \nlevels of groupings of the data, from each record belonging to its own group, \nto all records belonging to a single group. Usually, for predictive modeling, the \ndisadvantage of using hierarchical clustering is the size of data; often, there \nare far too many records for hierarchical clusters to be effi cient and practical.\nHowever, if rather than beginning with individual records, we instead begin \nwith the cluster centers found by K-Means as the input data (refer to Table 7-2), \nthe number of records is small, and hierarchical clustering becomes very attrac-\ntive. Consider a K-Means model with 21 clusters. After aggregating the data by \ncluster label and computing the mean value for each input, you have 21 records \nand 14 inputs. The hierarchical clustering model will the", "K-Means model with 21 clusters. After aggregating the data by \ncluster label and computing the mean value for each input, you have 21 records \nand 14 inputs. The hierarchical clustering model will then identify groups of \n\n206 \nChapter 7 \u25a0 Interpreting Descriptive Models\ncluster centers that are most alike and group them together. The sequence of \nclusters found by hierarchical clustering shows clearly which clusters are most \nrelated to other clusters, and, by extension, which clusters are most dissimilar \nfrom other clusters.\nDecision Trees\nDecision trees are excellent choices for cluster interpretation because they are \nsupervised models known for their ease of interpretation. (If you are unfamiliar \nwith decision trees, Chapter 8 includes a full explanation.) In addition, deci-\nsion trees scale well with large numbers of inputs or large numbers of levels \nin the target variable. If the cluster model contains 100 clusters, decision trees \ncan fi nd patterns for each of the clusters,", "ees scale well with large numbers of inputs or large numbers of levels \nin the target variable. If the cluster model contains 100 clusters, decision trees \ncan fi nd patterns for each of the clusters, provided there are enough records to \nfi nd the patterns.\nTo use decision trees as an interpreter of clustering models, the same protocol \nis followed as was done for the ANOVA. The target variable is set to the cluster \nlabel column, and the same inputs used to build the clusters are selected to \nbuild the decision tree. The depth of the tree should be kept relatively shallow \nto maintain the interpretation of the tree, usually four to fi ve levels deep at most, \nalthough there are reasons for building more complex trees if the complexity \nof the cluster model warrants it.\nAn additional benefi t of building decision trees is that there is no need to be \ntied to the normalized data as one is with building the ANOVA or hierarchical \nclusters. Decision trees are not distance-based algorithm", "fi t of building decision trees is that there is no need to be \ntied to the normalized data as one is with building the ANOVA or hierarchical \nclusters. Decision trees are not distance-based algorithms and therefore are \nunaffected by outliers and skewed distributions. Therefore, rather than using \nthe normalized inputs, one can use the original, unnormalized inputs for every \nvariable in the cluster model. Dummy variables can be kept with the 0 and 1 \nvalues, or even used in their categorical form. LASTGIFT and NGIFTALL do \nnot have to be log transformed and scaled. \nA third benefi t to using decision trees is that they are inherently interaction \ndetectors. The ANOVA approach is effective at fi nding single variable relation-\nships to the clusters. The tree can fi nd when two or more variables together help \nform the cluster. This is closer to what one expects from the cluster models: \nmultiple variables co-occurring to form clusters. Because the original form of \nthe variables can b", "iables together help \nform the cluster. This is closer to what one expects from the cluster models: \nmultiple variables co-occurring to form clusters. Because the original form of \nthe variables can be used in building the decision trees, interpretation of the \nclusters will be even more transparent than the other approaches described so \nfar, which relied on normalized data.\nConsider a tree that is built to interpret the 3-cluster K-Means model. For the \npurpose of this discussion, the rules generated by the tree are considered, where \nthe rules are the combination of decisions the tree makes to try to predict the \ncluster label. The rules are broken down into three groups, one for each cluster \nlabel. Table 7-4 lists seven rules found by the  decision tree created to  predict \nthe three clusters.\n\n \nChapter 7 \u25a0 Interpreting Descriptive Models \n207\nTable 7-4: Decision Tree Rules to Predict Clusters\nCLUSTER \nLABEL\nRULE\nNUMBER OF \nRECORDS \nMATCHING \nRULE\nNUMBER OF \nRECORDS \nWITH CLUSTER", "clusters.\n\n \nChapter 7 \u25a0 Interpreting Descriptive Models \n207\nTable 7-4: Decision Tree Rules to Predict Clusters\nCLUSTER \nLABEL\nRULE\nNUMBER OF \nRECORDS \nMATCHING \nRULE\nNUMBER OF \nRECORDS \nWITH CLUSTER \nLABEL IN RULE\nTREE \nACCURACY\nCluster 1, \nRule 1\nDOMAIN2 = 0 and \nRFA_2F > 2\n6,807\n6,807\n100%\nCluster 1, \nRule 2\nDOMAIN2 = 0 and \nRFA_2F = 2 and \nE_RFA_2A = 1\n1,262\n1,262\n100%\nCluster 2, \nRule 1\nDOMAIN2 = 1 and \nRFA_2F > 2\n6,751\n6,751\n100%\nCluster 2, \nRule 2\nDOMAIN2 = 1 and \nRFA_2F = 2 and \nE_RFA_2A = 1\n1,304\n1,304\n100%\nCluster 3, \nRule 1\nRFA_2F = 1 and \nE_RFA_2A = 0\n21,415\n21,415\n100%\nCluster 3, \nRule 2\nRFA_2F = 1 and \nE_RFA_2A = 1\n2,432\n2,411\n99%\nCluster 3, \nRule 3\nRFA_2F = 2 and \nE_RFA_2A = 0 and \nF_RFA_2A = 1\n5,453\n5,344\n98%\nThe seven simple rules found by the tree are effective in fi nding the key drivers \nof the clusters. For example, Cluster 3 has two distinct groups in its defi nition. \nFirst, as you saw in Figure 7-1, when RFA_2F = 1 the donor belongs to Cluster 3. \nSecond, even ", "ng the key drivers \nof the clusters. For example, Cluster 3 has two distinct groups in its defi nition. \nFirst, as you saw in Figure 7-1, when RFA_2F = 1 the donor belongs to Cluster 3. \nSecond, even if RFA_2F = 2, as long as F_RFA_2A is 1, the donor still belongs to \nCluster 3. This second interaction went undetected in the ANOVA and wasn\u2019t \nclearly identifi ed by the mean values of Table 7-1.\nOnce the rules are found, visualization can make the rules clearer. Cluster \n1 and Cluster 2 rules are based largely on RFA_2F and DOMAIN2, and \nFigure 7-2 shows a visual cross-tab indicating where the rules apply in the plot. \nIn Figure 7-2, the x axis is RFA_2F, with the box for Cluster 3, Rule 1 above the \nvalue RFA_2F = 1 in unnormalized units. The y axis is DOMAIN2 with its \ntwo values 0 and 1.\nIt is not unusual that trees built to predict cluster membership have high \naccuracy, although the more clusters in the target variable, the more complex \nthe tree must be to uncover all the patterns", "\nIt is not unusual that trees built to predict cluster membership have high \naccuracy, although the more clusters in the target variable, the more complex \nthe tree must be to uncover all the patterns and rules. In this 3-cluster model, \nthese seven simple rules match more than 95 percent of the records in the \ndata, and therefore represent the vast majority of key patterns in the  \ncluster model, as shown in Table  7-5.\n\n208 \nChapter 7 \u25a0 Interpreting Descriptive Models\n1\nCluster 3,\nRule 1\nCluster 2, Rule 1\nCluster 1, Rule 1\n0\n1\n2\n3\n4\ncluster_1\ncluster_2\ncluster_3\nFigure 7-2:  Visualization of decision tree rules\nTable 7-5: Percentage of Records Matching Rule in Decision Tree\nCLUSTER\n# RULES\n# RECORDS REPRESENTED BY KEY RULES\n% OF \nCLUSTER\nCluster 1\n2\n8,069\n94.5\nCluster 2\n2\n8,055\n94.6\nCluster 3\n3\n29,300\n95.6\nIrrelevant Variables\nThe 3-cluster model used as an example in this chapter is a simple model with \nonly 11 inputs. When cluster models are more complex, unlike this model, many \nv", "ter 3\n3\n29,300\n95.6\nIrrelevant Variables\nThe 3-cluster model used as an example in this chapter is a simple model with \nonly 11 inputs. When cluster models are more complex, unlike this model, many \nvariables may not show any relationship with the clusters when interpreted \nthrough any of these three methods.\nWhat if some variables do not show up as signifi cant in any interpretation \nmodel? These are good candidates for removal from the cluster model simply \nbecause they are not adding signifi cantly to the formation of the clusters. If \nthey truly have little infl uence in the clusters, their removal won\u2019t infl uence the \ncluster centers, particularly when the number of irrelevant variables is relatively \nsmall compared to the number of signifi cant variables. A model rebuilt with \nfewer variables will deploy more quickly, however; this may be a consideration \nfor some applications. \n\n \nChapter 7 \u25a0 Interpreting Descriptive Models \n209\nCluster Prototypes\nA prototype is an actual recor", " variables will deploy more quickly, however; this may be a consideration \nfor some applications. \n\n \nChapter 7 \u25a0 Interpreting Descriptive Models \n209\nCluster Prototypes\nA prototype is an actual record in the modeling data that is a representative of the \ncluster rather than a statistical summary used to describe the cluster. The operation \nof identifying prototypes is the inverse of identifying outliers; prototypes fi nd \nminimum distance records in the data as opposed to maximum distance records. \nCluster distances are computed between cluster centers and each record in the \ncluster. Some software will compute the distances and identify prototypes as part \nof the clustering algorithm, but most will not, leaving it to the modeler to identify. \nSometimes identifying records that typify the cluster can be helpful in gain-\ning additional insight into the meaning of each cluster. The cluster center, the \nmean value of each input to the model, may not actually be a data point, and \ntherefo", " cluster can be helpful in gain-\ning additional insight into the meaning of each cluster. The cluster center, the \nmean value of each input to the model, may not actually be a data point, and \ntherefore is not truly representative of the records in the cluster. However, the \nrecords with the smallest distance to the cluster center are the most typical data \npoints in the cluster (See Table 7-6). \nTable 7-6: Cluster Prototypes\nFIELD\nCLUSTER 1, \nPROTOTYPE 1\nCLUSTER 2, \nPROTOTYPE 1\nCLUSTER 3, \nPROTOTYPE 1\nDistance\n0.375\n0.301\n0.313\nLASTDATE\n0.492\n0.492\n0.045\nFISTDATE\n0.895\n0.895\n0.915\nRFA_2F\n0.667\n0.667\n0\nD_RFA_2A\n0.3\n0.3\n0.3\nE_RFA_2A\n0.7\n0.7\n0.3\nF_RFA_2A\n0.3\n0.3\n0.7\nDOMAIN3\n0.3\n0.3\n0.3\nDOMAIN2\n0.3\n0.7\n0.3\nDOMAIN1\n0.7\n0.3\n0.3\nNGIFTALL_log10\n0.392\n0.375\n0.230\nLASTGIFT_log10\n0.347\n0.347\n0.434\nBut while these values are accurate and correct, they are not intuitive. Once \nthe distance from the cluster center has been computed (in the normalized units) \nand the prototypes have been found, the ", "ut while these values are accurate and correct, they are not intuitive. Once \nthe distance from the cluster center has been computed (in the normalized units) \nand the prototypes have been found, the original values of the key variables \ncan be identifi ed by joining the record by the record ID or by de-normalizing \nthe data so that the variables associated with the minimum distance record can \n\n210 \nChapter 7 \u25a0 Interpreting Descriptive Models\nbe shown. This level of interpretation also helps decision-makers as they tie the \nclusters to what a record means, donors in the case of the KDD Cup 1998 data.\nTable 7-7 shows the same table of prototypes with the variables transformed \nback to their original, pre-normalized units. Now you can see more clearly \nwhat values are typical of each cluster. Cluster 3 has more gifts (NGIFTALL) \nand smaller donation amounts (LASTGIFT) compared to Clusters 1 and 2. The \nsocio-economic status of the donors (DOMAIN1 and DOMAIN2) is key in dif-\nferentiating", "r. Cluster 3 has more gifts (NGIFTALL) \nand smaller donation amounts (LASTGIFT) compared to Clusters 1 and 2. The \nsocio-economic status of the donors (DOMAIN1 and DOMAIN2) is key in dif-\nferentiating Cluster 1 from Cluster 2.\nTable 7-7: Cluster Prototypes in Natural Units\nFIELD\nCLUSTER 1, \nPROTOTYPE 1\nCLUSTER 2, \nPROTOTYPE 1\nCLUSTER 3, \nPROTOTYPE 1\nDistance\n0.375\n0.301\n0.313\nLASTDATE\n9601\n9601\n9512\nFISTDATE\n9111\n9109\n9203\nRFA_2F\n3\n3\n1\nD_RFA_2A\n0\n0\n0\nE_RFA_2A\n1\n1\n0\nF_RFA_2A\n0\n0\n1\nDOMAIN3\n0\n0\n0\nDOMAIN2\n0\n1\n0\nDOMAIN1\n1\n0\n0\nNGIFTALL\n10\n10\n19\nLASTGIFT\n12\n11\n5\nThese tables show only one prototype for each cluster, but you can show \nmore prototypes as well. The second prototype is the record that has the second \nsmallest distance to the cluster mean, the third prototype the record that has \nthe third smallest distance to the cluster mean, and so on. \nCluster Outliers\nK-Means clustering and Kohonen SOMs require every record to fall into one and \nonly one node. If there are outliers in the dat", "third smallest distance to the cluster mean, and so on. \nCluster Outliers\nK-Means clustering and Kohonen SOMs require every record to fall into one and \nonly one node. If there are outliers in the data, they still have to be assigned to a \nnearest cluster but at a large distance from the cluster center. An outlier in this \ncontext is multidimensional, based on all of the inputs in the data rather than \nany single variable. Sometimes multidimensional outliers are called anomalies.\nOutliers are problematic for several reasons. First, outliers infl uence the cluster \ncenters disproportionately, distorting the cluster centers. Second, operationally, \nthese outliers are different from most of the examples in the cluster, and the \n\n \nChapter 7 \u25a0 Interpreting Descriptive Models \n211\nfurther they are from the cluster center, the less confi dence we have that they \ncan be treated the same way as other records in the cluster. \nThere is no precise defi nition of an outlier. However, if one begins", "are from the cluster center, the less confi dence we have that they \ncan be treated the same way as other records in the cluster. \nThere is no precise defi nition of an outlier. However, if one begins with the \nassumption of a Normal distribution, a rule of thumb is to use three standard \ndeviations from the mean as a metric because 99.8 percent of the data falls within \nplus or minus three standard deviations from the mean. Other commonly used \nthresholds include two standard deviations (95.6 percent of the data falls within \nthis range) or even 90 percent of the data, calling data outliers that fall outside \nplus or minus 90 percent of the data.\nOutliers are sometimes interesting in of themselves, particularly for risk \nanalysis and fraud detection. The outliers are unlike anything else in the data \nand have been forced into becoming members of clusters they are not particu-\nlarly similar to. They are, therefore, good candidates for more careful scrutiny.\nOutliers in customer analyti", "in the data \nand have been forced into becoming members of clusters they are not particu-\nlarly similar to. They are, therefore, good candidates for more careful scrutiny.\nOutliers in customer analytics may be ignored because they don\u2019t fi t into an \naccepted segment, or they can be treated differently from the other customers \nin the segment, essentially forming a subcluster within the cluster. Identifying \noutliers and treating them as separate subgroups may be more effi cient and prag-\nmatically effective than adding clusters to the model specifi cation and rebuilding.\nAs an example, consider Table 7-8, a table containing the cluster centers for \nthe three clusters and the largest outlier from each cluster. The bold entries in \nTable 7-8 indicate values that are outliers in the cluster. Distances were computed \nin normalized units, but the values in the table are displayed in natural units \nto make the results easier to understand. Interestingly, LASTDATE has large \nvalues as outlie", " Distances were computed \nin normalized units, but the values in the table are displayed in natural units \nto make the results easier to understand. Interestingly, LASTDATE has large \nvalues as outliers for each cluster even though it isn\u2019t a key variable.\nTable 7-8: Cluster Outliers\nFIELD\nCLUSTER \n1, CENTER\nCLUSTER \n1, OUTLIER\nCLUSTER \n2, CENTER\nCLUSTER \n2, OUTLIER\nCLUSTER \n3, CENTER\nCLUSTER \n3, OUTLIER\nDistance\n0\n1.123\n0\n1.140\n0\n1.088\nLASTDATE\n9566\n9702\n9563\n9702\n9539\n9701\nFISTDATE\n9069\n8801\n9064\n7401\n9173\n8609\nRFA_2F\n3.1\n1\n3.1\n1\n1.2\n1\nD_RFA_2A\n0.21\n0\n0.23\n0\n0.00\n0\nE_RFA_2A\n0.50\n1\n0.50\n1\n0.08\n1\nF_RFA_2A\n0.17\n0\n0.16\n0\n0.67\n0\nDOMAIN3\n0.37\n1\n0.00\n0\n0.17\n0\nDOMAIN2\n0.00\n0\n1.00\n1\n0.47\n1\nDOMAIN1\n0.54\n0\n0.00\n0\n0.32\n0\nNGIFTALL\n10.1\n17\n9.7\n67\n18.5\n15\nLASTGIFT\n11.5\n0\n11.6\n8\n5.1\n0\n\n212 \nChapter 7 \u25a0 Interpreting Descriptive Models\nAdding a few additional clusters to the model by increasing K slightly or \nby adding a few more nodes in a dimension of an SOM may not capture these \noutliers into thei", "Interpreting Descriptive Models\nAdding a few additional clusters to the model by increasing K slightly or \nby adding a few more nodes in a dimension of an SOM may not capture these \noutliers into their own cluster, depending on how many additional clusters are \nadded and the shape of the data. Eventually, if enough clusters are added, it is \ninevitable that the outliers will be characterized by their own clusters, although \nperhaps the outliers from a single cluster in a simpler model may require a dozen \nor more clusters to characterize them fully if the data points are dispersed in \nevery direction from the center of the cluster.\nSummary\nInterpreting unsupervised learning models, clustering models in particular, is a \nchallenge for predictive modelers because there are no clear and standard met-\nrics like those used to assess supervised learning models. You should consider \ncomputing both within-cluster statistics to describe each cluster and between-\ncluster statistics to determine ", "ard met-\nrics like those used to assess supervised learning models. You should consider \ncomputing both within-cluster statistics to describe each cluster and between-\ncluster statistics to determine which variables are most important in separating \nthe clusters. The former is found commonly in predictive analytics software, and \neven when the software does not compute within-cluster statistics, the analyst \ncan compute them easily with simple aggregations of variables.\nSoftware rarely computes between-cluster differences and reports the results; \nthis chapter described several approaches you can use to identify the variables \nthat defi ne the clusters.\nRemember to beware of outliers, variables with large magnitudes, and highly \nskewed variables that can bias statistics that summarize the clusters. Most of these \nproblems should be identifi ed and corrected during Data Preparation, though \nif any problems remain, they can still be uncovered from the cluster statistics.\n\n213\nThis chapte", "clusters. Most of these \nproblems should be identifi ed and corrected during Data Preparation, though \nif any problems remain, they can still be uncovered from the cluster statistics.\n\n213\nThis chapter explains key ideas behind algorithms used in predictive \nmodeling. The algorithms included in the chapter are those that are found most \ncommonly in the leading predictive analytics software packages. There are, of \ncourse, many more algorithms available than those included here, including \nsome of my favorites. \nPredictive modeling algorithms are supervised learning methods, meaning \nthat the algorithms try to fi nd relationships that associate inputs to one or more \ntarget variables. The target variable is key: Good predictive models need target \nvariables that summarize the business objectives well. \nConsider the data used for the 1998 KDD Cup Competition (http://\nkdd.ics.uci.edu/databases/kddcup98/kddcup98.html). The business objective \nis to identify lapsed donors that can be recove", "tives well. \nConsider the data used for the 1998 KDD Cup Competition (http://\nkdd.ics.uci.edu/databases/kddcup98/kddcup98.html). The business objective \nis to identify lapsed donors that can be recovered with a future mailing. There \nare two potential target variables for this task: TARGET_B, the binary (fl ag) \nvariable indicating whether or not a donor responded to the recovery mailing, \nor TARGET_D, the amount of the donation the donor gave if he or she responded \nto the recovery campaign. The algorithm you use depends in part on the target \nvariable type: classifi cation for categorical target variables and regression for \ncontinuous target variables. \nAssume that the business objective states that a classifi cation model should be \nbuilt to predict the likelihood a lapsed donor will respond to a mailing. In this \nC H A P T E R \n8\nPredictive Modeling\n\n214 \nChapter 8 \u25a0 Predictive Modeling\ncase, you can build TARGET_B models with a classifi cation algorithm. But which \nones? What are", "espond to a mailing. In this \nC H A P T E R \n8\nPredictive Modeling\n\n214 \nChapter 8 \u25a0 Predictive Modeling\ncase, you can build TARGET_B models with a classifi cation algorithm. But which \nones? What are the strengths and weaknesses of the different approaches? This \nchapter describes, primarily qualitatively, how the predictive modeling algo-\nrithms work and which settings you can change to improve predictive accuracy.\nPredictive modeling algorithms can be placed into a few categories: Some \nalgorithms are local estimators (decision trees, nearest neighbor), some are global \nestimators that do not localize (linear and logistic regression), and still others \nare functional estimators that apply globally but can localize their predictions \n(neural networks). \nThere are dozens of good predictive modeling algorithms, and dozens of \nvariations of these algorithms. This chapter, however, describes only the algo-\nrithms most commonly used by practitioners and found in predictive analytics \nsoft", "e modeling algorithms, and dozens of \nvariations of these algorithms. This chapter, however, describes only the algo-\nrithms most commonly used by practitioners and found in predictive analytics \nsoftware. Moreover, only the options and features of the algorithms that have \nthe most relevance for practitioners using software to build the models are \nincluded in the discussion. \nDecision Trees\nDecision trees are among the most popular predicting modeling techniques in \nthe analytics industry. The 2009, 2010, 2011, and 2013 Rexer Analytics Data Miner \nsurveys, the most comprehensive public surveys of the predictive analytics com-\nmunity, showed decision trees as the number 1 or number 2 algorithm used by \npractitioners, with the number of users ranging between 67 and 74 percent. Of \nthe supervised learning algorithms, only Regression was used often by even \nmore than one-third of the respondents. \nWhy such popularity? First, decision trees are touted as easy to understand. \nAll trees can", "rvised learning algorithms, only Regression was used often by even \nmore than one-third of the respondents. \nWhy such popularity? First, decision trees are touted as easy to understand. \nAll trees can be read as a series of \u201cif-then-else\u201d rules that ultimately generate \na predicted value, either a proportion of a categorical variable value likely to \noccur or an average value of the target variable. These are much more accessible \nto decision-makers than mathematical formulas. In addition, decision trees can \nbe very easy to deploy in rule-based systems or in SQL. \nA second reason that decision trees are popular is that they are easy to build. \nDecision tree learning algorithms are very effi cient and scale well as the number \nof records or fi elds in the modeling data increase.\nThird, most decision tree algorithms can handle both nominal and con-\ntinuous inputs. Most algorithms either require all inputs to be numeric (linear \nregression, neural networks, k-nearest neighbor) or all inp", "t decision tree algorithms can handle both nominal and con-\ntinuous inputs. Most algorithms either require all inputs to be numeric (linear \nregression, neural networks, k-nearest neighbor) or all inputs to be categorical \n(as Na\u00efve Bayes). \n\n \nChapter 8 \u25a0 Predictive Modeling \n215\nFourth, decision trees have a built-in variable selection. Coupled with their \nability to scale with large numbers of inputs, they are excellent for data exploration \nwith hundreds of variables of unknown power in predicting the target variable.\nFifth, decision trees are non-parametric, making no assumptions about dis-\ntributions for inputs or the target variable. They can therefore be used in a wide \nvariety of datasets without any transformations of inputs or outputs. Continuous \nvariables do not have to conform to any particular distribution, can be multi-\nmodal, and can have outliers.\nSixth, many decision tree algorithms can handle missing data automatically \nrather than requiring the modeler to impute mi", "m to any particular distribution, can be multi-\nmodal, and can have outliers.\nSixth, many decision tree algorithms can handle missing data automatically \nrather than requiring the modeler to impute missing values prior to building \nthe models. The method of imputation depends on the decision tree algorithm.\nThis chapter begins with an overview of the kinds of decision trees currently \nused by practitioners. It then describes how decision trees are built, including \nsplitting criteria and pruning options. Last, it enumerates typical options avail-\nable in software to customize decision trees. \nThe Decision Tree Landscape\nDecision trees are relatively new to the predictive modeler\u2019s toolbox, coming \nfrom the Machine Learning world and entering into data mining software in \nthe 1990s. The fi rst tree algorithm still in use was the AID algorithm (Automatic \nInteraction Detection, 1963), later adding the chi-square test to improve vari-\nable selection in what became the CHAID algorithm (Chi", "rst tree algorithm still in use was the AID algorithm (Automatic \nInteraction Detection, 1963), later adding the chi-square test to improve vari-\nable selection in what became the CHAID algorithm (Chisquare Automatic \nInteraction Detection, 1980). \nMeanwhile, in independent research, two other algorithms were developed. \nRoss Quinlan developed an algorithm called ID3 in the 1970s (documented in \n1986), which used Information Gain as the splitting criterion. In 1993, he improved \nthe algorithm with the development of C4.5 (1993), which used Gain Ratio \n(normalized Information Gain) as the splitting criterion. C4.5 was subsequently \nimproved further in the algorithm Quinlan called C5.0, including improve-\nments in misclassifi cation costs, cross-validation, and boosting (ensembles). \nIt also signifi cantly improved the speed in building trees and built them with \nfewer splits while maintaining the same accuracy. The algorithm is sometimes \nreferred to as just C5.\nAnother approach to buil", " signifi cantly improved the speed in building trees and built them with \nfewer splits while maintaining the same accuracy. The algorithm is sometimes \nreferred to as just C5.\nAnother approach to building decision trees occurred concurrent with the develop-\nment of ID3. The CART algorithm is described fully by Breiman, Friedman, Olshen, \nand Stone in the 1984 book Classifi cation and Regression Trees (Chapman and Hall/\nCRC). CART uses the Gini Index as the primary splitting criterion and has been \nemulated in every major data mining and predictive analytics software package.\nThese are the three most common algorithms in use in predictive analytics \nsoftware packages, although other tree algorithms are sometimes included, \n\n216 \nChapter 8 \u25a0 Predictive Modeling\nsuch as the QUEST algorithm (Quick, Unbiased and Effi cient Statistical Tree), \nwhich splits based on quadratic discriminant analysis, and other versions of \ntrees that include fuzzy or probabilistic splits. \nDecision tree termino", "uick, Unbiased and Effi cient Statistical Tree), \nwhich splits based on quadratic discriminant analysis, and other versions of \ntrees that include fuzzy or probabilistic splits. \nDecision tree terminology is very much like actual trees except that decision \ntrees are upside down: Roots of decision trees are at the top, and leaves are at \nthe bottom.\nA split is a condition in the tree where data is divided into two or more sub-\ngroups. These are \u201cif\u201d conditions, such as \u201cif petal width is <= 0.6, then go left, \notherwise go right.\u201d The fi rst split in a tree is called the root split or root node \nand is often considered the most important split in the tree. After the data \nis split into subgroups, trees formed on each of these subgroups are called a \n\u201cbranch\u201d of the tree. \nWhen the decision tree algorithm completes the building of a tree, terminal \nor leaf nodes are collectors of records that match the rule of the tree above the \nterminal node. Every record in the training dataset must ", " tree algorithm completes the building of a tree, terminal \nor leaf nodes are collectors of records that match the rule of the tree above the \nterminal node. Every record in the training dataset must ultimately end up in \none and only one of the terminal nodes. \nIn Figure 8-1, you see a simple tree built using the classic Iris dataset, which \nhas 150 records and 3 classes of 50 records each. Only two of the four candidate \ninputs in the Iris data are used for this tree: Petal length and petal width, some-\ntimes abbreviated as pet_length and pet_width. The root node of the tree splits \non petal width <= 0.6. When this condition is true, one goes down the left path \nof the tree and ends up in a terminal node with all 50 records belonging to the \nclass setosa. On the right side of the root split, you encounter a second condition: \npetal width > 1.7. If this is also true, records end up in the terminal node with \nthe class virginica. Otherwise, records end up in the terminal node with reco", "lit, you encounter a second condition: \npetal width > 1.7. If this is also true, records end up in the terminal node with \nthe class virginica. Otherwise, records end up in the terminal node with records \nthat are largely belonging to the class versicolor.\nThe score for a record is the proportion of records for the most populated \nclass in a terminal node. For example, the terminal node that contains all setosa \nIrises has a score of 100 related to the class setosa. Or put another way, any Iris \nthat is found that has a petal width \u2264 0.6 inches has 100 percent likelihood of \nbelonging to the setosa Iris species based on this data.\nThere is, therefore, one rule for every terminal node. For the tree in \nFigure 8-1, these are:\n \n\u25a0If pet_width is \u2264 0.6, then the record belongs to class setosa with \nprobability 100 percent.\n \n\u25a0If pet_width > 0.6 and petal width > 1.7, then the record belongs to species \nvirginica with probability 97.8. \n \n\u25a0If pet_width is > 0.6 and \u2264 1.7, then the record be", "with \nprobability 100 percent.\n \n\u25a0If pet_width > 0.6 and petal width > 1.7, then the record belongs to species \nvirginica with probability 97.8. \n \n\u25a0If pet_width is > 0.6 and \u2264 1.7, then the record belongs to species versicolor \nwith probability 90.7 percent.\nTrees are considered interpretable models because they can be read as rules. The \ninterpretability, however, is severely reduced as the depth of the tree increases. \n\n \nChapter 8 \u25a0 Predictive Modeling \n217\nThe depth is the number of conditions between the root split and the deepest \nterminal node, including the root split. In the tree shown in Figure 8-1, the depth \nis 2: fi rst splitting on pet_width equal to 0.6 and then splitting on pet_width \nequal to 1.7. \nhe \ufb01rst\nRoot Split: t\nree\nsplit in the tr\nSplit: a condition that\ndivides data into two\nor more subgroups\nTerminal Nodes\nT\nFigure 8-1:  Simple decision tree built for the iris data\nIn a more complex tree (see Figure 8-2), every terminal has fi ve conditions. \nThe tree itsel", "data into two\nor more subgroups\nTerminal Nodes\nT\nFigure 8-1:  Simple decision tree built for the iris data\nIn a more complex tree (see Figure 8-2), every terminal has fi ve conditions. \nThe tree itself, even with a depth equal to only fi ve, is too complex for the details \nof each node to be seen on this page. The terminal node at the far-bottom right \ncan be expressed by this rule: If RFA_2F > 1 and CARDGIFT > 3 and LASTGIFT \n> 7 and AGE > 82 and RFA_2F > 2, then 46 of 100 donors do not respond. This \nrule that defi nes the records that fall into a terminal node, one of the 16 terminal \nnodes in the tree, has already lost considerable transparency and interpretabil-\nity. What if there were 10 or 15 conditions in the rule? Decision trees are only \ninterpretable if they are relatively simple. Figure 8-1 has a depth equal to two \n\n218 \nChapter 8 \u25a0 Predictive Modeling\nand therefore is easy to see and understand. I fi nd that trees become more dif-\nfi cult to interpret once the depth excee", "e. Figure 8-1 has a depth equal to two \n\n218 \nChapter 8 \u25a0 Predictive Modeling\nand therefore is easy to see and understand. I fi nd that trees become more dif-\nfi cult to interpret once the depth exceeds three. If the tree is many levels deep, \nyou can still use the fi rst two or three levels to provide a simpler explanation \nof the tree to others. \nFigure 8-2: Moderately complex decision tree\nBuilding Decision Trees\nDecision trees belong to a class of recursive partitioning algorithms that is very \nsimple to describe and implement. For each of the decision tree algorithms \ndescribed previously, the algorithm steps are as follows:\n \n1. For every candidate input variable, assess the best way to split the data \ninto two or more subgroups. Select the best split and divide the data into \nthe subgroups defi ned by the split.\n \n2. Pick one of the subgroups, and repeat Step 1 (this is the recursive part of \nthe algorithm). Repeat for every subgroup. \n \n3. Continue splitting and splitting until", "bgroups defi ned by the split.\n \n2. Pick one of the subgroups, and repeat Step 1 (this is the recursive part of \nthe algorithm). Repeat for every subgroup. \n \n3. Continue splitting and splitting until all records after a split belong to the \nsame target variable value or until another stop condition is applied. The \nstop condition may be as sophisticated as a statistical signifi cance test, or \nas simple as a minimum record count.\nThe determination of best can be made in many ways, but regardless of the \nmetric, they all provide a measure of purity of the class distribution. Every input \nvariable is a candidate for every split, so the same variable could, in theory, be \nused for every split in a decision tree. \nHow do decision trees determine \u201cpurity\u201d? Let\u2019s return to the Iris dataset \nand consider the variables petal width and petal length with three target vari-\nable classes, setosa (circle), versicolor (X-shape), and virginica (triangle), shown \nin Figure 8-3. It is obvious from the", " consider the variables petal width and petal length with three target vari-\nable classes, setosa (circle), versicolor (X-shape), and virginica (triangle), shown \nin Figure 8-3. It is obvious from the scatterplot that one good split will cleave \nout the setosa values by splitting on petal length anywhere between 2.25 and \n2.75, or by splitting on petal width anywhere between the values 0.6 and 0.8. \n\n \nChapter 8 \u25a0 Predictive Modeling \n219\npetal width\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00\n3.25\n3.50\npetal length\n3.75\n4.00\n4.25\n4.50\n4.75\n5.00\n5.25\n5.50\n5.75\n6.00\n6.25\n6.50\n6.75\n6.90\n0.100\n0.300\n0.500\n0.700\n0.899\n1.099\n1.299\n1.499\n1.699\n1.899\n2.099\n2.300\n2.500\nsetosa\nversicolor\nvirginica\nCircle\nX-Shape\nTriangle\nFigure 8-3: Iris data scatterplot of petal width versus petal length\nIndeed, the CART algorithm fi rst split could be on either petal length or petal \nwidth\u2014both variables can identify perfectly the setosa class. Suppose petal \nlength is selected. The second split selected by ", "the CART algorithm fi rst split could be on either petal length or petal \nwidth\u2014both variables can identify perfectly the setosa class. Suppose petal \nlength is selected. The second split selected by CART is at petal width > 1.75. The \nresulting regions from these two splits are shown in Figure 8-4. The shapes of \nthe regions correspond to the shape of the target variable that is predominant.\nHowever, what if the algorithm selected a different fi rst split (equally as good). \nThe decision regions created by the algorithm when the fi rst split is petal width \nare shown in Figure 8-5. This tree has the same accuracy as the one visualized \nin Figure 8-4, but clearly defi nes different regions: In the fi rst tree, large values \nof petal length and small values of petal width are estimated to be part of the \nclass versicolor, whereas in the second tree with the same classifi cation accuracy \ncalls the same part of the decision space setosa. \nDecision trees are nonlinear predictors, meaning ", "e part of the \nclass versicolor, whereas in the second tree with the same classifi cation accuracy \ncalls the same part of the decision space setosa. \nDecision trees are nonlinear predictors, meaning the decision boundary between \ntarget variable classes is nonlinear. The extent of the nonlinearities depends on \nthe number of splits in the tree because each split, on its own, is only a piecewise \nconstant separation of the classes. As the tree becomes more complex, or in other \nwords, as the tree depth increases, more piecewise constant separators are built \ninto the decision boundary, providing the nonlinear separation. Figure 8-6 shows \nscatterplots of decision boundaries from a sequence of three decision trees. At \nthe top, the fi rst split creates a single constant decision boundary. At the middle, \na second split, vertical, creates a simple stair step decision boundary. The third \ntree, at the bottom, the next stair step is formed by a second horizontal constant \ndecision boundary", "t the middle, \na second split, vertical, creates a simple stair step decision boundary. The third \ntree, at the bottom, the next stair step is formed by a second horizontal constant \ndecision boundary, formed midway up the y axis. The decision boundary is now \na nonlinear composition of piecewise constant boundaries.\n\n220 \nChapter 8 \u25a0 Predictive Modeling\npetal length\nSplit 2:\npetal width > 1.75\nSplit 1:\npetal length > 2.45\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00\n3.25\n3.50\n3.75\n4.00\n4.25\n4.50\n4.75\n5.00\n5.25\n5.50\n5.75\n6.00\n6.25\n6.50\n6.75\n6.90\npetal width\n0.100\n0.300\n0.500\n0.700\n0.899\n1.099\n1.299\n1.499\n1.699\n1.899\n2.099\n2.300\n2.500\nsetosa\nversicolor\nvirginica\nCircle\nX-Shape\nTriangle\nFigure 8-4: First two decision tree splits of iris data\nSplit 2:\npetal width > 1.75\nSplit 1:\npetal width > 0.8\npetal length\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00\n3.25\n3.50\n3.75\n4.00\n4.25\n4.50\n4.75\n5.00\n5.25\n5.50\n5.75\n6.00\n6.25\n6.50\n6.75\n6.90\npetal width\n0.100\n0.300\n0.500\n0.700\n0.899\n1.099\n1.299\n1.4", " 0.8\npetal length\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00\n3.25\n3.50\n3.75\n4.00\n4.25\n4.50\n4.75\n5.00\n5.25\n5.50\n5.75\n6.00\n6.25\n6.50\n6.75\n6.90\npetal width\n0.100\n0.300\n0.500\n0.700\n0.899\n1.099\n1.299\n1.499\n1.699\n1.899\n2.099\n2.300\n2.500\nsetosa\nversicolor\nvirginica\nCircle\nX-Shape\nTriangle\nFigure 8-5: Alternative decision tree splits of iris data\n\n \nChapter 8 \u25a0 Predictive Modeling \n221\n0.501\n1.001\n1.501\n2.001\n2.501\n3.001\n3.501\n4.001\n4.499\n0.501\n1.001\n1.501\n2.001\n2.501\n3.001\n3.501\n4.001\n4.499\n0.501\n1.001\n1.501\n2.001\n2.501\n3.001\n3.501\n4.001\n4.499\n5.005\n0.005\n10.005\n20.005\n30.005\n40.005\n50.005\n60.005\n70.005\n80.005\n90.005\n99.966\n15.005\n25.005\n35.005\n45.005\n55.005\n65.005\n75.005\n85.005\n95.005\n5.005\n0.005\n10.005\n20.005\n30.005\n40.005\n50.005\n60.005\n70.005\n80.005\n90.005\n99.966\n15.005\n25.005\n35.005\n45.005\n55.005\n65.005\n75.005\n85.005\n95.005\n5.005\n0.005\n10.005\n20.005\n30.005\n40.005\n50.005\n60.005\n70.005\n80.005\n90.005\n99.966\n15.005\n25.005\n35.005\n45.005\n55.005\n65.005\n75.005\n85.005\n95.005\nFigure 8-6: Nonlinea", "55.005\n65.005\n75.005\n85.005\n95.005\n5.005\n0.005\n10.005\n20.005\n30.005\n40.005\n50.005\n60.005\n70.005\n80.005\n90.005\n99.966\n15.005\n25.005\n35.005\n45.005\n55.005\n65.005\n75.005\n85.005\n95.005\nFigure 8-6: Nonlinear decision boundaries for trees\nDecision Tree Splitting Metrics\nThe three most popular decision tree algorithms are CART, C5.0, and CHAID. \nTable 8-1 provides a brief comparison of these algorithms. Not all implementations \n\n222 \nChapter 8 \u25a0 Predictive Modeling\nof the algorithms necessarily include all of the options in the table. For example, \nsome implementations of the CART algorithm do not include the ability to \nchange the priors.\nThe CART and C5.0 decision trees are similar in several regards. They both \nbuild trees to full-size, deliberately overfi tting the tree and then they prune the \ntree back to the depth that trades off error with complexity in a suitable man-\nner. Additionally, they both can use continuous and categorical input variables. \nCHAID, on the other hand, splits onl", " \ntree back to the depth that trades off error with complexity in a suitable man-\nner. Additionally, they both can use continuous and categorical input variables. \nCHAID, on the other hand, splits only if the split passes a statistically signifi cant \ntest (chi-square). The chi-square test is only applied to categorical variables, so \nany continuous inputs and output must be binned into categorical variables. \nUsually, the software bins the continuous variables automatically. You can bin \nthe data yourself as well. \nDecision Tree Knobs and Options\nOptions available in most software for decision trees include:\n \n\u25a0Maximum depth: The number of levels deep a tree is allowed to go. This \noption limits the complexity of a tree even if the size of the data would \nallow for a more complex tree.\n \n\u25a0Minimum number of records in terminal nodes: This option limits the \nsmallest number of records allowed in a terminal node. If the winning \nsplit results in fewer than the minimum number of records i", "\u25a0Minimum number of records in terminal nodes: This option limits the \nsmallest number of records allowed in a terminal node. If the winning \nsplit results in fewer than the minimum number of records in one of the \nterminal nodes, the split is not made and growth in this branch ceases. \nThis option is a good idea to prevent terminal nodes with so few cases, \nif you don\u2019t have confi dence that the split will behave in a stable way on \nnew data. My usual preference is 30, but sometimes make this value as \nhigh as several hundred.\n \n\u25a0Minimum number of records in parent node: Similar to the preceding \noption, but instead applies the threshold to the splitting node rather than \nthe children of the node. Once there are fewer than the number of records \nspecifi ed by this option, no further split is allowed in this branch. This \nvalue is typically made larger than the minimum records in a terminal \nnode, often twice as large. My usual preference is 50.\n \n\u25a0Bonferroni correction: An option for C", "is allowed in this branch. This \nvalue is typically made larger than the minimum records in a terminal \nnode, often twice as large. My usual preference is 50.\n \n\u25a0Bonferroni correction: An option for CHAID only, this correction adjusts \nthe chi-square statistic for multiple comparisons. As the number of levels \nin the input and target variable increases, and there are more possible \ncombinations of input values to use in predicting the output values, the \nprobability that you may fi nd a good combination increases, and sometimes \nthese combinations may just exist by chance. The Bonferroni correction \npenalizes the chi-square statistic proportional to the number of possible \ncomparisons that could be made, reducing the likelihood a split will be \nfound by chance. This option is usually turned on by default.\n\nTable 8-1: Characteristics of Three Decision Tree Algorithms\nTREE \nALGORITHM\nSPLITTING \nCRITERION\nINPUT \nVARIABLES\nTARGET \nVARIABLE\nBINARY OR \nMULTI\uf6baWAY \nSPLITS\nCOMPLEXITY \nREGULARIZ", "d on by default.\n\nTable 8-1: Characteristics of Three Decision Tree Algorithms\nTREE \nALGORITHM\nSPLITTING \nCRITERION\nINPUT \nVARIABLES\nTARGET \nVARIABLE\nBINARY OR \nMULTI\uf6baWAY \nSPLITS\nCOMPLEXITY \nREGULARIZATION\nHANDLING OF CLASS \nIMBALANCE\nCART\nGini index, \nTwo-ing \ncategorical or \ncontinuous\ncategorical or \ncontinuous\nBinary\nPruning\nPriors or Misclassi\ufb01 cation costs\nC5.0\nGain Ratio, \nbased on \nentropy\ncategorical or \ncontinuous\ncategorical\nBinary (continuous \nvariables) Multi-\nway (categorical \nvariables)\nPruning\nMisclassi\ufb01 cation costs\nCHAID\nchi-square \ntest\ncategorical\ncategorical\nBinary or Multi-way\nSigni\ufb01 cance test\nMisclassi\ufb01 cation costs\n\n224 \nChapter 8 \u25a0 Predictive Modeling\nReweighting Records: Priors\nIn predictive modeling algorithms, each record has equal weight and therefore \ncontributes the same amount to the building of models and measuring accuracy. \nConsider, however, a classifi cation problem with 95 percent of the records with \nthe target class labeled as 0 and 5 percent la", "utes the same amount to the building of models and measuring accuracy. \nConsider, however, a classifi cation problem with 95 percent of the records with \nthe target class labeled as 0 and 5 percent labeled as 1. A modeling algorithm \ncould conclude that the best way to achieve the maximum classifi cation accu-\nracy is to predict every record belongs to class 0, yielding 95 Percent Correct \nClassifi cation! Decision trees are no exception to this issue, and one can become \nfrustrated when building a decision tree yields no split at all because the great \nmajority of records belong to a single target variable value. \nOf course this is rarely what you actually want from a model. When you \nhave an underrepresented target class, it is typical that the value occurring \nless frequently is far more important than the numbers alone indicate. For \nexample, in fraud detection, the fraud rate may be 0.1 percent, but you can\u2019t \ndismiss fraudulent cases just because they are rare. In fact, it is oft", "e important than the numbers alone indicate. For \nexample, in fraud detection, the fraud rate may be 0.1 percent, but you can\u2019t \ndismiss fraudulent cases just because they are rare. In fact, it is often the rare \nevents that are of most interest. \nOne way to communicate to the decision tree algorithm that the underrep-\nresented class is important is to adjust the prior probabilities, usually called the \npriors. Most modeling algorithms, by considering every record to have equal \nweight, implicitly assign prior probabilities based on the training data propor-\ntions. In the case with the target class having 5 percent of the target class equal \nto 1, the prior probability of the target class equaling 1 is 5 percent. \nHowever, what if the training data is either wrong (and you know this to be \nthe case) or misleading? What if you really want the 0s and 1s to be considered \nequally important? The solution to these problems is to change the mathematics \nthat compute impurity so that the reco", "\nthe case) or misleading? What if you really want the 0s and 1s to be considered \nequally important? The solution to these problems is to change the mathematics \nthat compute impurity so that the records with target variable equal to 1 are \nweighted equally with those equal to 0. This mathematical up-weighting has \nthe same effect as replicating 1s so that they are equal in number with 0s, but \nwithout the extra computation necessary to process these additional records. \nThe tree algorithms that include an option of setting priors include CART and \nQUEST. For the practitioner, this means that one can build decision trees using \nthe CART and QUEST algorithms when large imbalances exist in the data. In \nfact, this is a standard, recommended practice when using these algorithms for \nclassifi cation problems.\nReweighting Records: Misclassi\ufb01 cation Costs\nMisclassifi cation costs, as the name implies, provide a way to penalize classifi -\ncation errors asymmetrically. Without misclassifi cati", "cation problems.\nReweighting Records: Misclassi\ufb01 cation Costs\nMisclassifi cation costs, as the name implies, provide a way to penalize classifi -\ncation errors asymmetrically. Without misclassifi cation costs, each error made \n\n \nChapter 8 \u25a0 Predictive Modeling \n225\nby the classifi er has equal weight and infl uence in model accuracy statistics. \nThe number of occurrences of a particular value of the target variable has the \ngreatest infl uence in the relative importance of each target variable value. For \nexample, if 95 percent of the target variable has a value of 0 and only 5 percent \nhas a value of 1, the learning algorithm treats the class value 0 as 19 times more \nimportant than the class value 1. This is precisely why a learning algorithm \nmight conclude that the best way to maximize accuracy is to merely label every \nrecord as a 0, giving 95 Percent Correct Classifi cation.\nPriors provide one way to overcome this bias. Misclassifi cation costs pro-\nvide a second way to do this ", "e accuracy is to merely label every \nrecord as a 0, giving 95 Percent Correct Classifi cation.\nPriors provide one way to overcome this bias. Misclassifi cation costs pro-\nvide a second way to do this and are available as an option in most software \nimplementations of decision trees. For binary classifi cation, if you would like to \novercome the 95 percent to 5 percent bias in the data, you could specify a mis-\nclassifi cation cost of 19 when the algorithms label a record whose actual value \nis 1 with a model prediction of 0. With this cost in place, there is no advantage \nfor the algorithm to label all records as 0 or 1; they have equal costs.\nTypically, the interface for specifying misclassifi cation costs shows something \nthat appears like a confusion matrix with rows representing actual target vari-\nable values and columns the predicted values. Table 8-2 shows an example of a \nmisclassifi cation cost matrix with the cost of falsely dismissing target variable \nvalues equal to 1 set t", " target vari-\nable values and columns the predicted values. Table 8-2 shows an example of a \nmisclassifi cation cost matrix with the cost of falsely dismissing target variable \nvalues equal to 1 set to 19. The diagonal doesn\u2019t contain any values because these \nrepresent correct classifi cation decisions that have no cost associated with them.\nTable 8-2: Misclassi\ufb01 cation Cost Matrix for TARGET_B\nCOST MATRIX\nTARGET = 0\nTARGET = 1\nTarget = 0\n\u2014\n1\nTarget = 1\n19\n\u2014\nIn the example with 5 percent of the population having the target variable \nequal to 1 and 95 percent equal to 0, let\u2019s assume you have 10,000 records so that \n500 examples have 1 for the target, and 9,500 have 0. With misclassifi cation costs \nset to 19 for false dismissals, if all 500 of these examples were misclassifi ed as 0, \nthe cost would be 19 \u00d7 500 = 9500, or equal to the cost of classifying all the 0s as \n1s. In practice, you don\u2019t necessarily have to increase the cost all the way up to \n19: Smaller values (even a third ", " cost would be 19 \u00d7 500 = 9500, or equal to the cost of classifying all the 0s as \n1s. In practice, you don\u2019t necessarily have to increase the cost all the way up to \n19: Smaller values (even a third of the value, so 6 or 7) may suffi ce for purposes \nof building models that are not highly biased toward classifying 0s correctly.\nThe C5.0 algorithm will not create any splits at all for data with a large imbal-\nance of the target variable, like the example already described with only 5 percent \nof the target equal to 1; there is not enough Information Gain with any split to \njustify building a tree. However, setting misclassifi cation costs like the ones in \n\n226 \nChapter 8 \u25a0 Predictive Modeling\nTable 8-2 communicate to the algorithm that the underrepresented class is more \nimportant than the sheer number of examples, and a tree will be built even \nwithout resampling the data.\nIn the case of binary classifi cation, there is no mathematical difference between \nusing priors and misclassifi", "e sheer number of examples, and a tree will be built even \nwithout resampling the data.\nIn the case of binary classifi cation, there is no mathematical difference between \nusing priors and misclassifi cation costs to make the relative infl uence of records \nwith target variable value equal to 1 and those with target variable value equal \nto 0 the same. Misclassifi cation costs can, however, have a very different effect \nwhen the target variable has many levels.\nConsider the nasadata dataset, with seven target variable values (alfalfa, clover, \ncorn, soy, rye, oats, and wheat). Priors adjust the relative occurrences of each, \nas if you replicated records to the point where each class value has the same \nnumber of records. Misclassifi cation costs, on the other hand, provide pairwise \nweightings of misclassifi cations, and therefore there are 21 combinations of \ncosts to set above the diagonal and 21 below, for a total of 42. These costs are \nnot symmetric: If the example has an actual t", "ings of misclassifi cations, and therefore there are 21 combinations of \ncosts to set above the diagonal and 21 below, for a total of 42. These costs are \nnot symmetric: If the example has an actual target value equal to alfalfa and \nthe model predicts clover, one may want to set this cost differently than the \nconverse. Table 8-3 shows a full misclassifi cation cost matrix for the nasadata \ndataset. The actual target variable value is in the rows, and the predicted val-\nues in the columns. For example, misclassifi cation costs for the target variable \nequal to alfalfa but a model predicting the target variable equal to clover are set \nto 10, whereas the converse is set to the default value of 1. Additionally, wheat \nmisclassifi ed as soy is penalized with a cost of 8. \nTable 8-3: Misclassi\ufb01 cation Cost Matrix for Nasadata Crop Types\nCOST \nMATRIX\nALFALFA\nCLOVER\nCORN\nSOY\nRYE\nOATS\nWHEAT\nAlfalfa\n-\n10\n1\n1\n1\n1\n1\nClover\n1\n-\n1\n1\n1\n1\n1\nCorn\n1\n1\n-\n1\n1\n1\n1\nSoy\n1\n1\n1\n-\n1\n1\n1\nRye\n1\n1\n1\n1\n-\n1\n1\nOat", "i\ufb01 cation Cost Matrix for Nasadata Crop Types\nCOST \nMATRIX\nALFALFA\nCLOVER\nCORN\nSOY\nRYE\nOATS\nWHEAT\nAlfalfa\n-\n10\n1\n1\n1\n1\n1\nClover\n1\n-\n1\n1\n1\n1\n1\nCorn\n1\n1\n-\n1\n1\n1\n1\nSoy\n1\n1\n1\n-\n1\n1\n1\nRye\n1\n1\n1\n1\n-\n1\n1\nOats\n1\n1\n1\n1\n1\n-\n1\nWheat\n1\n1\n1\n8\n1\n1\n-\nMisclassifi cation costs, therefore, are tremendously fl exible. Figure 8-7 shows a \nsimple decision tree using only the default misclassifi cations costs: All are equal \n\n \nChapter 8 \u25a0 Predictive Modeling \n227\nto 1. Band11 splits alfalfa and clover into Node 1 to the left, with approximately \n50 percent of the cases belonging to each (55 alfalfa and 62 clover records).\nFigure 8-7: Nasadata decision tree with no misclassification costs\nMisclassifi cation costs help the classifi er change the kinds of errors that are \nacceptable. A decision tree built with misclassifi cation costs from Table 8-3 is \nshown in Figure 8-8. The fi rst split no longer groups alfalfa and clover together\u2014\nthe misclassifi cation costs made this split on Band11 too expensive\u2014and s", "sclassifi cation costs from Table 8-3 is \nshown in Figure 8-8. The fi rst split no longer groups alfalfa and clover together\u2014\nthe misclassifi cation costs made this split on Band11 too expensive\u2014and splits \non Band9 as the root split instead.\n\n228 \nChapter 8 \u25a0 Predictive Modeling\nFigure 8-8: Nasadata decision tree with misclassification costs\nHowever, with this fl exibility comes diffi culty. There are often no known \nor theoretical values to use for the costs, leaving the modeler with nothing \nmore than a best guess to decide the values to use for misclassifi cation costs. \n\n \nChapter 8 \u25a0 Predictive Modeling \n229\nSome software provides mechanisms to search over ranges of misclassifi cation \ncosts to help determine the optimum values. Nevertheless, misclassifi cation \ncosts can still be valuable for fi ne-tuning classifi ers.\nOther Practical Considerations for Decision Trees\nFollowing are some practical considerations to make when using decision trees.\nDecision trees are created throug", "luable for fi ne-tuning classifi ers.\nOther Practical Considerations for Decision Trees\nFollowing are some practical considerations to make when using decision trees.\nDecision trees are created through a greedy search (forward selection). \nTherefore, they can never \u201cgo back\u201d to revisit a split in a tree based on informa-\ntion learned subsequent to that split. They can be fooled in the short run only \nto be suboptimal when the tree is completed. Removing the variable in the root \nsplit sometimes helps by forcing the tree to fi nd another way to grow. Some \nsoftware even provides a way to automate this kind of exploratory analysis.\nDecision trees incorporate only one variable for each split. If no single variable \ndoes a good job of splitting on its own, the tree won\u2019t start off well and may \nnever fi nd good multivariate rules. Trees need attributes that provide some lift \nright away or they may be fooled. If the modeler knows multivariate features \nthat could be useful, they should be ", " \nnever fi nd good multivariate rules. Trees need attributes that provide some lift \nright away or they may be fooled. If the modeler knows multivariate features \nthat could be useful, they should be included as candidates for the model. As an \nalternative, other techniques, such as association rules, could be used to fi nd good \ninteraction terms that decision trees may miss. You can then build new derived \nvariables from these interactions and use them as inputs to the decision tree.\nTrees are considered weak learners, or unstable models: Small changes in data \ncan produce signifi cant changes in how the tree looks and behaves. Sometimes \nthe differences between the winning split and the fi rst runner-up is very small, \nsuch that if you rebuild the tree on a different data partition, the winning/\nrunner-up splits can be reversed. Examining competitors to the winning split \nand surrogate splits can be very helpful in understanding how valuable the \nwinning splits are, and if other var", " winning/\nrunner-up splits can be reversed. Examining competitors to the winning split \nand surrogate splits can be very helpful in understanding how valuable the \nwinning splits are, and if other variables may do nearly as well. Trees are biased \ntoward selecting categorical variables with large numbers of levels (high cardi-\nnality data). If you fi nd a large number of levels in categorical variables, turn on \ncardinality penalties or consider binning these variables to have fewer levels. \nTrees can \u201crun out of data\u201d before fi nding good models. Because each split \nreduces how many records remain, subsequent splits are based on fewer and \nfewer records and therefore have less statistical power.\nSingle trees are often not as accurate as other algorithms in predictive accu-\nracy on average; in particular, because of greedy, forward variable selection; \nthe piecewise constant splits; and the problem of running out of data. Neural \nnetworks and support vector machines will often outperfo", "; in particular, because of greedy, forward variable selection; \nthe piecewise constant splits; and the problem of running out of data. Neural \nnetworks and support vector machines will often outperform trees. Consider \nbuilding ensembles of trees to increase accuracy if model interpretation is \nnot important. \n\n230 \nChapter 8 \u25a0 Predictive Modeling\nLogistic Regression\nLogistic regression is a linear classifi cation technique for binary classifi cation. \nThe history of logistic regression is as follows:\nConsider the KDD Cup 1998 data and the question, \u201cWhich lapsed donors \nto a charity can be recovered?\u201d Dozens of giving patterns for each donor were \ncollected and can be used to identify the patterns related to recovering and \nunrecovered lapsed donors. Two of the historic attributes that were collected \nwere how many gifts a lapsed donor has given during his or her relationship \nwith the charity (NGFITALL) and how much the lapsed donor gave for his or \nher last gift (LASTGIFT). Figure ", "ere collected \nwere how many gifts a lapsed donor has given during his or her relationship \nwith the charity (NGFITALL) and how much the lapsed donor gave for his or \nher last gift (LASTGIFT). Figure 8-9 shows how a logistic regression model pre-\ndicts the likelihood that the donor can be recovered, 1 for recovered and 0 for \nnot recovered. The TARGET_B outcome values predicted by the model (0 or 1) \nare separated by a line found by the logistic regression model: a linear decision\nboundary. If three inputs were included in the model, the decision boundary \nchanges from a line to a plane that separates the predicted values equal to 0 \nfrom the predicted values equal to 1. With more than three inputs, the decision \nboundary becomes a hyper-plane.\nLASTGIFT\nNGIFTALL\nT\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n22\n24\n26\n28\n30\n32\n34\n36\n38\n40\n42\n44\n46\n48\n50\nFigure 8-9: Linear decision boundary based on NGIFTALL and LASTGIFT\nThe core of a logistic regression model is the odds r", "45\n50\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n22\n24\n26\n28\n30\n32\n34\n36\n38\n40\n42\n44\n46\n48\n50\nFigure 8-9: Linear decision boundary based on NGIFTALL and LASTGIFT\nThe core of a logistic regression model is the odds ratio: the ratio of the out-\ncome probabilities. \noddsratio\nP\nP\nP\n=\n=\n( )\n( )\n( )\n( )\n1 P\n\u2212(\n\n \nChapter 8 \u25a0 Predictive Modeling \n231\nBear in mind that the odds ratio is not the same as the likelihood an event \nwill occur. For example, if an event occurs in every 1 of 5 events (20 percent \nlikelihood), the odds ratio is 0.2/(1 \u2013 0.2) = 0.2/0.8 = 0.25. The odds ratio of the \nevent not occurring is 0.8/0.2 = 4.0, or in other words, the odds of the event not \noccurring is 4 times that of it occurring.\nLogistic regression does not build linear models of the odds ratio, but rather \nof the log of the odds ratio. Consider the data in Table 8-4. P(0) is the probability \nthat the target variable has value equal to 0 given the input value specifi ed, and \nP(1) is the comparable measure for a target va", "odds ratio. Consider the data in Table 8-4. P(0) is the probability \nthat the target variable has value equal to 0 given the input value specifi ed, and \nP(1) is the comparable measure for a target variable equal to 1.\nTable 8-4: Odds Ratio Values for a Logistic Regression Model\nINPUT VALUE\nP\uf6ae0\uf6af\nP\uf6ae1\uf6af\nODDS RATIO\nLOG ODDS RATIO\n0\n0.927\n0.073\n0.079\n\u20131.103\n5\n0.935\n0.065\n0.07\n\u20131.156\n10\n0.942\n0.058\n0.062\n\u20131.208\n15\n0.948\n0.052\n0.055\n\u20131.26\n20\n0.954\n0.046\n0.049\n\u20131.313\n25\n0.959\n0.041\n0.043\n\u20131.365\n30\n0.963\n0.037\n0.038\n\u20131.417\n35\n0.967\n0.033\n0.034\n\u20131.47\nIf you plot the input value versus the odds ratio (see Figure 8-10), you can see \nthat the relationship between the input and the odds ratio is nonlinear (expo-\nnential). The same is true if you plot P(1) versus the input. This fl attens out into a \nlinear relationship when you compute the log of the odds ratio (see Figure 8-11). \n0\n0.000\n0.010\n0.020\n0.030\n0.040\n0.050\n0.060\n0.070\n0.080\n0.090\n10\n20\n30\n40\nodds ratio\nExpon. (odds ratio)\ny = 0.0789e\u22120.0", "ear relationship when you compute the log of the odds ratio (see Figure 8-11). \n0\n0.000\n0.010\n0.020\n0.030\n0.040\n0.050\n0.060\n0.070\n0.080\n0.090\n10\n20\n30\n40\nodds ratio\nExpon. (odds ratio)\ny = 0.0789e\u22120.024x\nR2 = 1\nFigure 8-10: Odds ratio versus model input\n\n232 \nChapter 8 \u25a0 Predictive Modeling\n\u22121.600\n\u22121.400\n\u22121.200\n\u22121.000\n\u22120.800\n\u22120.600\n\u22120.400\n\u22120.200\n0.000\n0\n10\n20\n30\n40\nlog odds ratio\nLinear (log odds ratio)\ny = \u22120.0105x \u2212 1.1032\nR2 = 1\n0\nFigure 8-11: Log odds ratio versus model input\nThis linear relationship is the statistical model logistic regression in computing:\noddsratio\nP\nw\nw\nx\nw\nx\nn\nn\nx\n=\n=\n+\nw\n\u00d7\n+\nx\n+\n\u00d7\nwn\n( )\n( )\n1 P\n\u2212(\n0\n1\nw\n1\nIn the odds ratio equation, the values w0, w1, and so forth are the model \ncoeffi cients or weights. The coeffi cient w0 is the constant term in the model, \nsometimes called the bias term. The variables x0, x1, and so forth are the inputs \nto the model. For example, x1 may be LASTGIFT and x2 may be NGFITALL. \nWithout providing any derivation, the calculatio", "metimes called the bias term. The variables x0, x1, and so forth are the inputs \nto the model. For example, x1 may be LASTGIFT and x2 may be NGFITALL. \nWithout providing any derivation, the calculation of the probability that the \noutcome is equal to 1 is: \nPr(target\ne\nw\nw\nx\nw\nx\nw\nx\nn\nn\nx\n=\n= +\n\u2212\n+\nw\n\u00d7\n+\nx\n\u00d7\n+\nx\n+\n\u00d7\nwn\n1\n1\n1\n0\n1\nw\n1\n2\nw\n2\n)\n(\n)\n The probability formula is the function known as the logistic curve and takes \non the shape shown in Figure 8-12. In contrast to the linear regression model \nwhose predicted values are unbounded, the logistic curve is bounded by the \nrange 0 to 1. The middle of the distribution, approximately in the x axis range \n\u20132 to 2 in the fi gure, is approximately linear. However, the edges have severe \nnonlinearities to smoothly scale the probabilities to the bounded range. When \nyou change the input value from 0 to +2, the probability goes from 0.5 to 0.88, \nwhereas when you change the input from +4 to +6, the probability changes \nonly from 0.98 to 0.99", "e bounded range. When \nyou change the input value from 0 to +2, the probability goes from 0.5 to 0.88, \nwhereas when you change the input from +4 to +6, the probability changes \nonly from 0.98 to 0.998.\nHow is the probability calculation, with values scaled nonlinearly so that it \nhas a range between 0 and 1, and the linear decision boundary? Assume that the \nprior probability of the target variable, TARGET_B, is 50 percent. All of the data \npoints that lie on the line that separates the predicted TARGET_B values equal \nto 0 from those equal to 1 have predicted probability values, P(1), equal to 0.5. \n\n \nChapter 8 \u25a0 Predictive Modeling \n233\nThe further from the line a data point resides, the larger the value of P(1) or P(0). \nFor a data point in the upper left of Figure 8-9, LASTGIFT = 5 and NGIFTALL \n= 70, the P(1) is 0.82: a high likelihood the donor can be recovered, and a data \npoint far from the decision boundary. The further from the decision boundary, \nthe smaller the relative i", "nd NGIFTALL \n= 70, the P(1) is 0.82: a high likelihood the donor can be recovered, and a data \npoint far from the decision boundary. The further from the decision boundary, \nthe smaller the relative increase in P(1) until it approaches 1.0.\n\u221210\n\u22128\n\u22126\n\u22124\n\u22122\n0\n0\n0.1\n0 1\n0.2\n0 2\n0.3\n0\n0.4\n0 4\n0.5\n0 5\n0.6\n0 6\n0.7\n0 7\n0.8\n0 8\n0.9\n0 9\n1\n2\n4\n6\n8\n10\nFigure 8-12: Logistic curve\nInterpreting Logistic Regression Models\nTo interpret logistic regression models, you\u2019ll examine several numbers most \nsoftware packages provide in their summary reports:\n \n\u25a0Coeffi cient: Sometimes shown as the Greek symbol beta (\u00df). This is the \nparameter found by the logistic regression algorithm; one coeffi cient per \ninput variable plus one constant (bias term).\n \n\u25a0Standard error of the coeffi cient (SE): A measure of certainty of the coef-\nfi cient found by logistic regression. Smaller values of standard error imply \na smaller level of uncertainty of the coeffi cient value. \n \n\u25a0Confi dence Interval (CI): The range of", "tainty of the coef-\nfi cient found by logistic regression. Smaller values of standard error imply \na smaller level of uncertainty of the coeffi cient value. \n \n\u25a0Confi dence Interval (CI): The range of values the coeffi cient is expected \nto fall between. The CI is computed as:\nCI\nSE\n= \u00b1\n\u03b2\nThis is sometimes helpful when evaluating models. You can compute the \nPr(1) not just for the coeffi cient with value \u00df, but also for the coeffi cient \nplus the SE and minus the SE, thereby computing the sensitivity of the \nlogistic regression probabilities.\n\n234 \nChapter 8 \u25a0 Predictive Modeling\n \n\u25a0z statistic or Wald test: z is calculated by the equation:\nz\nSE\n= \u03b2\nThe larger the value of z, the more likely the term associated with the \ncoeffi cient is signifi cant in the model. \n \n\u25a0Pr(>|z|): The p value associated with the z statistic. Often, analysts consider \nvalues less than 0.05 to indicate a signifi cant predictor, although there is \nno theoretical reason for making this inference; it is histori", "associated with the z statistic. Often, analysts consider \nvalues less than 0.05 to indicate a signifi cant predictor, although there is \nno theoretical reason for making this inference; it is historical precedence. \nThe p value does not indicate how much accuracy the variable provides, \nper se, but rather how likely it is that the coeffi cient is different from 0. \nThe more records one has in the training data, the smaller SE and the p\nvalue will be. Therefore, the more records you have, the more stable the \ncoeffi cients become. \nSome implementations of logistic regression include variable selection options, \nusually forward selection or backward elimination of variables. The p value is \noften the metric used to decide to include or exclude a variable, by including \nnew variables only if the p value is less than some value (typically 0.05), or \nexcluding a variable if its p value is greater than some value (often 0.1). These \nthresholds are usually confi gurable.\nIf variable selectio", " the p value is less than some value (typically 0.05), or \nexcluding a variable if its p value is greater than some value (often 0.1). These \nthresholds are usually confi gurable.\nIf variable selection options are not available, the practitioner can use the p\nvalues to decide which variables should be excluded. For example, if you use \nlogistic regression to build a model to predict lapsed donors with target variable \nTARGET_B, and include inputs NGIFTALL, LASTGIFT, FISTDATE, RFA_2F, \nRFA_2A, and AGE, the following model was created, with coeffi cients and \nsummary statistics shown in Table 8-5.\nTable 8-5: Logistic Regression Model Report\nVARIABLE\nCOEFF.\nSTD. ERR.\nZ\uf6baSCORE\nP>|Z|\nNGIFTALL\n 0.0031\n0.0024\n 1.2779\n0.2013\nLASTGIFT\n 0.0008\n0.0014\n 0.5702\n0.5685\nFISTDATE\n\u20130.0003\n0.0000664\n\u20133.9835\n0.0000679\nRFA_2F=2\n 0.2686\n0.0405\n 6.636\n3.22E\u201311\nRFA_2F=3\n 0.3981\n0.047\n 8.4638\n0\nRFA_2F=4\n 0.5814\n0.0538\n10.7987\n0\nRFA_2A=E\n\u20130.1924\n0.0535\n\u20133.5937\n0.0003\nRFA_2A=F\n\u20130.3558\n0.0613\n\u20135.8085\n6.31E\u201309\n\n \n", "0.0000679\nRFA_2F=2\n 0.2686\n0.0405\n 6.636\n3.22E\u201311\nRFA_2F=3\n 0.3981\n0.047\n 8.4638\n0\nRFA_2F=4\n 0.5814\n0.0538\n10.7987\n0\nRFA_2A=E\n\u20130.1924\n0.0535\n\u20133.5937\n0.0003\nRFA_2A=F\n\u20130.3558\n0.0613\n\u20135.8085\n6.31E\u201309\n\n \nChapter 8 \u25a0 Predictive Modeling \n235\nVARIABLE\nCOEFF.\nSTD. ERR.\nZ\uf6baSCORE\nP>|Z|\nRFA_2A=G\n\u20130.5969\n0.075\n\u20137.9642\n      1.89E\u201315\nAGE\n\u20130.0014\n0.0011\n\u20131.3129\n0.1892\nConstant\n\u20130.381\n0.6351\n\u20130.5999\n0.5486\nThe worst predictor according to the p value is LASTGIFT, and therefore this \nvariable is a good candidate for removal. This doesn\u2019t mean that LASTGIFT \nisn\u2019t a good predictor by itself, only that in combination with the other inputs, \nit is either relatively worse, or the information in LASTGIFT also exists in other \nvariables rendering LASTGIFT unnecessary to include in the model. After \nremoving LASTGIFT, the model is rebuilt, resulting in the model coeffi cient \nvalues and summary statistics shown in Table 8-6.\nTable 8-6: Logistic Regression Model after Removing LASTGIFT\nVARIABLE\nCOEFF.\nSTD. ER", "TGIFT, the model is rebuilt, resulting in the model coeffi cient \nvalues and summary statistics shown in Table 8-6.\nTable 8-6: Logistic Regression Model after Removing LASTGIFT\nVARIABLE\nCOEFF.\nSTD. ERR.\nZ\uf6baSCORE\nP>|Z|\nNGIFTALL\n0.0031\n0.0024\n1.2772\n0.2015\nFISTDATE\n\u20130.0003\n       0.0000664\n\u20133.9834\n       0.0000679\nRFA_2F=2\n0.2671\n0.0404\n6.6136\n3.75E\u201311\nRFA_2F=3\n0.396\n0.0469\n8.4465\n0\nRFA_2F=4\n0.5787\n0.0536\n10.7924\n0\nRFA_2A=E\n\u20130.1898\n0.0534\n\u20133.558\n0.0004\nRFA_2A=F\n\u20130.349\n0.0601\n\u20135.8092\n6.28E\u201309\nRFA_2A=G\n\u20130.5783\n0.0673\n\u20138.5932\n0\nAGE\n\u20130.0014\n0.0011\n\u20131.3128\n0.1893\nConstant\n\u20130.3743\n0.635\n\u20130.5894\n0.5556\nNow NGIFTALL and AGE are the only remaining variables with p values \ngreater than 0.05. You can continue the process until all the variables have p \nvalues greater than 0.05 if the goal is to identify variables that are all statistically \nsignifi cant at the 0.05 level. \nHowever, it is possible that variables with p values above the 0.05 signifi cance \nlevel still provide predictive accuracy. If a", " variables that are all statistically \nsignifi cant at the 0.05 level. \nHowever, it is possible that variables with p values above the 0.05 signifi cance \nlevel still provide predictive accuracy. If accuracy is the objective, you should \nuse accuracy to decide which variables to include or exclude.\nOther Practical Considerations for Logistic Regression\nIn addition to the principles of building logistic regression models described \nso far, successful modelers often include other data preparation steps. \n\n236 \nChapter 8 \u25a0 Predictive Modeling\nThe following four considerations are among the most common I apply during \nmodeling projects.\nInteractions\nLogistic regression is a \u201cmain effect\u201d model: The form of the model described \nso far has been a linearly weighted sum of inputs with no interaction terms. \nSometimes, however, it is the interaction of two variables that is necessary for \naccurate classifi cation. \nConsider a simple example in Figure 8-13: The cross data set created from \ntwo i", "on terms. \nSometimes, however, it is the interaction of two variables that is necessary for \naccurate classifi cation. \nConsider a simple example in Figure 8-13: The cross data set created from \ntwo inputs, x and y, and a binary target variable z represented by X-shapes \n(target variable = +1) and circles (target variable = \u20131). This example is just an \nextension of the famous \u201cXOR\u201d problem cited by Minsky and Pappert in their \nbook Perceptrons (MIT, 1969). Clearly there is no linear separator that classifi es \nthis dataset. \nA logistic regression model using only a linear combination of inputs x and \ny\u2014a main effect model\u2014only classifi es approximately 50 percent of the cases \ncorrectly. The predicted values from the logistic regression model are shown \nin Figure 8-14\u2014clearly not an effective classifi er. \n\u22129.000\n\u22129.999\n\u22127.999\n\u22125.999\n\u22123.999\n\u22121.999\n0.001\n2.001\n4.001\n6.001\n8.001\n9.994\n\u22128.000\n\u22126.000\n\u221210.000\n\u22127.000\n\u22125.000\n\u22124.000\n\u22122.000\n\u22123.000\n\u22121.000\n1.000\n0.000\n3.000\n4.000\n2.000\n5.000\n6.0", "tive classifi er. \n\u22129.000\n\u22129.999\n\u22127.999\n\u22125.999\n\u22123.999\n\u22121.999\n0.001\n2.001\n4.001\n6.001\n8.001\n9.994\n\u22128.000\n\u22126.000\n\u221210.000\n\u22127.000\n\u22125.000\n\u22124.000\n\u22122.000\n\u22123.000\n\u22121.000\n1.000\n0.000\n3.000\n4.000\n2.000\n5.000\n6.000\n7.000\n8.000\n9.993\n9.000\nFigure 8-13: Cross data\n\n \nChapter 8 \u25a0 Predictive Modeling \n237\n\u22129.000\n\u22128.000\n\u22126.000\n\u221210.000\n\u22127.000\n\u22125.000\n\u22124.000\n\u22122.000\n\u22123.000\n\u22121.000\n1.000\n0.000\n3.000\n4.000\n2.000\n5.000\n6.000\n7.000\n8.000\n9.993\n9.000\n\u22129.999\n\u22127.999\n\u22125.999\n\u22123.999\n\u22121.999\n0.001\n2.001\n4.001\n6.001\n8.001\n9.994\nFigure 8-14: Linear separation of cross data\nConsider adding an interaction term to the model: x \u00d7 y. A histogram of this \nnew feature, shown in Figure 8-15, shows the value of this interaction: The model \nthat was able to do no better than a \u201crandom\u201d guess using x and y by themselves \nhas become, with an interaction term, an excellent model. \n0\n36\n72\n108\n144\n180\n216\n252\n288\n324\n360\n406\nFigure 8-15: Histogram of interaction variable\n\n238 \nChapter 8 \u25a0 Predictive Modeling\nMissing Values\nLogistic re", "th an interaction term, an excellent model. \n0\n36\n72\n108\n144\n180\n216\n252\n288\n324\n360\n406\nFigure 8-15: Histogram of interaction variable\n\n238 \nChapter 8 \u25a0 Predictive Modeling\nMissing Values\nLogistic regression cannot have any missing values. How missing values are \nhandled is implementation dependent, although often the course of action \nin software is listwise deletion. All of the typical missing value imputation \nmethods apply.\nDummy Variables\nLogistic regression is a numeric algorithm: All inputs must be numeric. Categorical \nvariables must therefore be transformed into a numeric format, the most com-\nmon of which is 1/0 dummy variables: one column for each value in the variable \nminus one. If a categorical variable has eight levels, create seven dummy vari-\nables only; the contribution for the eighth will be taken up by the bias (constant) \nterm. If all values are represented in the dummy variable, the multicollinearity \nin the data will cause the terms combing from the same target ", "for the eighth will be taken up by the bias (constant) \nterm. If all values are represented in the dummy variable, the multicollinearity \nin the data will cause the terms combing from the same target variable to have \nz values that fail signifi cance tests.\nConsider a simple logistic regression model with one categorical input hav-\ning four levels: A, B, C, and D. Dummy variables are created for each level so \nthat there are now four candidate inputs to the logistic regression model. If all \nfour are included in the model, you may see bizarre standard error values like \nthose in Table 8-7. The p values equal to 0.9999 make these four variables seem \nunrelated to the target. Note that even the constant term is problematic.\nTable 8-7: Logistic Regression Model with n Dummy Variables\nVARIABLE\nBETA\nSE\nWALD TEST, Z\nP>|Z|\nX1=D\n2.38\n28,630.90\n8.31E\u201305\n0.9999\nX1=E\n2.80\n28,630.90\n9.78E\u201305\n0.9999\nX1=F\n3.24\n28,630.90\n0.0001\n0.9999\nX1=G\n3.49\n28,630.90\n0.0001\n0.9999\nConstant\n\u20130.1471\n28,630.90\n\u20135.14", "\nBETA\nSE\nWALD TEST, Z\nP>|Z|\nX1=D\n2.38\n28,630.90\n8.31E\u201305\n0.9999\nX1=E\n2.80\n28,630.90\n9.78E\u201305\n0.9999\nX1=F\n3.24\n28,630.90\n0.0001\n0.9999\nX1=G\n3.49\n28,630.90\n0.0001\n0.9999\nConstant\n\u20130.1471\n28,630.90\n\u20135.14E\u201306\n1\nThe clue, however, is that the SE values are so large. This indicates a numeri-\ncal degeneracy in their calculation. The general principle is this: For categori-\ncal variables with n levels, only include n \u20131 dummy variables as inputs to a \nlogistic regression model. If you remove one of the dummy variables, such as \n\n \nChapter 8 \u25a0 Predictive Modeling \n239\nX1=G, the problem goes away and the model looks fi ne (Table 8-8). It doesn\u2019t \nmatter which dummy variable is removed; you can choose to remove the dummy \nthat is the least interesting to report.\nTable 8-8: Logistic Regression Model with n\u20131 Dummy Variable\nVARIABLE\nBETA\nSE\nWALD TEST, Z\nP>|Z|\nX1=D\n\u20131.1062\n0.08\n\u201314.0\n0\nX1=E\n\u20130.6865\n0.07\n\u201310.01\n0\nX1=F\n\u20130.2438\n0.06\n\u20133.7863\n0.0002\nConstant\n3.3387\n0.0558\n59.7835\n0\nMulti-Class Classi\ufb01 ca", "ith n\u20131 Dummy Variable\nVARIABLE\nBETA\nSE\nWALD TEST, Z\nP>|Z|\nX1=D\n\u20131.1062\n0.08\n\u201314.0\n0\nX1=E\n\u20130.6865\n0.07\n\u201310.01\n0\nX1=F\n\u20130.2438\n0.06\n\u20133.7863\n0.0002\nConstant\n3.3387\n0.0558\n59.7835\n0\nMulti-Class Classi\ufb01 cation\nIf the target variable has more than two levels, logistic regression cannot be used \nin the form described here. However, there are two primary ways practitioners \nextend the logistic regression classifi er.\nThe most common way is to explode the multi-class variable with N levels \ninto N \u20131 dummy variables, and build a single logistic regression model for \neach of these dummy variables. Each variable, as a binary variable, represents \na variable for building a \u201cone vs. all\u201d classifi er, meaning that a model built from \neach of these dummy variables will predict the likelihood a record belongs to \na particular level of the target variable in contrast to belonging to any of the \nother levels. While you could build N classifi ers, modelers usually build N\u20131 \nclassifi ers and the predicti", "ongs to \na particular level of the target variable in contrast to belonging to any of the \nother levels. While you could build N classifi ers, modelers usually build N\u20131 \nclassifi ers and the prediction for the Nth level is merely one minus the sum of \nall the other probabilities:\nP\ng\nP\ng\ni\nN\n(\n)\nTarget\nN\n(\n)\nTarget\ni\nN\n=\n\u2212\n\u2211\n1\n1\nWhen deploying models using this approach, N\u20131 model scores are generated \nand a seventh score is computed from the other six. For the nasadata dataset, six \nmodels are created which generate six probabilities. The seventh probability is \none minus the sum of the other six. Typically, the maximum probability from \nthe seven classes is considered the winner, although other post-processing steps \ncan be incorporated as well to calculate the winning class.\n\n240 \nChapter 8 \u25a0 Predictive Modeling\nThe benefi t of this approach is that the number of classifi ers scales linearly \nwith the number of levels in the target variable, and the interpretation of \nthe resulting", "ter 8 \u25a0 Predictive Modeling\nThe benefi t of this approach is that the number of classifi ers scales linearly \nwith the number of levels in the target variable, and the interpretation of \nthe resulting models is straightforward. However, this approach groups all \nthe remaining target levels into a single value, 0, regardless of whether they are \nrelated to one another or not, and therefore the model may suffer in accuracy \nas a result.\nAn alternative that is used sometimes is to build a model for every pair of \ntarget variable levels: the pairwise classifi er approach. This approach requires \nsampling records to fi nd just those that apply to each pair of levels a classifi er \nis built for. For that levels, this requires building N \u00d7 (N\u20131)/2 classifi ers. For \nexample, the nasadata contains 7 classes, and therefore building a classifi er for \neach pair of classes requires building 21 classifi ers ((7 \u00d7 6)/2).\nIt is certainly true that the pairwise approach requires building many \nmore m", " 7 classes, and therefore building a classifi er for \neach pair of classes requires building 21 classifi ers ((7 \u00d7 6)/2).\nIt is certainly true that the pairwise approach requires building many \nmore models than the one-vs.-all approach; it isn\u2019t necessarily the case that \ncomputationally it is more expensive. The nasadata has 424 records, but only \napproximately 60 per class. Every classifi er therefore is built on only 120 records \nrather than the full dataset, so the computation burden depends also on the \neffi ciency of the algorithm. \nDeploying the pairwise set of classifi ers requires that a new score (a column) \nbe created for every classifi er. Each level will have N\u20131 classifi ers with its level \ninvolved. For the nasadata example, you have 21 classifi ers and each target \nvariable level is involved in 6 of them. You can decide on the winner through \na number of approaches, including averaging the probabilities for each level \n(6 each in the nasadata example) or counting votes ", " level is involved in 6 of them. You can decide on the winner through \na number of approaches, including averaging the probabilities for each level \n(6 each in the nasadata example) or counting votes where each level has the \nlarger probability. \nBeware of implementations that automatically handle multilevel target vari-\nables by creating dummies. Some implementations use the same model structure \nfor each one-vs-all classifi er even if some variables are not good predictors for \na particular one-vs-all model.\nNeural Networks\nArtifi cial Neural Networks (ANN) began as linear models in 1943 with the \nintroduction of a model that emulated the behavior of a neuron with McCulloch \nand Pitts. These were linear models with a threshold, although they were not \nintroduced primarily for their biological analogy but rather for their computa-\ntional fl exibility. In the 1950s and 60s, there was a fl urry of activity surrounding \nthese ideas, primarily using single nodes, although some began exper", "gical analogy but rather for their computa-\ntional fl exibility. In the 1950s and 60s, there was a fl urry of activity surrounding \nthese ideas, primarily using single nodes, although some began experimenting \n\n \nChapter 8 \u25a0 Predictive Modeling \n241\nwith multiple layers of nodes, including Frank Rosenblatt, Bernie Widrow, \nRoger Barron, and others. However, ANNs were primarily known by the single \nneuron in the early days, either the perceptron as designed by Rosenblatt or an \nADALINE (Adaptive Linear Neuron) by Widrow.\nThe momentum gained in neural network research during the 1950s and 60s \nhit a wall in 1969 with the publication of the book Perceptrons (The MIT Press) \nby Marvin Minsky and Seymour Pappert, which proved that the perceptron \ncould not predict even elementary logic functions such as the exclusive or (XOR) \nfunction. While this is true, a second hidden layer in a neural network can easily \nsolve the problem, but the fl exibility of 2-layer neural networks wasn\u2019t able to ", "s such as the exclusive or (XOR) \nfunction. While this is true, a second hidden layer in a neural network can easily \nsolve the problem, but the fl exibility of 2-layer neural networks wasn\u2019t able to \nbreak through the impression gained from the Minsky and Pappert book about \nthe defi ciencies of perceptrons. Neural networks remained in the background \nuntil the 1980s.\nThe resurgence of neural networks in the 1980s was due to the research in \nParallel Distributed Processing (PDP) systems by David Rumelhart, James \nMclelland, and others. It was in 1986 that the backpropagation algorithm was \npublished. Backpropagation enabled the learning of the neural network weights \nof a multilayer network of elements with continuous (initially sigmoidal) activa-\ntion functions. It was designed to be simple from the start, or in the words of \nRumelhart, \u201cWe\u2019ll just pretend like it [the neural network] is linear, and fi gure \nout how to train it as if it were linear, and then we\u2019ll put in these sigmoi", "ple from the start, or in the words of \nRumelhart, \u201cWe\u2019ll just pretend like it [the neural network] is linear, and fi gure \nout how to train it as if it were linear, and then we\u2019ll put in these sigmoids.\u201d \nAnother signifi cant development was the increase in speed of computers so \nthey could handle the computational load of neural networks. This allowed \nresearchers to begin experimenting with neural networks on a wide range of \napplications. The fi rst international neural network conference of the Institute \nof Electrical and Electronics Engineers (IEEE) was held in 1987, giving further \ncredibility to neural networks as an accepted approach to data analysis.\nIn 1989, George Cybenko proved that a sigmoidal neural network with one \nhidden layer can approximate any function under some mild constraints. This \nfl exibility of the neural network to adapt to any reasonable function became a \npowerful rallying cry for practitioners, and by the 1990s, neural networks were \nbeing widely imple", " constraints. This \nfl exibility of the neural network to adapt to any reasonable function became a \npowerful rallying cry for practitioners, and by the 1990s, neural networks were \nbeing widely implemented in software both in standalone, neural network\u2013only \nsoftware packages as well as options in data mining software that contained \na suite of algorithms.\nANNs are a broad set of algorithms with great variety in how they are trained, \nand include networks that are only feedforward as well as those with feedbacks. \nMost often, when the phrase \u201cneural network\u201d is used in predictive analytics, \nthe intent is a specifi c neural network called the mutli-layer perceptron (MLP). \nThe neural networks described in this section therefore are limited to MLPs. \n\n242 \nChapter 8 \u25a0 Predictive Modeling\nBuilding Blocks: The Neuron\nThe perceptron is an ANN comprised of a single neuron. In an MLP, neurons \nare organized in layers in a fully connected, feedforward network. Each neuron \nis simply a linear", "uilding Blocks: The Neuron\nThe perceptron is an ANN comprised of a single neuron. In an MLP, neurons \nare organized in layers in a fully connected, feedforward network. Each neuron \nis simply a linear equation just like linear regression as shown in the following \nequation. Figure 8-16 shows a representation of a single neuron. Just as with the \nlogistic regression model, the values w0, w1, and so forth are the weights, and \nthe values x0, x1, and so forth are the inputs to the neural network.\ny\nw\nw\nw\nx\nn\nw\nw\nx\nw\nx\nw\nn\n=\n+\nw\n\u00d7\n+\nx\n\u00d7\n+\nx\n+\n\u00d7\nw\n0\nw\nw\nThis equation is often called the transfer function in an ANN. In the early \ndays of neural networks, in the 1960s, this linearly weighted sum would be \nthresholded at some value so that the output of the neuron would be either 1 \nor 0. The innovation in the 1980s was the introduction of a soft-thresholding \napproach by means of an activation function. \nZ1\nZ\nX1\nX\nX2\nX\nX3\nX\nXn\nX\n...\nFigure 8-16: Single neuron\nThe neuron contains one other ele", "vation in the 1980s was the introduction of a soft-thresholding \napproach by means of an activation function. \nZ1\nZ\nX1\nX\nX2\nX\nX3\nX\nXn\nX\n...\nFigure 8-16: Single neuron\nThe neuron contains one other element after the linear function. The output \nof the linear function is then transformed by the activation function, often called \na squashing function, that transforms the linear output to a number between 0 \nand 1. The most common squashing function is the sigmoid, which is the same \nfunction as the logistic regression logistic curve:\nz\nlogstic y\ne y\n1\n1\n1\n=\n=\nlogstic\n\u2212\n\u2212\n)\ny\n( )\ny\nOther functions used for the squashing function include the hyperbolic \ntangent (tanh) and arctangent (arctan). Determining which activation function \nto use is not usually very important: Any of them suffi ce for the purpose of \ntransforming the transfer function. One difference between these activation \nfunctions is that the logistic curve ranges from 0 to 1, whereas tanh and arctan \nrange from \u20131 to +1. \n\n \nC", "he purpose of \ntransforming the transfer function. One difference between these activation \nfunctions is that the logistic curve ranges from 0 to 1, whereas tanh and arctan \nrange from \u20131 to +1. \n\n \nChapter 8 \u25a0 Predictive Modeling \n243\nz\ny\ne\ne\ne\ne\ny\ny\ne\ny\ny\ne\n1\n=\ny\n= tanh( )\nThe key is that the activation functions are continuous and nonlinear. Because \nthey are continuous, you can compute derivatives, an essential property of the \nneurons so that the learning algorithms used to train neural networks can be \napplied. Neurons, except for output layer neurons for reasons to be described \nlater, must be nonlinear for the neural network to be able to estimate nonlinear \nfunctions and nonlinear relationships in the data. It is precisely the nonlinear \naspect of the neuron that gives the neural network predictive power and fl exibility.\nA single neuron builds a linear model: a linear decision boundary for clas-\nsifi cation or a linear estimator for continuous-valued prediction. Such models \n", "network predictive power and fl exibility.\nA single neuron builds a linear model: a linear decision boundary for clas-\nsifi cation or a linear estimator for continuous-valued prediction. Such models \nare not particularly interesting because there are many linear modeling algo-\nrithms that are as accurate as the single neuron but are algorithmically far more \neffi cient. However, when layers are added to the ANN, they become far more \nfl exible and powerful as predictors.\nA layer is a set of neurons with common inputs, as shown in Figure 8-17. The \nneurons, in general, will have different coeffi cients, all learned by the training \nalgorithms used by the ANN.\nSome layers in the ANN are called hidden layers because their outputs are \nhidden to the user. In the network shown in Figure 8-18, there are two hidden \nlayers in the ANN. This network also has an output layer: one for every target \nvariable in the data. If you build classifi cation models for the nasadata, each of \nthe seven targ", "here are two hidden \nlayers in the ANN. This network also has an output layer: one for every target \nvariable in the data. If you build classifi cation models for the nasadata, each of \nthe seven target variable classes will have its own output layer neuron. Note \nthat this is in contrast to logistic regression models where completely separate \nmodels for each target variable value are built.\nX1\nX\nX2\nX\nX3\nX\ny1\ny\ny2\ny\nym\ny\nXn\nX\nneuron or node:\nsingle combination\nof inputs\nlayer: set of neurons\nwith common inputs\n...\n...\n...\n...\n...\nFigure 8-17: Neural network terminology, part 1\n\n244 \nChapter 8 \u25a0 Predictive Modeling\nX1\nX\nX2\nX\nX3\nX\ny1\ny\ny2\ny\nym\ny\nXn\nX\ninput \u201clayer\u201d: the\ninputs to the network\nhidden layers: layer between\ninput layer and output layer,\noutputs are \u201chidden\u201d to analyst\nOutput layer: output\nfrom these neurons isi\nmodel outputs(s)\n...\n...\n...\n...\n...\nFigure 8-18: Neural network terminology, part 2\nFor classifi cation modeling, the output layer neurons usually have the same \nsig", "er: output\nfrom these neurons isi\nmodel outputs(s)\n...\n...\n...\n...\n...\nFigure 8-18: Neural network terminology, part 2\nFor classifi cation modeling, the output layer neurons usually have the same \nsigmoid activation function already described, one that transforms the neuron \nto a value between 0 and 1. For continuous-valued prediction, however, software \nimplementations of ANNs most often use what is called a \u201clinear\u201d activation \nfunction, which means that no transformation is applied to the linearly weighted \nsum transfer function. For target variables that are unbounded, you can also \nuse an exponential activation function in the output layer neurons.\nThe cost function for a neural network predicting a continuous-valued output \nminimizes squared error just like linear regression. In fact, without the squash-\ning functions for each neuron in a neural network, the network would just be a \nlarge, ineffi cient, linear regression model. The nonlinear squashing functions \nenable the networ", ", without the squash-\ning functions for each neuron in a neural network, the network would just be a \nlarge, ineffi cient, linear regression model. The nonlinear squashing functions \nenable the network to fi nd very complex relationships between the inputs and \noutput without any intervention by the user. \nFor classifi cation, many predictive analytics software packages use cross-\nentropy as the cost function rather than minimizing squared error because it \nseems to be a more appropriate measure of error for a dichotomous output clas-\nsifi cation problem. It has been shown, however, that even a squared-error cost \ncan fi nd solutions every bit as good as cross-entropy for classifi cation problems.\nA neural network for classifi cation with no hidden layer and a single output \nneuron is essentially a logistic regression model when the neuron has a logistic \nactivation function, especially if the cost function is cross-entropy. With the \nsquared error cost function, there is no guarantee ", "ntially a logistic regression model when the neuron has a logistic \nactivation function, especially if the cost function is cross-entropy. With the \nsquared error cost function, there is no guarantee the model will be the same.\nNeural Network Training\nANNs are iterative learners, in contrast to algorithms like linear regression, \nwhich learn the coeffi cients in a single processing step. The learning process \nis similar to how I learned to catch fl y balls as a boy. First, imagine my father \n\n \nChapter 8 \u25a0 Predictive Modeling \n245\nhitting a fl y ball to me in the outfi eld. In the beginning, I had absolutely no idea \nwhere the ball was going to land, so I just watched it until it landed, far to my \nleft. Since my goal was to catch the ball, the distance between where the ball \nlanded and where I stood was the error. Then my father hit a second fl y ball, \nand, because of the prior example I had seen, I moved (hesitantly) toward where \nthe ball landed last time, but this time the ball l", "here I stood was the error. Then my father hit a second fl y ball, \nand, because of the prior example I had seen, I moved (hesitantly) toward where \nthe ball landed last time, but this time the ball landed to my right. Wrong again. \nBut then something began to happen. The more fl y balls my father hit, the \nmore I was able to associate the speed the ball was hit, the steepness of the hit, \nand the left/right angle\u2014the initial conditions of the hit\u2014and predict where \nthe ball was going to land. Major league outfi elders are so good at this that for \nmany fl y balls, they arrive at the spot well before the ball arrives and just wait. \nThis is exactly what neural networks do. First, all the weights in the ANN are \ninitialized to small random values to \u201ckick start\u201d the training. Then, a single \nrecord is passed through the network, with all the multiplies, adds, squashing \ncomputed all the way through neurons in hidden layers and the output layer, \nyielding a prediction from each output ne", " \nrecord is passed through the network, with all the multiplies, adds, squashing \ncomputed all the way through neurons in hidden layers and the output layer, \nyielding a prediction from each output neuron in the neural network. The error \nis computed between the actual target value and the prediction for that record. \nWeights are adjusted in the ANN proportional to the error. The process for \ncomputing the weights for one output neuron is described here, but the same \nprinciples apply for more than one output neuron.\nThe process is repeated next for the second record, including the forward \ncalculations ending in predictions from the output layer neuron(s) and error \ncalculations, and then weights are updated by backpropagation of the errors. \nThis continues until the entire set of records has passed through the ANN. \nFigure 8-19 is a representation of the process where each jump is the result of \na single record passing through the network and weights being updated. The \nrepresentatio", " has passed through the ANN. \nFigure 8-19 is a representation of the process where each jump is the result of \na single record passing through the network and weights being updated. The \nrepresentation here shows a network converging to the minimum error solu-\ntion if the error surface is a parabola, as is the case when minimizing squared \nerrors for a linear model such as linear regression. The actual error surface is \nmuch more complex than this.\nNeural networks iterate to\nminimum solution in one\nstep (iterative solution)\nFigure 8-19: Iterative convergence to the bottom of a quadratic error curve\n\n246 \nChapter 8 \u25a0 Predictive Modeling\nAn epoch or pass occurs when every record in the training data has passed \nthrough the neural network and weights have been updated. Training a neural \nnetwork can take dozens, hundreds, or even thousands of training epochs; you \ncan understand why ANNs have a reputation for taking a long time to train. \nThe training time on commonly available computer h", "ork can take dozens, hundreds, or even thousands of training epochs; you \ncan understand why ANNs have a reputation for taking a long time to train. \nThe training time on commonly available computer hardware is acceptable for \nmost problems. \nThe forward and backward process continues until a stop condition applies \nto end training. The diffi culty in converging to a solution is non-trivial because \nof the non-linearity of neural networks. Rather than the solution following the \nsmooth quadratic error surface shown in Figure 8-19, it is more like the con-\nceptual error curve shown in Figure 8-20 with many dips and hills. These dips \nare local minima. The challenge for training a neural network is to traverse the \nerror surface at just the right pace. \nError\nLocal\nminimum\nGlobal\nminimum\nFigure 8-20: Local and global minima\nIf the algorithm takes too small of a step, it can easily end up in a local mini-\nmum, as shown on the left in Figure 8-21. A larger learning rate is suffi cient to \n", "ure 8-20: Local and global minima\nIf the algorithm takes too small of a step, it can easily end up in a local mini-\nmum, as shown on the left in Figure 8-21. A larger learning rate is suffi cient to \njump over the local minimum on the right, enabling the algorithm to fi nd the \nglobal minimum. \nError\nLocal\nminimum\nGlobal\nminimum\nError\nLocal\nminimum\nGlobal\nminimum\nFigure 8-21: Iterating to local and global minima\n\n \nChapter 8 \u25a0 Predictive Modeling \n247\nYou never know if the global minimum has been found because you never \nknow if all of the points of the error surface have been found. In the left sequence \nof errors of Figure 8-22, you have found a minimum. You only know it is a local \nminimum if you build a second neural network and fi nd a better solution, such \nas the sequence of errors on the right of Figure 8-22. Is this minimum a global \nminimum? You can never know for sure.\nError\nIs this a local\nminimum?\nIs this a local or\nglobal minimum?\nFigure 8-22: Local or global minimum?\nExp", "on the right of Figure 8-22. Is this minimum a global \nminimum? You can never know for sure.\nError\nIs this a local\nminimum?\nIs this a local or\nglobal minimum?\nFigure 8-22: Local or global minimum?\nExperimentation therefore is key with neural networks. Modelers typically \nbuild several networks with varying parameters, which include varying how \nmany neurons to include in hidden layers, learning rates, even changing the \nrandom number seed that generates the initial weights for the network.  \nThe Flexibility of Neural Networks\nAs universal approximators, neural networks are very fl exible predictors. One \nexample is the ability of the ANN to predict the sombrero function without \nhaving to create derived variables to unravel the nonlinearities in the data. The \nsombrero function is defi ned by the equation:\nz =\nsqrt\ny\nsqrt\ny\n2\n2\n2\n2\nsin(\n(\n)\nx + y\n2\n2\n+ y\n)\n(\n)\nx + y\n2\n2\n+ y\nand has two classes, represented by 1 (X-shapes) and 0 (circles). The (X-shapes) class \nis characterized by those", " by the equation:\nz =\nsqrt\ny\nsqrt\ny\n2\n2\n2\n2\nsin(\n(\n)\nx + y\n2\n2\n+ y\n)\n(\n)\nx + y\n2\n2\n+ y\nand has two classes, represented by 1 (X-shapes) and 0 (circles). The (X-shapes) class \nis characterized by those data points with z > 0 and the (circles) class with z <= 0. \nFigure 8-23 shows the function. \n\n248 \nChapter 8 \u25a0 Predictive Modeling\n\u22129.000\n\u22128.000\n\u22126.000\n\u221210.000\n\u22127.000\n\u22125.000\n\u22124.000\n\u22122.000\n\u22123.000\n\u22121.000\n1.000\n0.000\n3.000\n4.000\n2.000\n5.000\n6.000\n7.000\n8.000\n9.993\n9.000\n\u22129.999\n\u22127.999\n\u22125.999\n\u22123.999\n\u22121.999\n0.001\n2.001\n4.001\n6.001\n8.001\n9.994\nFigure 8-23: Sombrero function\nThis function is clearly not linearly separable; any line drawn will necessarily \ncontain both classes on either side of the line (see Figure 8-24). This function \ncannot, therefore, be estimated by logistic regression models without transform-\ning the inputs to \u201clinearize\u201d the relationship between inputs and the output. \nLikewise, decision trees will have great diffi culty with this function because \nof the smooth curves th", "thout transform-\ning the inputs to \u201clinearize\u201d the relationship between inputs and the output. \nLikewise, decision trees will have great diffi culty with this function because \nof the smooth curves throughout the decision boundaries between the values. \n\u221210.000\n\u22129.000\n\u22129.999\n\u22127.999\n\u22125.999\n\u22123.999\n\u22121.999\n0.001\n2.001\n4.001\n6.001\n8.001\n9.994\n\u22128.000\n\u22127.000\n\u22126.000\n\u22125.000\n\u22124.000\n\u22123.000\n\u22122.000\n\u22121.000\n0.000\n1.000\n2.000\n4.000\n5.000\n6.000\n7.000\n8.000\n9.000\n9.993\n3.000\nFigure 8-24: Linear classifier for sombrero function\n\n \nChapter 8 \u25a0 Predictive Modeling \n249\nAn MLP with two hidden layers containing twelve hidden units per hidden \nlayer was trained on this data. As training of the neural network progresses \nthrough 10, 100, 200, 500, 1000 and 5000 epochs, you see in Figure 8-25 the true \nshape of the sombrero function forming. With only 10 or 100 epochs, the som-\nbrero pattern has not yet been learned. The gist of the sombrero function is seen \nafter 500 epochs, and after 5000 epochs, the sombrer", " sombrero function forming. With only 10 or 100 epochs, the som-\nbrero pattern has not yet been learned. The gist of the sombrero function is seen \nafter 500 epochs, and after 5000 epochs, the sombrero shape is clear.\n\u22129.999\n\u22129.000\n\u221210.000 \u22128.000 \u22126.000 \u22124.000 \u22122.000 0.000\n2.000\n4.000\n6.000\n8.000\n9.993\n\u22127.000 \u22125.000 \u22123.000 \u22121.000 1.000\n3.000\n5.000\n7.000\n9.000\n\u22129.000\n\u221210.000 \u22128.000 \u22126.000 \u22124.000 \u22122.000 0.000\n2.000\n4.000\n6.000\n8.000\n9.993\n\u22127.000 \u22125.000 \u22123.000 \u22121.000 1.000\n3.000\n5.000\n7.000\n9.000\n\u22127.999\n\u22125.999\n\u22123.999\n\u22121.999\n0.001\n2.001\n4.001\n6.001\n8.001\n9.994\n\u22129.999\n\u22127.999\n\u22125.999\n\u22123.999\n\u22121.999\n0.001\n2.001\n4.001\n6.001\n8.001\n9.994\n\u22129.999\n\u22129.000\n\u221210.000 \u22128.000 \u22126.000 \u22124.000 \u22122.000 0.000\n2.000\n4.000\n6.000\n8.000\n9.993\n\u22127.000 \u22125.000 \u22123.000 \u22121.000 1.000\n3.000\n5.000\n7.000\n9.000\n\u22129.000\n\u221210.000 \u22128.000 \u22126.000 \u22124.000 \u22122.000 0.000\n2.000\n4.000\n6.000\n8.000\n9.993\n\u22127.000 \u22125.000 \u22123.000 \u22121.000 1.000\n3.000\n5.000\n7.000\n9.000\n\u22127.999\n\u22125.999\n\u22123.999\n\u22121.999\n0.001\n2.001\n4.001\n6.001\n8.001\n9.994\n\u22129.999\n\u22127.999\n\u22125.999\n", "\u22126.000 \u22124.000 \u22122.000 0.000\n2.000\n4.000\n6.000\n8.000\n9.993\n\u22127.000 \u22125.000 \u22123.000 \u22121.000 1.000\n3.000\n5.000\n7.000\n9.000\n\u22127.999\n\u22125.999\n\u22123.999\n\u22121.999\n0.001\n2.001\n4.001\n6.001\n8.001\n9.994\n\u22129.999\n\u22127.999\n\u22125.999\n\u22123.999\n\u22121.999\n0.001\n2.001\n4.001\n6.001\n8.001\n9.994\n\u22129.999\n\u22129.000\n\u221210.000 \u22128.000 \u22126.000 \u22124.000 \u22122.000 0.000\n2.000\n4.000\n6.000\n8.000\n9.993\n\u22127.000 \u22125.000 \u22123.000 \u22121.000 1.000\n3.000\n5.000\n7.000\n9.000\n\u22129.000\n\u221210.000 \u22128.000 \u22126.000 \u22124.000 \u22122.000 0.000\n2.000\n4.000\n6.000\n8.000\n9.993\n\u22127.000 \u22125.000 \u22123.000 \u22121.000 1.000\n3.000\n5.000\n7.000\n9.000\n\u22127.999\n\u22125.999\n\u22123.999\n\u22121.999\n0.001\n2.001\n4.001\n6.001\n8.001\n9.994\n\u22129.999\n\u22127.999\n\u22125.999\n\u22123.999\n\u22121.999\n0.001\n2.001\n4.001\n6.001\n8.001\n9.994\nFigure 8-25: Decision boundaries as neural network learns after 10, 100, 200, 500, 1000, and 5000 \nepochs\nNeural Network Settings\nANNs have several options available in software for the building of the neural \nnetworks:\n \n\u25a0Learning rate (for MLPs only): Adjust the amount of change in weights \nper iteration. Larger learning rates hel", " have several options available in software for the building of the neural \nnetworks:\n \n\u25a0Learning rate (for MLPs only): Adjust the amount of change in weights \nper iteration. Larger learning rates help the MLP to train faster by making \nlarger jumps. In addition, larger learning rates help the MLP guard against \n\n250 \nChapter 8 \u25a0 Predictive Modeling\nconverging to a local minimum. However, a learning rate that is too large \ncould allow the MLP to jump over a global minimum. The solution some \nsoftware uses to the dilemma of learning rate size is to schedule the learn-\ning rate so it decreases as the number of epochs increases. Good values for \nlearning rates range from 0.01 to 0.5. The value 0.1 is a good starting value.\n \n\u25a0Learning rate (for MLPs only): Adjust the amount of change in weights \nper iteration. Larger learning rates help the MLP to train faster by making \nlarger jumps. In addition, larger learning rates help the MLP guard against \nconverging to a local minimum. However, a ", "ts \nper iteration. Larger learning rates help the MLP to train faster by making \nlarger jumps. In addition, larger learning rates help the MLP guard against \nconverging to a local minimum. However, a learning rate that is too large \ncould allow the MLP to jump over a global minimum. The solution some \nsoftware uses to the dilemma of learning rate size is to schedule the learn-\ning rate so it decreases as the number of epochs increases. Good values for \nlearning rates range from 0.01 to 0.5. The value 0.1 is a good starting value.\n \n\u25a0Momentum (for MLPs only): Helps speed up convergence with backprop \nby smoothing weight changes by adding a fraction of the prior weight \nchange to the current rate. It also has the effect, if the prior weight change \nand current weight changes have the same sign, of amplifying the mag-\nnitude of the weight adjustment. Good values of momentum range from \n0.3 to 0.9, with the larger value.\n \n\u25a0Number iterations/epochs: The maximum number of passes through the", "gn, of amplifying the mag-\nnitude of the weight adjustment. Good values of momentum range from \n0.3 to 0.9, with the larger value.\n \n\u25a0Number iterations/epochs: The maximum number of passes through the \ndata for training the network. This is included for time savings and in \nsome cases to guard against overfi tting the network.\n \n\u25a0Number of hidden layers and number of neurons in each hidden layer: \nThis specifi es the neural network architecture. While theoretically only \none hidden layer is necessary, in practice more complex problems can \nbenefi t from adding a second hidden layer to the network. No theory exists \nto specify exactly how many neurons are included in a neural network, \nalthough more neurons are needed for noisier data and more complex data. \nMany software packages now provide the ability to search through a wide \nrange of architectures so the modeler doesn\u2019t have to iterate manually.\n \n\u25a0Stopping criteria: Some implementations of neural networks allow stopping \nonce the ", "de the ability to search through a wide \nrange of architectures so the modeler doesn\u2019t have to iterate manually.\n \n\u25a0Stopping criteria: Some implementations of neural networks allow stopping \nonce the training accuracy reaches an acceptable level so that overfi tting the \ndata is avoided. Some implementations allow training for a pre-specifi ed \nnumber of minutes. Another option available in some software is to stop \ntraining when the error on testing data increases for many consecutive \nepochs, indicating that overfi tting has begun in the network.\n \n\u25a0Weight updates: Some implementations of neural networks also provide a \nsetting for how often the weights are updated. The default is that weights \nare updated after every single record passes through the network. However, \n100 or more records can be accumulated fi rst, and the average error on \nthese 100 records is computed to update the weights. This approach will \nspeed up training considerably but at the expense of reducing the infl u", "s can be accumulated fi rst, and the average error on \nthese 100 records is computed to update the weights. This approach will \nspeed up training considerably but at the expense of reducing the infl u-\nence of some records that might produce large errors on their own. \n\n \nChapter 8 \u25a0 Predictive Modeling \n251\nThe most popular learning algorithm for training neural networks is still \nthe backpropagation algorithm, which is a fi rst order gradient descent \nmethod. Dozens of alternatives to backpropagation exist and that was a \ngreat topic for doctoral dissertations in the 1990s. \n \n\u25a0Quick propagation: Assumes the error surface is quadratic. If the slope of \nthe error change with respect to the weight change (\u0394error/\u0394weight, the error \ngradient) switches sign, an approximation to the bottom of the error parabola \nis computed, and the weights are adjusted to the value that achieves that \nminimum-error estimate. Do not let the change in weights exceed a maxi-\nmum value. It is usually several", " the error parabola \nis computed, and the weights are adjusted to the value that achieves that \nminimum-error estimate. Do not let the change in weights exceed a maxi-\nmum value. It is usually several times faster convergence than backprop. \n \n\u25a0Resilient propagation (Rprop): Recognizes that the magnitude of the \nerror gradient is often noisy, so it considers only the sign of the slope. If \nthe error gradient is the same sign, increase the learning rate. If the value \nis not the same, reduce the learning rate. The amount of the increase or \ndecrease is not always changeable.\n \n\u25a0Second-order methods: These methods converge in far fewer epochs than \nbackpropagation but require signifi cantly more computation and memory. \nAlgorithmically they are signifi cantly more complex as well, which is a \nprimary reason they are included in only a few tools. Conjugate Gradient, \nGauss-Newton, and Levenberg Marquardt algorithms are three methods \nthat are found in software tools.\nNeural Network Prunin", " \nprimary reason they are included in only a few tools. Conjugate Gradient, \nGauss-Newton, and Levenberg Marquardt algorithms are three methods \nthat are found in software tools.\nNeural Network Pruning\nThus far, the assumption in building the neural network has been that an archi-\ntecture is specifi ed and a neural network is trained. But which architecture \ngenerates the smallest testing dataset error? A modeler can build multiple neural \nnetworks with different architectures and determine empirically which is best. \nA few software implementations build in the ability to build many networks \nwith different architectures. However, what about the inputs? Are all of them \nnecessary, or are some irrelevant at best or even harmful?\nPruning algorithms for neural networks exist but are rarely implemented \nin mainstream predictive analytics software. These algorithms progressively \nremove inputs and neurons from the network in an attempt to improve error \non the test data. The simplest approa", "plemented \nin mainstream predictive analytics software. These algorithms progressively \nremove inputs and neurons from the network in an attempt to improve error \non the test data. The simplest approach to removing inputs and neurons is to \nidentify those that have the least infl uence on the network predictions; if the \nweights throughout a neural network associated with an input are small, the \ninput is essentially being ignored by the network and can be safely removed \nand the network retrained without the input. \nThe advantage of pruning is that the fi nal neural network will be faster to \ndeploy because of the fewer inputs, neurons, and weights that create additional \nmultiplies and adds in the model. Moreover, the network could become more \nstable by removing terms. \n\n252 \nChapter 8 \u25a0 Predictive Modeling\nBuilding neural networks with pruning can take considerable time, however, \nand therefore it is often left as a fi nal step in the modeling-building process, \nafter a good set of", "\u25a0 Predictive Modeling\nBuilding neural networks with pruning can take considerable time, however, \nand therefore it is often left as a fi nal step in the modeling-building process, \nafter a good set of inputs for the model are found and a good baseline archi-\ntecture is found. \nInterpreting Neural Networks\nNeural networks have the reputation of being \u201cblack boxes,\u201d meaning that \nthe interpretation of why predicted values are small or large cannot be deter-\nmined. This reputation is unfair to a large degree; while neural networks are \nnot transparent, the input variables that most infl uence the predictions can be \ndetermined in a variety of ways.\nMost software implementations of neural networks generate a set of vari-\nable infl uence scores for each variable in the neural network. These scores are \nsometimes called variable importance or infl uence. The methods used to determine \nthe infl uence of each variable fall into two general categories. The fi rst category \nexamines weights asso", "res are \nsometimes called variable importance or infl uence. The methods used to determine \nthe infl uence of each variable fall into two general categories. The fi rst category \nexamines weights associated with each input (attached to the fi rst hidden layer) \nand subsequent weights between the fi rst hidden layer and the output layer. \nThese approaches may use partial derivatives or just multiply the weights to \nderive the relative infl uence of the variables. \nThe second category determines the infl uence indirectly by changing values \nof each input and measuring the amount of change in the output. The larger \nthe relative change, the more infl uence the variable has. Some techniques hold \nall variables but one constant at the mean and randomly vary a single variable \nfrom its mean value plus or minus one standard deviation. The variables are \nthen scored by the relative change in the predictions as each of the inputs \nare wiggled. A more extreme version of this approach is to remov", "e plus or minus one standard deviation. The variables are \nthen scored by the relative change in the predictions as each of the inputs \nare wiggled. A more extreme version of this approach is to remove a single vari-\nable completely, retrain the neural network, and examine how much the error \nincreases. This last technique has the danger, however, that the neural network \nmight converge in such a way that the two networks with and without the input \nare not comparable; it is best to start the training of the network without the \ninput with the same weights found in the network being assessed, if possible.\nHowever, there are also other methods to determine which variables are \ninfl uential. In one approach, decision trees are trained from the same inputs as \nwere used in training the neural network, but the target variable for the tree \nis the neural network output rather than the actual target variable. The tree \ntherefore fi nds the key patterns the neural network is fi nding with the", " network, but the target variable for the tree \nis the neural network output rather than the actual target variable. The tree \ntherefore fi nds the key patterns the neural network is fi nding with the advan-\ntage that the decision tree generates rules that are more easily interpreted than \nneural network weights. \n\n \nChapter 8 \u25a0 Predictive Modeling \n253\nNeural Network Decision Boundaries\nSince neural networks are universal approximators and can fi t very complex \nfunctions, many modelers believed they would never need to use logistic regres-\nsion again. However, the reality is that sometimes the more complex neural \nnetworks either overfi t the data or fail to converge to an optimum solution. In \nother words, just because a neural network in theory is a universal approximately \ndoesn\u2019t mean that in practice it will fi nd that solution. Therefore, it is usually a \ngood idea to build models using several algorithms.\nNevertheless, neural networks are usually better predictors than k-NN, l", "n that in practice it will fi nd that solution. Therefore, it is usually a \ngood idea to build models using several algorithms.\nNevertheless, neural networks are usually better predictors than k-NN, logistic \nregression, or decision trees. As you can see in Figure 8-26, decision boundaries \nfor the nasadata dataset are largely linear, although there are some nonlinear \nregions, such as between rye, corn, and oats. Moreover, the nonlinear decision \nboundaries are very smooth.\n\u22122.615\n\u22122.374\n\u22122.174\n\u22121.974\n\u22121.774\n\u22121.574\n\u22121.374\n\u22121.174\n\u22120.974\n\u22120.774\n\u22120.574\n\u22120.374\n\u22120.174\n0.026\n0.226\n0.426\n0.626\n0.826\n1.026\n1.226\nBand 1\nBand 11\n1.426\n1.626\n1.801\n\u22122.115\n\u22121.615\n\u22121.115\n\u22120.615\n\u22120.115\n0.385\n0.885\n1.415\nalfalfa\nclover\ncorn\noats\nrye\nsoy\nwheat\nCircle\nHorizontal Line\nAsterisk\nCross\nDiamond\nVertical Line\nX-Shape\nFigure 8-26: Neural network decision regions on nasadata\nOther Practical Considerations for Neural Networks\nANNs, like other numeric algorithms including linear regression, logistic regres-\nsion", "Shape\nFigure 8-26: Neural network decision regions on nasadata\nOther Practical Considerations for Neural Networks\nANNs, like other numeric algorithms including linear regression, logistic regres-\nsion, k-nearest neighbor, and support vector machines requires numeric input \ndata, and cannot have missing data. Typically, categorical data is represented \nnumerically through the creation of dummy variables, as described in the \ndiscussion of logistic regression. Missing values can be imputed using typical \nimputation methods. \n\n254 \nChapter 8 \u25a0 Predictive Modeling\nVariable selection for neural networks is done sometimes using the variable \nimportance measure. For example, you may decide to keep only the top 20 \nvariables in the network, and then retrain the neural network with only these \nvariables. Some modelers use other algorithms that select variables automati-\ncally, like decision trees, to do variable selection for neural networks. This kind \nof variable selection is not optimal, but", "ables. Some modelers use other algorithms that select variables automati-\ncally, like decision trees, to do variable selection for neural networks. This kind \nof variable selection is not optimal, but sometimes is practical. \nK-Nearest Neighbor\nThe nearest neighbor algorithm is a non-parametric algorithm that is among \nthe simplest of algorithms available for classifi cation. The technique was fi rst \ndescribed by E. Fix and J. L. Hodges in 1951 in their paper \u201cDiscriminatory \nAnalysis: Nonparametric Discrimination: Consistency Properties\u201d (a work that \nwas later republished in 1989) and by 1967 had been studied suffi ciently that its \ntheoretical qualities were known, namely that k-NN is a universal approxima-\ntor with worst-case error that is bounded. The precise nature of its theoretical \nproperties is not a concern here; nearest neighbor has proven to be an effective \nmodeling algorithm. Because of these properties and the ease of training the mod-\nels, nearest neighbor is included", "l \nproperties is not a concern here; nearest neighbor has proven to be an effective \nmodeling algorithm. Because of these properties and the ease of training the mod-\nels, nearest neighbor is included in most predictive analytics software packages.\nThe nearest neighbor algorithm is a so-called \u201clazy learner,\u201d meaning that \nthere is little done in the training stage. In fact, nothing is done: The training \ndata is the model. In essence, the nearest neighbor model is a lookup table.\nThe k-NN Learning Algorithm\nThe k-NN learning algorithm is quite simple: It begins with the data itself. In \nother words, k-NN is merely a lookup table that you use to predict the target \nvalue of new cases unseen during training. The character \u201ck\u201d refers to how \nmany neighbors surrounding the data point you need to make the prediction.\u201d \nThe mathematics behind the k-NN algorithm resides in how you compute the \ndistance from a data point to its neighbors. \nConsider the simple example in Figure 8-27. There are", " to make the prediction.\u201d \nThe mathematics behind the k-NN algorithm resides in how you compute the \ndistance from a data point to its neighbors. \nConsider the simple example in Figure 8-27. There are two classes of data in \nthe training set\u2014one class colored black and the other colored gray\u2014and a total \nof 13 data points in two dimensions (the x-axis direction and y-axis direction). \nThe 13 data points therefore are the model. To classify a new data point, x, using \nthe 1-NN algorithm, you compare each and every one of the data points in the \ntraining data to x, computing the distance calculated from each of these com-\nparisons. In Figure 8-27, the closest data point is gray, so x will be labeled as gray. \nHowever, suppose you instead use three nearest neighbors to classify x. \nFigure 8-28 shows the ring within which the three nearest neighbors reside. If \nyou decide which class to predict for x by majority vote, the prediction this time \nis the black class, different from the predict", "8 shows the ring within which the three nearest neighbors reside. If \nyou decide which class to predict for x by majority vote, the prediction this time \nis the black class, different from the prediction using the 1-NN model.\n\n \nChapter 8 \u25a0 Predictive Modeling \n255\nX\nFigure 8-27: 1-nearest neighbor solution\nX\nFigure 8-28: 3-NN solution\nThe more neighbors you include in the prediction, the smoother the predic-\ntions. For binary classifi cation, it is common to use an odd number of nearest \nneighbors to avoid ties in voting. \nThere is no theory to tell you exactly how many neighbors to use. A small \nnumber of nearest neighbors can be very effective in fi nding small, subtle shifts \nin the data space, but can also be very susceptible to noise in the data. This is \nwhat appears to be the case in Figure 8-28: The gray dot inside the ring is an \nisolated example in that region, and a 3-NN solution is more robust. However, \nas the number of nearest neighbors increases, the classifi er becomes", "in Figure 8-28: The gray dot inside the ring is an \nisolated example in that region, and a 3-NN solution is more robust. However, \nas the number of nearest neighbors increases, the classifi er becomes less local-\nized, smoother, less susceptible to noise, but also less able to fi nd pockets of \nhomogeneous behavior in the data, similar to a decision tree with only one split.\n\n256 \nChapter 8 \u25a0 Predictive Modeling\nTherefore, the number of nearest neighbors you choose is often discovered \nthrough an iterative search, beginning with a small number of nearest neigh-\nbors, like 1, and continuing to increase the number of nearest neighbors until \nthe testing data error rate begins to increase. Figure 8-29, for example, shows \nthe decision regions for a 1-NN model built on the nasadata dataset with its \nseven crop types. The x axis is Band 1 and the y axis is Band 11. The shapes of \nthe data points refer to the testing data classifi cation predictions. Notice that \nwhile there are general, hom", "h its \nseven crop types. The x axis is Band 1 and the y axis is Band 11. The shapes of \nthe data points refer to the testing data classifi cation predictions. Notice that \nwhile there are general, homogeneous regions for each crop type, there are \nstill some local pockets of behavior with these regions. The corn crop type has \nsome veins within the soy region in the upper left, and also in the rye region \nin the top-middle of the data space. The wheat predictions have a local pocket \nwithin the rye region, and alfalfa has a small group within the clover region in \nthe lower right of the plot.\nalfalfa\nclover\ncorn\noats\nrye\nsoy\nwheat\nCircle\nHorizontal Line\nAsterisk\nCross\nDiamond\nVertical Line\nX-Shape\n0.000\n0.000\n0.050\n0.100\n0.150\n0.200\n0.250\n0.300\n0.350\n0.400\n0.450\n0.500\n0.550\n0.600\nBand 1\nBand 11\n0.650\n0.700\n0.750\n0.800\n0.850\n0.900\n0.950\n0.999\n0.100\n0.200\n0.300\n0.400\n0.500\n0.600\n0.700\n0.800\n0.900\n0.999\nFigure 8-29: 1-NN decision regions for nasadata\nIncreasing the number of nearest neigh", "nd 11\n0.650\n0.700\n0.750\n0.800\n0.850\n0.900\n0.950\n0.999\n0.100\n0.200\n0.300\n0.400\n0.500\n0.600\n0.700\n0.800\n0.900\n0.999\nFigure 8-29: 1-NN decision regions for nasadata\nIncreasing the number of nearest neighbors to 3 reduces these localized \npockets of behavior signifi cantly, as shown in Figure 8-30. There are now only \na few data points in the chart that do not correspond to the dominant crop type \nin the regions. \n\n \nChapter 8 \u25a0 Predictive Modeling \n257\nalfalfa\nclover\ncorn\noats\nrye\nsoy\nwheat\nCircle\nHorizontal Line\nAsterisk\nCross\nDiamond\nVertical Line\nX-Shape\n0.000\n0.000\n0.050\n0.100\n0.150\n0.200\n0.250\n0.300\n0.350\n0.400\n0.450\n0.500\n0.550\n0.600\nBand 1\nBand 11\n0.650\n0.700\n0.750\n0.800\n0.850\n0.900\n0.950\n0.999\n0.100\n0.200\n0.300\n0.400\n0.500\n0.600\n0.700\n0.800\n0.900\n0.999\nFigure 8-30: 3-NN decision regions for nasadata\nWhen you increase k to 7, all seven regions become homogeneous, although \nnot always smooth (see Figure 8-31).\nalfalfa\nclover\ncorn\noats\nrye\nsoy\nwheat\nCircle\nHorizontal Line\nAsterisk\nCr", "ns for nasadata\nWhen you increase k to 7, all seven regions become homogeneous, although \nnot always smooth (see Figure 8-31).\nalfalfa\nclover\ncorn\noats\nrye\nsoy\nwheat\nCircle\nHorizontal Line\nAsterisk\nCross\nDiamond\nVertical Line\nX-Shape\n0.000\n0.000\n0.050\n0.100\n0.150\n0.200\n0.250\n0.300\n0.350\n0.400\n0.450\n0.500\n0.550\n0.600\nBand 1\nBand 11\n0.650\n0.700\n0.750\n0.800\n0.850\n0.900\n0.950\n0.999\n0.100\n0.200\n0.300\n0.400\n0.500\n0.600\n0.700\n0.800\n0.900\n0.999\nFigure 8-31: 7-NN decision regions for nasadata\n\n258 \nChapter 8 \u25a0 Predictive Modeling\nIn general, what value of k is correct? There is no theoretical answer to this \nquestion, but you can fi nd an empirical solution by assessing model performance \non testing data for each of the values of k you would like to test. As the value of \nk increases, the errors on testing data usually reach a minimum asymptotically.\nFor example, Table 8-9 shows the number of  nearest neighbors, k, scored by \nAUC. The reduction in AUC from k equal to 101 to 121 is only 0.18%, s", " testing data usually reach a minimum asymptotically.\nFor example, Table 8-9 shows the number of  nearest neighbors, k, scored by \nAUC. The reduction in AUC from k equal to 101 to 121 is only 0.18%, showing \nconvergence.\nTable 8-9: The Number of Nearest Neighbors, K, Scored by AUC\nNUMBER OF NN\nAUC\n% AUC REDUCTION\n1\n0.521\n3\n0.536\n2.83%\n11\n0.555\n3.65%\n21\n0.567\n2.14%\n41\n0.580\n2.23%\n61\n0.587\n1.23%\n81\n0.591\n0.57%\n101\n0.594\n0.66%\n121\n0.596\n0.18%\nDistance Metrics for k-NN\nThe primary distance metric used with the k-NN algorithm is Euclidean dis-\ntance. Assume there are m records in the training data, and n inputs to the \nk-NN model. Collect the inputs into a vector called \u201cx\u201d of length n. The vector \nof inputs we are intending to predict based on the k-NN model is called y and \nis also of length n. The Euclidean distance can be expressed by the formula:\n( , )\n(\n)\nx y\n,\ny\ni\nn\n=\n(\n=\u2211\n2\n1\nwhere D is the Euclidean distance between the training vector x and the vector \nto classify, y. The summatio", "n. The Euclidean distance can be expressed by the formula:\n( , )\n(\n)\nx y\n,\ny\ni\nn\n=\n(\n=\u2211\n2\n1\nwhere D is the Euclidean distance between the training vector x and the vector \nto classify, y. The summation merely adds the squares of the differences between \ntraining vector x and the vector to score called y. The distance will be computed \nm times, once for every record in the training data. The 1-NN algorithm fi nds \nthe smallest of these distances and assigns to new data point y the target vari-\nable label of the nearest record.\nEuclidean distance is the most commonly used distance metric, although \nothers are sometimes found in software packages, including the Manhattan \n\n \nChapter 8 \u25a0 Predictive Modeling \n259\ndistance, the Hamming distance, and the Mahalanobis distance. The Manhattan \ndistance, rather than computing the squares of the differences between the \ntraining vector and the vector to score the new value, y, computes the absolute \nvalue of the differences. \nD x y\nx\nyiy\ni\nn\n( ,x\n", "nce, rather than computing the squares of the differences between the \ntraining vector and the vector to score the new value, y, computes the absolute \nvalue of the differences. \nD x y\nx\nyiy\ni\nn\n( ,x\n)\nx\n=\n=\u2211\n1\nThe Mahalanobis distance is similar to the Euclidean distance except that \nthe inputs are normalized by the covariance matrix. Covariance is analogous \nto variance (the square of the standard deviation) but measures the variance on \ntwo variables rather than a single one. To compute covariance, you multiply the \nstandard deviation of variable 1, the standard deviation of variable 2, and the \ncorrelation between variables 1 and 2. The effect of normalizing by the covari-\nance matrix is that the data is scaled to the unit sphere. The scaling takes into \naccount the magnitudes of the variables if some variables have a larger spread \nthan others. \nOther Practical Considerations for k-NN\nThe k-NN algorithm usually has few options you need to set besides the value \nof k. Other common ", "ariables if some variables have a larger spread \nthan others. \nOther Practical Considerations for k-NN\nThe k-NN algorithm usually has few options you need to set besides the value \nof k. Other common considerations you should consider include which distance \nmetric to use, how to handle categorical variables, and how many inputs to \ninclude in the models. \nDistance Metrics\nEuclidean distance is known to be sensitive to magnitudes of the input data; \nlarger magnitudes can dominate the distance measures. In addition, skew in \ndistributions of the inputs can also effect the distance calculations. For example, \nconsider a k-NN model built from two inputs from the KDD Cup 1998 data: \nRAMNTALL and CARDGIFT. The fi rst 13 of 4,844 records in a sample of that \ndata are shown in Table 8-10 and represent the training data. The mean value \non the entire dataset (not just the 13 records shown here) for RAMNTALL is \n112 and for CARDGIFT is 6.12, so RAMNTALL is more than 18 times larger on \naverage ", "esent the training data. The mean value \non the entire dataset (not just the 13 records shown here) for RAMNTALL is \n112 and for CARDGIFT is 6.12, so RAMNTALL is more than 18 times larger on \naverage than CARDGIFT.\n\n260 \nChapter 8 \u25a0 Predictive Modeling\nTable 8-10: Two Inputs from KDD Cup 1998 Data\nRECORD, I\nRAMNTALL\nCARDGIFT\n1\n98\n6\n2\n119\n9\n3\n61\n10\n4\n123\n12\n5\n68\n6\n6\n102\n14\n7\n132\n5\n8\n94\n8\n9\n38\n3\n10\n30\n5\n11\n44\n2\n12\n25\n1\n13\n35\n2\nMean\n112\n6.12\nIf you assume that you are evaluating a new datapoint with the value of \nRAMNTALL equal to 98 and CARDGIFT equal to 6, Table 8-11 shows the dis-\ntances from these 13 records in the training data. The distance in the fi rst record \nrepresents the distance from the record to score to the fi rst record in the training \ndata. Notice that most of the distance values for RAMNTALL are much larger \nthan they are for CARDGIFT (they are more than 10 times larger), precisely \nbecause RAMNTALL itself is larger. RAMNTALL therefore is the primary \ncontributor to To", "values for RAMNTALL are much larger \nthan they are for CARDGIFT (they are more than 10 times larger), precisely \nbecause RAMNTALL itself is larger. RAMNTALL therefore is the primary \ncontributor to Total Distance.\nTable 8-11: Euclidean Distance between New Data Point and Training Records \nRECORD \nNUMBER \nIN \nTRAINING \nDATA\nTRAINING \nRAMNTALL\nTRAINING \nCARDGIFT\nNEW \nRAMNTALL \nTO SCORE\nNEW \nCARDGIFT \nTO SCORE\nDISTANCE\n1\n0\n5\n98\n6\n5.00\n2\n21\n3\n98\n6\n21.21\n3\n37\n4\n98\n6\n37.22\n4\n25\n6\n98\n6\n25.71\n\n \nChapter 8 \u25a0 Predictive Modeling \n261\nRECORD \nNUMBER \nIN \nTRAINING \nDATA\nTRAINING \nRAMNTALL\nTRAINING \nCARDGIFT\nNEW \nRAMNTALL \nTO SCORE\nNEW \nCARDGIFT \nTO SCORE\nDISTANCE\n5\n30\n0\n98\n6\n30.00\n6\n4\n8\n98\n6\n8.94\n7\n34\n1\n98\n6\n34.01\n8\n4\n2\n98\n6\n4.47\n9\n60\n3\n98\n6\n60.07\n10\n68\n1\n98\n6\n68.01\n11\n54\n4\n98\n6\n54.15\n12\n73\n5\n98\n6\n73.17\nFor this reason, practitioners usually transform the input variables so their \nmagnitudes are comparable, a process often called normalization. Let\u2019s assume \nyou are transforming RAMNTALL: The most", "17\nFor this reason, practitioners usually transform the input variables so their \nmagnitudes are comparable, a process often called normalization. Let\u2019s assume \nyou are transforming RAMNTALL: The most common transformations applied \nto continuous data are shown in Table 8-12. The functions Mean, Min, Max, and \nStdDev are not presented in mathematical terms but rather in functional terms \nand refer to an operation on the entire column in the training data. \nTable 8-12: Two Transformations for Scaling Inputs\nTRANSFORMATION \nNAME\nTRANSFORMATION FORMULA\nZ-score\nRAMNTALL\nRAMNTALL\nStdDev\nz = (\n(\nRAMNTALL\nMean\n\u2212\n))\n(\n)\nRAMNTALL\nMin-max\n(\n(\n))\n(\n)\n(\nRAMNTALL\nR\n( AMNTALL\nMax\nMin\nminmax =\n\u2212\nRAMNTALL\nR\n)\nThe z-score is very appealing for k-NN and other algorithms whose errors are \nbased on Euclidean distance or squared error because the z-score has zero mean \nand unit standard deviation. A typical range for the z-scored variables will be \nbetween \u20133 and +3, and all variables will therefore have s", "distance or squared error because the z-score has zero mean \nand unit standard deviation. A typical range for the z-scored variables will be \nbetween \u20133 and +3, and all variables will therefore have similar magnitudes, \nan important consideration for k-NN.\nHowever, computing the z-score assumes the distribution is normal. If, for \nexample, the distribution is severely skewed, the mean and standard deviation \nwill be biased and the z-score units won\u2019t accurately refl ect the distribution. \n\n262 \nChapter 8 \u25a0 Predictive Modeling\nMoreover, and more importantly for k-NN, the values of the variable won\u2019t be \nin the range \u20133 to +3. RAMNTALL actually has z-score values as high as 18.\nAn alternative is to use the min-max transformation. The formula in Table \n8-11 converts the variable from its original units to a range from 0 to 1. You \ncould scale this result to the range \u20131 to +1 if you prefer to have a center value 0 \nin the transformed variable; you merely multiply the min-max transformatio", "inal units to a range from 0 to 1. You \ncould scale this result to the range \u20131 to +1 if you prefer to have a center value 0 \nin the transformed variable; you merely multiply the min-max transformation \nresult by 2 and then subtract 1.\nOne advantage of the min-max transformation is that it is guaranteed to have \nbounded minimum and maximum values, and therefore the transformation \nwill put all the transformed variables on the same scale. Also, if one is includ-\ning dummy variables in the nearest neighbor model, they, too, will be on the \nscale of 0 to 1. \nHowever, just as skewed distributions cause problems for z-scores, they also \ncause problems for the min-max transformation. Suppose RAMNTALL has one \nvalue that is 10,000, the second largest value is 1,000, and the minimum value is \n0. The 10,000 value will map to the transformed value 1, but the second highest \nvalue will map to 0.1, leaving 90 percent of the range (from 0.1 to 1) completely \nempty except for the one value at 1. Thi", "he 10,000 value will map to the transformed value 1, but the second highest \nvalue will map to 0.1, leaving 90 percent of the range (from 0.1 to 1) completely \nempty except for the one value at 1. This variable would then be at a severe \ndisadvantage for use in a k-NN model because its values are almost always \nless than 0.1, whereas other variables may have values distributed throughout \nthe 0 to 1 range.\nTherefore, you must take great care before applying any normalization that \nthe distribution of the variable to be transformed is not heavily skewed and \ndoes not have outliers. Methods to treat these kinds of data were described in \nChapter 4. The modeler must choose the course of action based on the pros and \ncons of each possible correction to the data. \nHandling Categorical Variables\nk-NN is a numerical algorithm requiring all inputs to be numeric. Categorical \nvariables must therefore be transformed into a numeric format, the most com-\nmon of which is 1/0 dummy variables: one co", "\nk-NN is a numerical algorithm requiring all inputs to be numeric. Categorical \nvariables must therefore be transformed into a numeric format, the most com-\nmon of which is 1/0 dummy variables: one column for each value in the variable. \nIf many key variables are categorical and dummies have been created to \nmake them numeric, consider matching the continuous variables with the \ncategorical variables by using min-max normalization for the continuous vari-\nables. However, k-NN models with many categorical variables can easily be \ndominated by the categorical variables because the distances generated by the \ndummy variables are either the minimum possible distance, 0 (if the new data \npoint value matches a neighbor) or the maximum possible distance 1 (if the new \n\n \nChapter 8 \u25a0 Predictive Modeling \n263\ndata point doesn\u2019t match the neighbor). The maximum distance will infl uence \nthe overall distance more than continuous variables whose Euclidean distance \nwill have the full range 0 to 1.", "deling \n263\ndata point doesn\u2019t match the neighbor). The maximum distance will infl uence \nthe overall distance more than continuous variables whose Euclidean distance \nwill have the full range 0 to 1. \nOne alternative to min-max scaling is to rescale the dummy variables of the \nnew data points from 1 to 0.7 so the maximum distance is reduced. A second \nalternative is to rescale the dummy variables so that the 0 values are coded as \n\u20132 and the 1 values are coded as +2, values that are smaller than the maximum \nand larger than the minimum. \nThe Curse of Dimensionality\nOne of the challenges with k-NN and other distance-based algorithms is the \nnumber of inputs used in building a model. As the number of inputs increases, \nthe number of records needed to populate the data space suffi ciently increases \nexponentially. If the size of the space increases without increasing the number \nof records to populate the space, records could be closer to the edge of the data \nspace than they are to each", "creases \nexponentially. If the size of the space increases without increasing the number \nof records to populate the space, records could be closer to the edge of the data \nspace than they are to each other, rendering many, if not all, records as outliers. \nThis is the curse of dimensionality. There is very little help from theory for us \nhere; descriptions of how much space is too much, and how much additional \nvariables can negatively impact k-NN models (if they do at all), have not been \nforthcoming.\nOne solution to the curse of dimensionality is to keep dimensionality low; \ninclude only a dozen to a few dozen inputs in k-NN models. Additionally, the \nquality of the inputs is important. If too many irrelevant features are included \nin the model, the distance will be dominated by noise, thus reducing the con-\ntrast in distances between the target variable values. Third, you should exclude \ninputs that are correlated with other inputs in the modeling data (exclude one \nof the two corr", "thus reducing the con-\ntrast in distances between the target variable values. Third, you should exclude \ninputs that are correlated with other inputs in the modeling data (exclude one \nof the two correlated variables, not both). Redundant variables give the false \nimpression of smaller distances. However, reducing dimensionality too much \nwill result in poorer predictive accuracy in models.\nWeighted Votes\nSome k-NN software includes an option to weight the votes of the nearest neigh-\nbors by the distance the neighbor is from the data point. The reason for this is \nto compensate for situations where some of the nearest neighbors are very far \nfrom the data point and intuitively would not be expected to contribute to the \nvoting. The weighting diminishes the infl uence. \n\n264 \nChapter 8 \u25a0 Predictive Modeling\nNa\u00efve Bayes\nThe Na\u00efve Bayes algorithm is a simplifi cation of the Bayes classifi er, an algorithm \nwith a long and successful history. It\u2019s named after English mathematician and \nPre", "edictive Modeling\nNa\u00efve Bayes\nThe Na\u00efve Bayes algorithm is a simplifi cation of the Bayes classifi er, an algorithm \nwith a long and successful history. It\u2019s named after English mathematician and \nPresbyterian minister Thomas Bayes, developer of Bayes\u2019 theorem, or Bayes\u2019 \nrule, in the 1740s. But it wasn\u2019t until the 1930s that it became a contender in data \nanalysis. It was proposed by Harold Jeffreys as an alternative to the approaches \ndeveloped and advocated by English statistician Sir Ronald Aylmer Fisher, \nincluding the use of statistical tests and p-values. However, it wasn\u2019t until the \n1980s that Bayesian methods become more mainstream in data analysis, and \nin the 1990s, the Na\u00efve Bayes algorithm began appearing in technical writings \nand as an algorithm used in data mining competitions.\nBayes\u2019 Theorem\nBefore describing the Bayes\u2019 theorem and Na\u00efve Bayes algorithm, fi rst consider \ntwo variables, and a probability that each is \u201cTrue\u201d labeled A and B. \n \n\u25a0P(A) is the probability ", "ions.\nBayes\u2019 Theorem\nBefore describing the Bayes\u2019 theorem and Na\u00efve Bayes algorithm, fi rst consider \ntwo variables, and a probability that each is \u201cTrue\u201d labeled A and B. \n \n\u25a0P(A) is the probability that a variable has value A (or that A is the \u201ctrue\u201d \nvalue).\n \n\u25a0P(~A) is the probability that a variable doesn\u2019t have the value A.\n \n\u25a0P(B) is the probability that a second variable has the value B (or that B is \nthe \u201ctrue\u201d value).\n \n\u25a0P(~B) is the probability that the second variable doesn\u2019t have the value B.\n \n\u25a0P(A and B) is the probability that both A and B are true.\n \n\u25a0P(A or B) is the probability that either A or B is true.\nThe values P(A) and P(B) are called the prior probabilities that A is true or that B \nis true respectively, or just priors. These values are either set by a domain expert \nwho knows the likelihood an event has occurred, calculated from historical \ndata, or can be supposed in an experiment. Another way some describe these \nprobabilities is as the initial degree of be", "rt \nwho knows the likelihood an event has occurred, calculated from historical \ndata, or can be supposed in an experiment. Another way some describe these \nprobabilities is as the initial degree of belief that A is true (or B is true). \nUsually, in predictive modeling problems, the values are calculated from the \ndata. However, as described in the Chapter 8 it is sometimes advantageous to \noverride the historical values in the data if the modeler would like to alter how \nthe algorithms interpret the variable.\nA conditional probability is the probability that a condition is true, given that \na second condition is true. For example,\nP(A|B) is the probability that A is true given that B is true, and\nP(B|A) is the probability that B is true given that A is true.\n\n \nChapter 8 \u25a0 Predictive Modeling \n265\nSome describe P(A|B) as a posterior probability: the degree of belief having \naccounted for B. Note that P(A|B) is not, in general, equal to P(B|A) because the \ntwo conditional probabilities ", "ing \n265\nSome describe P(A|B) as a posterior probability: the degree of belief having \naccounted for B. Note that P(A|B) is not, in general, equal to P(B|A) because the \ntwo conditional probabilities presuppose different subsets of data (the former \nwhen B is true and the latter when A is true).\nThe conditional probabilities are formulated as follows:\nP(A|B) = P(A and B) \u00f7 P(B), and\nP(B|A) = P(A and B) \u00f7 P(A)\nIf you rewrite these equations, you have:\nP(A and B) = P(A|B) \u00d7 P(B) and\nP(A and B) = P(B|A) \u00d7 P(A)\nThis is sometimes called the \u201cchain rule\u201d of conditional probabilities.\nNow consider Figure 8-32 with its abstract representation of these two condi-\ntions. The overlap between the two conditions is the \u201cA and B\u201d region in the \nfi gure. This region relates to the P(A and B) in the equations already described: \nP(A and B) is the product of the conditional probability P(A|B) and the prior of \nB, P(B). Or, stated another way, the intersection of the two sets is the conditional \nprobabi", "ions already described: \nP(A and B) is the product of the conditional probability P(A|B) and the prior of \nB, P(B). Or, stated another way, the intersection of the two sets is the conditional \nprobability times the prior.\nA\nB\nB\nA\nA\nand\nB\nFigure 8-32: Conditional probability\nBayes\u2019 theorem tells you how to compute the probability that A is true given \nB is true, P(A|B), with the formula:\nP(A|B) = P(B|A) \u00d7 P(A) \u00f7 P(B). \nThis is the form that you usually see in the literature, but the A and B values \nare just placeholders for actual conditions you fi nd in their data.\nFor example, consider a doctor\u2019s offi ce with a patient coming in who is sneezing \n(S) that wants to be able to diagnose if the patient has a cold (C) or pneumonia \n(P). Let\u2019s assume that there are probabilities that have already been computed \nor assumed, including:\nP(S) = 0.3 (30 percent of patients come in sneezing)\nP(C) = 0.25 (25 percent of patients come in with a cold)\nP(P) = 1 x 10-6 (1 in a million patients come in w", "y been computed \nor assumed, including:\nP(S) = 0.3 (30 percent of patients come in sneezing)\nP(C) = 0.25 (25 percent of patients come in with a cold)\nP(P) = 1 x 10-6 (1 in a million patients come in with pneumonia)\n\n266 \nChapter 8 \u25a0 Predictive Modeling\nThese three are prior probabilities. The role of \u201cA\u201d in Bayes\u2019 theorem will be \nthe probability the diagnosis is a cold, and then in a second computation, the \nprobability that the diagnosis is pneumonia. In the language of predictive analyt-\nics, the target variables or the outcomes of interest are \u201ccold\u201d or \u201cpneumonia.\u201d\nYou also need conditional probabilities for Bayes\u2019 theorem to work, namely \nP(S|C) and P(S|P), or in words, what is the probability that the patient sneezes \nbecause he has a cold, and what is the probability the patient sneezes because \nhe has pneumonia. Let\u2019s assume both of these are 100 percent.\nThe solution for the cold is:\nP(C|S) = P(S|C) \u00d7 P(C) \u00f7 P(S)\nP(C|S) = 1.0 \u00d7 0.25/0.3 = 0.83\nP(P|S) = P(P|C) \u00d7 P(P) \u00f7 P(S) \nP", "eezes because \nhe has pneumonia. Let\u2019s assume both of these are 100 percent.\nThe solution for the cold is:\nP(C|S) = P(S|C) \u00d7 P(C) \u00f7 P(S)\nP(C|S) = 1.0 \u00d7 0.25/0.3 = 0.83\nP(P|S) = P(P|C) \u00d7 P(P) \u00f7 P(S) \nP(P|S) = 1.0 \u00d7 10-6 \u00f7 0.3 = 3 \u00d7 10-6\nIn summary, the probability that the patient has a cold is 83 percent, whereas \nthe probability the patient has pneumonia is only 3 chances in a million. Clearly, \nthe disparity of the two probabilities is driven by the priors associated with the \noutcomes.\nOne key difference between the Bayes approach and other machine learning \napproaches described so far is the Bayes approach considers only one class at \na time. The patient example had only two outcomes, so two calculations were \nmade. For the nasadata, with seven target value classes, seven probabilities will \nbe computed, one each for alfalfa, clover, corn, soy, oats, rye, and wheat. To predict \na single outcome, you take the largest probability as the most likely outcome. \nSo far, the description o", " will \nbe computed, one each for alfalfa, clover, corn, soy, oats, rye, and wheat. To predict \na single outcome, you take the largest probability as the most likely outcome. \nSo far, the description of the Bayes classifi er has been in probabilistic terms. \nMathematically, for normal distributions, the Bayes classifi er algorithm com-\nputes a mean value for every input (a mean vector) and the covariance matrix \n(the variance in one dimension, covariance in more than one). The maximum \nprobability solution is the one with the smallest normalized distance to the \nmean of each population.\nConsider the same simple data shown in Figure 8-27 for k-NN. With k-NN, \nthe distance from the new value \u201cx\u201d and each of the data points was computed. \nThe Bayes classifi er, on the other hand, only needs to compute the distance from \n\u201cx\u201d to the mean of each class: gray and black in this case, or two distances. The \nrings around the mean values are normalized units of covariance. As you can \nsee in Figur", " compute the distance from \n\u201cx\u201d to the mean of each class: gray and black in this case, or two distances. The \nrings around the mean values are normalized units of covariance. As you can \nsee in Figure 8-33, \u201cx\u201d is closer to the gray class mean, less than one unit of \ncovariance, than to the black class, so it is assigned the label \u201cgray\u201d by the clas-\nsifi er. In addition, the distance from the mean for each class can be appended \nto the data much like probabilities in logistic regression and confi dences with \nneural networks.\n\n \nChapter 8 \u25a0 Predictive Modeling \n267\nThis distance is not the Euclidean distance used in k-NN but a normalized \ndistance, where the covariance matrix is the normalizer. The effect of this nor-\nmalization is that magnitudes of the inputs are handled automatically (unlike \nk-NN), and rather than building piecewise linear decision boundaries as is done \nwith k-NN, the Bayes classifi er constructs quadratic decision boundaries, so it is \nmore fl exible and matche", "(unlike \nk-NN), and rather than building piecewise linear decision boundaries as is done \nwith k-NN, the Bayes classifi er constructs quadratic decision boundaries, so it is \nmore fl exible and matches the normal distributions assumed in the data perfectly.\nmean\nX\nFigure 8-33: Bayes classifier distances\nThe Bayes classifi er is an excellent algorithm, but what happens when there \nare many inputs to be considered in the probability calculations? Consider the \npatient example again, and instead of having just one symptom (sneezing), \nassume there are two symptoms: sneezing and fever (F). The Bayes\u2019 theorem \nsolution for colds is now much more complex. \nFor two inputs, you must compute additional probabilities and conditional \nprobabilities, including the following:\nP(F) = probability of fever in patients\nP(F and S|C)\nP(F and ~S|C)\nP(~F and S|C)\nP(~F and ~S|C)\nP(F and S|P)\nP(F and ~S|P)\nP(~F and S|P)\nP(~F and ~S|P)\nAs the number of inputs increases, the number of conditional probabilities", "r in patients\nP(F and S|C)\nP(F and ~S|C)\nP(~F and S|C)\nP(~F and ~S|C)\nP(F and S|P)\nP(F and ~S|P)\nP(~F and S|P)\nP(~F and ~S|P)\nAs the number of inputs increases, the number of conditional probabilities \nto compute also increases, especially those that include combinations of inputs. \nThe problem becomes one of data size: Is there enough data to compute all of the \n\n268 \nChapter 8 \u25a0 Predictive Modeling\nconditional probabilities? Have they all occurred often enough for these measures \nto be reliable? Mathematically, the Bayes classifi er requires the computation of \na covariance matrix. Is there enough data for these measures to be stable?\nThe Na\u00efve Bayes Classi\ufb01 er\nOne solution to the problem of computing large numbers of conditional prob-\nabilities is to assume independence of the input variables, meaning you assume \nthat input variables are unrelated to one another. With the assumption of inde-\npendence, the conditional probabilities involving combinations of inputs are \nzero and you a", "iables, meaning you assume \nthat input variables are unrelated to one another. With the assumption of inde-\npendence, the conditional probabilities involving combinations of inputs are \nzero and you are left with only the conditional probabilities relating the output \nvariable to the input variables, an enormous simplifi cation. This is the assumption \nbehind the Na\u00efve Bayes classifi er: We naively assume independence of inputs.\nFor the patient example, the calculation of the probability of having a cold \nfrom the measured inputs becomes:\nP(C|S and F) = P(S|C) \u00d7 P(F|C) \u00d7 P(C) \u00f7 P(S) and\nP(P|S and F) = P(S|C) \u00d7 P(F|P) \u00d7 P(P) \u00f7 P(S) \nThe complex interaction of all the input variables has now become a simple \nmultiplication of fi rst order conditional probabilities. Not only does this simplify \nthe building of models, but it also simplifi es the interpretation. \nIf one applies the Na\u00efve Bayes algorithm to the nasadata, just including Band1 \nand Band11 as inputs, the decision regions shown", "y \nthe building of models, but it also simplifi es the interpretation. \nIf one applies the Na\u00efve Bayes algorithm to the nasadata, just including Band1 \nand Band11 as inputs, the decision regions shown in Figure 8-34 are found. The \nshapes correspond to the maximum probability regardless of its value. The \nregions are largely linear and tend to be \u201cboxy\u201d due to the categorical inputs. \nThese regions are very similar to those found by the 7-NN algorithm (refer to \nFigure 8-31) in the upper third of the scatterplot, though it does not have the \nnonlinear decision regions; the decision boundaries are all perpendicular or \nparallel to the Band 1 (x axis) and Band 11 (y axis). \nInterpreting Na\u00efve Bayes Classi\ufb01 ers\nNa\u00efve Bayes models are interpreted by examining the conditional probabilities \ngenerated in the training data. Most often, software reports these as lists of \nprobabilities, like the ones shown in Table 8-13. The training data for the Na\u00efve \nBayes model is a stratifi ed sample, wit", "erated in the training data. Most often, software reports these as lists of \nprobabilities, like the ones shown in Table 8-13. The training data for the Na\u00efve \nBayes model is a stratifi ed sample, with 50 0s and 50 percent 1s, so the baseline \nrate for comparison in the table is 50 percent. The model report is very easy to \nunderstand because each variable has its own list of probabilities. For RFA_2F, \nit is clear that when RFA_2F is equal to 4, 65 percent of the donors that match \nthis value are responders, which is the group with the highest probability. Only \n\n \nChapter 8 \u25a0 Predictive Modeling \n269\nthe RFA_2F equal to 1 group is lower than the average response rate equal to \n50 percent. \n\u221211.599\n\u221268.094\n\u221258.094\n\u221248.094\n\u221238.094\n\u221228.094\n\u221218.094\n\u22128.094\n1.906\n11.906\n21.906\n31.906\n37.877\n\u221213.599\n\u22129.599\n\u22125.599\n\u22121.599\n2.401\nBand 1\nBand 11\n6.401\n10.391\n\u22127.599\n\u22123.599\n0.401\n4.401\n8.401\nalfalfa\nclover\ncorn\noats\nrye\nsoy\nwheat\nCircle\nHorizontal Line\nAsterisk\nCross\nDiamond\nVertical Line\nX-Shape\n", "3.599\n\u22129.599\n\u22125.599\n\u22121.599\n2.401\nBand 1\nBand 11\n6.401\n10.391\n\u22127.599\n\u22123.599\n0.401\n4.401\n8.401\nalfalfa\nclover\ncorn\noats\nrye\nsoy\nwheat\nCircle\nHorizontal Line\nAsterisk\nCross\nDiamond\nVertical Line\nX-Shape\nFigure 8-34: Na\u00efve Bayes model for nasadata\nTable 8-13: Na\u00efve Bayes Probabilities for RFA_2F\nCROSSTAB\nRFA_2F = 1\nRFA_2F = 2\nRFA_2F = 3\nRFA_2F = 4\nCounts with TARGET_ \nB = 0\n1268\n510\n382\n266\nCounts with TARGET_ \nB = 1, counts\n930\n530\n471\n494\nPercent Target_B = 1\n42.3%\n51.0%\n55.2%\n65.0%\nEach variable has its own table of results, and since Na\u00efve Bayes assumes each \ninput variable is independent, there are no tables of interactions between inputs. \nSome software packages also provide a table with a list of the most predictive \ninput variables in the model, a report that is very useful to any analyst. \nOther Practical Considerations for Na\u00efve Bayes\nAdditional data preparation steps to consider when you build Na\u00efve Bayes \nmodels include:\nNa\u00efve Bayes requires categorical inputs. Some implementat", "yst. \nOther Practical Considerations for Na\u00efve Bayes\nAdditional data preparation steps to consider when you build Na\u00efve Bayes \nmodels include:\nNa\u00efve Bayes requires categorical inputs. Some implementations will bin \nthe data for you, and others require you to bin the data prior to building \nthe Na\u00efve Bayes models. Supervised binning algorithms can help generate \nbetter bins than simple equal width or equal count bins. \n\n270 \nChapter 8 \u25a0 Predictive Modeling\nNa\u00efve Bayes is susceptible to correlated variables. Na\u00efve Bayes can suffer \nfrom poor classifi cation accuracy if input variables are highly correlated \nbecause of the assumption of independence and therefore the replica-\ntion of probabilities from multiple variables in the model. It is best if the \nvariables of only one of the correlated variables is included in the model.\nNa\u00efve Bayes does not fi nd interactions. If you know that there are inter-\nactions in the data, create them explicitly for the Na\u00efve Bayes model. \nThis is similar ", " variables is included in the model.\nNa\u00efve Bayes does not fi nd interactions. If you know that there are inter-\nactions in the data, create them explicitly for the Na\u00efve Bayes model. \nThis is similar to the strategy you use with logistic regression models to \nensure the interactions are considered by the algorithm, and can improve \naccuracy signifi cantly.\nRegression Models\nRegression models predict a continuous target variable rather than a categori-\ncal target variable. In some ways, this is a more diffi cult problem to solve than \nclassifi cation. Classifi cation has two or a few values to predict: two for the KDD \nCup 1998 dataset, three for the Iris data set, and seven for the nasadata dataset, \nto name three examples. The models have to predict the outcome correctly for \nthese groups. Regression models must predict every value contained in the \ntarget variable well to have high accuracy. \nRegression belongs to the supervised learning category of algorithms along \nwith classifi ca", ". Regression models must predict every value contained in the \ntarget variable well to have high accuracy. \nRegression belongs to the supervised learning category of algorithms along \nwith classifi cation. I am using the term \u201cregression\u201d for this kind of model, but \nseveral other terms are used to convey the same idea. Some modelers call these \nmodels \u201ccontinuous valued\u201d prediction models; others call them \u201cestimation\u201d \nor \u201cfunction estimation\u201d models. \nThe most common algorithm predictive modelers use for regression problems \nis linear regression, an algorithm with a rich history in statistics and linear \nalgebra. Another popular algorithm for building regression models is the neural \nnetwork, and most predictive analytics software has both linear regression and \nneural networks. Many other algorithms are also used for regression, including \nregression trees, k-NN, and support vector machines.  In fact, most classifi cation \nalgorithms have a regression form. \nThroughout this section", "her algorithms are also used for regression, including \nregression trees, k-NN, and support vector machines.  In fact, most classifi cation \nalgorithms have a regression form. \nThroughout this section, the KDD Cup 1998 data will be used for illustrations, \nthough instead of using the binary, categorical target variable TARGET_B that was \nused to illustrate classifi cation modeling, a different target variable is predicted \nfor regression problems, TARGET_D. This target variable is the amount a donor \ngave when he or she responded to the mail campaign to recover lapsed donors. \n\n \nChapter 8 \u25a0 Predictive Modeling \n271\nLinear Regression\nMost analysts are familiar with linear regression, easily the most well-known \nof all algorithms used in regression modeling. Even analysts who don\u2019t know \nabout linear regression are using it when they are adding a \u201ctrend line\u201d in Excel. \nFigure 8-35 shows a scatterplot with a trend line added to show the best fi t of \nthe data points using linear regress", "out linear regression are using it when they are adding a \u201ctrend line\u201d in Excel. \nFigure 8-35 shows a scatterplot with a trend line added to show the best fi t of \nthe data points using linear regression. The linear regression algorithm fi nds \nthe slope of the output with respect to the input. In the model in Figure 8-35, \nevery time the variable on the x axis increases 10 units, the target variable on \nthe y axis increases approximately 2.5 units.\n0\n0\n5\n10\n15\n20\n25\n30\n5\n10\n15\n20\n25\n30\nFigure 8-35: Regression line\nThis discussion presents linear regression from a predictive modeler\u2019s per-\nspective, not a statistician\u2019s perspective. Predictive modelers usually use linear \nregression in the same way other algorithms are used, primarily to predict a \ntarget variable accurately. \nThe form of a linear regression model is identical to the equations already \ndescribed for the neuron within a neural network:\ny\nw\nw\nw\nx\nn\nw\nw\nx\nw\nx\nw\nn\n=\n+\nw\n\u00d7\n+\nx\n\u00d7\n+\nx\n+\n\u00d7\nw\n0\nw\nw\n\n272 \nChapter 8 \u25a0 Predictive ", "a linear regression model is identical to the equations already \ndescribed for the neuron within a neural network:\ny\nw\nw\nw\nx\nn\nw\nw\nx\nw\nx\nw\nn\n=\n+\nw\n\u00d7\n+\nx\n\u00d7\n+\nx\n+\n\u00d7\nw\n0\nw\nw\n\n272 \nChapter 8 \u25a0 Predictive Modeling\nAs before, the y variable is the target variable\u2014the output. The inputs to the \nmodel are x1, x2, and so forth. The weights or coeffi cients are w0, w1, w2, and so \nforth. To be more precise, however, linear regression only includes the output \nvariable, y, and a single input variable, x; we regress the target variable on the \ninput. The full equation is multiple regression where we regress the target vari-\nable on all the input variables. \nOften, the variables y and x have subscripts, like the letter i in the regression \nequation just shown, which refer to the record number; each record will have \nits own predicted value based on the input values in that record. In addition, \nthe predicted target value is often represented with a carat above it, usually \npronounced as \u201chat,\u201d indi", "ord will have \nits own predicted value based on the input values in that record. In addition, \nthe predicted target value is often represented with a carat above it, usually \npronounced as \u201chat,\u201d indicating it is a predicted value in contrast to the actual \nvalue of the target variable, y. \nThe difference between the target variable value in the data and the value \npredicted by the regression model, y minus y hat, is called a residual and is often \nannotated with the letter e, as in the equation:\ne\ny\ny\ni\niy\n\u2212\nyi\n\u02c6\nStrictly speaking, residuals are different than the regression error term, \nwhich is defi ned as the difference between the target variable value and what \nstatisticians call the true regression line, which is the perfect regression model that \ncould be built if the data satisfi ed all of the regression assumptions perfectly, \nand the training data contained all of the patterns that could possibly exist. \nPredictive modelers don\u2019t usually include any discussions or descriptio", "ed all of the regression assumptions perfectly, \nand the training data contained all of the patterns that could possibly exist. \nPredictive modelers don\u2019t usually include any discussions or descriptions of \nthis kind of idealized model, and therefore often treat the terms residuals and \nerrors synonymously. \nResiduals are visualized by dropping a perpendicular line between the data \nvalue and the regression line. Figure 8-36 shows the target variable, TARGET_D, \nfi t with a regression line created from input variable LASTGIFT. The residuals \nare shown as vertical lines, the distance between the actual value of TARGET_D \nand the predicted value of the TARGET_D (y hat in the equation) as predicted \nby the linear model. You can see there are large errors above and below the \nregression line. If one or more of the data points above the regression line were \nremoved, the slope of the regression line would certainly change.\nA plot of the residuals versus LASTGIFT is shown in Figure 8-37. Not", "If one or more of the data points above the regression line were \nremoved, the slope of the regression line would certainly change.\nA plot of the residuals versus LASTGIFT is shown in Figure 8-37. Note that \nmagnitudes of residuals are approximately the same regardless of the value of \nLASTGIFT, the residuals are both positive and negative, and there is no shape \nto the residuals. If, however, the shape of the residuals versus LASTGIFT is \nquadratic, there is a quadratic relationship between LASTGIFT and TARGET_D \nthat is missing in the model and should be included by adding the square of \nLASTGIFT as an input. Any detectable pattern in the residual plot indicates \nthat one or more derived variables can be included to help improve the fi t of \nthe model. \n\n \nChapter 8 \u25a0 Predictive Modeling \n273\n0\n0\n5\n10\n15\n20\n25\n30\n5\n10\nLASTGIFT\nRegression Line\nresidual\nTARGET_D\n15\n20\n25\n30\nFigure 8-36: Residuals in linear regression\n\u221210.00\n\u22125.00\n0.00\n5\n1\n5\n0\n1\n0\n5\n20\n25\n30\n5.00\n10.00\n15.00\n0\nLASTGIFT\n", " \n273\n0\n0\n5\n10\n15\n20\n25\n30\n5\n10\nLASTGIFT\nRegression Line\nresidual\nTARGET_D\n15\n20\n25\n30\nFigure 8-36: Residuals in linear regression\n\u221210.00\n\u22125.00\n0.00\n5\n1\n5\n0\n1\n0\n5\n20\n25\n30\n5.00\n10.00\n15.00\n0\nLASTGIFT\nResiduals\nFigure 8-37: Residuals vs. LASTGIFT\nThe infl uence of large residuals is magnifi ed because the linear regression \nminimizes the square of the residuals. The reason that outliers, especially outli-\ners that have large magnitudes compared to the rest of the input values, create \n\n274 \nChapter 8 \u25a0 Predictive Modeling\nproblems for regression models is because of the squared error metric. This \nis also the reason that positive skew has such infl uence on regression models; \nmodels will be biased toward trying to predict the positive tail of a distribution \nbecause of the squared error metric.\nHowever, this is not always bad. Consider the KDD Cup 1998 data again. \nTARGET_D is positively skewed, as most monetary variables are. Therefore, \nlinear regression models trying to predict TARG", "tric.\nHowever, this is not always bad. Consider the KDD Cup 1998 data again. \nTARGET_D is positively skewed, as most monetary variables are. Therefore, \nlinear regression models trying to predict TARGET_D will be biased toward \npredicting the positive tail of TARGET_D over predicting the smaller values of \nTARGET_D. But is this bad? Don\u2019t we want to identify the large donors well, \nperhaps even at the expense of worse predictive accuracy for smaller donors? The \nquestion of transforming TARGET_D is not just a question of linear regression \nassumptions; it is also a question of which cases we want the linear regression \nmodel to predict well.  \nLinear Regression Assumptions\nThe linear regression algorithm makes many assumptions about the data; I\u2019ve \nseen lists of assumptions numbering anywhere from four to ten. Four of the \nassumptions most commonly listed include the following. First, the relationship \nbetween the input variables and the output variable is assumed to be linear. If \nthe", "ere from four to ten. Four of the \nassumptions most commonly listed include the following. First, the relationship \nbetween the input variables and the output variable is assumed to be linear. If \nthe relationship is not linear, you should create derived variables that transform \nthe inputs so that the relationship to the target becomes linear. Figure 8-38 shows \nwhat appears to be a quadratic relationship between the input on the x axis and \nthe output on the y axis. The linear fi t from a regression model is shown by the \nline, which obviously doesn\u2019t fully capture the curvature of the relationship in \nthe data: The data violates the assumption that the relationship is linear. \n\u221210\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\n10\n20\n30\n40\n50\n60\n0\n1\nFigure 8-38: Linear model fitting nonlinear data\n\n \nChapter 8 \u25a0 Predictive Modeling \n275\nA residual plot, like Figure 8-37, would reveal the quadratic relationship \nbetween the residuals and the input presented by the x axis. The corrective \naction is to create a", " Predictive Modeling \n275\nA residual plot, like Figure 8-37, would reveal the quadratic relationship \nbetween the residuals and the input presented by the x axis. The corrective \naction is to create a derived variable to linearize the relationship. For example, \nrather than building a model using the input directly, if the square of the input \nis used instead, the scatterplot is now linear, as shown in Figure 8-39.\n0\n10\n20\n30\n40\n50\n60\n70\n0\n10\n20\n30\n40\n50\n60\nFigure 8-39: Linear model after transforming nonlinear data\nThe second assumption is that the inputs are uncorrelated with each other, \nor, in other words, the correlations between input variables are all zero. If this \nisn\u2019t the case, then the regression coeffi cient for one variable may carry some \nof the infl uence from another variable, blurring the infl uence of a variable on \nthe outcome. Consider the following model for TARGET_D:\nTARGET_D = 2.3261 + 0.5406 \u00d7 LASTGIFT + 0.4319 \u00d7 AVGGIFT\nThis equation says that every increase i", "ble, blurring the infl uence of a variable on \nthe outcome. Consider the following model for TARGET_D:\nTARGET_D = 2.3261 + 0.5406 \u00d7 LASTGIFT + 0.4319 \u00d7 AVGGIFT\nThis equation says that every increase in LASTGIFT of $10 will produce an \nincrease in the amount the lapsed donor gives to the recovery mailing (TARGET_D) \n$5.41 (5.406 rounded to the nearest penny). However, LASTGIFT and AVGGIFT \nare correlated with a correlation coeffi cient equal to 0.817. If AVGGIFT is removed \nfrom the model, the new regression equation is:\nTARGET_D = 3.5383 + 0.7975 \u00d7 LASTGIFT\nNow the interpretation of the relationship between LASTGIFT and TARGET_D \nis different: Every increase in LASTGIFT of $10 will produce an increase of \n$7.975 in the amount the lapsed donor gives. The difference is the correlation \nbetween the two variables, meaning they share variance, and therefore, they \nshare infl uence on the target variable. \nHigh levels of correlation, therefore, should be avoided if possible in regres-\nsion m", "en the two variables, meaning they share variance, and therefore, they \nshare infl uence on the target variable. \nHigh levels of correlation, therefore, should be avoided if possible in regres-\nsion models, though they cause more problems for interpretation of the models \n\n276 \nChapter 8 \u25a0 Predictive Modeling\nrather than the model accuracy. However, if variables are correlated extremely \nhigh, 0.95 or above as a rule of thumb, the correlation can cause instability that \nis sometimes called destructive collinearity. One symptom of this is seen when \nthe linear regression coeffi cients of two highly correlated terms (positively) are \nboth very high and opposite in sign. In these cases, the two terms cancel each \nother out and even though the terms appear to be infl uential, they are not, at \nleast not on the training data. However, if new data contains values of these two \nvariables that deviate from the high correlation, the large coeffi cients produce \nwild fl uctuations in the predict", "\nleast not on the training data. However, if new data contains values of these two \nvariables that deviate from the high correlation, the large coeffi cients produce \nwild fl uctuations in the predicted values. \nMost statistics software contains collinearity diagnostics that alert you when \nthere is dangerous collinearity with the inputs to the model. Even if you have \nalready removed highly correlated variables during Data Preparation, the col-\nlinearity could exist in three or more variables. Because of this, some practition-\ners routinely apply Principal Component Analysis to fi nd these high levels \nof correlation before selecting inputs for modeling. If you used the principal \ncomponents (PC) themselves as inputs to a linear regression model, you are \nguaranteed to eliminate collinearity because each PC is independent of all oth-\ners, so the correlations are zero. If you instead use the single variable that loads \nhighest on each PC, you will reduce the possibility for collinearit", "ity because each PC is independent of all oth-\ners, so the correlations are zero. If you instead use the single variable that loads \nhighest on each PC, you will reduce the possibility for collinearity.\nThe remaining assumptions you fi nd in the literature all relate to the residual, \nwhich are assumed to be normally distributed with a mean of zero and equal. \nIn other words, residuals should contain only the noise in the data, also referred \nto as the unexplained variance. \nThis last set of assumptions, especially the normality assumption, is the rea-\nson many analysts transform inputs so that they are normally distributed. For \nexample, with positively skewed inputs, many will transform the inputs with a \nlog transform, and this is exactly the approach that was described in Chapter 4. \nThis is not necessary, strictly speaking, to comply with regression assumptions, \nbut beginning with inputs and outputs that are normally distributed increases \nthe likelihood that the assumptions will", "his is not necessary, strictly speaking, to comply with regression assumptions, \nbut beginning with inputs and outputs that are normally distributed increases \nthe likelihood that the assumptions will be met. \nVariable Selection in Linear Regression\nLinear regression fi ts all of the inputs to the target variable. However, you will \nusually do some variable selection, especially during the exploratory phase of \nbuilding models. This step is very important to modelers: Identifying the best \nvariables to include in the model can make the difference between deploying \nan excellent model or a mediocre model.\nPerhaps the simplest method of variable selection is called forward selection, \nwhich operates in the same way variable selection occurs in decision trees. \nIn forward selection, a model is built using all of the input variables, one at a \ntime. The variable with the smallest error is kept in the model. Then, all of the \nremaining variables are added to this variable one at a time, and", "s built using all of the input variables, one at a \ntime. The variable with the smallest error is kept in the model. Then, all of the \nremaining variables are added to this variable one at a time, and a model is built \n\n \nChapter 8 \u25a0 Predictive Modeling \n277\nfor each of these combinations. The best two-term model is kept. This process \ncontinues until a condition applies to stop the adding of more variables, most \noften to prevent the model from overfi tting the training data.\nSome software provides the option to do backward selection, where the fi rst \nmodel is built using all of the inputs, each variable is removed one at a time \nfrom the set of inputs, a model is built using each of these subsets, and the best \nmodel is retained, now with one variable removed. The process of removing \nthe variable that contributes the least to reducing fi tting error continues until \nno input variables remain or a stop condition applies. \nA third approach, called stepwise variable selection, is a co", "he variable that contributes the least to reducing fi tting error continues until \nno input variables remain or a stop condition applies. \nA third approach, called stepwise variable selection, is a combination of these \ntwo. The algorithm begins by fi nding the best single variable (forward selec-\ntion), and alternatively adds a new variable or removes a variable until a stop \ncondition applies. Stepwise regression is preferred because it usually fi nds better \nvariable subsets to include in the regression model.\nHow does forward, backward, or stepwise regression decide which variables \nto include or remove? Chapter 4 described several methods that are frequently \nused, including the Akaike information criterion (AIC) and minimum descrip-\ntion length (MDL). Other metrics that are sometimes used include the Bayesian \ninformation criterion (BIC) and Mallow\u2019s Cp. All of these metrics function the \nsame way: They compute a complexity penalty for the regression model from \nthe number of inp", " used include the Bayesian \ninformation criterion (BIC) and Mallow\u2019s Cp. All of these metrics function the \nsame way: They compute a complexity penalty for the regression model from \nthe number of inputs in the model and the number of records the model is built \nfrom. The more records you have, the more terms that can be added to the model \nwithout incurring a higher penalty.\nFigure 8-40 shows a representation of what these metrics do. As the number of \ninputs in a regression model increases, the error of the regression model (fi tting \nerror in the fi gure) becomes smaller. Each term added to the model, however, \nincurs a complexity penalty that grows linearly with the number of terms in \nthe model. These two numbers added together provide a number that trades \noff fi tting with complexity. In the fi gure, fi ve inputs provide the best tradeoff \nbetween fi tting error and complexity. \n0\n0\n1\n2\nBest tradeoff\nbetween \ufb01tting\nand complexity\nFitting error\nComplexity penalty\nAIC, MDL, BlC me", "y. In the fi gure, fi ve inputs provide the best tradeoff \nbetween fi tting error and complexity. \n0\n0\n1\n2\nBest tradeoff\nbetween \ufb01tting\nand complexity\nFitting error\nComplexity penalty\nAIC, MDL, BlC measure\n3\n4\nNumber of terms in regression model\nRelative Error\n5\n6\n7\n8\n9\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFigure 8-40: Trading fitting error and complexity\n\n278 \nChapter 8 \u25a0 Predictive Modeling\nInterpreting Linear Regression Models\nLinear regression equations are interpreted in two ways: through the coeffi cients \nand the p values. The coeffi cients describe the amount of change to expect in the \ntarget variable prediction for every unit change in the input variable. This kind \nof interpretation has already been discussed. But which input variables are most \ninfl uential in predicting the target variable? If all the inputs were scaled to the \nsame range, such as with min-max normalization or z-score normalization, the \nregression coeffi cients themselves show the infl uence of the input", "arget variable? If all the inputs were scaled to the \nsame range, such as with min-max normalization or z-score normalization, the \nregression coeffi cients themselves show the infl uence of the inputs, and the input \nassociated with the largest coeffi cient magnitude is the most important variable. \nConsider the simple linear regression model summarized in Table 8-14, predict-\ning TARGET_D. The largest coeffi cient in the model (excluding the constant) is \nRFA_2F. The values of RFA_2F are 1, 2, 3, and 4. However, the smallest p values \nare for LASTGIFT and AVGGIFT, whose p values don\u2019t show any value at all \nto four signifi cant digits. These two are the strongest predictors in the model. \nAn average value of LASTGIFT, 14, is nearly seven times larger than the average \nvalue for RFA_2F. If RFA_2F had the same infl uence on TARGET_D as LASTGIFT, \nits coeffi cient would be seven times larger than LASTGIFT.\nTable 8-14: Regression Model Coe\ufb03  cients for TARGET_D Model\nVARIABLE\nCOEFFICIENT", "f RFA_2F had the same infl uence on TARGET_D as LASTGIFT, \nits coeffi cient would be seven times larger than LASTGIFT.\nTable 8-14: Regression Model Coe\ufb03  cients for TARGET_D Model\nVARIABLE\nCOEFFICIENT\nP\nConstant\n13.4704\n0.0473\nLASTGIFT\n0.4547\n0.0000\nAVGGIFT\n0.4478\n0.0000\nRFA_2F\n\u20130.5847\n0.0002\nNGIFTALL\n\u20130.0545\n0.0349\nFISTDATE\n\u20130.0009\n0.2237\nSome modelers use a p value of 0.05 as the rule of thumb to indicate which \nvariables are signifi cant predictors and which should be removed from the \nmodel. Using this principle, FISTDATE is not a signifi cant predictor and there-\nfore should be removed from the model. \nHowever, this approach has several problems. First, there is no theoretical \nreason why 0.05 is the right threshold. As the number of records increases, the \np values for inputs will decrease even if the model is no more predictive just \nbecause of the way p is calculated. Second, there are other more direct ways \nto assess which inputs should be included, such as AIC, BIC, and MDL.", "crease even if the model is no more predictive just \nbecause of the way p is calculated. Second, there are other more direct ways \nto assess which inputs should be included, such as AIC, BIC, and MDL. Third, \njust because a variable is not a signifi cant predictor doesn\u2019t necessarily mean \n\n \nChapter 8 \u25a0 Predictive Modeling \n279\nthat it is harmful to the model. In this model, removing FISTDATE from the \nmodel actually reduces model accuracy on testing data according to one com-\nmon measure, R2, from 0.589 to 0.590, a tiny change. Clearly, FISTDATE isn\u2019t \ncontributing signifi cantly to model accuracy, nor is it contributing to overfi tting \nthe target variable.\nUsing Linear Regression for Classi\ufb01 cation\nLinear regression is usually considered as an algorithm that applies only to \nproblems with continuous target variables. But what would happen if lin-\near regression tried to predict a categorical target variable coded as 0 and 1? \nFigure 8-41 shows what this kind of problem would look l", "ith continuous target variables. But what would happen if lin-\near regression tried to predict a categorical target variable coded as 0 and 1? \nFigure 8-41 shows what this kind of problem would look like. Linear regression \nfi nds a line that tries to minimize the squares of the errors. With only two values \nof the target variable, the regression line obviously cannot fi t the data well at \nall. Notice that what it tries to do in the fi gure is put the line in the center of the \ndensity of data points with values 0 and 1. \nA regression fi t of the dummy variable, while creating a poor estimate of the \nvalues 0 and 1, can create a score that rank-orders the data. In the fi gure, it is \nclear that the smaller the values of the input, the larger the value of the predicted \nvalue. Intuitively this makes sense: Smaller values of the input are more likely \nto have the value 1 because they are further from the place where the target \nswitches from the value 1 to the value 0.\nIn practice, I ha", "this makes sense: Smaller values of the input are more likely \nto have the value 1 because they are further from the place where the target \nswitches from the value 1 to the value 0.\nIn practice, I have found this phenomenon true for problems with a business \nobjective that calls for rank-ordering the population and acting on a subset of \nthe scored records, such as fraud detection, customer acquisition, and customer \nattrition. In fact, for some problems, the linear regression model sometimes is \nthe best model for selecting records at the top end of the rank-ordered list. \n\u22120.6\n\u22120.4\n\u22120.2\n0\n0\n10\n20\n30\n40\n50\nInput\nTarget\n60\n70\n80\n90\n100\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n80\n0\nFigure 8-41: Using linear regression for classification\n\n280 \nChapter 8 \u25a0 Predictive Modeling\nOther Regression Algorithms\nMost classifi cations also have a regression form, including neural networks, \ndecision trees, k-NN, and support vector machines. Regression Neural Networks \nare identical to classifi cation neural netw", "st classifi cations also have a regression form, including neural networks, \ndecision trees, k-NN, and support vector machines. Regression Neural Networks \nare identical to classifi cation neural networks except that output layer nodes \nusually have a linear activation function rather than the usual sigmoidal \nactivation function. The linear activation function actually does nothing at all \nto the linearly weighted sum. They therefore behave very similarly to a linear \nregression function. Figure 8-42 shows how the linear activation function is \nrepresented in neural network pictures.\ny1\ny\nZ1\nZ\nZ2\nZ\nZ3\nZ\nZn\nZ\n...\nFigure 8-42: Linear activation function for regression output nodes\nThe remaining nodes in the neural network, including all the neurons in the \nhidden layer, still have the sigmoid activation functions. The neural network \nis still a very fl exible nonlinear model. The advantage over linear regression is \nclear; neural networks can fi nd nonlinear relationships between inputs", "id activation functions. The neural network \nis still a very fl exible nonlinear model. The advantage over linear regression is \nclear; neural networks can fi nd nonlinear relationships between inputs and a \ncontinuous-valued target variable without the need to create derived variables.\nRegression trees use the same decision tree form, but terminal nodes compute \nthe mean or median value of the continuous target value. The ideal regression \ntrees will create terminal nodes with small standard deviations of the target \nvariable, and the mean or median values in the terminal nodes will be distinct \nfrom one another. Regression trees have the same advantages over numeric \nalgorithms that classifi cation trees have. They are insensitive to outliers, missing \nvalues, and strange distributions of inputs and the target variable. \nModifying the k-NN algorithm for regression is simple. Rather than voting \nto determine the value of a new data point, an average of the target value for \nnearest ne", "f inputs and the target variable. \nModifying the k-NN algorithm for regression is simple. Rather than voting \nto determine the value of a new data point, an average of the target value for \nnearest neighbors is computed. All other aspects of the k-NN algorithm are the \nsame, including the principles for data preparation. \n\n \nChapter 8 \u25a0 Predictive Modeling \n281\nSummary\n This chapter described the predictive modeling algorithms most commonly found \nin software, and therefore the algorithms most commonly used in practice. All \nof the predictive modeling algorithms described in this chapter fall under the \ncategory of supervised learning, which predicts one or more target variables \nin the data from a set of candidate input variables. Classifi cation algorithms \npredict categorical target variables, and regression algorithms predict continu-\nous target variables. \nEach algorithm comes with its own set of assumptions about the data, and \nhas its own strengths and weaknesses. I recommend th", "iables, and regression algorithms predict continu-\nous target variables. \nEach algorithm comes with its own set of assumptions about the data, and \nhas its own strengths and weaknesses. I recommend that predictive modelers \nlearn at least three different algorithms well enough to understand how to \nmodify the parameters that change the way the models are built. Additionally, \nunderstanding the assumptions algorithms make about data helps you diagnose \nproblems when they arise. This mindset develops as you build more models \nand experience models performing well and poorly.\n\n\n283\nThis chapter explains how to assess model accuracy so that you can select the \nbest model to deploy and have a good estimate of how well the model will \nperform operationally. In some ways, this should be a very short chapter. If you \nassess model accuracy by using percent correct classifi cation for classifi cation \nproblems or average squared error for regression problems, your choices would \nbe simple. Howev", "ort chapter. If you \nassess model accuracy by using percent correct classifi cation for classifi cation \nproblems or average squared error for regression problems, your choices would \nbe simple. However, because the choice of the model assessment metric should \nbe tied to operational considerations rather than algorithmic expedience, you \nshould keep several methods of model assessment in your toolbox.\nEvery algorithm has its own way of optimizing model parameters. Linear \nregression minimizes squared error. Neural networks minimize squared \nerror or sometimes cross-entropy; k-NN minimizes the distance between data \npoints. However, businesses often don\u2019t care about the root mean squared error, \nR-squared, the Gini Index, or entropy. Rather, the business may care about \nreturn on investment, expected profi t, or minimizing false alarm rates for the \nnext best 1,000 cases to be investigated. This chapter explores the most common \nways models are assessed from a business perspective.\nMod", "t, expected profi t, or minimizing false alarm rates for the \nnext best 1,000 cases to be investigated. This chapter explores the most common \nways models are assessed from a business perspective.\nModel assessment should be done fi rst on testing data to obtain an estimate \nof model accuracy for every model that has been built. Two decisions are made \nbased on test data assessment: an assessment of which model has the best \naccuracy and therefore should be selected for deployment, and a conclusion if \nmodel accuracy is high enough for the best model to be used for deployment. \nThe validation data can also be used for model assessment but usually only to \nC H A P T E R \n9\nAssessing Predictive Models\n\n284 \nChapter 9 \u25a0 Assessing Predictive Models\nestimate the expected model accuracy and performance once it is deployed. \nThroughout discussions in the chapter, the assumption is that model assessment \nfor selection purposes will be done using test data.\nBatch Approach to Model Assessment\nThe", "rmance once it is deployed. \nThroughout discussions in the chapter, the assumption is that model assessment \nfor selection purposes will be done using test data.\nBatch Approach to Model Assessment\nThe fi rst approach to assessing model accuracy is a batch approach, which means \nthat all the records in the test or validation data are used to compute the accu-\nracy without regard to the order of predictions in the data. A second approach \nbased on a rank-ordered sorting of the predictions will be considered next. \nThroughout this chapter, the target variable for binary classifi cation results will \nbe shown as having 1s and 0s, although any two values can be used without \nloss of generality, such as \u201cY\u201d and \u201cN,\u201d or \u201cgood\u201d and \u201cbad.\u201d\nPercent Correct Classi\ufb01 cation\nThe most straightforward assessment of classifi cation models is percent correct \nclassifi cation (PCC). The PCC metric matches the predicted class value from a \nmodel with actual class value. When the predicted value matches th", "essment of classifi cation models is percent correct \nclassifi cation (PCC). The PCC metric matches the predicted class value from a \nmodel with actual class value. When the predicted value matches the actual \nvalue, the record is counted as a correct classifi cation, and when they don\u2019t \nmatch, the record is counted as an incorrect classifi cation. \nConsider a binary classifi cation problem with 50 percent of the records labeled \nas 1 and the remaining 50 percent labeled as 0. The worst you can do with a \npredictive model built properly is random guessing, which can be simulated by \nfl ipping a fair coin and using heads as a guess for 1 and tails as a guess for 0; \na random guess would be correct 50 percent of the time for binary classifi cation. \n(It is possible for a model to predict worse than a random guess on testing data \nif it has been overfi t on the training data, but we assume care has already been \ntaken to avoid overfi t.) A well-built classifi er therefore will always hav", "rse than a random guess on testing data \nif it has been overfi t on the training data, but we assume care has already been \ntaken to avoid overfi t.) A well-built classifi er therefore will always have PCC \non testing or validation data greater than or equal to 50 percent, and less than \nor equal to 100 percent.\nThe lift of a model is the ratio of model accuracy divided by the accuracy of a \nbaseline measure, usually the expected performance of a random guess. If the \nproportion of 1s is 50 percent, the minimum lift of a model, if it does no better \nthan a random guess, is 1 (50 percent \u00f7 50 percent), and the maximum lift of \na model, if it is a perfect predictor, is 2 (100 percent \u00f7 50 percent). \nWhat if the proportion of 1s is not 50 percent, but some smaller percentage \nof the data? The baseline PCC from a random guess will always be the propor-\ntion of 1s in the data, and lift is the ratio of PCC to this original proportion. \nIf the proportion of 1s in the testing data is 5 percent", " baseline PCC from a random guess will always be the propor-\ntion of 1s in the data, and lift is the ratio of PCC to this original proportion. \nIf the proportion of 1s in the testing data is 5 percent, the baseline (random) \nexpected performance of a classifi er will be 5 percent correct classifi cation, and \nthe maximum lift of a model is now 20: 100 percent PCC \u00f7 5 percent at random. \n\n \nChapter 9 \u25a0 Assessing Predictive Models \n285\nTherefore, the lower the baseline percentage of 1s in the target variable, the \nhigher the potential lift that is achievable by a model. This method of calculat-\ning lift works for classifi cation problems regardless of the number of levels in \nthe target variable.\nIf there are more than two levels for the target variable, PCC is still computed \nthe same way: the proportion of records classifi ed correctly. However, the more \nlevels in the target variable, the more diffi cult the classifi cation problem is and \nthe lower the random baseline accuracy is. Fo", "the proportion of records classifi ed correctly. However, the more \nlevels in the target variable, the more diffi cult the classifi cation problem is and \nthe lower the random baseline accuracy is. For the three-class problem, if the \nproportion of each value is 33.3 percent, a random guess is also 33.3 percent and \ntherefore the maximum lift is 3 instead of 2 for binary classifi cation. In general, \nthe maximum lift can differ by class if the proportion of each class value is \ndifferent from the others. If, for example, class 1 is represented in 10 percent of \nthe records, class 2 in 20 percent of the records, and class 3 in 70 percent of the \nrecords, you would expect that a random guess would estimate 10 percent of \nthe records belong to class 1, 20 percent belong to class 2, and 70 percent belong \nto class 3. The maximum lift values for estimates of class 1, class 2, and class 3\n are therefore 10 (100 percent \u00f7 10 percent), 5 (100 percent \u00f7 20 percent), and \n1.4 (100 percent \u00f7 70 p", "cent belong \nto class 3. The maximum lift values for estimates of class 1, class 2, and class 3\n are therefore 10 (100 percent \u00f7 10 percent), 5 (100 percent \u00f7 20 percent), and \n1.4 (100 percent \u00f7 70 percent). \nTable 9-1 summarizes the differences between the possible lift compared to \nthe baseline rate. It is immediately obvious that the smaller the baseline rate, \nthe larger the possible lift. When comparing models by the lift metric, you must \nalways be aware of the base rate to ensure models with a large baseline rate are \nnot perceived as poor models because their lift is low, and conversely, models with \nvery small baseline rates aren\u2019t considered good models even if the lift is high.\nTable 9-1: Maximum Lift for Baseline Class Rates\nBASELINE RATE OF CLASS VALUE \nMAXIMUM LIFT\n50 percent\n2\n33 percent\n3\n25 percent\n4\n10 percent\n10\n5 percent\n20\n1 percent\n100\n0.1 percent\n1000\nTable 9-2 shows 20 records for a binary classifi cation problem. Each record con-\ntains the target variable, the", "t\n2\n33 percent\n3\n25 percent\n4\n10 percent\n10\n5 percent\n20\n1 percent\n100\n0.1 percent\n1000\nTable 9-2 shows 20 records for a binary classifi cation problem. Each record con-\ntains the target variable, the model\u2019s predicted probability that the record belongs \nto class 1, and the model\u2019s predicted class label based on a probability threshold \nof 0.5. (If the probability is greater than 0.5, the predicted class label is assigned 1; \n\n286 \nChapter 9 \u25a0 Assessing Predictive Models\notherwise, it is assigned the value 0.) In this sample data, 13 of 20 records are \nclassifi ed correctly, resulting in a PCC of 65 percent. \nTable 9-2: Sample Records with Actual and Predicted Class Values\nACTUAL TARGET \nVALUE\nPROBABILITY \nTARGET = 1\nPREDICTED \nTARGET VALUE\nCONFUSION \nMATRIX QUADRANT\n0\n0.641\n1\nfalse alarm\n1\n0.601\n1\ntrue positive\n0\n0.587\n1\nfalse alarm\n1\n0.585\n1\ntrue positive\n1\n0.575\n1\ntrue positive\n0\n0.562\n1\nfalse alarm\n0\n0.531\n1\nfalse alarm\n1\n0.504\n1\ntrue positive\n0\n0.489\n0\ntrue negative\n1\n0.488\n0\nfal", "\n1\n0.601\n1\ntrue positive\n0\n0.587\n1\nfalse alarm\n1\n0.585\n1\ntrue positive\n1\n0.575\n1\ntrue positive\n0\n0.562\n1\nfalse alarm\n0\n0.531\n1\nfalse alarm\n1\n0.504\n1\ntrue positive\n0\n0.489\n0\ntrue negative\n1\n0.488\n0\nfalse dismissal\n0\n0.483\n0\ntrue negative\n0\n0.471\n0\ntrue negative\n0\n0.457\n0\ntrue negative\n1\n0.418\n0\nfalse dismissal\n0\n0.394\n0\ntrue negative\n0\n0.384\n0\ntrue negative\n0\n0.372\n0\ntrue negative\n0\n0.371\n0\ntrue negative\n0\n0.341\n0\ntrue negative\n1\n0.317\n0\nfalse dismissal\n\n \nChapter 9 \u25a0 Assessing Predictive Models \n287\nConfusion Matrices\nFor binary classifi cation, the classifi er can make an error in two ways: The \nmodel can predict a 1 when the actual value is a 0 (a false alarm), or the model \ncan predict a 0 when the actual value is a 1 (a false dismissal). PCC provides \na measure of classifi cation accuracy regardless of the kind of errors: Either kind \nof error counts as an error.\nA confusion matrix provides a more detailed breakdown of classifi cation \nerrors through computing a cross-tab of actual", "acy regardless of the kind of errors: Either kind \nof error counts as an error.\nA confusion matrix provides a more detailed breakdown of classifi cation \nerrors through computing a cross-tab of actual class values and predicted class \nvalues, thus revealing the two types of errors the model is making (false alarms \nand false dismissals) as well as the two ways the model is correct (true positives \nand true negatives), a total of four possible outcomes. In Figure 9-1, these four \noutcomes are listed in the column Confusion Matrix Quadrant.\nThe diagonal of the confusion matrix represents correct classifi cation counts: \ntrue negatives, when the actual and predicted values are both 0, and true posi-\ntives when both the actual and predicted values are both 1. The errors are on \nthe off-diagonal, with false negatives in the lower-left quadrant when the actual \nvalue is 1 and predicted value is 0, and false positives in the upper-right quad-\nrant when the actual value is 0 and the predicted ", "l, with false negatives in the lower-left quadrant when the actual \nvalue is 1 and predicted value is 0, and false positives in the upper-right quad-\nrant when the actual value is 0 and the predicted value is 1. A perfect confu-\nsion matrix, therefore, has values equal to 0 in the off-diagonal quadrants, and \nnon-zero values in the diagonal quadrants.\nOnce you divide the predictions into these four quadrants, several metrics \ncan be computed, all of which provide additional insight into the kinds of \nerrors the models are making. These metrics are used in pairs: sensitivity and \nspecifi city, type I and type II errors, precision and recall, and false alarms \nand false dismissals. Their defi nitions are shown in Table 9-3. Engineers often \nuse the terms false alarms and false dismissals; statisticians often use type I and \ntype II errors or sensitivity and specifi city, particularly in medical applications; \nand computer scientists who do machine learning often use precision and recall,", "atisticians often use type I and \ntype II errors or sensitivity and specifi city, particularly in medical applications; \nand computer scientists who do machine learning often use precision and recall, \nparticularly in information retrieval. The table shows that the defi nitions of \nseveral of these measures are identical.\nSome classifi ers will excel at one type of accuracy at the expense of another, \nperhaps having a high sensitivity but at the expense of incurring false alarms \n(low specifi city), or vice versa. If this is not a desirable outcome, the practitioner \ncould rebuild the model, changing settings such as misclassifi cation costs or \ncase weights to reduce false alarms.\n\n288 \nChapter 9 \u25a0 Assessing Predictive Models\nPredicted Class\nConfusion Matrix\n0\n(actual value is\nnegative)\n0\n(predicted value is\nnegative)\n1\n(predicted value is\npositive)\ntn\n(true negative)\nfp\n(false positive,\nfalse alarm)\ntp\n(true positive)\nfn\n(false negative,\nfalse dismissal)\nTotal negative\npredictions\ntn", "\n(predicted value is\nnegative)\n1\n(predicted value is\npositive)\ntn\n(true negative)\nfp\n(false positive,\nfalse alarm)\ntp\n(true positive)\nfn\n(false negative,\nfalse dismissal)\nTotal negative\npredictions\ntn + fn\nTotal positive\npredictions\ntp + fp\nTotal\nExamples\ntp + tn +\nfp + fn\nTotal Actual\n(down)\nTotal actual\nnegatives\ntn + fp\nTotal actual\npositives\ntp + fn\n1\n(actual value is\npositive)\nTotal Predicted\n(across)\nActual Class\nFigure 9-1:  Confusion Matrix Components\nTable 9-3: Confusion Matrix Measures\nCONFUSION MATRIX MEASURES\nWHAT IT MEASURES\nQUADRANTS USED \nIN COMPUTATION\n+\n=\n+\n+\n+\nt\nt\np\nn\nPCC\nt\nt\nf\nf\np\nn\np\nn\nOverall accuracy\ntn\nfn\ntp\nfp\n=\n+\nfp\nFalse Alarm Rate FA\nt\nf\nn\np\n(\n)\nActual negative \ncases misclassi\ufb01 ed \nas positive\ntn\nfp\n=\n+\nfn\nFalse Dismissal Rate FD\nt\nf\np\nn\n(\n)\nActual positive \ncases misclassi\ufb01 ed \nas negative\nfn\ntp\n=\n+\ntp\nPrecision\nt\nf\np\np\nPredicted positive \ncases classi\ufb01 ed \ncorrectly \ntp\nfp\n=\n\u2212\n=\n+\ntp\nRecall\nFD\nt\nf\np\nn\n1\nActual positive cases \nclassi\ufb01 ed correctly \nfn\ntp\n=\n", "es misclassi\ufb01 ed \nas negative\nfn\ntp\n=\n+\ntp\nPrecision\nt\nf\np\np\nPredicted positive \ncases classi\ufb01 ed \ncorrectly \ntp\nfp\n=\n\u2212\n=\n+\ntp\nRecall\nFD\nt\nf\np\nn\n1\nActual positive cases \nclassi\ufb01 ed correctly \nfn\ntp\n=\n=\n+\ntp\nSensitivity\nRecall\nt\nf\np\nn\nActual positive cases \nclassi\ufb01 ed correctly \nfn\ntp\n/(\n)\nt n\nt n\nf p\n=\n=\n+\n\u2212\n\u2212\n\u2212\nSpecificity\nTrue  Negative  Rate\nActual negative cases \nclassi\ufb01 ed correctly\ntn\nfp\n\n \nChapter 9 \u25a0 Assessing Predictive Models \n289\n=\n=\np\nn\np\nf\nType I Error Rate\nFA\nt + f\nActual negative cases \nmisclassi\ufb01 ed as \npositive \u2014\nIncorrect rejection of a \ntrue null hypothesis\ntn\nfp\n=\n=\n+\nfn\nType II Error Rate\nFD\nt\nf\np\nn\nActual positive cases\n misclassi\ufb01 ed as \nnegative \u2014\nFailure to reject a false \nnull hypothesis\nfn\ntp\nAs an example of a confusion matrix, consider a logistic regression model built \nfrom the KDD Cup 1998 dataset and assessed on 47,706 records in a test set, with \nresponders, coded as the value 1, comprising 5.1 percent of the population (2,418 \nof 47,706 records). The m", "el built \nfrom the KDD Cup 1998 dataset and assessed on 47,706 records in a test set, with \nresponders, coded as the value 1, comprising 5.1 percent of the population (2,418 \nof 47,706 records). The model was built from a stratifi ed sample of 50 percent \n0s and 50 percent 1s, and the confusion matrix represents the counts in each \nquadrant based on a predicted probability threshold of the model of 0.5. From \nthese counts, any of the confusion matrix metrics listed in Table 9-3 could be \ncomputed, although you would rarely report all of them for any single project. \nTable 9-4 shows every one of these computed for a sample of 47,706 records. This \nmodel has similar false alarm and false dismissal rates (40 percent and 43.3 per-\ncent, respectively) because the predicted probability is centered approximately \naround the value 0.5, typical of models built from stratifi ed samples. \nThe confusion matrix for this model is shown in Table 9-5. Rates from Table \n9-4 are computed from the counts", "ed approximately \naround the value 0.5, typical of models built from stratifi ed samples. \nThe confusion matrix for this model is shown in Table 9-5. Rates from Table \n9-4 are computed from the counts in the confusion matrix. For example, the \nfalse alarm rate, 40.0 percent is computed from true negatives and false posi-\ntives, 18,110 \u00f7 (27,178 + 18,110). \nA second model, built from the natural proportion (not stratifi ed), has a \nconfusion matrix for test data in Table 9-6. Once again, the confusion matrix \nis created by using a predicted probability threshold of 0.5, although because \nthe model was trained on a population with only 5.06 percent responders, \nonly two of the predictions exceeded 0.5. The second model appears to have \nhigher PCC, but only because the model prediction labels are nearly always 0. \nBe careful of the prior probability of the target variable values and the threshold \nthat is used in building the confusion matrix. Every software package defaults \nto equal pro", " are nearly always 0. \nBe careful of the prior probability of the target variable values and the threshold \nthat is used in building the confusion matrix. Every software package defaults \nto equal probabilities as the threshold for building a confusion matrix (0.5 for \nbinary classifi cation), regardless of the prior probability of the target variable. \n\n290 \nChapter 9 \u25a0 Assessing Predictive Models\nIf you threshold the model probabilities by 0.0506 rather than 0.5, the resulting \nconfusion matrix is nearly identical to the one shown in Table 9-5.\nTable 9-4: Comparison Confusion Matrix Metrics for Two Models\nMETRIC\nRATES, MODEL 1\nRATES, MODEL 2\nPCC\n59.8 percent\n94.9 percent\nFA\n40.0 percent\n0.0 percent\nFD\n43.3 percent\n100.0 percent\nPrecision\n7.0 percent\n0.0 percent\nRecall\n56.7 percent\n0.0 percent\nSensitivity\n56.7 percent\n0.0 percent\nSpeci\ufb01 city\n60.0 percent\n100.0 percent\nType I Rate\n40.0 percent\n0.0 percent\nType II Rate\n43.3 percent\n100.0 percent\nTable 9-5: Confusion Matrix for Model 1\nC", "rcent\nSensitivity\n56.7 percent\n0.0 percent\nSpeci\ufb01 city\n60.0 percent\n100.0 percent\nType I Rate\n40.0 percent\n0.0 percent\nType II Rate\n43.3 percent\n100.0 percent\nTable 9-5: Confusion Matrix for Model 1\nCONFUSION \nMATRIX, \nMODEL 1\n0\n1\nTOTAL\n0\n27,178\n18,110\n45,288\n1\n1,047\n1,371\n2,418\nTotal\n28,225\n19,481\n47,706\nTable 9-6: Confusion Matrix for Model 2\nCONFUSION \nMATRIX, \nMODEL 2\n0\n1\nTOTAL\n0\n45,286\n2\n45,288\n1\n2,418\n0\n2,418\nTotal\n47,704\n2\n47,706\nBeware of comparing individual metrics by themselves. One model may \nappear to have a low precision in comparison to a second model, but it may only \nbe because the second model has very few records classifi ed as 1, and therefore, \n\n \nChapter 9 \u25a0 Assessing Predictive Models \n291\nwhile the false alarm rate is lower, the true positive rate is also much smaller. \nFor example, precision from Table 9-6 is 0 percent because no records were \npredicted to be responders and were actually value responders. In Table 9-5, \nprecision was equal to 7.0 percent. Howev", "er. \nFor example, precision from Table 9-6 is 0 percent because no records were \npredicted to be responders and were actually value responders. In Table 9-5, \nprecision was equal to 7.0 percent. However, the overall classifi cation accuracy \nfor the model in Table 9-6 is 94.9 percent (45,286 + 0) \u00f7 47,706, much larger than \nthe 59.8 percent PCC for Table 9-5. \nConfusion Matrices for Multi-Class Classi\ufb01 cation\nIf there are more than two levels in the target variable, you can still build a con-\nfusion matrix, although the confusion matrix measures listed in Table 9-3 don\u2019t \napply. Table 9-7 shows a confusion matrix for the seven-level target variable in \nthe nasadata dataset, with the classes re-ordered to show the misclassifi cations \nmore clearly. The actual values are shown in rows, and the predictive values \nin the columns. Note that for this classifi er there are three groupings of clas-\nsifi cation decisions: alfalfa and clover form one group; corn, oats, and soy form \na second gro", "he predictive values \nin the columns. Note that for this classifi er there are three groupings of clas-\nsifi cation decisions: alfalfa and clover form one group; corn, oats, and soy form \na second group; and rye and wheat form a third group. \nTable 9-7: Multi-Class Classi\ufb01 cation Confusion Matrix\nMULTI\uf6baCLASS CONFUSION \nMATRIX\nALFALFA\nCLOVER\nCORN\nOATS\nSOY\nRYE\nWHEAT\nAlfalfa\n28\n27\n0\n0\n0\n0\n0\nClover\n5\n57\n0\n0\n0\n0\n0\nCorn\n0\n0\n50\n7\n4\n0\n0\nOats\n0\n0\n4\n59\n1\n0\n0\nSoy\n0\n0\n3\n2\n56\n0\n0\nRye\n0\n0\n0\n0\n1\n53\n7\nWheat\n0\n0\n0\n0\n1\n15\n44\nROC Curves\nReceiver Operating Characteristic (ROC) curves provide a visual trade-off \nbetween false positives on the x-axis and true positives on the y-axis. They are \nessentially, therefore, a visualization of confusion matrices, except that rather \nthan plotting a single confusion matrix, all confusion matrices are plotted. Each \ntrue positive/false positive pair is found by applying a different threshold on \nthe predicted probability, ranging from 0 to 1. A sample ROC curve is sh", "rix, all confusion matrices are plotted. Each \ntrue positive/false positive pair is found by applying a different threshold on \nthe predicted probability, ranging from 0 to 1. A sample ROC curve is shown \nin Figure 9-2. \nAt the extremes, a threshold of 1 means that you only predict 1 for the model if \nthe predicted probability exceeds 1. Because the maximum predicted probability \n\n292 \nChapter 9 \u25a0 Assessing Predictive Models\nis 1, the threshold is never exceeded so you never have any false alarms (the \nfalse alarm rate is 0). Likewise, you also don\u2019t have any actual target values of \n1 classifi ed correctly (the sensitivity is also 0). This is the bottom left of the ROC \ncurve. At the other extreme, if you threshold the predictions at 0, every record \nis predicted to have the value 1, so the sensitivity and false alarm rates are \nboth 1, the upper right of the ROC curve.\nThe interesting part of the ROC curve is what happens in between these extremes. \nThe higher the sensitivity for low", "he sensitivity and false alarm rates are \nboth 1, the upper right of the ROC curve.\nThe interesting part of the ROC curve is what happens in between these extremes. \nThe higher the sensitivity for low false alarm rates, the steeper the vertical rise \nyou see at the left end of the ROC curve; a perfect ROC curve has a vertical line \nfor sensitivity up to value 1 at the false alarm rate equal to 0, and then is hori-\nzontal at the sensitivity value equal to 1 for all values of the false alarm rate. In \nthe sample ROC curve, thresholds between 0.2 and 0.8 at uniform intervals of \n0.1 are shown. The distance between the thresholds is not uniform in this ROC \ncurve; the distance depends on the distribution of probabilities.\n0\n0\n0.050\n0.100\n0.150\nThreshold\n>= 0.8\nThreshold\n<= 0.2\nThreshold\n= 0.7\nThreshold = 0.6\nThreshold = 0.5\nThreshold = 0.4\nThreshold = 0.3\n0.200\n0.250\n0.300\n0.350\n0.399\n0.449\n0.499\n1 \u2013 Speci\ufb01city (False PositiveRate)\nSensitivity (True Positive Rate)\n0.549\n0.600\n0.650\n0.700\n0", "Threshold = 0.6\nThreshold = 0.5\nThreshold = 0.4\nThreshold = 0.3\n0.200\n0.250\n0.300\n0.350\n0.399\n0.449\n0.499\n1 \u2013 Speci\ufb01city (False PositiveRate)\nSensitivity (True Positive Rate)\n0.549\n0.600\n0.650\n0.700\n0.750\n0.800\n0.850\n0.900\n0.950\n1.000\n0.100\n0.200\n0.300\n0.400\n0.500\n0.600\n0.700\n0.799\n0.899\n1.000\nFigure 9-2:  Sample ROC curve\nOne of the most commonly used single-number metrics to compare classifi ca-\ntion models is Area under the Curve (AUC), usually referring to the area under \nthe ROC curve. For a ROC curve with x- and y-axes containing rates rather than \nrecord counts, a perfect model will have AUC equal to 1. A random model will \nhave AUC equal to 0.5 and is represented by the diagonal line between the \nextremes: coordinates (0,0) and (1,1). A larger AUC value can be achieved by \n\n \nChapter 9 \u25a0 Assessing Predictive Models \n293\nthe ROC curve stretching to the upper left of the ROC curve, meaning that the \nfalse alarm rate does not increase as quickly as the sensitivity rate compared to", "er 9 \u25a0 Assessing Predictive Models \n293\nthe ROC curve stretching to the upper left of the ROC curve, meaning that the \nfalse alarm rate does not increase as quickly as the sensitivity rate compared to \na random selection of records. The AUC for Figure 9-2 is 0.614.\nOften, practitioners will display the ROC curves for several models in a single \nplot, showing visually the differences in how the models trade off false positives \nand true positives. In the sample shown in Figure 9-3, Model 2 is the best choice \nif you require false alarm rates to be less than 0.4. However, the sensitivity is \nstill below 0.6. If you can tolerate higher false alarm rates, Model 3 is the better \nchoice because its sensitivity is highest.\n0\n0\n0.050\n0.100\n0.150\nModel 2\nModel 1\nModel 3\n0.200\n0.250\n0.300\n0.350\n0.399\n0.449\n0.499\n1 \u2013 Speci\ufb01city (False PositiveRate)\nSensitivity (True Positive Rate)\n0.549\n0.600\n0.650\n0.700\n0.750\n0.800\n0.850\n0.900\n0.950\n1.000\n0.100\n0.200\n0.300\n0.400\n0.500\n0.600\n0.700\n0.799\n0.899\n1.0", "9\n0.449\n0.499\n1 \u2013 Speci\ufb01city (False PositiveRate)\nSensitivity (True Positive Rate)\n0.549\n0.600\n0.650\n0.700\n0.750\n0.800\n0.850\n0.900\n0.950\n1.000\n0.100\n0.200\n0.300\n0.400\n0.500\n0.600\n0.700\n0.799\n0.899\n1.000\nFigure 9-3:  Comparison of three models\nRank-Ordered Approach to Model Assessment\nIn contrast to batch approaches to computing model accuracy, rank-ordered \nmetrics begin by sorting the numeric output of the predictive model, either the \nprobability or confi dence of a classifi cation model or the actual predicted output \nof a regression model. The rank-ordered predictions are binned into segments, \nand summary statistics related to the accuracy of the model are computed either \nindividually for each segment, or cumulatively as you traverse the sorted fi le list. \nThe most common segment is the decile, with10 percent of the dataset\u2019s records \nin each segment. Other segment sizes used commonly include 5 percent groups, \ncalled vinigtiles (also called vingtiles, twentiles, or demi-deciles", " the decile, with10 percent of the dataset\u2019s records \nin each segment. Other segment sizes used commonly include 5 percent groups, \ncalled vinigtiles (also called vingtiles, twentiles, or demi-deciles in software) and \n1 percent groups called percentiles. Model results in this section are shown with \nvinigtiles, although you could use any of the segment sizes without changing \nthe principles.\n\n294 \nChapter 9 \u25a0 Assessing Predictive Models\nRank-ordered approaches work particularly well; the model will identify \na subset of the scored records to act on. In marketing applications, you treat \nthose who are most likely to respond to the treatment, whether the treatment \nis a piece of mail, an e-mail, a display ad, or a phone call. For fraud detection, \nyou may want to select the cases that are the most suspicious or non-compliant \nbecause there are funds to treat only a small subset of the cases. In these kinds \nof applications, so long as the model performs well on the selected population, ", "he most suspicious or non-compliant \nbecause there are funds to treat only a small subset of the cases. In these kinds \nof applications, so long as the model performs well on the selected population, \nyou don\u2019t care how well it rank orders the remaining population because you \nwill never do anything with it. \nThe three most common rank-ordered error metrics are gains charts, lift charts, \nand ROI charts. In each of these charts, the x-axis is the percent depth of the \nrank-ordered list of probabilities, and the y-axis is the gain, lift, or ROI produced \nby the model at that depth.\nGains and Lift Charts\nGain refers to the percentage of the class value of interest found cumulatively \nin the rank-ordered list at each fi le depth. Without loss of generality, assume \nthe value of interest is a 1 for a binary classifi cation problem. Figure 9-4 shows a \ntypical gains chart, where the upper curve represents the gain due to the model. \nThis particular dataset has the target variable value of i", "1 for a binary classifi cation problem. Figure 9-4 shows a \ntypical gains chart, where the upper curve represents the gain due to the model. \nThis particular dataset has the target variable value of interest represented in 5.1 \npercent of the data, although this rate is not known from the gains chart itself. \nThe contrasting straight line is the expected gain due to a random draw from \nthe data. For a random draw, you would expect to fi nd 10 percent of the 1s in \nthe fi rst 10 percent of the data, 20 percent in the fi rst 20 percent of the records, \nand so on: a linear gain. The area between the random line and the model gain \nis the incremental improvement the model provides. The fi gure shows through \nthe 10 percent depth, the model selects nearly 20 percent of the 1s, nearly twice \nas many as a random draw.\nThe depth you use to measure a model\u2019s gain depends on the business objec-\ntive: It could be gain at the 10 percent depth, 30 percent depth, 70 percent depth, \nor even the area ", "many as a random draw.\nThe depth you use to measure a model\u2019s gain depends on the business objec-\ntive: It could be gain at the 10 percent depth, 30 percent depth, 70 percent depth, \nor even the area between the model gain and the random gain over the entire \ndataset, much like the AUC metric computed from the ROC curve. With a specifi c \nvalue of gain determined by the business objective, the predictive modeler can \nbuild several models and select the model with the best gain value.\nIf a model is perfect, with 5.1 percent of the data with a target value equal to \n1, the fi rst model vinigtile will be completely populated with 1s, the remaining \n1s falling in the second vinigtile. Because the remainder of the data does not \nhave any more 1s, the remaining values for the gains chart are fl at at 100 per-\ncent. A perfect gains chart is shown in Figure 9-5. If you ever see a perfect gains \nchart, you\u2019ve almost certainly allowed an input variable into the model that is \n\n \nChapter 9 \u25a0 Asse", "at at 100 per-\ncent. A perfect gains chart is shown in Figure 9-5. If you ever see a perfect gains \nchart, you\u2019ve almost certainly allowed an input variable into the model that is \n\n \nChapter 9 \u25a0 Assessing Predictive Models \n295\nincorporating target variable information. Find the variable that is most impor-\ntant to the model and correct it or remove it from the list of candidate inputs.\n0%\n0%\n10%\n20%\n30%\n40%\n50%\nDepth\nGains Chart\n60%\n70%\n80%\n90%\n100%\n10%\n20%\n30%\n40%\n50%\n60%\nCumulative Gain\n70%\n80%\n90%\n100%\nImprovement from Model vs. Random Selection\nAt 10% depth, model\n\ufb01nds nearly 2x as\nmany 1s as a random\nselection\nFigure 9-4:  Sample gains chart\n0%\n0.0%\n10.0%\n20.0%\n30.0%\n40.0%\n50.0%\n60.0%\n70.0%\n80.0%\n90.0%\n100.0%\n10%\n20%\n30%\n40%\n50%\nDepth\nPerfect Gains Chart\nCumulative Gain\n60%\n70%\n80%\n90%\n100%\nFigure 9-5:  Perfect gains chart\n\n296 \nChapter 9 \u25a0 Assessing Predictive Models\nLift is closely related to gain, but rather than computing the percentage of \n1s found in the rank ordered list,", "90%\n100%\nFigure 9-5:  Perfect gains chart\n\n296 \nChapter 9 \u25a0 Assessing Predictive Models\nLift is closely related to gain, but rather than computing the percentage of \n1s found in the rank ordered list, we compute the ratio between the 1s found \nby the model and the 1s that would have been found with a random selection \nat the same depth. Figure 9-6 shows a cumulative lift chart for the same data \nused for the gains chart in Figure 9-4. In a lift chart, the random selection line \nhas a lift of 1.0 through the population. The cumulative lift chart will always \nconverge to 1.0 at the 100 percent depth. Figure 9-7 shows the segment lift chart, \nthe lift for each segment (vinigtiles in this chart). When computing lift per seg-\nment, the segment lift must eventually descend below the lift of 1.0, typically \nhalfway through the fi le depth. One advantage of seeing the segment lift is \nthat you can also see how few 1s are still selected by the model in the bottom \nsegments. For Figure 9-7, the ", " typically \nhalfway through the fi le depth. One advantage of seeing the segment lift is \nthat you can also see how few 1s are still selected by the model in the bottom \nsegments. For Figure 9-7, the lift is just above 0.5.\n0.00\n0.50\n1.00\n1.50\n2.00\n2.50\nDepth\nCumulative Lift Chart\nCumulative Lift\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nFigure 9-6:  Sample cumulative lift chart\nSometimes it is useful to measure model lift from segment to segment rather \nthan cumulatively. Cumulative lift charts aggregate records as the depth increases \nand can mask problems in the models, particularly at the higher model depths. \nConsider Figures 9-8 and 9-9, which show the cumulative lift and segment lift \nfor a different model than was used for the lift charts in Figures 9-6 and 9-7. The \ncumulative lift chart is worse than the prior model but has a smooth appearance \nfrom vinigtile to vinigtile. However, in Figure 9-9, the segment lift chart is erratic, \nwith positive and negative slopes from vin", "ift chart is worse than the prior model but has a smooth appearance \nfrom vinigtile to vinigtile. However, in Figure 9-9, the segment lift chart is erratic, \nwith positive and negative slopes from vinigtile to vinigtile. Erratic segment lift \n\n \nChapter 9 \u25a0 Assessing Predictive Models \n297\ncharts created from testing or validation data are indicative of models that are \noverfi t: A stable model will have monotonic lift values from segment to segment. \n0.00\n0.50\n1.00\n1.50\n2.00\n2.50\nDepth\nSegment Lift\nSegment Lift\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nFigure 9-7:  Sample segment lift chart\n0%\n0.00\n0.20\n0.40\n0.60\n0.80\n1.00\n1.20\n1.40\n1.60\n1.80\n10%\n20%\n30%\n40%\n50%\nDepth\nCumulative Lift Chart\nCumulative Lift\n60%\n70%\n80%\n90%\n100%\nFigure 9-8:  Cumulative lift chart for overfit model\n\n298 \nChapter 9 \u25a0 Assessing Predictive Models\n0%\n0.00\n0.20\n0.40\n0.60\n0.80\n1.00\n1.20\n1.40\n1.60\n1.80\n10%\n20%\n30%\n40%\n50%\nDepth\nSegment Lift\nSegment Lift\n60%\n70%\n80%\n90%\n100%\nFigure 9-9:  Segment lift chart for ", "apter 9 \u25a0 Assessing Predictive Models\n0%\n0.00\n0.20\n0.40\n0.60\n0.80\n1.00\n1.20\n1.40\n1.60\n1.80\n10%\n20%\n30%\n40%\n50%\nDepth\nSegment Lift\nSegment Lift\n60%\n70%\n80%\n90%\n100%\nFigure 9-9:  Segment lift chart for overfit model\nWhen there are multiple levels in the target variable\u2014multi-class classifi cation \nproblems\u2014gains and lift charts can still be computed, but for only one value of \nthe target variable at a time. For example, with the nasadata, you would build \na separate lift chart for each of the seven classes. \nCustom Model Assessment\nWhen you use a rank-ordered metric to assess model performance, the actual \nvalue predicted by the model no longer matters: Only the metric matters. \nFor gains, the metric is the percentage of 1s found by the model. For lift, the \nmetric is the ratio of the percentage of 1s found to the average rate. For ROC, \nit is the comparison of true alerts to false alarms. In each of these cases, each \nrecord has equal weight. However, some projects can benefi t from wei", "tage of 1s found to the average rate. For ROC, \nit is the comparison of true alerts to false alarms. In each of these cases, each \nrecord has equal weight. However, some projects can benefi t from weighting \nrecords according to their costs or benefi ts, creating a custom assessment \nformula. \nFor example, one popular variation on rank-ordered methods is computing \nthe expected cumulative profi t: a fi xed or variable gain minus a fi xed or variable \ncost. Fixed cost and gain is a straightforward refi nement of a gains chart, and \nmany software tools include profi t charts already. For example, the variable gain \nin a marketing campaign can be the purchase amount of a new customer\u2019s fi rst \nvisit, and the variable cost the search keyword ad cost. Or for a fraud detection \n\n \nChapter 9 \u25a0 Assessing Predictive Models \n299\nproblem, the variable gain can be the amount of fraud perpetrated by the offender \nand the variable cost the investigation costs. \nConsider an application like the KDD C", "sessing Predictive Models \n299\nproblem, the variable gain can be the amount of fraud perpetrated by the offender \nand the variable cost the investigation costs. \nConsider an application like the KDD Cup 1998 data that has fi xed cost (the \ncost of the mailing) and variable gain (the donation amount). Figure 9-10 shows \nthe profi t chart for this scenario. This particular model generates a maximum \nprofi t of nearly $2,500 at the depth of 25,000 donors. This maximum profi t is \nnearly two times as much compared to contacting all the donors (depth of 43,000). \n\u2212500\n0\n0\n5000\n10000\n15000\n20000\nRank Ordered Record Number\nNet Revenue\n25000\n30000\n35000\n40000\n45000\n500\n1000\n1500\n2000\n2500\n3000\nFigure 9-10:  Profit chart\nCustom model assessments can be applied to regression models just as eas-\nily as classifi cation models precisely because it is only the rank-ordering that \nmatters, not the actual prediction value. By extension, this also means that \ncompletely different models, built for diff", "as classifi cation models precisely because it is only the rank-ordering that \nmatters, not the actual prediction value. By extension, this also means that \ncompletely different models, built for different target variables, can be assessed \nand compared directly, on a common scale. \nIn addition, custom model assessments can also be applied to batch met-\nrics, adding to the list of confusion matrix metrics already listed in Table 9-3. \nFor example, consider a business objective that stipulates false alarms have four \ntimes the cost of false dismissals. Table 9-8 shows the same data already shown \nin Table 9-2 with the additional column showing the cost of an error. Correct \nclassifi cations receive a cost of 0, false dismissals a cost of 1, and false alarms \na cost of 4. The sum of these costs is the model score and the best model is the \nlowest-scoring model for the cost function. \n\n300 \nChapter 9 \u25a0 Assessing Predictive Models\nTable 9-8: Custom Cost Function for False Alarms and False ", "sts is the model score and the best model is the \nlowest-scoring model for the cost function. \n\n300 \nChapter 9 \u25a0 Assessing Predictive Models\nTable 9-8: Custom Cost Function for False Alarms and False Dismissals\nACTUAL \nTARGET \nVALUE\nPROBABILITY \nTARGET = 1\nPREDICTED \nTARGET VALUE\nCONFUSION \nMATRIX \nQUADRANT\nCOST OF ERROR, \nFALSE ALARM 4X\n0\n0.641\n1\nfalse alarm\n4\n1\n0.601\n1\ntrue positive\n0\n0\n0.587\n1\nfalse alarm\n4\n1\n0.585\n1\ntrue positive\n0\n1\n0.575\n1\ntrue positive\n0\n0\n0.562\n1\nfalse alarm\n4\n0\n0.531\n1\nfalse alarm\n4\n1\n0.504\n1\ntrue positive\n0\n0\n0.489\n0\ntrue negative\n0\n1\n0.488\n0\nfalse dismissal\n1\n0\n0.483\n0\ntrue negative\n0\n0\n0.471\n0\ntrue negative\n0\n0\n0.457\n0\ntrue negative\n0\n1\n0.418\n0\nfalse dismissal\n1\n0\n0.394\n0\ntrue negative\n0\n0\n0.384\n0\ntrue negative\n0\n0\n0.372\n0\ntrue negative\n0\n0\n0.371\n0\ntrue negative\n0\n0\n0.341\n0\ntrue negative\n0\n1\n0.317\n0\nfalse dismissal\n1\nWhich Assessment Should Be Used?\nIn general, the assessment used should be the one that most closely matches \nthe business objectives defi ned", "e\n0\n0\n0.341\n0\ntrue negative\n0\n1\n0.317\n0\nfalse dismissal\n1\nWhich Assessment Should Be Used?\nIn general, the assessment used should be the one that most closely matches \nthe business objectives defi ned at the beginning of the project during Business \nUnderstanding. If that objective indicates that the model will be used to select \none-third of the population for treatment, then model gain or lift at the 33 \npercent depth is appropriate. If all customers will be treated, then computing \nAUC for a batch metric may be appropriate. If the objective is to maximize the \nrecords selected by the model subject to a maximum false alarm rate, a ROC \nis appropriate.\nThe metric used for model selection is of critical importance because the model \nselected based on one metric may not be a good model for a different metric. \n\n \nChapter 9 \u25a0 Assessing Predictive Models \n301\nConsider Figure 9-11: a scatterplot of 200 models and their rank based on \nAUC at the 70 percent depth and the root mean squared (R", "a different metric. \n\n \nChapter 9 \u25a0 Assessing Predictive Models \n301\nConsider Figure 9-11: a scatterplot of 200 models and their rank based on \nAUC at the 70 percent depth and the root mean squared (RMS) error. The cor-\nrelation between these two rankings is 0.1\u2014almost no relationship between the \ntwo rankings; the ranking based on AUC cannot be determined at all from the \nranking of RMS error. Therefore, if you want to use a model operationally in \nan environment where minimizing false positives and maximizing true posi-\ntives is important, choosing the model with the best RMS error model would \nbe sub-optimal. \n0\n0\n50\n100\nAUC 70% Rank\nTest RMS Error Ranking vs. AUC 70% Rank\nTest RMS Error Ranking\n150\n200\n250\n50\n100\n150\n200\n250\nFigure 9-11:  Scatterplot of AUC vs. RMS Error\nAssessing Regression Models\nThe metrics most predictive analytics software packages provide to assess \nregression models are batch methods computed for the data partition you are \nusing to assess the models (traini", "ession Models\nThe metrics most predictive analytics software packages provide to assess \nregression models are batch methods computed for the data partition you are \nusing to assess the models (training, testing, validation, and so on). The most \ncommonly used metric is the coeffi cient of determination known as R2, pro-\nnounced \u201cr squared.\u201d R2 measures the percentage of the variance of the target \nvariable that is explained by the models. You can compute it by subtracting \nthe ratio of the variance of the residuals and the variance of the target variable \nfrom 1. If the variance of the residuals is 0, meaning the model fi t is perfect, R2 \nis equal to 1, indicating a perfect fi t. If the model explains none of the variance \nin the model, the variance of the residuals will be just as large as the variance \nof the target variable, and R2 will be equal to 0. You see R2 even in an Excel \ntrend line equation.\nWhat value of R2 is good? This depends on the application. In social science \npro", " as the variance \nof the target variable, and R2 will be equal to 0. You see R2 even in an Excel \ntrend line equation.\nWhat value of R2 is good? This depends on the application. In social science \nproblems, an R2 of 0.3 might be excellent, whereas in scientifi c applications you \nmight need an R2 value of 0.7 or higher for the model to be considered a good \nfi t. Some software packages also include a modifi cation of R2 that penalizes \n\n302 \nChapter 9 \u25a0 Assessing Predictive Models\nthe model as more terms are added to the model, much like what is done with \nAIC, BIC, and MDL.\nFigure 9-12 shows two regression models, the fi rst with a relatively high R2 \nvalue and the second with a nearly zero R2 value. In the plot at the left, there is \na clear trend between LASTGIFT and TARGET_D. At the right, the relationship \nbetween the variables is nearly random; there is very little explained variance \nfrom the linear model.\n0\n0\n5\n10\n15\n20\n25\n30\n5\n10\n15\nLASTGIFT\nR2 = 0.50426\nTARGET_D\n20\n25\n30\n0\n0\n", "ght, the relationship \nbetween the variables is nearly random; there is very little explained variance \nfrom the linear model.\n0\n0\n5\n10\n15\n20\n25\n30\n5\n10\n15\nLASTGIFT\nR2 = 0.50426\nTARGET_D\n20\n25\n30\n0\n0\n5\n10\n15\n20\n25\n30\n4\n2\n6\n10\n8\nRandom Input\nR2 = 0.02681\nTARGET_D\n12\n16\n14\n18\nFigure 9-12:  R2 for two linear models\nR2 is only one of the many ways regression models are assessed. Table 9-9 \nshows a list of other common measures of model accuracy. Even if these are \nnot included in your predictive modeling software, they are easy to compute.\nTable 9-9: Batch Metrics for Assessing Regression Models\nMEASURE\nEXPLANATION OF MEASURE\nR2\nPercent variance explained\nMean squared \nerror (MSE)\nAverage error. If the model predictions are unbiased positively or \nnegatively, this should be equal to 0.\nMean absolute \nerror (MAE)\nCompute the absolute value of the error before averaging. This \nprovides an average magnitude of error measure and is usually pre-\nferred over MSE for comparing models.\nMedian erro", "solute \nerror (MAE)\nCompute the absolute value of the error before averaging. This \nprovides an average magnitude of error measure and is usually pre-\nferred over MSE for comparing models.\nMedian error\nA more robust measure of errors than MSE because outliers in error \ndon\u2019t in\ufb02 uence the median\nMedian absolute \nerror\nA more robust measure of error magnitudes\nCorrelation\nThe square root of R2, this is an intuitive way to assess how similar \nmodel predictions are to the target values. Correlations are sensitive \nto outliers.\n\n \nChapter 9 \u25a0 Assessing Predictive Models \n303\nMEASURE\nEXPLANATION OF MEASURE\nAverage percent \nerror\nCompute the percentage of the error rather than the mean. This nor-\nmalizes by the magnitude of the target variable, showing the relative \nsize of the error compared to the actual target value.\nAverage absolute \npercent error\nCompute the percentage of the absolute value of the error rather \nthan the mean. As with average percent error, this shows the relative \nsize ", " the actual target value.\nAverage absolute \npercent error\nCompute the percentage of the absolute value of the error rather \nthan the mean. As with average percent error, this shows the relative \nsize of the error.\nThe error metric you use depends on your business objective. If you care about \ntotal errors, and larger errors are more important than smaller errors, mean \nabsolute error is a good metric. If the relative size of the error is more important \nthan the magnitude of the error, the percent error metrics are more appropriate. \nTable 9-10 shows several of the error metrics for the KDD Cup 1998 data using \nfour regression models, each predicting the target variable TARGET_D. The \nlinear regression model has the largest R2 in this set of models. Interestingly, \neven though the linear regression model had the highest R2 and lowest mean \nabsolute error, the regression trees had lower median absolute error values, \nmeaning that the most typical error magnitude is smaller for the trees", "egression model had the highest R2 and lowest mean \nabsolute error, the regression trees had lower median absolute error values, \nmeaning that the most typical error magnitude is smaller for the trees.\nTable 9-10: Regression Error Metrics for Four Models\nMEASURE\nLINEAR \nREGRESSION\nNEURAL \nNETWORK\nCART \nREGRESSION \nTREE\nCHAID \nREGRESSION \nTREE\nR2\n0.519\n0.494\n0.503\n0.455\nMean Error\n\u20130.072\n\u20130.220\n\u20130.027\n0.007\nMean Absolute Error\n4.182\n4.572\n4.266\n4.388\nMedian Absolute Error\n2.374\n3.115\n2.249\n2.276\nCorrelation\n0.720\n0.703\n0.709\n0.674\nRank-ordered model assessment methods apply to regression problems in \nthe same way they apply to classifi cation problems, even though they often are \nnot included in predictive analytics software. Rank-ordered methods change \nthe focus of model assessment from a record-level error calculation to the rank-\ning, where the accuracy itself matters less than getting the predictions in the \nright order. \nFor example, if you are building customer lifetime value (CL", " record-level error calculation to the rank-\ning, where the accuracy itself matters less than getting the predictions in the \nright order. \nFor example, if you are building customer lifetime value (CLV) models, you \nmay determine that the actual CLV value is not going to be particularly accurate. \nHowever, identifying the customers with the most potential, the top 10 percent \nof the CLV scores, may be strategically valuable for the company. Figure 9-11 \nshowed that batch assessment measures might not be correlated with rank-\nordered assessment methods for classifi cation. The same applies to regression.\n\n304 \nChapter 9 \u25a0 Assessing Predictive Models\nFor example, Table 9-11 shows a decile-by-decile summary of two models \nbuilt on the KDD Cup 1998 data. The top decile for the linear regression model \nfi nds donors who gave on average 32.6 dollars. The average for donors who \ngave a donation was 15.4, so the top decile of linear regression scores found gift \namounts with a lift of 2.1 (32.", "n model \nfi nds donors who gave on average 32.6 dollars. The average for donors who \ngave a donation was 15.4, so the top decile of linear regression scores found gift \namounts with a lift of 2.1 (32.6 \u00f7 15.4). \nTable 9-11: Rank-Ordering Regression Models by Decile\nDECILE\nLINEAR REGRESSION, \nMEAN TARGET_D\nNEURAL NETWORK, \nMEAN TARGET_D\n1\n32.6\n32.3\n2\n22.1\n21.4\n3\n18.9\n19.1\n4\n17.1\n16.8\n5\n15.3\n15.0\n6\n13.2\n13.1\n7\n11.3\n11.2\n8\n10.5\n10.2\n9\n7.6\n7.8\n10\n6.0\n6.6\nThe deciles are shown in tabular form, but they can also be shown in graphi-\ncal form, as in the chart in Figure 9-13. The fi gure shows that the model predicts \nthe highest donors in the top decile particularly well; the average TARGET_D \nvalues for the top decile is larger than you expect based on the decreasing trend \nof the remaining deciles. \n1\n0\n5\n10\n15\n20\nAverage TARGET_D value in decile\n25\n30\n35\n2\n3\n4\n5\n6\nDecile\n7\n8\n9\n10\nFigure 9-13:  Average actual target value by decile\n\n \nChapter 9 \u25a0 Assessing Predictive Models \n305\nSummary\nA pr", "es. \n1\n0\n5\n10\n15\n20\nAverage TARGET_D value in decile\n25\n30\n35\n2\n3\n4\n5\n6\nDecile\n7\n8\n9\n10\nFigure 9-13:  Average actual target value by decile\n\n \nChapter 9 \u25a0 Assessing Predictive Models \n305\nSummary\nA predictive model is only good if it answers a business question effectively. \nPredictive modelers determine the meaning of the words \u201cgood\u201d and \u201ceffec-\ntively\u201d during Business Understanding by translating the business objectives \ninto modeling objectives. When these objectives are defi ned clearly, identifying \nthe best model is easy: The best model is the one that optimizes that objective.\nThe objectives sometimes require metrics that consider all the records, what \nI called batch objectives. PCC and the confusion matrix metrics are the most \ncommonly used batch objectives for classifi cation. Even here, however, there are \ndifferences between the ways a confusion matrix can be used to assess models, \ndepending on how you trade off false alerts, false dismissals, true alerts, and \ntrue dism", "Even here, however, there are \ndifferences between the ways a confusion matrix can be used to assess models, \ndepending on how you trade off false alerts, false dismissals, true alerts, and \ntrue dismissals. For regression problems, the most commonly used metrics are \nR2 and average squared error. \nVery often, however, the models are built to select a sub-population to treat. \nIn these problems, a rank-ordered metric is more appropriate. For classifi ca-\ntion models, ROC curves, gains charts, lift charts, and profi t charts are the \nmost popular methods to assess these problems. In addition to these methods, \nsome business objectives lead modelers to create customized model assessment \nmetrics. Similar rank-ordered metrics are appropriate for regression problems. \nRegardless of the method, you should match the metric to the business objec-\ntive or else you may pick a model that won\u2019t be the best operationally.\n\n\n307\nModel Ensembles, or simply \u201censembles,\u201d are combinations of two or mor", " you should match the metric to the business objec-\ntive or else you may pick a model that won\u2019t be the best operationally.\n\n\n307\nModel Ensembles, or simply \u201censembles,\u201d are combinations of two or more \npredictions from predictive models into a single composite score. The chapters \non building supervised and unsupervised learning models focused on how to \nbuild the best single model and how to determine which of the single models \nyou should select for deployment. The ensemble approach turns this thinking \naround. Rather than building models and selecting the single best model to \ndeploy, why not build many models and use them all in deployment? \nThis chapter describes how to build model ensembles and why under-\nstanding how to build them is important\u2014some would say essential\u2014for \npredictive modelers.\nMotivation for Ensembles\nPractitioners build model ensembles for one reason: improved accuracy. In \nstudy after study over the past two decades, ensembles nearly always improve \nmodel pre", "ve modelers.\nMotivation for Ensembles\nPractitioners build model ensembles for one reason: improved accuracy. In \nstudy after study over the past two decades, ensembles nearly always improve \nmodel predictive accuracy and rarely predict worse than single models. In the \n1990s, when ensembles were beginning to appear in the data-mining literature, \nC H A P T E R \n10\nModel Ensembles\n\n308 \nChapter 10 \u25a0 Model Ensembles\nthe improved accuracy of ensembles on held-out data began to appear in an \nalmost magical way. By the 2000s, ensembles became essential to winning data \nmining competitions. \nConsider two examples, and these are not unique. First, the Pacifi c-Asia \nConference on Knowledge Discovery (PAKDD) has a data mining competition \neach year called the PAKDD Cup. In 2007, twenty-eight submissions were received \nand the top fi ve solutions were model ensembles, including three submissions \nwith ensembles of trees, one with an ensemble of Probit models (similar to \nlogistic regression mod", "ssions were received \nand the top fi ve solutions were model ensembles, including three submissions \nwith ensembles of trees, one with an ensemble of Probit models (similar to \nlogistic regression models), and one neural network ensemble. \nA second recent example of ensembles winning competitions is the Netfl ix \nprize, an\u00a0open competition that solicited entries to predict user ratings of \nfi lms based on historical ratings. The prize was $1,000,000 for any team that \ncould reduce the root means squared error (RMSE) of the existing Netfl ix \ninternal algorithm by 10 percent or more. The winner, runner-up, and nearly \nall the leaders used model ensembles in their submissions. In fact, the winning \nsubmission was the result of an ensemble containing hundreds of \npredictive models.\nModel ensembles not only can improve model accuracy, but they can also \nimprove model robustness. Through averaging multiple models into a single \nprediction, no single model dominates the fi nal predicted valu", "mbles not only can improve model accuracy, but they can also \nimprove model robustness. Through averaging multiple models into a single \nprediction, no single model dominates the fi nal predicted value of the models, \nreducing the likelihood that a fl aky prediction will be made (so long as all the \nmodels don\u2019t agree on the fl aky prediction). \nThe improved accuracy comes with costs, however. First, model interpretation, \nif it ever existed, is lost. A single decision tree is a favorite choice of predictive \nmodelers when model interpretation is paramount. However, if an ensemble of \n1,000 trees is the fi nal model, the determination of why a prediction was made \nrequires assessing the predictions of 1,000 models. Techniques exist for aiding in \ninterpreting the ensemble but the transparency of a single decision tree is gone.\nSecond, model ensembles can become too computationally expensive for \ndeployment. In the Netfl ix prize, the winning submission was never used \ndirectly to make ", "ncy of a single decision tree is gone.\nSecond, model ensembles can become too computationally expensive for \ndeployment. In the Netfl ix prize, the winning submission was never used \ndirectly to make movie recommendations because of its complexity and inability \nto scale and make the movie predictions quickly enough. Sometimes accuracy \nitself isn\u2019t enough for a model or ensemble of models to be useful.\nThe Wisdom of Crowds\nIn the popular book The Wisdom of Crowds (Random House, 2005), author James \nSurowiecki proposes that better decisions can be made if rather than relying \non a single expert, many (even uninformed) opinions can be aggregated into \na decision that is superior to the expert\u2019s opinion, often called crowdsourcing. \nSurowiecki describes four characteristics necessary for the group opinion \nto work well and not degenerate into the opposite effect of poor decisions as \n\n \nChapter 10 \u25a0 Model Ensembles \n309\nevidenced by the \u201cmadness of crowds\u201d: diversity of opinion, independ", " group opinion \nto work well and not degenerate into the opposite effect of poor decisions as \n\n \nChapter 10 \u25a0 Model Ensembles \n309\nevidenced by the \u201cmadness of crowds\u201d: diversity of opinion, independence, \ndecentralization, and aggregation (see Figure 10-1). The fi rst three characteristics \nrelate to how the individual decisions are made: They must have information \ndifferent than others in the group and not be affected by the others in the group. \nThe last characteristic merely states that the decisions must be combined. \nDiversity of opinion\nDecentralization\nAggregation\nIndependence\n\u2022 Vary algorithms and/or data\n\u2022 Check correlations of\n prediction\n\u2022 Related to independence\n\u2022 Include multiple analyses\n\u2022 Voting\n\u2022 Averaging or weighted\n averaging\n\u2022 Sampling\n\u2022 Different algorithms\nFigure 10-1:  Characteristics of good ensemble decisions\nModel ensembles have very similar characteristics. Each predictive model \nhas a voice in the fi nal decision. The diversity of opinion can be measured ", "0-1:  Characteristics of good ensemble decisions\nModel ensembles have very similar characteristics. Each predictive model \nhas a voice in the fi nal decision. The diversity of opinion can be measured by \nthe correlation of the predictive values themselves: If all of the predictions are \nhighly correlated with one another, or in other words, if the models nearly all \nagree, there is no point in combining them. The decentralization characteristic \ncan be achieved by resampling data or through case weights: Each model uses \neither different records from a common data set or at least uses the records with \nweights that are different from the other models. \nBias Variance Tradeo\ufb00 \nBefore describing model ensembles, it is useful to understand the principle in \nstatistical literature of the bias-variance tradeoff. Bias refers to model error and \nvariance refers to the consistency in predictive accuracy of models applied to \nother data sets. The best models have low bias (low error, high accura", "ariance tradeoff. Bias refers to model error and \nvariance refers to the consistency in predictive accuracy of models applied to \nother data sets. The best models have low bias (low error, high accuracy) and \nlow variance (consistency of accuracy from data set to data set). \nUnfortunately, there is always a tradeoff between these two building predic-\ntive models. You can achieve low bias on training data, but may suffer from \nhigh variance on held-out data because the models were overfi t. The k-NN \nalgorithm with k=1 is an example of a low bias model (perfect on training data), \nbut susceptible to high variance on held-out data. Conversely, you can achieve \nlow variance from data set to data set but at the cost of higher bias. These are \ntypically simple models that lack the fl exibility and power to predict accurately, \nsuch as single split trees called decision stumps.\n\n310 \nChapter 10 \u25a0 Model Ensembles\nIn a simple example, Figure 10-2 shows a data set with a simple linear \nregressi", "and power to predict accurately, \nsuch as single split trees called decision stumps.\n\n310 \nChapter 10 \u25a0 Model Ensembles\nIn a simple example, Figure 10-2 shows a data set with a simple linear \nregression model. \nThis model will have relatively low variance as it is a smooth predictor, \nalthough it has bias. A second curve fi t is shown in Figure 10-3, this time with \nlow bias, but because of the complexity of the model it is in danger of overfi t-\nting the data and will likely have large errors on subsequent data. These larger \nerrors are shown in Figure 10-4. Four vertical, dashed lines show the errors \nof these four data points, clearly large errors induced by the differences between \nthe diamond data points the model was built from and the second box data. \nEvery new data set, with comparable variations in the data, will also have large \nerrors like those shown in Figure 10-4, resulting in high error variance from \nthis model.\nTraining Data\n0.000\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n\u22121.000\n\u22122", "rable variations in the data, will also have large \nerrors like those shown in Figure 10-4, resulting in high error variance from \nthis model.\nTraining Data\n0.000\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n\u22121.000\n\u22122.000\n\u22123.000\n\u22124.000\n1.000\n2.000\n3.000\nFigure 10-2:  Low variance fit to data\n0\n2\n4\n6\n8\n10\n12\nTraining Data\n14\n16\n18\n20\n0.000\n\u22121.000\n\u22122.000\n\u22123.000\n\u22124.000\n1.000\n2.000\n3.000\nFigure 10-3:  Low bias fit to data\n\n \nChapter 10 \u25a0 Model Ensembles \n311\n0.000\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n\u22121.000\n\u22122.000\n\u22123.000\n\u22124.000\n1.000\n2.000\n3.000\nTraining Data\nTesting Data\nFigure 10-4:  Errors on new data\nBagging\nLeo Brieman fi rst published a description of the Bootstrap Aggregating (bag-\nging) algorithm in 1996. The idea is quite simple yet powerful: Build multiple \ndecision trees from resampled data and combine the predicted values through \naveraging or voting. The resampling method Breiman used was bootstrap \nsampling (sampling with replacement), which creates replicates of some records \nin the training data, al", "redicted values through \naveraging or voting. The resampling method Breiman used was bootstrap \nsampling (sampling with replacement), which creates replicates of some records \nin the training data, although on average, 37 percent of the records will not be \nincluded at all in the training data. \nAlthough bagging was fi rst developed for decision trees, the idea can be \napplied to any algorithm that produces predictions with suffi cient variation in \nthe predicted values. Other algorithms that are good candidates include neural \nnetworks, Na\u00efve Bayes, k-nearest neighbor (for low values of k), and to a lesser \ndegree, even logistic regression. k-nearest neighbor is not a good candidate for \nbagging if the value of k is already large; the algorithm already votes or aver-\nages predictions and with larger values of k, predictions are already very stable \nwith low variance. \nHow many bootstrap samples, sometimes called replicates, should be created? \nBrieman stated, \u201cMy sense of it is that f", "with larger values of k, predictions are already very stable \nwith low variance. \nHow many bootstrap samples, sometimes called replicates, should be created? \nBrieman stated, \u201cMy sense of it is that fewer are required when y is numerical \nand more are required with an increasing number of classes.\u201d He typically \nused 10\u201325 bootstrap replicates, with signifi cant improvements occurring with \nas few as 10 replicates.\nOverfi tting the models is an important requirement to building good bagged \nensembles. By overfi tting each model, the bias is low, but the decision tree will \n\n312 \nChapter 10 \u25a0 Model Ensembles\ngenerally have worse accuracy on held-out data. But bagging is a variance \nreduction technique; the averaging of predictions smoothes the predictions to \nbehave in a more stable way on new data. \nFor example, consider models based on the KDD Cup 1998 data set. Thirty \n(30) bootstrap samples were created from a stratifi ed sample of the target vari-\nable. From these 30 samples, 30 de", "new data. \nFor example, consider models based on the KDD Cup 1998 data set. Thirty \n(30) bootstrap samples were created from a stratifi ed sample of the target vari-\nable. From these 30 samples, 30 decision trees were built. Only LASTGIFT and \nRFA_2F were included as inputs to simplify visualization of results. Each model \nwas a CART-styled decision tree that was somewhat overfi t, per Breiman\u2019s \nrecommendation. \nResults shown are based on test data only, not training data or out-of-sample \ndata from the bootstrap samples. Table 10-1 shows the average model Area \nunder the Curve (AUC) for these 30 models. The 30 models were remarkably \nconsistent in their AUC metrics; the AUC values ranged from 0.530 to 0.555. If \nthe models built to be included in the ensemble are poor predictors, or in other \nwords have high bias, it is better to exclude those models from the ensemble as \nthey will degrade the accuracy of the ensemble. \nBagged ensembles were created from averaging the model predicted", "ther \nwords have high bias, it is better to exclude those models from the ensemble as \nthey will degrade the accuracy of the ensemble. \nBagged ensembles were created from averaging the model predicted prob-\nabilities rather than voting. A 10-model ensemble is shown in the table as well: \nIts AUC is better than even the best individual tree. Finally, a 30-model ensemble \nwas created and had the best AUC on test data. Bagged ensembles with more \nthan 30 trees were not any better than using 30 trees. While the gains are modest \non this data set, they are statistically signifi cant. These results are summarized \nin Table 10-1.\nTable 10-1: Bagging Model AUC\nMETHOD\nAUC\nAverage tree \n0.542\nBest tree \n0.555\n10-tree ensemble\n0.561\n30-tree bagging ensemble\n0.566\n\n \nChapter 10 \u25a0 Model Ensembles \n313\nThe diversity of model predictions is a key element in creating effective \nensembles. One way to measure the diversity of predictions is to examine the \ncorrelation of predicted values. If the correla", "The diversity of model predictions is a key element in creating effective \nensembles. One way to measure the diversity of predictions is to examine the \ncorrelation of predicted values. If the correlations between model predictions \nare always very high, greater than 0.95, there is little additional predictive infor-\nmation each model brings to the ensemble and therefore little improvement \nin accuracy is achievable. Generally, it is best to have correlations less than 0.9 \nat most. The correlations should be computed from the model propensities or \npredicted probabilities rather than the {0,1} classifi cation value itself. \nIn the modeling example, the 30 models produce pairwise correlations that \nrange from 0.212 to 0.431: all quite low. When estimates produce similar per-\nformance scores (such as the AUC here) but are not correlated with each other, \nthey achieve their accuracy by predicting accurately on different records, the \nideal scenario for effective ensembles. Records that a", "s (such as the AUC here) but are not correlated with each other, \nthey achieve their accuracy by predicting accurately on different records, the \nideal scenario for effective ensembles. Records that are easy to classify have \nno need for ensembles; any single model will classify these records correctly. \nLikewise, accuracy on records that are never classifi ed correctly in any model \ncannot be improved; these records are lost causes. It is the disagreement \nbetween models that provides the potential. The hope is that the majority of \nmodels in the ensemble will predict these correctly and thus \u201coutvote\u201d the \npoor decisions. \nA fi nal way to examine how the Bagging Ensemble turns individual models \ninto a composite prediction can be seen through the decision boundaries created \nby the models. Figure 10-5 shows decision regions for nine of the models. The \nx-axis in each plot is LASTGIFT; the y-axis is RFA_2F. RFA_2F values (1, 2, 3, \nor 4) have been randomly jittered so they can be seen", "s. Figure 10-5 shows decision regions for nine of the models. The \nx-axis in each plot is LASTGIFT; the y-axis is RFA_2F. RFA_2F values (1, 2, 3, \nor 4) have been randomly jittered so they can be seen more clearly. A predicted \nclass \u201c1\u201d is shown as a light gray square and a predicted class \u201c0\u201d by a dark gray \nsquare. It is easy to see from these plots why the correlations between models \nis so low: each model creates different decision regions.\nAfter averaging 30 models, the resulting decision regions that produced \nthe increase in AUC are shown in Figure 10-6. The regions are generally more \nhomogeneous than in single models.\n\n5.005\n15.005\n20.005\n25.005\n30.005\n40.005\n45.005\n50.005\n55.005\n60.005\n65.005\n70.005\n75.005\n80.005\n90.005\n85.005\n95.005\n99.966\n35.005\n0.005\n0.501\n1.001\n1.501\n2.001\n2.501\n3.001\n3.501\n4.001\n4.499\n10.005\n5.005\n15.005\n20.005\n25.005\n30.005\n40.005\n45.005\n50.005\n55.005\n60.005\n65.005\n70.005\n75.005\n80.005\n90.005\n85.005\n95.005\n99.966\n35.005\n0.005\n0.501\n1.001\n1.501\n2.001\n2.", ".501\n3.001\n3.501\n4.001\n4.499\n10.005\n5.005\n15.005\n20.005\n25.005\n30.005\n40.005\n45.005\n50.005\n55.005\n60.005\n65.005\n70.005\n75.005\n80.005\n90.005\n85.005\n95.005\n99.966\n35.005\n0.005\n0.501\n1.001\n1.501\n2.001\n2.501\n3.001\n3.501\n4.001\n4.499\n10.005\n5.005\n15.005\n20.005\n25.005\n30.005\n40.005\n45.005\n50.005\n55.005\n60.005\n65.005\n70.005\n75.005\n80.005\n90.005\n85.005\n95.005\n99.966\n35.005\n0.005\n0.501\n1.001\n1.501\n2.001\n2.501\n3.001\n3.501\n4.001\n4.499\n10.005\n5.005\n15.005\n20.005\n25.005\n30.005\n40.005\n45.005\n50.005\n55.005\n60.005\n65.005\n70.005\n75.005\n80.005\n90.005\n85.005\n95.005\n99.966\n35.005\n0.005\n0.501\n1.001\n1.501\n2.001\n2.501\n3.001\n3.501\n4.001\n4.499\n10.005\n5.005\n15.005\n20.005\n25.005\n30.005\n40.005\n45.005\n50.005\n55.005\n60.005\n65.005\n70.005\n75.005\n80.005\n90.005\n85.005\n95.005\n99.966\n35.005\n0.005\n0.501\n1.001\n1.501\n2.001\n2.501\n3.001\n3.501\n4.001\n4.499\n10.005\n5.005\n15.005\n20.005\n25.005\n30.005\n40.005\n45.005\n50.005\n55.005\n60.005\n65.005\n70.005\n75.005\n80.005\n90.005\n85.005\n95.005\n99.966\n35.005\n0.005\n0.501\n1.001\n1.501\n2.001\n2.501\n", "\n3.001\n3.501\n4.001\n4.499\n10.005\n5.005\n15.005\n20.005\n25.005\n30.005\n40.005\n45.005\n50.005\n55.005\n60.005\n65.005\n70.005\n75.005\n80.005\n90.005\n85.005\n95.005\n99.966\n35.005\n0.005\n0.501\n1.001\n1.501\n2.001\n2.501\n3.001\n3.501\n4.001\n4.499\n10.005\n5.005\n15.005\n20.005\n25.005\n30.005\n40.005\n45.005\n50.005\n55.005\n60.005\n65.005\n70.005\n75.005\n80.005\n90.005\n85.005\n95.005\n99.966\n35.005\n0.005\n0.501\n1.001\n1.501\n2.001\n2.501\n3.001\n3.501\n4.001\n4.499\n10.005\n5.005\n15.005\n20.005\n25.005\n30.005\n40.005\n45.005\n50.005\n55.005\n60.005\n65.005\n70.005\n75.005\n80.005\n90.005\n85.005\n95.005\n99.966\n35.005\n0.005\n0.501\n1.001\n1.501\n2.001\n2.501\n3.001\n3.501\n4.001\n4.499\n10.005\n5.005\n15.005\n20.005\n25.005\n30.005\n40.005\n45.005\n50.005\n55.005\n60.005\n65.005\n70.005\n75.005\n80.005\n90.005\n85.005\n95.005\n99.966\n35.005\n0.005\n0.501\n1.001\n1.501\n2.001\n2.501\n3.001\n3.501\n4.001\n4.499\n10.005\nValues...\n1\n0\nX-Shape\nCircle\nShapes\nFigure 10-5:  Decision regions for nine bagged trees\n\n \nChapter 10 \u25a0 Model Ensembles \n315\n0.005\n5.005\n0.501\n0.751\n1.001\n1.251\n1.501\n1.75", "01\n3.501\n4.001\n4.499\n10.005\nValues...\n1\n0\nX-Shape\nCircle\nShapes\nFigure 10-5:  Decision regions for nine bagged trees\n\n \nChapter 10 \u25a0 Model Ensembles \n315\n0.005\n5.005\n0.501\n0.751\n1.001\n1.251\n1.501\n1.751\n2.001\n2.251\n2.501\n2.751\n3.001\n3.251\n3.501\n3.751\n4.001\n4.251\n4.499\n10.005\n15.005\n20.005\n25.005\n30.005\n35.005\n40.005\n45.005\n50.005\n55.005\n60.005\n65.005\n70.005\n75.005\n80.005\n85.005\n90.005\n95.005\n99.966\nValues...\n1\n0\nX-Shape\nCircle\nShapes\nFigure 10-6:  Decision region for bagging ensemble\nContrast these fi gures to a heat map of the actual response rates plotted on the \nsame axes as shown in Figure 10-7. Since the target variable, TARGET_B, only \nhas values 0 and 1, continuous values comparable to the model probabilities are \ncreated by binning the two variables (LASTGIFT and RFA_2F) and computing \nthe average response rate in each bin. Each square was colored using colors \ncomparable to the points in the scatterplots shown in Figures 10-5 and 10-6. \nThe bagging ensemble clearly has identifi", "ng \nthe average response rate in each bin. Each square was colored using colors \ncomparable to the points in the scatterplots shown in Figures 10-5 and 10-6. \nThe bagging ensemble clearly has identifi ed the trends in the data (Figure 10-6), \nespecially the  light gray region at the upper left and the dark gray region at the \nbottom right. The vertical stripe in Figure 10-6 can be seen in the vertical band \nfor LASTGIFT having values between 7 and 10 dollars. \nBootstrap sampling in bagging is the key to introducing diversity in the \nmodels. One can think of the bootstrap sampling methodology as creating \ncase weights for each record: Some records are included multiple times in the \ntraining data\u2014their weights are 1, 2, 3, or more\u2014and others are not included \nat all\u2014their weights are equal to 0. \nSome software implementations of decision trees will include an option for \nbagging or provide the capability to construct bagged models. Even if this is \nnot the case, however, bagging is stra", "to 0. \nSome software implementations of decision trees will include an option for \nbagging or provide the capability to construct bagged models. Even if this is \nnot the case, however, bagging is straightforward to implement in most predic-\ntive analytics software packages. For example, if a modeler is building logistic \nregression models, creating bagged ensembles of logistic regression models \nrequires only the creation of 10 samples, 10 models, and logic to combine the \npredictions. The same is true for creating bagged ensembles using any algorithm. \n\n316 \nChapter 10 \u25a0 Model Ensembles\n[0.0,7]\n1\n2\n3\n4\n(7,10]\n(10,11]\n(11,15]\n(15,17]\nLASTGIFT\nRFA_2F\n(17,20]\n(21,25]\n(25,100]\n0.0950\n0.0702\n0.0871\n0.0864\n0.0484\n0.0651\n0.0571\n0.0691\n0.0857\n0.0640\n0.0569\n0.0610\n0.0302\n0.0353\n0.0580\n0.0623\n0.0585\n0.0594\n0.0563\n0.0550\n0.0538\n0.0448\n0.0381\n0.0461\n0.0388\n0.0476\n0.0520\n0.0406\n0.0438\n0.0368\n0.0323\n0.0288\nFigure 10-7:  Decision region for actual target variable\nOther techniques also exist for vary", "5\n0.0594\n0.0563\n0.0550\n0.0538\n0.0448\n0.0381\n0.0461\n0.0388\n0.0476\n0.0520\n0.0406\n0.0438\n0.0368\n0.0323\n0.0288\nFigure 10-7:  Decision region for actual target variable\nOther techniques also exist for varying case weights rather than using a \nbootstrap sample. One could select randomly which records to include in train-\ning and which to exclude, which assigns case weights of 1 and 0 to each record \nrespectively. Cross-validation sampling is one way to accomplish this weighting. \nOne could even create a random, real-valued case weight for each record. These \nare not as commonly found in commercial software but are easy to implement \nthrough custom manipulation of the data.\nBoosting\nRobert Shapire and Yoav Freund introduced the boosting algorithm in \nseparate publications in 1990. In a 1995 joint publication they fi rst described \nthe AdaBoost algorithm (pronounced \u201cadd-a-boost\u201d). The idea, like bagging, \nis also straightforward. \nFirst, one builds a simple classifi cation model; the model on", " publication they fi rst described \nthe AdaBoost algorithm (pronounced \u201cadd-a-boost\u201d). The idea, like bagging, \nis also straightforward. \nFirst, one builds a simple classifi cation model; the model only needs to be \nslightly better than random chance, so for a binary classifi cation problem, only \nslightly better than a 50 percent correct classifi cation. In this fi rst pass, each \nrecord is used in the algorithm with equal case weights, as one would do nor-\nmally in building a predictive model. The errors in the predicted values are \nnoted. Records correctly classifi ed have their case weights reduced and records \nincorrectly classifi ed have their case weights increased, and a second simple \nmodel is built. In other words, for the second model, records that were incor-\nrectly classifi ed are encouraged through case weights to be considered more \nstrongly in the construction of the model. The records that are diffi cult to classify, \nmeaning that initially, the models classify them in", " are encouraged through case weights to be considered more \nstrongly in the construction of the model. The records that are diffi cult to classify, \nmeaning that initially, the models classify them incorrectly, keep getting case \n\n \nChapter 10 \u25a0 Model Ensembles \n317\nweights increased more and more, communicating to the algorithm to pay more \nattention to these records until, hopefully, they are fi nally classifi ed correctly. \nBoosting is often repeated tens or even hundreds of times. After the tens \nor hundreds of iterations, the fi nal predictions are made based on a weighted \naverage of the predictions from all the models. \nConsider the simple example in Figure 10-8 based on only 10 data points. The \ndata contains two class values represented by a dark gray circle and a light gray \nsquare. The size of the symbol represents the case weight for that data point, \nwith the initial size representing a weight of 1. In the fi rst decision stump, cases \nto the left of the split, represented", "re. The size of the symbol represents the case weight for that data point, \nwith the initial size representing a weight of 1. In the fi rst decision stump, cases \nto the left of the split, represented by the vertical dotted line, are predicted to \nbe light gray, and those to the right are predicted to be dark gray. Therefore, \nthere are three misclassifi ed data points (all light gray), and the remaining \ndata points, including all the dark gray points, are classifi ed correctly. These \nmisclassifi ed data points are given increased weight and the correctly classi-\nfi ed data points decreased weight, as shown in the upper right of the fi gure \nand labeled \u201cReweighting.\u201d The amount of the reweighting shown in Figure \n10-8 only represents the increase or decrease and should not be interpreted as \nshowing a precise reweighting.\nThe second decision stump is then computed based on the reweighted data, \nand the process of reweighting the data points is repeated, resulting in the plot \nto the", "d as \nshowing a precise reweighting.\nThe second decision stump is then computed based on the reweighted data, \nand the process of reweighting the data points is repeated, resulting in the plot \nto the right of the second stump split. After the third reweighting, you can see \nthat the light gray data point at the upper right has now been misclassifi ed three \ntimes and therefore has the largest weight of any data point. Two data points \nhave been classifi ed correctly three times, and their weights are the smallest; \nthey are at the top left (light gray) and bottom right dark gray. \nIf you examine all three splits from the three decision stumps, you can see in \nFigure 10-9 that there are six decision regions defi ned by the three splits. Each \nregion will have a different model score based on the weighted sum of the three \nmodels, with the region at the upper left having a probability of being light \ngray equal to 1 and the region to the lower right having a probability of being \nlight ", "on the weighted sum of the three \nmodels, with the region at the upper left having a probability of being light \ngray equal to 1 and the region to the lower right having a probability of being \nlight gray equal to 0. If additional iterations of the boosting algorithm were to \ntake place, the high weight of the light gray square at the upper right would \nundoubtedly result in a decision stump that separates this data point from the \nothers in the data to classify it correctly.\nAs a reminder, this process is done automatically in software implementations \nof boosting and does not have to be done by the modeler in a step-by-step man-\nner. For example, the C5 classifi cation tree algorithm has a boosting algorithm \nbuilt in, using a variant of the original AdaBoost algorithm.  \nBoosting methods are designed to work with weak learners, that is, simple \nmodels; the component models in a boosted ensemble are simple models with \nhigh bias, though low variance. The improvement with boosting is ", "s are designed to work with weak learners, that is, simple \nmodels; the component models in a boosted ensemble are simple models with \nhigh bias, though low variance. The improvement with boosting is better, as \n\n318 \nChapter 10 \u25a0 Model Ensembles\nwith Bagged models, when algorithms that are unstable predictors are used. \nDecision trees are most often used in boosted models. Na\u00efve Bayes is also used \nbut with fewer improvements over a single model.\nFirst Stump\nSecond Stump\nThird Stump\nReweighting\nReweighting\nReweighting\nFigure 10-8:  AdaBoost reweighting\nFigure 10-9:  AdaBoost decision regions for a simple example\n\n \nChapter 10 \u25a0 Model Ensembles \n319\nModel accuracy from boosting typically is better than single decision trees \nor even from bagging. Figure 10-10 shows the AUC for 30 individual trees, \naveraging predictions from 3 groups of 10 trees (non-overlapping), bagging, \nand AdaBoost built from 100 iterations. All results are from test data. In each \ntree, only LASTGIFT and RFA_2F w", "l trees, \naveraging predictions from 3 groups of 10 trees (non-overlapping), bagging, \nand AdaBoost built from 100 iterations. All results are from test data. In each \ntree, only LASTGIFT and RFA_2F were used as inputs. Clearly, AdaBoost has \nthe highest accuracy; this was true even with only 10 iterations. The box and \nwhiskers plot for the single models shows the minimum and maximum AUC \nvalues. Note that bagging has signifi cantly higher AUC than even the best single \nmodel, and AdaBoost signifi cantly higher than bagging.\nBoosted trees created from the same KDD Cup 1998 data as the results shown \nin Figures 10-6 have far simpler decision regions than individual trees and the \nbagged trees, even after 100 iterations. Figure 10-11 shows the decision regions \nfor the AdaBoost predictions. The regions from AdaBoost have smoother transi-\ntions between the decisions and the regions themselves are more homogeneous. \nAUC\n0.53\n0.52\n1 single model\n10 Bagged Trees\n30 Bagged Trees\nModel Group\n", " The regions from AdaBoost have smoother transi-\ntions between the decisions and the regions themselves are more homogeneous. \nAUC\n0.53\n0.52\n1 single model\n10 Bagged Trees\n30 Bagged Trees\nModel Group\nAdaboost,\n100 Iterations\n0.54\n0.55\n0.56\n0.57\n0.58\n0.59\n0.60\n0.61\nFigure 10-10:  Comparison of AUC for individual trees and ensembles\n\n320 \nChapter 10 \u25a0 Model Ensembles\n0\n5\n10\n15\n20\n0.501\n0.751\n1.001\n1.251\n1.501\n1.751\n2.001\n2.251\n2.501\n2.751\n3.001\n3.251\n3.501\n3.751\n4.001\n4.251\n4.499\n25\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\n80\n85\n90\n95\n100\nValues...\n1\n0\nX-Shape\nCircle\nShapes\nFigure 10-11:  Decision regions for the AdaBoost ensemble\nImprovements to Bagging and Boosting\nBagging and boosting were the first ensemble methods that appeared in \npredictive analytics software, primarily with decision tree algorithms. Since \ntheir introduction, many other approaches to building ensembles have been \ndeveloped and made available particularly in open source software. The three \nmost popular and successful approa", "thms. Since \ntheir introduction, many other approaches to building ensembles have been \ndeveloped and made available particularly in open source software. The three \nmost popular and successful approaches are random forests, stochastic gradient \nboosting, and heterogeneous ensembles.\nRandom Forests\nRandom Forests (RF) was introduced by Breiman in 2000 as a modifi cation to \nthe bagging algorithm. As with bagging, the algorithm begins with bootstrap \nsampled data, and one decision tree is built from each bootstrap sample. There \nis, however, an important twist to the RF algorithm: At each split in the tree, \nrather than considering all input variables as candidates, only a random subset \nof variables is considered. \nThe number of variables one considers is a parameter in the algorithm speci-\nfi cation, but the default is to use the square root of the total number of candidate \ninputs. For example, if there were 100 candidate inputs for the model, a random \n10 inputs are candidates for e", "-\nfi cation, but the default is to use the square root of the total number of candidate \ninputs. For example, if there were 100 candidate inputs for the model, a random \n10 inputs are candidates for each split. This also means that it is unlikely that \nthe same inputs will be available for splits at parent and children nodes in the \n\n \nChapter 10 \u25a0 Model Ensembles \n321\ntree, forcing the tree to fi nd alternate ways to maximize the accuracy of subse-\nquent splits in the tree. Therefore, there is a two-fold diversity in the trees. First, \ndiversity is encouraged by case weights: Some patterns are over-emphasized \nand others under-emphasized in each tree. Additionally, diversity is encouraged \nthrough input weighting (\u201con\u201d vs. \u201coff\u201d).\nRF models are typically more accurate than bagging and are often more \naccurate than AdaBoost. \nStochastic Gradient Boosting\nAdaBoost is only one of many boosting algorithms currently documented \nin the literature, though it and the C5 boosting algorithm are", " often more \naccurate than AdaBoost. \nStochastic Gradient Boosting\nAdaBoost is only one of many boosting algorithms currently documented \nin the literature, though it and the C5 boosting algorithm are most common \nin commercial predictive modeling software; dozens of boosting algorithms \ncan be found in the open source software packages. \nOne interesting boosting algorithm is the Stochastic Gradient Boosting (SGB) \nalgorithm created by Jerry Friedman of Stanford University. Friedman developed \nan algorithm published in 2001 that he called Multiple Additive Regression Trees \n(MART) and was later branded as TreeNet\u00ae by Salford Systems in its software \ntool. Like other boosting algorithms, the MART algorithm builds successive, \nsimple trees and combines them additively. Typically, the simple trees are more \nthan stumps and will contain up to six terminal nodes. \nAfter building the fi rst tree, errors (also called residuals) are computed. The \nsecond tree and all subsequent trees then use ", "trees are more \nthan stumps and will contain up to six terminal nodes. \nAfter building the fi rst tree, errors (also called residuals) are computed. The \nsecond tree and all subsequent trees then use residuals as the target variable. \nSubsequent trees identify patterns that relate inputs to small and large errors. \nPoor prediction of the errors results in large errors to predict in the next tree, \nand good predictions of the errors result in small errors to predict in the next \ntree. Typically, hundreds of trees are built and the fi nal predictions are an addi-\ntive combination of the predictions that is, interestingly, a piecewise constant \nmodel because each tree is itself a piecewise constant model. However, one \nrarely notices because typically hundreds of trees are included in the ensemble. \nThe TreeNet algorithm, an example of stochastic gradient boosting, has won \nmultiple data mining modeling competitions since its introduction and has \nproven to be an accurate predictor with t", "emble. \nThe TreeNet algorithm, an example of stochastic gradient boosting, has won \nmultiple data mining modeling competitions since its introduction and has \nproven to be an accurate predictor with the benefi t that very little data cleanup \nis needed for the trees before modeling. \nHeterogeneous Ensembles\nThe ensemble techniques described so far achieve variety through case weights, \nwhether by resampling (bootstrap sampling) or reweighting records based on \nprediction errors (boosting). These techniques use the same algorithm for every \nmodel, typically decision trees. However, variety can also be achieved through \nvarying the algorithm as well, creating a heterogeneous ensemble, comprised of \ntwo or more different algorithms. \n\n322 \nChapter 10 \u25a0 Model Ensembles\nAs you saw in Chapter 8, algorithms sometimes achieve the same accu-\nracy on data sets in different ways, disagreeing on individual records even \nwhile, on average, having the same accuracy. We observed bagged trees where \nt", " algorithms sometimes achieve the same accu-\nracy on data sets in different ways, disagreeing on individual records even \nwhile, on average, having the same accuracy. We observed bagged trees where \nthe correlation between model predictions ranged from 0.212 to 0.431 even \nthough they had the same accuracy.  \nHeterogeneous ensembles are combined similarly to bagged trees, most often \nthrough simple averaging or voting. However, one can also experiment with \nother ways to combine the models, such as selecting the maximum probability \nof the models or weighting the predictions by model accuracy (the better models \ncontribute more to the weighted average). \nIn a simple example using data known as the glass identification data \nset, Figure 10-12 shows minimum, maximum, and average error rates for six \ndifferent algorithms on validation data. The best single model on validation \ndata had a 28.1 percent error rate, the worst a 37.5 percent error rate, and the \naverage error rate is 30.8 perc", "for six \ndifferent algorithms on validation data. The best single model on validation \ndata had a 28.1 percent error rate, the worst a 37.5 percent error rate, and the \naverage error rate is 30.8 percent. \nThe fi gure shows all possible ensembles built from these six models: all \npossible two-way combinations, three-way combinations, four-way, fi ve-way, and \nthe single six-way combination. Note that the average error rates on validation \ndata decreases as more models are included in the ensembles. In fact, the worst \nthree-model ensemble is better than the average single model. The six-model \nensemble is among the best of all the ensembles, but it is not the best possible; \nyou never know which particular combination will be the best on validation data.\nEnsembles are described as methods that increase accuracy. Based on Figure \n10-12, it is clear that ensembles also reduce the risk of deploying a poor model. \n1\n0%\n5%\n10%\n15%\n20%\n25%\n30%\n35%\n40%\n2\n3\n4\nNumber Models Combined\nPercent Cla", "that increase accuracy. Based on Figure \n10-12, it is clear that ensembles also reduce the risk of deploying a poor model. \n1\n0%\n5%\n10%\n15%\n20%\n25%\n30%\n35%\n40%\n2\n3\n4\nNumber Models Combined\nPercent Classi\ufb01cation Error\n5\n6\nMax Error\nMin Error\nAverage Error\nFigure 10-12:  Heterogeneous ensemble example\n\n \nChapter 10 \u25a0 Model Ensembles \n323\nModel Ensembles and Occam\u2019s Razor\nOccam\u2019s razor is a core principle many predictive modelers use. The idea is that \nsimpler models are more likely to generalize better, so it is better to regular-\nize complexity, or in other words, simplify the models so that the inclusion of \neach term, coeffi cient, or split in a model is justifi ed by its reducing the error \nsuffi ciently to justify its inclusion. One way to quantify the relationship between \naccuracy and complexity is taken from information theory in the form of infor-\nmation theoretic criteria, such as the Akaike information criterion (AIC), the \nBayesian information criterion (BIC), and Minimum Des", " complexity is taken from information theory in the form of infor-\nmation theoretic criteria, such as the Akaike information criterion (AIC), the \nBayesian information criterion (BIC), and Minimum Description Length (MDL). \nStatisticians sometimes use these criteria for model selection during stepwise \nlinear regression, for example. Information theoretic criteria require a reduction \nin model error to justify additional model complexity.\nDo model ensembles violate Occam\u2019s razor? Ensembles, after all, are much \nmore complex than single models. Random Forests and Boosted Trees could \nhave hundreds of models combined into the fi nal prediction. Neural network \nensembles could contain dozens or hundreds of networks, each of which contain \nhundreds to thousands of weights. If the ensemble accuracy is better on held-\nout data than single models, yet these models are far more complex than single \nmodels, do we need another paradigm to help us choose which models will \ngeneralize well? The an", "racy is better on held-\nout data than single models, yet these models are far more complex than single \nmodels, do we need another paradigm to help us choose which models will \ngeneralize well? The answer is \u201cno\u201d as long as we think about the complexity \nof a model in different terms.\nJohn Elder describes the concept of Generalized Degrees of Freedom (GDF) \nthat measures the behavior of models, or as he writes, the fl exibility of a modeling \nprocess. The complexity of linear regression models increases linearly with the \nnumber of terms in the model. GDF confi rms this as well: A linear increase in \ncoeffi cients in a model produces a linear increase in the complexity of behavior \nin the model as well. However, this is not the case for all algorithms. Elder shows \nan example where Bagged trees built from 50 bootstrap samples have a lower \nGDF measure than a single tree. Bagging, in fact, smoothes the predictions as \na part of the averaging of predicted probabilities, making the model\u2019", "trees built from 50 bootstrap samples have a lower \nGDF measure than a single tree. Bagging, in fact, smoothes the predictions as \na part of the averaging of predicted probabilities, making the model\u2019s behavior \nsmoother and therefore simpler. \nWe should not fear that adding computational complexity (more terms, splits, \nor weights) will necessarily increase the behavioral complexity of models. In fact, \nsometimes the ensemble will signifi cantly reduce the behavioral complexity. \nInterpreting Model Ensembles\nThe interpretation of ensembles can become quite diffi cult. If you build an RF \nensemble containing 200 trees, how do you describe why a prediction has a \nparticular value? You can examine each of the trees individually, though this \nis clearly not practical. For this reason, ensembles are often considered black \n\n324 \nChapter 10 \u25a0 Model Ensembles\nbox models, meaning that what they do is not transparent to the modeler or \ndomain expert. \nNevertheless, one way to determine which i", "es are often considered black \n\n324 \nChapter 10 \u25a0 Model Ensembles\nbox models, meaning that what they do is not transparent to the modeler or \ndomain expert. \nNevertheless, one way to determine which inputs to the model are most \nimportant is to perform a sensitivity analysis much like one does with neural \nnetworks: When inputs are changed by some deviation from the mean, and all \nother inputs are held constant, how much does output probability change? One \ncan then plot the sensitivity of the change versus the value to visualize the sen-\nsitivity, seeing if the relationship between the input values and the predictions \nare linear or nonlinear, or if they are monotonic or non-monotonic. \nA simple sensitivity analysis can be achieved by building single-split decision \ntrees using an algorithm that allows for multi-way splits (such as CHAID) for \neach variable. All the variables can be scored, and the variable with the highest \nchi-square statistic can be considered the most important si", "hm that allows for multi-way splits (such as CHAID) for \neach variable. All the variables can be scored, and the variable with the highest \nchi-square statistic can be considered the most important single variable. As an \nalternative, if the inputs to the model are continuous variables, an ANOVA can \nbe computed with the predicted target variable class as the grouping variable.\nConsider the KDD Cup 98 data set, this time including 12 candidate input \nvariables to the ensemble models. Table 10-2 shows the ranking of the 12 vari-\nables based on an ANOVA, where the F statistic is the test of signifi cance. The F \nstatistic is computed on the test data set with 47,706 records. All of the variables \nhave statistically signifi cant differences in mean values, largely because of the \nlarge number of records in the test set. RFA_2F is by far the best predictor, its F \nstatistic being more than seven times larger than the second variable. This gives \nyou a sense for which variables are the larg", " of records in the test set. RFA_2F is by far the best predictor, its F \nstatistic being more than seven times larger than the second variable. This gives \nyou a sense for which variables are the largest contributors to the ensemble model. \nTable 10-2: AdaBoost Variable Ranking According to ANOVA (F Statistic)\nVARIABLE\nF STATISTIC, ADABOOST\nP VALUE, ADABOOST\nRANK, ADABOOST\nRFA_2F\n53,560.3 \n0.0\n1\nNGIFTALL\n \n 7,862.7 \n0.0\n2\nMINRAMNT\n 6,400.0 \n0.0\n3\nLASTGIFT\n \n  5,587.6 \n0.0\n4\nFISTDATE\n 3,264.9 \n0.0\n5\nNUMPROM\n 2,776.7 \n0.0\n6\nMAXRAMNT\n \n  2,103.0 \n0.0\n7\nRAMNTALL\n  \n      621.4 \n0.0\n8\nNUMPRM12\n  \n     343.8 \n0.0\n9\nHIT\n   \n    60.9 \n0.0\n10\nWEALTH1\n   \n    60.0 \n0.0\n11\nAGE\n   \n     36.5 \n0.0\n12\n\n \nChapter 10 \u25a0 Model Ensembles \n325\nA histogram of the top predictor, RFA_2F, color coded by the predicted tar-\nget variable from the AdaBoost ensemble is shown in Figure 10-13. The fi gure \nshows the strong relationship between RFA_2F and the predictions; if RFA_2F \nis 4, the vast majority of predict", "cted tar-\nget variable from the AdaBoost ensemble is shown in Figure 10-13. The fi gure \nshows the strong relationship between RFA_2F and the predictions; if RFA_2F \nis 4, the vast majority of predicted values are 1 (light gray). \n1\n0\n83\n166\n249\n332\n415\n498\n581\n664\n747\n830\n917\n2\n3\n4\nFigure 10-13:  RFA_2F histogram with AdaBoost ensemble overlay\nIf one computes an ANOVA on testing data for Random Forests, AdaBoost, \nand bagging models, the resulting rankings in Table 10-3. There is great \nconsistency in the story told by all three ensembles: RFA_2F is the top predictor, \nand the top six predictors are consistently top predictors. \nTable 10-3: Comparison of Variable Ranking for Three Ensemble Methods by ANOVA (F Statistic)\nVARIABLE\nRANK, RANDOM FORESTS\nRANK, ADABOOST\nRANK, BAGGING\nRFA_2F\n1\n1\n1\nNGIFTALL\n2\n2\n3\nMINRAMNT\n3\n3\n4\nLASTGIFT\n5\n4\n2\nNUMPROM\n4\n6\n6\nFISTDATE\n6\n5\n7\nMAXRAMNT\n8\n7\n5\nNUMPRM12\n7\n9\n8\nRAMNTALL\n9\n8\n11\nAGE\n10\n12\n9\nHIT\n11\n10\n12\nWEALTH1\n12\n11\n10\n\n326 \nChapter 10 \u25a0 Model Ensembles\n", "\n1\nNGIFTALL\n2\n2\n3\nMINRAMNT\n3\n3\n4\nLASTGIFT\n5\n4\n2\nNUMPROM\n4\n6\n6\nFISTDATE\n6\n5\n7\nMAXRAMNT\n8\n7\n5\nNUMPRM12\n7\n9\n8\nRAMNTALL\n9\n8\n11\nAGE\n10\n12\n9\nHIT\n11\n10\n12\nWEALTH1\n12\n11\n10\n\n326 \nChapter 10 \u25a0 Model Ensembles\nSummary\nModel ensembles are a new frontier for predictive modelers who are interested \nin accuracy, either by reducing errors in the models or by reducing the risk that \nthe models will behave erratically. Evidence for this is clear from the dominance \nof ensembles in predictive analytics and data mining competitions: Ensembles \nalways win.\nThe good news for predictive modelers is that many techniques for build-\ning ensembles are built into software already. The most popular ensemble \nalgorithms (bagging, boosting, and random forests) are available in nearly every \ncommercial or open source software tool. Building customized ensembles is \nalso supported in many software products, whether based on a single algorithm \nor through a heterogeneous ensemble. \nEnsembles aren\u2019t appropriate for eve", "ftware tool. Building customized ensembles is \nalso supported in many software products, whether based on a single algorithm \nor through a heterogeneous ensemble. \nEnsembles aren\u2019t appropriate for every solution\u2014the applicability of \nensembles is determined by the modeling objectives defi ned during Business \nUnderstanding\u2014but they should be in every predictive modeler\u2019s toolkit.\n\n327\nIn the early 2000s, I was a consultant on a contract that was analyzing maintenance \ndata for the Harrier aircraft. The business objective of the modeling project was \nto identify pre-cursor events that led to more expensive maintenance actions so \nthat proactive steps could be taken before expensive engine failures occurred. \nData was collected in two forms. During each fl ight, data was collected from a \nvariety of sensors and included alerts that indicated potential problems during \nthe fl ight, so the form of this data was time series with thousands of data points \nper aircraft per fl ight. A unique r", "riety of sensors and included alerts that indicated potential problems during \nthe fl ight, so the form of this data was time series with thousands of data points \nper aircraft per fl ight. A unique record in this data was a distinct fl ight-date-\ntime triplet. In addition, historical maintenance actions for each aircraft were \nstored in a database, including the part used to complete the maintenance action.\nThe data was transformed so each record represented a maintenance event, \nincluding features of the most recent in-fl ight data (thousands of time-series \ndata points fl attened into a single record), recent maintenance actions (prior \nto the current maintenance action), and the most recent maintenance action \ntaken. Each aircraft could have several entries in the data set: one record per \nfl ight the aircraft took.\nThe models were moderately effective, but something clearly was missing. There \nwas considerable \u201cnoise\u201d with maintenance actions for several reasons. First, the \nappli", "per \nfl ight the aircraft took.\nThe models were moderately effective, but something clearly was missing. There \nwas considerable \u201cnoise\u201d with maintenance actions for several reasons. First, the \napplication of a maintenance action didn\u2019t necessarily mean the maintenance \naction was needed. In fact, for some mechanics, there were clear favorite actions; \nC H A P T E R \n11\nText Mining\n\n328 \nChapter 11 \u25a0 Text Mining\nthey applied particular maintenance actions well in excess of norms. Second, \njust because a maintenance action wasn\u2019t done doesn\u2019t mean it wasn\u2019t needed.\nHowever, there was another source of data that, according to domain experts, \nwould have reduced the noise considerably and improved the target variable. \nThese were logs the mechanics kept at the depot. These books, sometimes just \nhand-written notes, contained explanations for the maintenance actions and \nother. In addition, they contained \u201cwatch\u201d items that could provide clues to \nfuture maintenance actions the aircraft w", "ust \nhand-written notes, contained explanations for the maintenance actions and \nother. In addition, they contained \u201cwatch\u201d items that could provide clues to \nfuture maintenance actions the aircraft would need. Unfortunately, these notes \nwere not included in the database, and even if they were, no processes were put \nin place to extract the key information from the text. \nThis story is typical of many industries: Key data is either not in the trans-\nactional data stores, or is in the form of free text that cannot be used directly \nin predictive modeling.\nMotivation for Text Mining \nThe value of predictive analytics is limited by the data that is used in the analysis. \nThus far in the chapters of this book, whenever data is described, it is numeric \nor text stored in a database or in a fl at fi le. However the data is stored, it is \ntransformed, through queries or ETL operations, into rectangular data: Each \nrecord contains the exact same number of rows. \nText mining is not yet a part ", "at fi le. However the data is stored, it is \ntransformed, through queries or ETL operations, into rectangular data: Each \nrecord contains the exact same number of rows. \nText mining is not yet a part of mainstream predictive analytics, though it is \non the short list for many organizations. But text mining is diffi cult, requiring \nadditional expertise and processing complementary to predictive modeling, \nbut not often taught in machine learning or statistics courses. \nNevertheless, text mining is being used increasingly as organizations recog-\nnize the untapped information contained in text. Social media, such as Twitter \nand Facebook, have been used effectively by organizations to uncover positive \nand negative trends that, when identifi ed through text mining, can be used to \nleverage the positive trends and provide corrective action to counteract any \nnegative comments. \nPerhaps the most recent powerful example of text mining is the stunning \nsuccess of IBM\u2019s Watson computer that c", "he positive trends and provide corrective action to counteract any \nnegative comments. \nPerhaps the most recent powerful example of text mining is the stunning \nsuccess of IBM\u2019s Watson computer that combines text mining with predictive \nmodeling and artifi cial intelligence techniques. Its fi rst success on the television \nshow Jeopardy!, while entertaining to watch, only scratches the surface of its \npotential in other fi elds such as healthcare analytics. \nSurveys frequently contain free-form text fi elds to capture opinions not easily \nquantifi ed as survey questions, but containing key views and explanations for \nwhy responders selected particular response values. News events, if captured \nin real time, help predict stock trends. Specifi cs of help-desk calls can help \norganizations identify immediately and automatically how to route the call to \nprovide the best customer experience. \n\n \nChapter 11 \u25a0 Text Mining \n329\nA Predictive Modeling Approach to Text Mining\nText mining can unc", "entify immediately and automatically how to route the call to \nprovide the best customer experience. \n\n \nChapter 11 \u25a0 Text Mining \n329\nA Predictive Modeling Approach to Text Mining\nText mining can uncover insights into data that extend well beyond information \nin structured databases. Processing steps that interpret the text to uncover these \npatterns and insights can range from simple extraction of words and phrases \nto a more complex linguistic understanding of what the data means. The for-\nmer treats words and phrases as dumb patterns, requiring minimal linguistic \nunderstanding of the data. This approach can apply, with minor modifi cations, \nto most languages. The latter requires extensive understanding of the language, \nincluding sentence structure, grammar, connotations, slang, and context. \nThis chapter takes the pattern extraction approach to text mining rather than \na natural language processing (NLP) approach. In the former, the purpose of \ntext mining is to convert unstruct", "context. \nThis chapter takes the pattern extraction approach to text mining rather than \na natural language processing (NLP) approach. In the former, the purpose of \ntext mining is to convert unstructured data to structured data, which then \ncan be used in predictive modeling. The meaning of the text, while helpful for \ninterpreting the models, is not of primary concern for creating the features. \nWhile text mining of patterns is simpler, there is still considerable complexity \nin the text processing and the features can provide signifi cant improvements \nin modeling accuracy. \nStructured vs. Unstructured Data\n\u201cStructured data\u201d is a label for the data that has been used throughout the chapters \nof this book. It is data that can, at least conceptually, be loaded into a spreadsheet. \nThere are rows and columns in the data, where each row represents the unit of \nanalysis for modeling, such as a customer, a business, a credit-card transaction, \nor a visit by a customer. The columns provide", "re rows and columns in the data, where each row represents the unit of \nanalysis for modeling, such as a customer, a business, a credit-card transaction, \nor a visit by a customer. The columns provide measurements, descriptions, and \nattributes related to the unit of analysis. Each cell in structured data is either \nfi lled or could be fi lled. The same number of columns exists for every row and \nthe defi nition of the cell in a column is consistent for all rows.\nStructured data often comes from databases, whether transactional, NoSQL, \nor ad hoc data stores. The data used for modeling is often created by combining \ntables from multiple data stores into a single view, table, document, or data fi le. \nStructured data is predictive modeling\u2013friendly because data in this form can \nbe loaded into any predictive analytics software package in a straightforward \nmanner. All of the data described so far in this book is structured data.\nUnstructured data is, in many ways an unfortunate label as", "nto any predictive analytics software package in a straightforward \nmanner. All of the data described so far in this book is structured data.\nUnstructured data is, in many ways an unfortunate label as there is almost \nalways structure to \u201cunstructured data.\u201d The layout of unstructured data is \nusually signifi cantly different than structured data. Rows may contain only a \nsingle text chunk, or there may be a variable number of attributes in a single row. \nEven the concept of a row may vary within a single fi le or document. Columns \ndon\u2019t exist naturally but could be generated through feature extraction. The \n\n330 \nChapter 11 \u25a0 Text Mining\nproblem of a variable number of columns could be exasperating because of the \nvariety of information in the unstructured data.\nSome examples of unstructured data are listed in Table 11-1.\nTable 11-1: Short List of Unstructured Data\nDATA \nDESCRIPTION\nWHAT MAKES IT UNSTRUCTURED\nMicrosoft Word, \nAdobe PDF \ufb01 les\nRows are not a unit of analysis, no column", "ured data are listed in Table 11-1.\nTable 11-1: Short List of Unstructured Data\nDATA \nDESCRIPTION\nWHAT MAKES IT UNSTRUCTURED\nMicrosoft Word, \nAdobe PDF \ufb01 les\nRows are not a unit of analysis, no columns in the data. Signi\ufb01 cant \ninvisible characters without information valuable for text mining.\nPowerpoint \ndocuments\nSlides are not rows. Metadata contains formatting information that \nusually is not interesting for text mining. \nHTML, XML\nRows are not necessarily a unit of analysis. Tags provide structure \n(semi-structured data), but should be stripped out before extracting \ntext patterns; not all information within tags is interesting for model-\ning. No columns in data.\nE-mail\nUnit of analysis could be an e-mail or a recipient. Data is semi-struc-\ntured, more valuable to group features (subject, body, signature, and \nso on).\nImages, video\nRows and columns have meaning within a single instance, not usually \nas a description of multiple images or videos.\nAudio/Music\nUnit of analysis could ", "ect, body, signature, and \nso on).\nImages, video\nRows and columns have meaning within a single instance, not usually \nas a description of multiple images or videos.\nAudio/Music\nUnit of analysis could be a song or a piece of a song. Columns would \nhave to be generated from features of the music, like time series \nanalysis.\nNew stories\nLength of article and subject matter varies. \nAs you can see, the idea of \u201csemi-structured\u201d data, sometimes referred to as \nweakly unstructured data, is mentioned a few times in the table. This type of \ndata has some cues to inform the analyst of the nature of text in the section, \nbut provides only a modest amount of help to the analyst with feature creation.\nWhy Text Mining Is Hard\nText mining is generally much more diffi cult than typical predictive modeling \nproblems. Problems include handling the variety of data, converting data from \nfree-form text into structured data through feature creation, and managing the \nvast number of features that can (and ", "g \nproblems. Problems include handling the variety of data, converting data from \nfree-form text into structured data through feature creation, and managing the \nvast number of features that can (and often are) created. But at the core, the most \ndiffi cult part of text mining is the mode of data itself: language. The discussion \nhere focuses on English, but most of the principles also apply to other languages. \nFirst, language is ambiguous and context is often required to understand the \nmeaning of words and phrases. Consider the two sentences:\n\n \nChapter 11 \u25a0 Text Mining \n331\nThe beam could bear more weight; we could both walk on it safely.\nThe heavy bear could still walk in the forest without a sound. \nIn the fi rst sentence, bear is a verb, meaning \u201cto support or carry,\u201d whereas \nin the second sentence it is a noun, meaning \u201ca large animal . . .\u201d Obviously, the \nsentences have very different meanings even though they share this key word. \nParts of speech could differentiate these u", "e second sentence it is a noun, meaning \u201ca large animal . . .\u201d Obviously, the \nsentences have very different meanings even though they share this key word. \nParts of speech could differentiate these uses of the same word, though part of \nspeech is not always enough.\nTypes of ambiguity are homonomy, synonymy, polysemy, and hyponymy. \nWith homonomy, the word is the same but the meaning is different through the \nevolution of the words through history. For example, consider two different \nmeanings for the same word \u201cbank\u201d:\nCindy jogged along the bank of the river.\nCityBank is the oldest bank in the city.\nIn the fi rst case, \u201cbank\u201d clearly has different meanings even though the part \nof speech is the same. In one case, it refers to a geographical description, in the \nother a specifi c fi nancial institution.\nWith polysemy, the same word is used but has different, albeit related, meanings. \nThe Central Bank raised its prime interest rates yesterday.\nThe new coffee shop was built right next t", "l institution.\nWith polysemy, the same word is used but has different, albeit related, meanings. \nThe Central Bank raised its prime interest rates yesterday.\nThe new coffee shop was built right next to the old bank.\nThe fi rst bank was established in Venice in 1857.\nIn all three cases, the concept of the bank as a fi nancial institution is in mind. \nHowever, in the fi rst case, \u201cbank\u201d is a broad concept, referring to an overseeing \nentity. In the second case, \u201cbank\u201d refers to a specifi c company, operating in the \ncity. In the third case, \u201cbank\u201d is a historical idea; while referring to a specifi c \nbuilding, it refers to more than just that bank; it implies a continuation of the \nhistory of the banking industry from that time to the present. \nSynonomy refers to different words that have similar meetings, sometimes \ninterchangeable meanings. For example, in these two sentences, \u201clarge\u201d and \n\u201cbig\u201d are used interchangeably:\nThe pile of trash was very large.\nThe pile of trash was very big.", "milar meetings, sometimes \ninterchangeable meanings. For example, in these two sentences, \u201clarge\u201d and \n\u201cbig\u201d are used interchangeably:\nThe pile of trash was very large.\nThe pile of trash was very big.\nYou must be careful of connotations, however, as sometimes the meanings \ncan be different enough to convey a different kind of meaning:\nBeth, now 25 years old, became a kind of big sister to Jenny.\nBeth, now 25 years old, became a kind of large sister to Jenny.\nObviously in the fi rst case, the connotation is positive, with Beth acting in a \ngenerous way. In the second case, the connotation is negative referring indirectly \n\n332 \nChapter 11 \u25a0 Text Mining\nto Beth\u2019s weight rather than her actions. This is only because of the use of these \nwords colloquially in English.\nA fourth source of ambiguity in language comes from hyponymy, a concept \nhierarchy that exists in words, where one word is a subset or subclass of another. \nDogs and cats are subclasses of animals, or broken legs and contusio", "ity in language comes from hyponymy, a concept \nhierarchy that exists in words, where one word is a subset or subclass of another. \nDogs and cats are subclasses of animals, or broken legs and contusions are \nsubclasses of injuries. Depending on the purpose of the analysis, the broader \nconcept (animal or injury) may be suffi cient or not specifi c enough. \nA fi fth type of ambiguity comes from spelling variants or misspellings of \nwords. Misspellings are sometimes obvious and easily corrected, though some-\ntimes the intended word is not clear except from the context. Spelling variants \nbecome similar to synonyms for the analyst: two different patterns that should \nbe interpreted the same way. Some variants are differences between American \nEnglish and British English, such as the pairs color and colour, favorite and \nfavourite, labeled and labelled, or theater and theatre. There are still some vari-\nants even within American English, such as donut and doughnut, gray and grey, \nor worsh", "or and colour, favorite and \nfavourite, labeled and labelled, or theater and theatre. There are still some vari-\nants even within American English, such as donut and doughnut, gray and grey, \nor worshiped and worshipped. \nA sixth type of ambiguity comes from abbreviations and acronyms: \nRenders search engines, SQL queries, Regex, . . .  ineffective\nText Mining Applications\nPredictive analytics has four kinds of analysis\u2014classifi cation, regression, clus-\ntering and rule mining\u2014which can be grouped into the two broader categories, \nsupervised and unsupervised learning. Text mining, in contrast, is often character-\nized as having seven or more types of analysis, fi ve of which are enumerated in \nTable 11-2. These are the analyses that predictive modelers are most likely to do. \nTable 11-2: Text Mining Applications\nTEXT MINING \nAPPLICATION\nDESCRIPTION\nInformation retrieval\nMatch documents from a list of keywords. Examples include web-\nsite searches, patent searches, and predictive modelin", "ext Mining Applications\nTEXT MINING \nAPPLICATION\nDESCRIPTION\nInformation retrieval\nMatch documents from a list of keywords. Examples include web-\nsite searches, patent searches, and predictive modeling confer-\nence abstracts.\nDocument clustering\nGroup documents based on keywords and phrases from each \ndocument.\nDocument \nclassi\ufb01 cation\nLabel documents from keywords and phrases for each document.\n\n \nChapter 11 \u25a0 Text Mining \n333\nTEXT MINING \nAPPLICATION\nDESCRIPTION\nSentiment analysis\nSpecial case of document classi\ufb01 cation. Predict positive, negative, \nor neutral sentiment of tweets, comments, or customer reviews \nbased on keywords and phrases.\nInformation \nextraction\nExtract and summarize keywords and phrases that match docu-\nment, for example, extracting courses taken and degrees from \nresumes.\nData Sources for Text Mining\nData containing text for predictive modeling can include a wide variety of \nsources. Text mining software can typically load each of these types of data:\n \n\u25a0Web pag", "sumes.\nData Sources for Text Mining\nData containing text for predictive modeling can include a wide variety of \nsources. Text mining software can typically load each of these types of data:\n \n\u25a0Web page scraping: An algorithm crawls through a website. Each web \npage is a document. Usually, metadata and tags are stripped out.\n \n\u25a0Twitter/Facebook feeds: Usually accessed via an API\n \n\u25a0Folders of text fi les: Each folder stores a particular class value for docu-\nment classifi cation or sentiment analysis. Document types can be plain \ntext, PDF, or Word documents.\n \n\u25a0Text fi elds within structured data: Free-form text fi eld within a structured \ndata set (for example, comment fi elds in a survey)\nLanguages common to predictive modelers, such as R and Python, have APIs \nthat can import these and a wide variety of additional data for text mining.\nData Preparation Steps\nTypical data preparation steps are shown in Figure 11-1. Not all of these steps \nnecessarily must be undertaken depending on t", "a wide variety of additional data for text mining.\nData Preparation Steps\nTypical data preparation steps are shown in Figure 11-1. Not all of these steps \nnecessarily must be undertaken depending on the specifi c application. \nPOS Tagging\nAfter any initial cleaning of the text, such as correcting misspellings, a Part of \nSpeech (POS) tagger is often invoked. Eighty-nine percent of English words \nhave only one part of speech, and therefore labeling POS doesn\u2019t provide any \nadditional information. However, as noted already, many words are ambiguous \nand POS tagging provides one method of disambiguating the text. In addition, \n\n334 \nChapter 11 \u25a0 Text Mining\nit can help recognize and group nouns or noun phrases, often a key part of a \ntext mining analysis.\nLoad Data\nFilter\nPunctuation\nTokens\nFilter Stop\nWords\nFilter\nNumbers\nStemming\nFilter Short\nWords\nPOS Tagging\nBag of Words\n(Create Tokens)\nFigure 11-1:  Typical text mining preprocessing steps\nA minimum set of POS includes nouns, verbs, a", "ilter Stop\nWords\nFilter\nNumbers\nStemming\nFilter Short\nWords\nPOS Tagging\nBag of Words\n(Create Tokens)\nFigure 11-1:  Typical text mining preprocessing steps\nA minimum set of POS includes nouns, verbs, adjectives, and adverbs. A \nslightly expanded list will sometimes include prepositions and conjunctions. \nSome lists contain more than 100 parts of speech. The most popular list comes \nfrom the Penn Treebank, containing the 36 parts of speech shown in Table 11-3.\nTable 11-3: Penn Treebank Parts of Speech (Word Level)\nTAG\nDESCRIPTION\nTAG\nDESCRIPTION\nCC\nCoordinating conjunction\nPRP$\nPossessive pronoun (prolog version \nPRP-S)\nCD\nCardinal number\nRB\nAdverb\nDT\nDeterminer\nRBR\nAdverb, comparative\nEX\nExistential there\nRBS\nAdverb, superlative\nFW\nForeign word\nRP\nParticle\nIN\nPreposition or subordinat-\ning conjunction\nSYM\nSymbol\nJJ\nAdjective\nTO\nto\nJJR\nAdjective, comparative\nUH\nInterjection\nJJS\nAdjective, superlative\nVB\nVerb, base form\nLS\nList item marker\nVBD\nVerb, past tense\nMD\nModal\nVBG\nVerb, gerund or", "conjunction\nSYM\nSymbol\nJJ\nAdjective\nTO\nto\nJJR\nAdjective, comparative\nUH\nInterjection\nJJS\nAdjective, superlative\nVB\nVerb, base form\nLS\nList item marker\nVBD\nVerb, past tense\nMD\nModal\nVBG\nVerb, gerund or present participle\nNN\nNoun, singular or mass\nVBN\nVerb, past participle\nNNS\nNoun, plural\nVBP\nVerb, non\u2013third person singular \npresent\n\n \nChapter 11 \u25a0 Text Mining \n335\nNNP\nProper noun, singular\nVBZ\nVerb, third person singular present\nNNPS\nProper noun, plural\nWDT\nWh-determiner\nPDT\nPredeterminer\nWP\nWh-pronoun\nPOS\nPossessive ending\nWP$\nPossessive wh-pronoun (prolog \nversion WP-S)\nPRP\nPersonal pronoun\nWRB\nWh-adverb\nText mining software typically has a POS tagger; rule-based taggers, such \nas the Brill POS tagger, are almost always available. Stochastic taggers are also \ncommon and can even be developed by predictive modelers if the built-in tag-\ngers are not suffi cient. Consider the following sentence with parts of speech \nlabeled above each word. All of the words are straightforward to tag ex", "oped by predictive modelers if the built-in tag-\ngers are not suffi cient. Consider the following sentence with parts of speech \nlabeled above each word. All of the words are straightforward to tag except for \nthe word \u201crace.\u201d In the fi rst tagged sentence, race is labeled a verb (VB) and in \nthe second sentence a noun (NN). Which is correct?\nNNP        VBZ    VBN   TO VB   NR\nSecretariat is expected to race tomorrow\nNNP        VBZ    VBN   TO NN  NR\nSecretariat is expected to race tomorrow\nA stochastic POS tagger uses a corpus of documents already tagged, such as \nthe Wall Street Journal corpus, to compute probabilities of pairs of words, such \nas the likelihood a verb (POS VB) follows the word \u201cto\u201d (POS TO) and the likeli-\nhood a noun (POS NN) follows TO. You might fi nd the following probabilities \nand conclude that the most likely POS for race is VB:\nP(NN|TO) = 0.00047\nP(VB|TO) = 0.83\nWhen tagging a larger block of text, the tags are sometimes shown within the \ntext. Consider the f", "obabilities \nand conclude that the most likely POS for race is VB:\nP(NN|TO) = 0.00047\nP(VB|TO) = 0.83\nWhen tagging a larger block of text, the tags are sometimes shown within the \ntext. Consider the following text posted in a predictive analytics blog: \nIn this talk, Mr. Pole discussed how Target was using Predictive Analytics includ-\ning descriptions of using potential value models, coupon models, and . . . yes . . . \npredicting when a woman is due (if you aren\u2019t the patient type, it is at about 34:30 \nin the video). \nThese models were very profi table at Target, adding an additional 30% to the \nnumber of women suspected or known to be pregnant over those already known \nto be (via self-disclosure or baby registries). \nThe tagged version appears like this:\nIn/IN this/DT talk/NN ,/, Mr./NNP Pole/NNP discussed/VBD how/WRB \nTarget/NNP was/VBD using/VBG Predictive/NNP Analytics/NNP including/\n\n336 \nChapter 11 \u25a0 Text Mining\nVBG descriptions/NNS of/IN using/VBG potential/JJ value/NN models/N", " Pole/NNP discussed/VBD how/WRB \nTarget/NNP was/VBD using/VBG Predictive/NNP Analytics/NNP including/\n\n336 \nChapter 11 \u25a0 Text Mining\nVBG descriptions/NNS of/IN using/VBG potential/JJ value/NN models/NNS ,/, \ncoupon/NN models/NNS ,/, and . . . yes . . . predicting/VBG when/WRB a/DT \nwoman/NN is/VBZ due/JJ (/( if/IN you/PRP are/VBP n\u2019t/RB the/DT patient/JJ \ntype/NN ,/, it/PRP is/VBZ at/IN about/IN 34:30/CD in/IN the/DT video/JJ )/) ./. \nThese/DT models/NNS were/VBD very/RB profi table/JJ at/IN Target/NNP ,/, \nadding/VBG an/DT additional/JJ 30/CD %/SYM to/TO the/DT number/NN of/\nIN women/NN suspected/VBD or/CC known/VBN to/TO be/VB pregnant/JJ \nover/IN those/DT already/RB known/VBN to/TO be/VB (/( via/IN self-disclosure/\nJJ or/CC baby/NN registries/NNS )/) ./. \nOften, the tagged version of the text is not exposed in text mining software \nbut is tracked for use once the tokens are created.\nTokens\nTokenization is a process that takes a stream of characters and splits them into \nindividual t", "e text is not exposed in text mining software \nbut is tracked for use once the tokens are created.\nTokens\nTokenization is a process that takes a stream of characters and splits them into \nindividual tokens, most often words. In English, the most obvious clues for \nhow to create tokens come from characters that split words, such as spaces and \npunctuation. The creation of tokens from streams of text is sometimes called \ncreating a bag of words, or BoW.\nWhite space does not guarantee the separation of tokens, such as in Asiatic \nlanguages (Chinese, Japanese, Korean) and German (compound nouns) where \nseveral tokens comprise a single character or word, and therefore these algo-\nrithms are language specifi c. Tokenization algorithms, therefore, are specifi c \nto a language \nStop Word and Punctuation Filters\nStop words are words that are uninteresting for the analysis. They are removed \nbecause they may occur so often that they don\u2019t differentiate documents or \nsentiment. Removing stop word", "uation Filters\nStop words are words that are uninteresting for the analysis. They are removed \nbecause they may occur so often that they don\u2019t differentiate documents or \nsentiment. Removing stop words is done primarily to reduce processing time \nand dimensionality of the tokens to be processed. A typical list of typical stop \nwords is:\na, an, and, are, as, at, be, but, by, for, if, in, into, is, it, no, not, of, on, or, \nsuch, that, the, their, then, there, these, they, this, to, was, will, with\nCare must be taken in determining which words are uninteresting prior to \nanalysis. Common stop words such as \u201cthe\u201d may be essential in identifying \nparticular names such as \u201cThe Who\u201d or identifying superlatives such as \u201cthe \nworst\u201d or \u201cthe best.\u201d \nPunctuation fi lters remove the punctuation tokens from the bag of words. \nUsually, though not always, punctuation is not interesting for predictive models. \n\n \nChapter 11 \u25a0 Text Mining \n337\nApplications may arise, for example, when knowing a token ", "s from the bag of words. \nUsually, though not always, punctuation is not interesting for predictive models. \n\n \nChapter 11 \u25a0 Text Mining \n337\nApplications may arise, for example, when knowing a token appears as the fi rst \nor the last token in a sentence may be interesting in an analysis.\nCharacter Length and Number Filters\nUsually, short words and long words are not of interest. One or two character \nwords are usually listed as stop words, though others such as \u201cme,\u201d \u201cwe,\u201d and \n\u201cso\u201d are usually not interesting. Software often has options to limit tokens to \na minimum and maximum length. Numbers are also typically fi ltered out to \nspeed up processing.\nStemming\nStemming normalizes a word by reducing it to its most basic element or stem. \nTypical normalization steps with stemming include:\n \n\u25a0Removing affi xes (prefi xes and suffi xes)\n \n\u25a0Removing plurals\n \n\u25a0Normalizing verb tenses\nTable 11-4 shows a short list of words and their stemmed form. The result of \nstemming is a reduction in th", "\n\u25a0Removing affi xes (prefi xes and suffi xes)\n \n\u25a0Removing plurals\n \n\u25a0Normalizing verb tenses\nTable 11-4 shows a short list of words and their stemmed form. The result of \nstemming is a reduction in the number of unique tokens, simplifying subse-\nquent processing steps.\nTable 11-4: Stemmed Words\nORIGINAL WORD\nSTEMMED WORD\nwalks\nwalk\nwalk\nwalk\nwalked\nwalk\nwalking\nwalk\nwalker\nwalker\nwalkers\nwalker\ndenormalization\ndenorm\nnormalize\nnormal\nnorms\nnorm\nnormed\nnorm\nrenormalize\nrenorm\ntitle\ntitl\n\n338 \nChapter 11 \u25a0 Text Mining\nHowever, there are also potential problems with stemming. First, stemming is \nlanguage dependent; each language must have its own stemming rules. Second, \nif stemming is too aggressive, the results will be confusing and misleading. If \nstemming is too gentle, there will be little reduction in the number of tokens.\nThe most popular stemming algorithm is the Porter Stemmer, usually \nconsidered a mild stemming algorithm. The Snowball Stemmer is perhaps the \nnext most popular a", "le reduction in the number of tokens.\nThe most popular stemming algorithm is the Porter Stemmer, usually \nconsidered a mild stemming algorithm. The Snowball Stemmer is perhaps the \nnext most popular and is usually considered superior to the Porter Stemmer. \nA third algorithm, the Lancaster Stemmer, is very aggressive and should be \nused with care.\nTable 11-5 shows a comparison of the stemming results after applying each of \nthese three algorithms. Note that the Snowball Stemmer is the only one of the \nthree that produces a variety of stemming results for this group of words, as you \nwould expect from this particular group. On the other hand, the token \u201corgani-\nzation\u201d reduces, surprisingly, to the stem organ with both Porter and Snowball. \nTable 11-5: Comparison of Stemming Algorithms\nWORD\nPORTER STEMMER\nSNOWBALL \nSTEMMER\nLANCASTER STEMMER\ngenerate\ngener\ngenerat\ngen\ngenerates\ngener\ngenerat\ngen\ngenerated\ngener\ngenerat\ngen\ngenerating\ngener\ngenerat\ngen\ngeneral\ngener\ngeneral\ngen\ngenerally\n", "\nPORTER STEMMER\nSNOWBALL \nSTEMMER\nLANCASTER STEMMER\ngenerate\ngener\ngenerat\ngen\ngenerates\ngener\ngenerat\ngen\ngenerated\ngener\ngenerat\ngen\ngenerating\ngener\ngenerat\ngen\ngeneral\ngener\ngeneral\ngen\ngenerally\ngener\ngeneral\ngen\ngeneric\ngener\ngeneric\ngen\ngenerically\ngener\ngeneric\ngen\ngenerous\ngener\ngenerous\ngen\ngenerously\ngener\ngenerous\ngen\norganization\norgan\norgan\norg\nurgency\nurgen\nurgen\nurg\nDictionaries\nA standard bag of words will identify as a token any combination of charac-\nters. Dictionaries can be valuable to reduce the set of tokens to be extracted by \nidentifying only the words of particular interest in an analysis. The dictionar-\nies can include proper nouns, special phrases, acronyms, titles, numbers, or \nany sequence of characters important in the analysis. They can also include all \n\n \nChapter 11 \u25a0 Text Mining \n339\nacceptable variations of a token, such as including MD, M.D., MD., and M D. \nUnless the dictionary is already defi ned, such as a medical dictionary of terms, \nit can be ", "Chapter 11 \u25a0 Text Mining \n339\nacceptable variations of a token, such as including MD, M.D., MD., and M D. \nUnless the dictionary is already defi ned, such as a medical dictionary of terms, \nit can be very time consuming to create.\nThe Sentiment Polarity Movie Data Set\nThe data used for examples in this chapter comes from the Sentiment Polarity \nData set introduced by Pang and Lee in 2004 (http://www.cs.cornell.edu/\npeople/pabo/movie-review-data/). The original data contained 1,000 positive \nand 1,000 negative movie reviews, each review in a separate text fi le and labeled \nwith the sentiment. The data described here was reduced to 101 positive and \n101 negative reviews. These reviews have on average 700 words each. \nAfter applying the processing steps outlined in Figure 11-1, Table 11-6 shows \nthe number of rows and terms remaining after each fi ltering step. \nTable 11-6: Record Counts and Term Counts after Data Preparation \nDATA PREPARATION \nSTAGE\nNUMBER OF ROWS\nNUMBER OF UNIQUE TERMS", "ows \nthe number of rows and terms remaining after each fi ltering step. \nTable 11-6: Record Counts and Term Counts after Data Preparation \nDATA PREPARATION \nSTAGE\nNUMBER OF ROWS\nNUMBER OF UNIQUE TERMS\nAll reviews\n202\nNA\nBag of Words Creation\n71,889\n18,803\nPunctuation Eraser\n70,348\n18,711\nStop Word Filter\n46,029\n17,959\nN Char Filter\n45,058\n17,778\nNumber Filter\n44,876\n17,683\nPorter Stemmer\n44,786\n17,171\nRemove POS tags\n40,984\n10,517\nGroup By Document\n203\n10,517\nThe processing begins with 202 reviews from the sentiment polarity data: \n101 positive reviews and 101 negative reviews. After identifying tokens and \nrunning the POS tagger, 18,803 tokens (terms) exist in the 202 documents. The \nterm count refers to one or more occurrences of the term in the document rather \nthan a full listing of every occurrence of each term. The 71,889 rows represent \ndocument-term pairs. Removing punctuation terms, stop words, 1- or 2-letter \nwords, and numbers reduces the number of terms down to 17,683, repr", "ery occurrence of each term. The 71,889 rows represent \ndocument-term pairs. Removing punctuation terms, stop words, 1- or 2-letter \nwords, and numbers reduces the number of terms down to 17,683, represented \nin 44,876 rows. \nStemming reduces the number of terms in this data set modestly. Interestingly, \nremoving the POS tags in this data reduces the number of terms. Table 11-7 \nshows a single term, \u201cboring,\u201d represented as three different parts of speech as \nit exists in fi ve different documents. After stripping the POS tags, documents \n\n340 \nChapter 11 \u25a0 Text Mining\n72 and 97 would have its TF value change as a result of grouping the values for \nthe two parts of speech.\nTable 11-7: Example of Terms Grouped after Stripping POS\nTERM \uf6b0POS\uf6b1\nDOCUMENT NUMBER\nboring[VBG(POS)]\n61\nboring[VBG(POS)]\n72\nboring[NN(POS)]\n72\nboring[JJ(POS)]\n73\nboring[VBG(POS)]\n81\nboring[JJ(POS)]\n97\nboring[VBG(POS)]\n97\nText Mining Features \nAfter all the data preparation and preprocessing has been completed, the da", ")]\n72\nboring[NN(POS)]\n72\nboring[JJ(POS)]\n73\nboring[VBG(POS)]\n81\nboring[JJ(POS)]\n97\nboring[VBG(POS)]\n97\nText Mining Features \nAfter all the data preparation and preprocessing has been completed, the data \nis still only a listing of tokens. For predictive modeling, these terms have to be \nconverted into a form useful for predictive modeling algorithms. If the unit of \nanalysis is the document, then the terms are exploded into additional columns \nin the data, one column per term. For the movie review data, if no stemming is \ndone and POS tags are retained, 17,683 terms remain (Table 11-6). Since a feature \nis created for each term, 17,683 new columns are created in the data. \nThis large number of new columns is often impractical, but more important, \nthe vast majority of these terms are rare\u2014the long tail effect. In fact, nearly two-\nthirds of the terms identifi ed in the data appear in only one document, making \nthem completely unusable for modeling. \nTable 11-8 shows the typical long-ta", "the long tail effect. In fact, nearly two-\nthirds of the terms identifi ed in the data appear in only one document, making \nthem completely unusable for modeling. \nTable 11-8 shows the typical long-tail effect of term extraction. Note that 11,439 \nterms appear in only one document of the 202 review (65 percent). Terms that \nappear in only one or even in a few reviews, of course, will not be useful in \npredicting sentiment. These can safely be removed from a document clustering \nor classifi cation application, though not necessarily from information retrieval.\n\n \nChapter 11 \u25a0 Text Mining \n341\nTable 11-8: Number of Times Terms Appear in Documents\nNUMBER OF DOCUMENTS IN \nWHICH THE TERM OCCURS\nNUMBER OF TERMS WITH THIS OCCURRENCE \nCOUNT\n1\n11,439\n2\n2,526\n3\n1,120\n4\n668\n5\n435\n6\n249\n7\n224\n8\n154\n9\n114\n10\n107\n11\u201320\n391\n21\u201330\n122\n31\u201350\n51\n51\u2013150\n33\nTerm Frequency\nTerm Frequency (TF) is a count of how often a term is used in each document. \nThe simplest representation of TF is to simply create a 1", "114\n10\n107\n11\u201320\n391\n21\u201330\n122\n31\u201350\n51\n51\u2013150\n33\nTerm Frequency\nTerm Frequency (TF) is a count of how often a term is used in each document. \nThe simplest representation of TF is to simply create a 1/0 label\u2014a fl ag\u2014indi-\ncating the existence of the term in the document. This is the same idea as was \ndescribed in Chapter 4 as \u201cexploding\u201d a categorical variable into dummies. \nSometimes this 1/0 form of TF is described as the \u201cBoolean frequency,\u201d although \nbecause it is not a frequency but merely an indicator of existence, this is a stretch \nin terminology. Nevertheless, it is a helpful description in comparison to other \nforms of TF.\nTable 11-9 shows a small table with fi ve terms and the TF fl ag form of the \nterm representation of the data. This version did not differentiate between dif-\nferent parts of speech, and therefore did not resolve situations where both parts \nof speech showed in a single document. These can be tracked separately or \ntogether depending on the interest of the", "\nferent parts of speech, and therefore did not resolve situations where both parts \nof speech showed in a single document. These can be tracked separately or \ntogether depending on the interest of the analyst. But this table is not yet ready \n\n342 \nChapter 11 \u25a0 Text Mining\nfor modeling because each record does not represent a document (a review in \nthis case), but rather represents a term within a document.\nTable 11-9: Boolean TF Values for Five Terms \nDOCUMENT \nID\nTERM\uf6b0POS\uf6b1\nDESPITE\nWORST\nENJOY\nMOVIE\nFILM\n1\ndespite[IN(POS)]\n1\n0\n0\n0\n0\n1\nenjoy[VB(POS)]\n0\n0\n1\n0\n0\n1\n\ufb01 lm[NN(POS) \nVB(POS)]\n0\n0\n0\n0\n1\n2\nmovie[NN(POS)]\n0\n0\n0\n1\n0\n2\n\ufb01 lm[NN(POS)]\n0\n0\n0\n0\n1\n5\nworst[JJS(POS)]\n0\n1\n0\n0\n0\n5\nmovie[NN(POS)]\n0\n0\n0\n1\n0\n5\n\ufb01 lm[NN(POS) \nVB(POS)]\n0\n0\n0\n0\n1\n77\nworst[JJS(POS)]\n0\n1\n0\n0\n0\n77\nenjoy[VB(POS)]\n0\n0\n1\n0\n0\n77\nmovie[NN(POS)]\n0\n0\n0\n1\n0\n77\n\ufb01 lm[NN(POS)]\n0\n0\n0\n0\n1\nTable 11-10 shows the form of the data that would be used in modeling once \nthe data is collapsed to one record per document (i.e., group by Do", "0\n77\nmovie[NN(POS)]\n0\n0\n0\n1\n0\n77\n\ufb01 lm[NN(POS)]\n0\n0\n0\n0\n1\nTable 11-10 shows the form of the data that would be used in modeling once \nthe data is collapsed to one record per document (i.e., group by Document ID). \nTable 11-10: Boolean TF Values for Five Terms after Rolling Up to Document Level\nDOCUMENT ID\nDESPITE\nWORST\nENJOY\nMOVIE\nFILM\nSENTIMENT\n1\n1\n0\n1\n0\n1\npos\n2\n0\n0\n0\n1\n1\npos\n5\n0\n1\n0\n1\n1\nneg\n77\n0\n1\n1\n1\n1\nneg\nTF based on the actual count is the next level of complexity in representing \nterms. Rather than simply fl agging if a term exists in the document, the actual \nnumber of times the term appears in the document is shown. If a term appears \nmore often in a document, it may be more strongly related to the target vari-\nable. Most often, TF will have a value equal to 1, just like the Boolean form. But \n\n \nChapter 11 \u25a0 Text Mining \n343\nsometimes the value could be quite large. Table 11-11 shows the same documents \nand terms with their TF values. \nTable 11-11: Boolean Form of TF Features\nD", " form. But \n\n \nChapter 11 \u25a0 Text Mining \n343\nsometimes the value could be quite large. Table 11-11 shows the same documents \nand terms with their TF values. \nTable 11-11: Boolean Form of TF Features\nDOCUMENT ID\nDESPITE\nWORST\nENJOY\nMOVIE\nFILM\nSENTIMENT\n1\n1\n0\n1\n0\n14\npos\n2\n0\n0\n0\n2\n2\npos\n5\n0\n1\n0\n6\n6\nneg\n77\n0\n1\n2\n13\n8\nneg\nIn the sentiment polarity data the term \u201cmovie\u201d has the maximum TF value \n(28), which is no surprise for movie review data. Note that the longer the docu-\nment, the more likely that TF will have larger values. If the documents are long \nand the raw TF value has a long tail, computing the log transform of TF can \nhelp normalize the values as described in Chapter 4. Table 11-12 shows the val-\nues of TF and the corresponding values for the log transform of TF\u2014actually, \nlog10( 1 + TF)\u2014for the sentiment polarity data. The Number of Terms Matching \ncolumn counts how many terms had a TF value in the TF column. \nThe vast majority of terms occurred only one time in any review (10,", "+ TF)\u2014for the sentiment polarity data. The Number of Terms Matching \ncolumn counts how many terms had a TF value in the TF column. \nThe vast majority of terms occurred only one time in any review (10,074). \nIf TF is used directly in modeling, a few terms would have values that are \nlarger\u2014only 19 terms were in a review 10 times. It is therefore heavily skewed \nand may bias models or summary statistics. The log transform will reduce the \namount of skew in TF. The Boolean frequency for each value of TF would be 1. \nTable 11-12: TF and Log10(1+TF) Values\nTF\nNUMBER OF TERMS MATCHING\nLOG10 TF\n1\n10,074\n0.301\n2\n2,427\n0.477\n3\n679\n0.602\n4\n260\n0.699\n5\n118\n0.778\n6\n84\n0.845\n7\n42\n0.903\n8\n28\n0.954\n9\n15\n1.000\n10\n19\n1.041\n\n344 \nChapter 11 \u25a0 Text Mining\nAnother technique that is sometimes used is to smooth the TF value by scal-\ning the value by the maximum TF value\u2014the min-max transform that puts TF \non a scale from 0 to 1. This approach reduces the magnitude of TF but does not \neliminate the long tail", " the TF value by scal-\ning the value by the maximum TF value\u2014the min-max transform that puts TF \non a scale from 0 to 1. This approach reduces the magnitude of TF but does not \neliminate the long tail. \nInverse Document Frequency\nDocument frequency (DF) represents the number of times the term appears in all \ndocuments in the corpus. The intent of this feature is to measure how popular a \nterm is. If the term is found in only a few documents, it could be perceived that \nit is more specifi c to those particular documents. This is especially helpful for \ndocument clustering and document classifi cation where the terms are helping \nto differentiate between documents. It is also helpful in Information Retrieval \nbecause the term will identify documents to be returned by the request because \nthe intent of the request is clearer.\nA lower value of DF, therefore, is better if the intent is fi nding words and \nterms that are specifi c to the documents of interest. However, many analysts \nprefer ", " intent of the request is clearer.\nA lower value of DF, therefore, is better if the intent is fi nding words and \nterms that are specifi c to the documents of interest. However, many analysts \nprefer larger values of the feature to indicate better, leading them to compute the \ninverse document frequency (IDF). IDF, then, is the total number of documents \nin the corpus divided by the number of documents with the term of interest in it. \nThis quotient is often heavily skewed as well, just like TF tends to be. Because \nof this, analysts usually take the log transform of the quotient and call the log-\ntransformed quotient IDF, usually with 1 added to the quotient before taking the \nlog transform. IDF is thus a constant for each term; the value will be repeated \nfor every document that term appears in, as shown in Figure 11-13.\nTable 11-13: Log Transformed IDF Feature Values\nDOCUMENT ID\nDESPITE\nWORST\nENJOY\nMOVIE\nFILM\nSENTIMENT\n1\n0.990\n0\n1.045\n0\n0.333\npos\n2\n0\n0\n0\n0.374\n0.333\npos\n5\n0\n0.928\n0\n", "ears in, as shown in Figure 11-13.\nTable 11-13: Log Transformed IDF Feature Values\nDOCUMENT ID\nDESPITE\nWORST\nENJOY\nMOVIE\nFILM\nSENTIMENT\n1\n0.990\n0\n1.045\n0\n0.333\npos\n2\n0\n0\n0\n0.374\n0.333\npos\n5\n0\n0.928\n0\n0.374\n0.333\nneg\n77\n0\n0.928\n1.045\n0.374\n0.333\nneg\nTF-IDF\nIDF on its own is not usually a good feature for building predictive models. \nHowever, if a term is rare, but occurs frequently within a single document, it may \nbe a term that conveys meaning and insight to that document. If, for example, \n\n \nChapter 11 \u25a0 Text Mining \n345\nyou are trying to cluster a corpus of abstracts from predictive analytics confer-\nences, terms that appear several times in only a few of the abstracts could be \ngood surrogates for what differentiates those talks from others. A few of the \nabstracts may include \u201cdecision trees\u201d several times, but not the majority of \nabstracts. A few other abstracts may include the text \u201csocial media\u201d or \u201csocial \nnetwork\u201d several times. The combination of high TF and high IDF (mean", "on trees\u201d several times, but not the majority of \nabstracts. A few other abstracts may include the text \u201csocial media\u201d or \u201csocial \nnetwork\u201d several times. The combination of high TF and high IDF (meaning terms \nthat are relatively rare in the corpus) could be the best separator of documents.\nThe simple product TF \u00d7 IDF is a good feature to include as a potential input \nfor predictive models. If a term appears frequently within a document (high TF) \nand doesn\u2019t appear frequently in the corpus (high IDF), then this term is highly \nconnected to the particular document. If a term appears frequently within a \ndocument but also in many other documents (low IDF), then this term is not \nhighly connected to the particular document. \nWhy multiplication? The multiplication operator is often used to measure \ninteractions between variables. One difference between using a multiplicative \napproach rather than an additive approach to combining TF and IDF\u2014creating \na feature TF + IDF\u2014is that the additi", "easure \ninteractions between variables. One difference between using a multiplicative \napproach rather than an additive approach to combining TF and IDF\u2014creating \na feature TF + IDF\u2014is that the additive version will produce a relatively higher \nscore value if only one of the two values is high. The multiplicative approach \nrequires both values to be higher for a high score to be the result. Nevertheless, \nTF \u00d7 IDF is the commonly used feature used in text mining.\nThe TF \u00d7 IDF values for the same documents and features from Tables 11-12 \nand 11-13 are shown in Table 11-14. The term \u201cworst\u201d has the same values for both \nDocument ID 5 and 7 because the TF value is the same for both, whereas \u201cenjoy\u201d \nhas different TF values for Document IDs 1 and 77, yielding different values. \nTable 11-14: TF-IDF Values for Five Terms\nDOCUMENT \nID\nDESPITE\nWORST\nENJOY\nMOVIE\nFILM\nSENTIMENT\n1\n0.2982\n0.000\n0.3147\n0.000\n1.2533\npos\n2\n0.000\n0.000\n0.000\n0.1125\n0.1003\npos\n5\n0.000\n0.2795\n0.000\n0.3159\n0.9006\nneg\n77\n", "-IDF Values for Five Terms\nDOCUMENT \nID\nDESPITE\nWORST\nENJOY\nMOVIE\nFILM\nSENTIMENT\n1\n0.2982\n0.000\n0.3147\n0.000\n1.2533\npos\n2\n0.000\n0.000\n0.000\n0.1125\n0.1003\npos\n5\n0.000\n0.2795\n0.000\n0.3159\n0.9006\nneg\n77\n0.000\n0.2795\n0.4987\n0.4284\n0.3181\nneg\nTF-IDF is used particularly in information retrieval applications because of \nthe benefi cial normalizing effect of the IDF component. For document classifi -\ncation and sentiment analysis, it may be worth trying as a feature, but doesn\u2019t \nnecessarily provide value; just because terms are specifi c to a particular docu-\nment doesn\u2019t mean that the terms are good at classifying groups of documents \nfrom other groups (positive sentiment from negative sentiment, for example).\n\n346 \nChapter 11 \u25a0 Text Mining\nCosine Similarity\nCosine similarity is the classical approach to measuring how similar two docu-\nments are to each other. The measure used could be TF-IDF, Boolean TF, TF, or \nsome other feature. If the Boolean TF value is used for each term, the similar", "pproach to measuring how similar two docu-\nments are to each other. The measure used could be TF-IDF, Boolean TF, TF, or \nsome other feature. If the Boolean TF value is used for each term, the similarity \nmeasure computes how many terms the documents have in common.\nIf there are 200 documents in the corpus, there are 200 \u00d7 200 = 40,000 similarity \nmeasures; the measure doesn\u2019t necessarily scale well to large numbers of docu-\nments. However, if you fi rst have clustered the document data, the similarity \nmeasure can be used after clustering to measure the distance of a document to \neach cluster. If there are M clusters, this is now M \u00d7 200 measures, a signifi cant \nreduction in computational complexity.\nThe cosine similarity, however, does not consider term order within the docu-\nment, only the existence of the terms in the document. \nMulti-Word Features: N-Grams\nAll of the features so far have focused on single terms. Clearly, multiple terms \ncould and often are more predictive than si", " the existence of the terms in the document. \nMulti-Word Features: N-Grams\nAll of the features so far have focused on single terms. Clearly, multiple terms \ncould and often are more predictive than single terms alone. These multiple word \nphrases are called N grams, defi ned as n-word phrases of adjacent words. The \nmost common n-grams are 2-word phrases called bigrams or digrams. Trigrams \nare 3-word phrases. Longer n-gram phrases are labeled by the number of adja-\ncent words: 4-grams, 5-grams, and so on.\n Consider the sentence, \u201cIt\u2019s one of the worst movies of all time.\u201d The bigrams \nfor this sentence are shown in the bulleted list where each bullet contains a \nsingle bigram. Note that even a period character (\u201c.\u201d) represents a term.\n \n\u25a0it \u2019s\n \n\u25a0\u2019s one\n \n\u25a0one of\n \n\u25a0of the\n \n\u25a0the worst\n \n\u25a0worst movies\n \n\u25a0movies of\n \n\u25a0of all\n \n\u25a0all time\n \n\u25a0time .\nThese bigrams then become new patterns that can be used as features if \nthey appear frequently enough in the corpus; they are evaluated the s", "\u25a0worst movies\n \n\u25a0movies of\n \n\u25a0of all\n \n\u25a0all time\n \n\u25a0time .\nThese bigrams then become new patterns that can be used as features if \nthey appear frequently enough in the corpus; they are evaluated the same way \nsingle-word terms are evaluated.\n\n \nChapter 11 \u25a0 Text Mining \n347\nReducing Keyword Features\nA common problem with text mining is the number of features that are created. \nTable 11-6 showed that after processing the sentiment polarity data set, 17,683 \nterms remained. If all of these variables converted to Boolean term frequency \nfeatures, there would be 17,683 new columns to use as inputs to a predictive \nmodel. Clearly this is too many inputs for most modeling approaches.\nA second problem with text mining features is sparseness; most of the terms \nfound in a corpus of documents occur rarely, resulting in poor predictive power. \nOf the 17,683 terms found in the corpus: \n \n\u25a0754 terms appear in 10 or more documents (5 percent of the documents)\n \n\u25a0231 terms appear in 20 or more docum", " rarely, resulting in poor predictive power. \nOf the 17,683 terms found in the corpus: \n \n\u25a0754 terms appear in 10 or more documents (5 percent of the documents)\n \n\u25a0231 terms appear in 20 or more documents (10 percent of the documents)\n \n\u25a084 terms appear in 30 or more documents (30 percent of the documents)\nRetaining only those terms that occur in suffi cient numbers of documents \n(reviews in the sentiment data) will reduce dimensionality signifi cantly. In \nsupervised learning problems, you can also remove terms that by themselves \nare poor predictors of the target variable. \nGrouping Terms\nSome methods of grouping terms together have already been described, such \nas the use of stemming and synonyms. Stemming groups words related by a \ncommon stem, and synonyms group terms that have similar meaning. Identifying \nsynonyms and replacing a term with its synonym can also reduce the num-\nber of terms. (Synonyms should be identified prior to removing sparsely \npopulated terms.) \nEven after r", "r meaning. Identifying \nsynonyms and replacing a term with its synonym can also reduce the num-\nber of terms. (Synonyms should be identified prior to removing sparsely \npopulated terms.) \nEven after reducing terms by eliminating sparse terms and grouping similar \nterms, numeric techniques can be used to reduce dimensionality. Principal \nComponent Analysis (PCA) can be applied to TF or TF-IDF features to reduce \nthe number of inputs to predictive models while still describing a high percent-\nage of the variance in the data. The components themselves can be used as the \ninputs to the supervised or unsupervised learning models. \nModeling with Text Mining Features\nThe steps described so far in this chapter outline how to convert unstructured \ndata into structured data that can be used for predictive modeling. The original \ntext fi elds become columns in a structured representation of the terms in the \ntext. Now that the data has been converted to a structured format, the principles \nfor su", "ictive modeling. The original \ntext fi elds become columns in a structured representation of the terms in the \ntext. Now that the data has been converted to a structured format, the principles \nfor supervised or unsupervised predictive models described in Chapters 6\u201310 \ncan be applied. \n\n348 \nChapter 11 \u25a0 Text Mining\nAs one simple example of creating models with the sentiment polarity data, \nconsider a K-Means model built from Boolean TF features. The full set of terms \nextracted by text mining techniques is far too large, far too correlated, and far \ntoo sparsely populated to apply directly. Only terms that were present in at least \n20 reviews but not more than 100 reviews are included to reduce the number \nof inputs to the model. If the term is present in fewer than 20 reviews, there \nis concern that patterns could occur by chance. If the term is present in more \nthan 100 reviews, it is too common a term. These two thresholds are just rules \nof thumb and should not be considered rule", " concern that patterns could occur by chance. If the term is present in more \nthan 100 reviews, it is too common a term. These two thresholds are just rules \nof thumb and should not be considered rules to apply to every data set.\nA 5-cluster model was built, summarized in Table 11-15, with only a few of \nthe terms included in the model displayed. The percentages convey what per-\ncentage of the reviews in the cluster have the sentiment (positive or negative) or \ninclude the term (worst, entertainment, etc.). Cluster 3 contains predominantly \npositive reviews; 91 percent of the reviews in Cluster 3 are positive. The terms \nthat appear most often in Cluster 3 include \u201centertainment,\u201d \u201centire,\u201d \u201cfamily,\u201d \nand \u201ceffective.\u201d In fact, 20 of the 23 (87%) of the reviews in Cluster 3 contain the \nterm \u201centire\u201d. Also note that the term \u201cworst\u201d never appears in these reviews.\nThe most negative cluster is Cluster 2 and contains terms \u201cstupid,\u201d \u201centire,\u201d \nand \u201cworst\u201d in relatively high proportions. T", "m \u201centire\u201d. Also note that the term \u201cworst\u201d never appears in these reviews.\nThe most negative cluster is Cluster 2 and contains terms \u201cstupid,\u201d \u201centire,\u201d \nand \u201cworst\u201d in relatively high proportions. The term \u201centire,\u201d therefore, can \nhave positive or negative sentiment, depending on the other terms that exist \nwith it in the review.\nThe key point is that the cluster model provides insight to how terms interact \nwith each other in the reviews, and can reveal how those interactions relate to \nsentiment. \nTable 11-15: Five-Cluster Model Results\nMEASURE \nOR TERM\nCLUSTER 1\nCLUSTER 2\nCLUSTER 3\nCLUSTER 4\nCLUSTER 5\nNumber of \nReviews\n73\n8\n23\n29\n29\nNegative \nReviews, \npercent\n58\n63\n9\n31\n62\nPositive \nReviews, \npercent\n42\n38\n91\n69\n38\nworst, \npercent\n21.9\n50.0\n0.0\n3.4\n20.7\nentertain-\nment, \npercent\n8.2\n0.0\n26.1\n10.3\n27.6\n\n \nChapter 11 \u25a0 Text Mining \n349\nentire, \npercent\n0.0\n100\n87.0\n0.0\n0.0\nhistory, \npercent\n11.0\n50.0\n8.7\n6.9\n13.8\nstupid, \npercent\n0.0\n62.5\n0.0\n10.3\n48.3\nsup-\nporting, \npercent\n0.0\n", "2\n0.0\n26.1\n10.3\n27.6\n\n \nChapter 11 \u25a0 Text Mining \n349\nentire, \npercent\n0.0\n100\n87.0\n0.0\n0.0\nhistory, \npercent\n11.0\n50.0\n8.7\n6.9\n13.8\nstupid, \npercent\n0.0\n62.5\n0.0\n10.3\n48.3\nsup-\nporting, \npercent\n0.0\n0.0\n26.1\n6.9\n55.2\nperfor-\nmances, \npercent\n0.0\n25.0\n17.4\n100\n0.0\nfamily, \npercent\n13.7\n0.0\n34.8\n17.2\n0.0\nenjoy, \npercent\n15.1\n25.0\n17.4\n6.9\n3.4\ne\ufb00 ective, \npercent\n15.1\n12.5\n21.7\n10.3\n6.9\nRegular Expressions\nRegular expressions (REs) are power pattern matchers for strings. Their power \ncomes from the syntax that includes a variety of wildcards to match strings in \nvery fl exible ways, enabling REs to match variable-length patterns of characters, \nexistence or non-existence of one or more characters, repeated patterns of one \nor more characters, and even gaps between characters or words of interest. The \ntypes of patterns that regular expressions can fi nd extend well beyond fi nding \nsingle terms in a corpus of documents, and are far more fl exible than standard \nstring operations found in", "rest. The \ntypes of patterns that regular expressions can fi nd extend well beyond fi nding \nsingle terms in a corpus of documents, and are far more fl exible than standard \nstring operations found in Excel, C, Java, or SQL. Packages, however, have been \ndeveloped not just for Python, Perl, R, and other languages, but also for C, Java, \nand other languages. \nREs are brute-force pattern matchers, however, and not semantic learners. \nThey don\u2019t care about language (English, Spanish, German, and so on), nor do \nthey care about the meaning of what they fi nd. Nevertheless, REs are often the \nbackbone text search and extraction. \nThe purpose of this section is not to provide a comprehensive description \nof REs, but rather to provide an overview of the power of REs and some typi-\ncal uses of them. Some examples of REs to fi nd specifi c kinds of patterns are \n\n350 \nChapter 11 \u25a0 Text Mining\nshown in Table 11-16. The quotation marks in the third column only set off the \ncharacters that match o", "m. Some examples of REs to fi nd specifi c kinds of patterns are \n\n350 \nChapter 11 \u25a0 Text Mining\nshown in Table 11-16. The quotation marks in the third column only set off the \ncharacters that match or don\u2019t match the regular expressions, but are not a part \nof the match itself.\nTable 11-16: Key Regular Expression Syntax Characters\nRE SYNTAX\nDESCRIPTION\nEXAMPLE\n.\nAny single character\nmo.el matches \u201cmodel,\u201d \u201cmotel,\u201d \u201cmobel,\u201d \n\u201cmo!el,\u201d and so on.\n^\nMatches the beginning \nof a line\n^A.* matches \u201cA Saturday\u201d but not \n\u201cSaturday.\u201d\n$\nMatches the end of a line\nend.$ matches \u201cthe end.\u201d but not \u201cthe \nends.\u201d\n\\b\nMatches a word \nboundary\nmodel\\b matches \u201cmodel\u201d from \u201cmodel \nprediction\u201d but not \u201cmodel\u201d in \u201cmodeling.\u201d\n*\nMatches zero or more \noccurrences of previous \nexpression\n.* matches any character, m* matches \u201c\u201d, \n\u201cmodel,\u201d and \u201cmom,\u201d but not \u201cant.\u201d\n+\nMatches one or more \noccurrences of previous \nexpression\nm+ matches \u201cmodel,\u201d \u201cmom,\u201d but not \u201c\u201d \nor \u201cant.\u201d\n?\nMatches 0 or 1 occur-\nrences of previous", "* matches \u201c\u201d, \n\u201cmodel,\u201d and \u201cmom,\u201d but not \u201cant.\u201d\n+\nMatches one or more \noccurrences of previous \nexpression\nm+ matches \u201cmodel,\u201d \u201cmom,\u201d but not \u201c\u201d \nor \u201cant.\u201d\n?\nMatches 0 or 1 occur-\nrences of previous \nexpression\n\u201cmo?el\u201d matches \u201cmodel,\u201d \u201cmotel,\u201d and \n\u201cmoel,\u201d but not \u201cmdel.\u201d\n[   ]\nMatches one charac-\nter inside the square \nbrackets\n\u201cmo[dt]el\u201d matches \u201cmodel\u201d and \u201cmotel,\u201d \nbut not \u201cMotel\u201d or \u201cmobel.\u201d\n[a-zA-Z0-9]\nMatches any lowercase \ncharacter between a and \nz, any uppercase charac-\nter between A and Z, and \nany number between 0 \nand 9\nMatches \u201cm,\u201d, \u201c1,\u201d and \u201cM,\u201d but not a \ncomma, space, colon, or dollar sign, to \nname a few non-matches.\n{a,b}\nMatches from a to b \noccurrences of the previ-\nous expression\na{1,3} matches \u201ca,\u201d \u201caa,\u201d and \u201caaa.\u201d\n\n \nChapter 11 \u25a0 Text Mining \n351\nUses of Regular Expressions in Text Mining\nREs are useful in text mining as an addition to standard processing techniques \nusually provided in text mining software. The following are some examples:\n \n\u25a0Non-dictionary ", "Expressions in Text Mining\nREs are useful in text mining as an addition to standard processing techniques \nusually provided in text mining software. The following are some examples:\n \n\u25a0Non-dictionary terms: Some applications of text mining have language \nvery specifi c to the domain. Consider auto insurance claims investiga-\ntions that include auto accidents with injuries to passengers. Most medi-\ncal descriptions can be found in medical dictionaries available publicly. \nHowever, shorthand terminology and insurance-specifi c terms may not \nbe available in dictionaries. REs can capture keywords specifi c to adjust-\ners with their variants as applied by different adjusters.\n \n\u25a0Abbreviations and Acronyms: This is related to the identifi cation of \nnon-dictionary terms. Some domains have a wealth of abbreviations and \nacronyms that are not specifi ed in a dictionary, such as MD and USA.\n \n\u25a0Multi-word features that may be separated by a number of words: \nSometimes n-grams are a useful idea,", "lth of abbreviations and \nacronyms that are not specifi ed in a dictionary, such as MD and USA.\n \n\u25a0Multi-word features that may be separated by a number of words: \nSometimes n-grams are a useful idea, but the key words are not adjacent. \nMoreover, word order may be critical to conveying the meaning of the \nmultiple word features. REs provide an easy way to capture the existence \nof these patterns.\nFor example, fl agging the existence of \u201cnegative\u201d for identifying negative \nsentiment is a good idea, but the word \u201cbut\u201d could negate the negative. \n\u201cNegative . . . but\u201d may be a helpful word pair to identify. Often, this \ncould be found with the trigram \u201cbut\u201d followed by a comma, followed \nby \u201cbut.\u201d However, consider the following sentence:\nThe fi lm has gotten some negative reviews (a friend of mine actually \nthinks it\u2019s the worst in the series), but I\u2019m not really sure why.\nThe RE \u201cnegative\\W+(?:\\w+\\W+){1,14}?but\u201d fi nds the word \u201cnega-\ntive\u201d followed by one or more non-word characters, f", "f mine actually \nthinks it\u2019s the worst in the series), but I\u2019m not really sure why.\nThe RE \u201cnegative\\W+(?:\\w+\\W+){1,14}?but\u201d fi nds the word \u201cnega-\ntive\u201d followed by one or more non-word characters, followed by 1 to 14 \nword/non-word pairs, followed by the word \u201cbut,\u201d and will therefore \nfi nd \u201cnegative\u201d followed by 1 to 14 words, followed by the word \u201cbut,\u201d \nso it will fi nd the underlined phrase within the sentence. The feature (let\u2019s \ncall it \u201cnegative . . . but\u201d) would then be populated with a 1 if the pattern \nmatched, 0 otherwise.\nThe fi lm has gotten some negative reviews (a friend of mine actually \nthinks it\u2019s the worst in the series), but I\u2019m not really sure why.\n\n352 \nChapter 11 \u25a0 Text Mining\n \n\u25a0Misspellings of words uncorrected by dictionary or phonetic spelling \ncorrection algorithms: Some words can be misspelled in the most unfore-\nseen, even clever, ways. In fraud detection applications, some analysts have \nfound that misspellings are done purposefully to help some transa", "rithms: Some words can be misspelled in the most unfore-\nseen, even clever, ways. In fraud detection applications, some analysts have \nfound that misspellings are done purposefully to help some transactions \navoid detection. Or, when text is typed or transcribed, seemingly random \nspelling variants are sometimes introduced. \nFor example, if a call center is identifying if a printer is \u201csticking,\u201d an RE can \nfi nd variants that won\u2019t be found with stemming algorithms, such as \u201cstck,\u201d \n\u201cstic,\u201d \u201cstick,\u201d \u201cstickin,\u201d \u201csticking,\u201d \u201csticks,\u201d and \u201csticky.\u201d\nThree examples of common REs used commonly for text mining on e-mail \nmessages or web pages are:\n \n\u25a0E-mail addresses: \\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}\\b\n \n\u25a0Phone numbers: \\(?[0-9]{3}\\)?[ -][0-9]{3}[ -][0-9]{4}\n \n\u25a0URLs: ^(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\u00a0\\.-]*)*\\/?$\nThese are shown as examples of the power of REs to fi nd simple, highly \nstructured patterns, like phone numbers, or more complex patterns like thos", "\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\u00a0\\.-]*)*\\/?$\nThese are shown as examples of the power of REs to fi nd simple, highly \nstructured patterns, like phone numbers, or more complex patterns like those \nfound in e-mail addresses and URLs. \nSummary\n Text mining is still a leading-edge approach for organizations, though its use \nis rapidly increasing. The mathematics behind text mining is fundamentally \ndifferent from what is found in predictive analytics algorithms, and therefore \nrequires that an analyst add different skills than he or she uses for predictive \nanalytics. Fortunately, text mining add-ins or packages are available in most \nof the most commonly-used predictive analytics software packages so that \nthe results from the text processing can be added to the structured data easily \nand therefore be used with standard supervised and unsupervised modeling \nalgorithms. \nPredictive modelers who need to customize text processing often rely on \nregular expressions, though not all pre", "\nand therefore be used with standard supervised and unsupervised modeling \nalgorithms. \nPredictive modelers who need to customize text processing often rely on \nregular expressions, though not all predictive analytics software packages \nsupport regular expressions well or at all. Modelers may have to use a separate \nscripting or programming language to leverage regular expressions. The benefi ts, \nhowever, often justify the burden of learning the new language.  \n\n353\nModel Deployment is the sixth and fi nal stage of the CRISP-DM process, \nincorporating activities that take place after models have been built, validated, \nand are ready for use. These activities include documenting the modeling proj-\nect, using the models operationally, monitoring the models as they are used \noperationally, and planning for rebuilding the models if and when necessary.\nMoving models through obstacles unrelated to predictive modeling within an \norganization so they can be deployed is often an achievement in", "nd planning for rebuilding the models if and when necessary.\nMoving models through obstacles unrelated to predictive modeling within an \norganization so they can be deployed is often an achievement in of itself; many \ngood models are never deployed because of technical reasons related to turning \nmodels into operational decisions or because internal politics create roadblocks \npreventing the models from being deployed. And even when deployment is the \ngoal of an organization, it can take weeks or months before the hurdles related \nto deployment are overcome. In some domain areas, government regulatory \nconstraints present additional obstacles to deployment.\nIt is curious that very little information exists, in the form of general \nprinciples, on how to deploy models. Information can be gleaned in blogs and \na few noteworthy books, but for the most part, analysts learn deployment by \nactually doing it. Most predictive analytics software vendors have developed \ninfrastructure for how to ", "aned in blogs and \na few noteworthy books, but for the most part, analysts learn deployment by \nactually doing it. Most predictive analytics software vendors have developed \ninfrastructure for how to deploy models effectively, including the development \nof specialized software designed specifi cally for the task of deploying, monitor-\ning, and maintaining predictive models.\nC H A P T E R \n12\nModel Deployment\n\n354 \nChapter 12 \u25a0 Model Deployment\nGeneral Deployment Considerations\nThe CRISP-DM description of model deployment consists of four stages as shown \nin Figure 12-1: planning deployment, monitoring and maintaining models, \ncreating a fi nal report, and reviewing the project and generating a list of lessons \nlearned. This chapter focuses on the fi rst two steps: the plan and implementa-\ntion of the plan.\nPlan\nDeployment\nDeployment\nPlan\nPlan Monitoring\n& Maintenance\nProduce Final\nReport\nReview Project\nExperience\nDocumentation\nFinal Report\nFinal\nPresentation\nMonitoring &\nMaintenance\nPl", "tion of the plan.\nPlan\nDeployment\nDeployment\nPlan\nPlan Monitoring\n& Maintenance\nProduce Final\nReport\nReview Project\nExperience\nDocumentation\nFinal Report\nFinal\nPresentation\nMonitoring &\nMaintenance\nPlan\nFigure 12-1:  CRISM-DM deployment steps\nSeveral general considerations should be included in the planning for model \ndeployment. First, plan for deployment at the beginning of the project. Knowing \nwhere the models are to be deployed and how quickly they need to operate is \ncritical in the development of data preparation and predictive models. If the \nmodel must generate predictions in real time, such as within 6 milliseconds, \na 1,000-model ensemble solution may not be the best approach. On the other \nhand, if the model can take minutes or more before the scores are generated, \nthe models can be very complex and still be used. \nSecond, predictive models themselves do not do anything except generate \nscores; the scores have to be interpreted and converted into actions for the model \nto ", "can be very complex and still be used. \nSecond, predictive models themselves do not do anything except generate \nscores; the scores have to be interpreted and converted into actions for the model \nto have any effect on the organization. Some models need only a threshold applied \nto the scores to turn them into a rule to use. Some models, like recommendation \nmodels, cross-sell models, and upsell models could have dozens, hundreds, or \neven thousands of scores that must be combined in some way to generate a list of \n\u201cnext best offers,\u201d ranging from simple \u201chighest probability wins\u201d logic to very \ncomplex, multidimensional, optimized decision logic. Analysts and modelers \nneed to coordinate with domain experts to determine the best course of action \nfor applying model predictions to the business objectives.\n\n \nChapter 12 \u25a0 Model Deployment \n355\nThird, deployment should be monitored to ensure the models are \nbehaving as they are expected to behave. Monitoring includes ensuring input \ndata", "ness objectives.\n\n \nChapter 12 \u25a0 Model Deployment \n355\nThird, deployment should be monitored to ensure the models are \nbehaving as they are expected to behave. Monitoring includes ensuring input \ndata is consistent with the data used to build the models; if the data is changing \nto the point that it differs signifi cantly from the data used to build the models, \nthe models themselves will not necessarily behave well any longer. Monitoring \nalso includes examining the model scores to ensure they are consistent with \nexpected scores. When deviations from the expected occur, it is time to rebuild \nthe models.\nDeployment Steps\nDeployment of predictive models includes several steps, outlined in Figure 12-2. \nImport/Select\nData to Score\nSelect Fields\nNeeded by\nModels\nPost-Process\nScores ->\nDecisions\nData to\nScore\nScored Data\nand Actions\nScore\nData\nRe-create\nDerived\nVariables\nClean Data\nFigure 12-2:  Deployment steps\nFirst, the data needing to be scored must be pulled from the data store, whe", "ions\nData to\nScore\nScored Data\nand Actions\nScore\nData\nRe-create\nDerived\nVariables\nClean Data\nFigure 12-2:  Deployment steps\nFirst, the data needing to be scored must be pulled from the data store, whether \nit is a SQL or a NoSQL database. The data must be in the same format as the \ndata used for modeling, including having the records representing the same \nunit of analysis and the columns representing the same information as was \nneeded by the models. \nRemember that any fi elds not used in the models directly or indirectly do not \nhave to be pulled from the database. Removing processing steps for variables \nthe modeler included while experimenting but which were not used in the fi nal \nmodels can reduce processing time signifi cantly. For example, if 500 variables \nwere candidate inputs, but only 20 variables were used in the fi nal models, \n480 variables do not need to be pulled from the database; have missing values \nimputed; and do not need to be scaled, normalized, or otherwise tra", "ut only 20 variables were used in the fi nal models, \n480 variables do not need to be pulled from the database; have missing values \nimputed; and do not need to be scaled, normalized, or otherwise transformed. \nAlso, no target is needed to generate model scores, so the processing needed to \ngenerate the target variable for modeling does not have to be replicated in scoring. \nNext, the data must be prepared in a manner consistent with the data \npreparation accomplished during model building, including binning, exploding \n\n356 \nChapter 12 \u25a0 Model Deployment\ninto dummies, missing value imputation, de-skewing, min-max or z-score \nnormalization, or any other operations done to the data. Some special consider-\nations should be remembered for data preparation, including those described in \nTable 12-1. Some of the considerations relate to how data is scaled or normalized \nin deployment compared to how they were scaled or normalized in training of \nthe models. Usually, scaling and normalization", "12-1. Some of the considerations relate to how data is scaled or normalized \nin deployment compared to how they were scaled or normalized in training of \nthe models. Usually, scaling and normalization factors applied in deployment \nwill be the same as those created during model training.\nTable 12-1:  Data Preparation Considerations Prior to Model Scoring\nDATA PREPARATION OPERATION\nCONSIDERATIONS\nMissing Value imputation based on nor-\nmal or uniform distribution\nRetain mean/standard deviation or min-max \nvalues found during model training rather than \nre-computing on the scoring population.\nMissing Value imputation based on \noriginal (modeling) data\nMust keep a representative sample of the \nvariable to draw from randomly.\nMin-max normalization\nKeep min and max value for each variable in \nlookup table; if value falls outside min-max, \nconsider clipping to min/max values in training \ndata, treating as missing, or excluding record \nfrom scoring.\nz-score normalization\nKeep mean and standard", "okup table; if value falls outside min-max, \nconsider clipping to min/max values in training \ndata, treating as missing, or excluding record \nfrom scoring.\nz-score normalization\nKeep mean and standard deviation of each \nvariable to be z-scored from training data.\nBinning\nKeep binning transformation lookup table. Be \nprepared to place variable values that were \nunseen during training into an \u201cother\u201d bin, treat \nas missing, or exclude from scoring.\nExploding into dummies\nBeware of values not seen during training. \nThey should be coded as 0 or be excluded from \nscoring\nLog transforms, polynomial transforms, \nand so on\nProceed normally.\nRatio features\nBeware of divide by zero. \nVariable ranges\nBeware of values in scoring data that exceed \nminimums and maximums of any data seen \nduring modeling, or categorical variable values \nthat were not present during training. \nWhere Deployment Occurs\nScoring can take place anywhere the organization decides is best for generating \nthe scores. The Busin", ", or categorical variable values \nthat were not present during training. \nWhere Deployment Occurs\nScoring can take place anywhere the organization decides is best for generating \nthe scores. The Business Understanding stage should describe where deployment \n\n \nChapter 12 \u25a0 Model Deployment \n357\ntakes place and how quickly scores are needed to make the models operational. \nModel scores are generated primarily in the predictive analytics software, on a \nserver running predictive analytics software deployment solutions, on a local \nserver as a standalone process, in a database, or in the cloud. A summary of the \npros and cons of these methods appears in Table 12-2.\nTable 12-2: Summary of Deployment Location Options\nDEPLOYMENT \nMETHOD \nWHEN TO USE IT\nBENEFITS\nDRAWBACKS\nIn predictive ana-\nlytics software\nWhen scores are \nnot time-critical or \nthe software pro-\nvides easy deploy-\nment steps\nAll data prep \nalready done; no \nmodel translation \nsteps needed\nOften the slowest \nof methods; usuall", "s software\nWhen scores are \nnot time-critical or \nthe software pro-\nvides easy deploy-\nment steps\nAll data prep \nalready done; no \nmodel translation \nsteps needed\nOften the slowest \nof methods; usually \nrequires an analyst \nto initiate scoring \nmanually\nIn a predictive \nanalytics soft-\nware deployment \nsolution\nFor larger projects \nthat justify addi-\ntional expense; for \nlarger organiza-\ntions with many \nmodels to deploy\nTight integration \nwith modeling \nsoftware; often \nvery e\ufb03  cient; \noften contains \nmodel monitoring \nand maintenance \noptions\nOften an expensive \nadd-on; sometimes \nvery complex software \nrequiring additional \ntraining\nHeadless predic-\ntive analytics \nsoftware (no GUI), \nbatch processing\nWhen you need \nautomation of \nmodel deployment \nAll data prep \nalready done; no \nmodel translation \nsteps needed; \nautomated \ndeployment\nErrors in running \nsoftware can stop \nscoring from complet-\ning (must make model-\ning steps \u201cfool proof\u201d).\nIn-database\nWhen you need \ntight integrat", "l translation \nsteps needed; \nautomated \ndeployment\nErrors in running \nsoftware can stop \nscoring from complet-\ning (must make model-\ning steps \u201cfool proof\u201d).\nIn-database\nWhen you need \ntight integration \nwith operational \nsystem, real-time \nor near real-time \nscoring, or large \ndata\nFast deployment\nMust translate data \npreparation and models \nto a form the database \ncan use (if data prepara-\ntion and modeling were \ndone outside of the \ndatabase).\nCloud\nWhen you need \nto scale to bigger \ndata on an occa-\nsional or regular \nbasis\nEasy scaling of \nhardware; can be \ncost e\ufb00 ective in \ncomparison to \nbuying software \nand hardware \nresources\nData prep, when not \nencoded in PMML, \nmust be translated; \nbandwidth limited by \nInternet connections.\n\n358 \nChapter 12 \u25a0 Model Deployment\nDeployment in the Predictive Modeling Software\nDeployment in the predictive modeling software is perhaps the most common \nmethod of deployment. It is by far the simplest of the deployment methods \nbecause if the mod", "he Predictive Modeling Software\nDeployment in the predictive modeling software is perhaps the most common \nmethod of deployment. It is by far the simplest of the deployment methods \nbecause if the models were built in the software, all of the steps the modeler \ndid to generate models and scores already exist and can be resused without \nmodifi cation. When models are scored only occasionally or on an ad hoc basis, \nthis method is effective because no additional software or hardware resources \nare needed. \nHowever, deploying models in the predictive analytics software is a manual \nprocess; the analyst has to launch the software, load the data, and export the \nscores to another location, often a database.\nDeployment in the Predictive Modeling Software Deployment Add-On\nMost predictive modeling software has solutions for model deployment, \nusually sold as an add-on to the modeling software. While these add-ons can be \nexpensive, they integrate tightly with the modeling software, usually ma", "g software has solutions for model deployment, \nusually sold as an add-on to the modeling software. While these add-ons can be \nexpensive, they integrate tightly with the modeling software, usually making \nthe transition from modeling to deployment easy for the analyst. Moreover, the \ndeployment options usually contain additional features to help the organiza-\ntion manage, monitor, and maintain models, saving the analyst from having to \nbuild custom software to accomplish all of these tasks. \nSome features typically included in these software add-ons include support \nfor alerting decision-makers when models have been run, what errors and \nanomalies were discovered in the data or model scores, and when the models \nneed to be refreshed and rebuilt.\nDeployment Using \u201cHeadless\u201d Predictive Modeling Software\nSome PA software allows the analyst to run a model deployment script without \nlaunching the software\u2019s GUI, enabling the analyst to automate when the soft-\nware is run, alleviating the a", "ling Software\nSome PA software allows the analyst to run a model deployment script without \nlaunching the software\u2019s GUI, enabling the analyst to automate when the soft-\nware is run, alleviating the analyst from having to run the models manually. No \nadditional data preparation or translation of the models needs to be done because \nthe same software used to build the models is also used to deploy the models. \nDeployment In-Database\nDeployment done in-database is an excellent option for real-time decisions, \nwhen the volume of data is too large to make extracting scoring data from a \ndatabase practical, or when the organization is database-centric and prefers \nto maintain a database solution. Examples of situations that call for real-time \ndecisions include digital models selecting display ads, rendering content on \n\n \nChapter 12 \u25a0 Model Deployment \n359\nwebsites based on visitor behavior, call center cross-sell and up-sell recommen-\ndations, health insurance claims fraud, and credit car", "ads, rendering content on \n\n \nChapter 12 \u25a0 Model Deployment \n359\nwebsites based on visitor behavior, call center cross-sell and up-sell recommen-\ndations, health insurance claims fraud, and credit card fraud alerts.\nTo run the models in a SQL or NoSQL database, all of the processing steps \ndone to the data in the predictive analytics software must be translated into a \nlanguage the database can use. This includes all data preparation operations \n(missing value imputation, scaling, normalization, and so on) as well as the mod-\nels. Most predictive analytics software will translate models into the Predictive \nModel Markup Language (PMML), and some will also translate models into \nother languages such as C, C#, Java, SAS, or even SQL. \nHowever, data preparation steps are not usually translated by modeling soft-\nware, leaving the analyst to convert these steps manually into code that can be \nused in the database. Great care must be taken to ensure the data preparation \nsteps are done corre", "d by modeling soft-\nware, leaving the analyst to convert these steps manually into code that can be \nused in the database. Great care must be taken to ensure the data preparation \nsteps are done correctly and completely so the models behave properly.\nDeployment in the Cloud\nCloud deployment is the newest of the deployment options in the analyst\u2019s \ntoolbox. Deployment can be done using a service company to consume the \ndata preparation and modeling code, most often PMML, or the organization \ncan create processing steps much like the headless deployment option, except \nthat the hardware the models run on is in the cloud rather than a server within \nthe organization. \nObviously, the greatest advantage is scalability; as the number of records to \nbe scored increases, more cloud resources can be purchased to keep with the \nscoring needs. For retail organizations, scaling on Black Friday, for example, \ncan be done in the cloud with short-term expansion of resources rather than \nhaving to pur", "e purchased to keep with the \nscoring needs. For retail organizations, scaling on Black Friday, for example, \ncan be done in the cloud with short-term expansion of resources rather than \nhaving to purchase hardware and software to handle the worst-case scenarios. \nEncoding Models into Other Languages\nModels deployed in the predictive analytics software can be used in the format \nnative to the software itself. However, when models are deployed outside of the \npredictive modeling environment, they have to be translated into another form \nthat other environments can consume. The most commonly used languages for \nencoding models are PMML, SQL, C#, Java, and SAS. But depending on the needs \nof an organization and the expertise of analysts and programmers within the \norganization, other languages are sometimes used as well, including Python, \n.NET, or even NoSQL scripting languages.\nFigure 12-3 shows a simple decision tree to predict target variable TARGET_B \nfrom one input variable, LASTGIF", "s are sometimes used as well, including Python, \n.NET, or even NoSQL scripting languages.\nFigure 12-3 shows a simple decision tree to predict target variable TARGET_B \nfrom one input variable, LASTGIFT. Decision trees are easy models to convert \nto rules and SQL because of their simplicity. The following Java code for this \ntree includes a condition to apply if LASTGIFT is not defi ned:\n\n360 \nChapter 12 \u25a0 Model Deployment\nif ( LASTGIFT > 14.030 && LASTGIFT > 17.015 ) return(3.686);\nif ( LASTGIFT > 14.030 && LASTGIFT <= 17.015 ) return(4.434);\nif ( LASTGIFT <= 14.030 && LASTGIFT > 8.250 ) return(6.111);\nif ( LASTGIFT <= 14.030 && LASTGIFT <= 8.250 ) return(8.275);\nif ( LASTGIFT == null ) return(-1);\nFigure 12-3:  Simple decision tree to be deployed\nA widely supported language for encoding models is PMML, an XML \ndocument specifi cally designed to support predictive modeling data preparation, \nmodels, and scoring. PMML has been defi ned since 1997 and continues to be \nsupported by nearly", "odels is PMML, an XML \ndocument specifi cally designed to support predictive modeling data preparation, \nmodels, and scoring. PMML has been defi ned since 1997 and continues to be \nsupported by nearly every predictive modeling software package in one form \nor another. Most data preparation steps are supported by PMML, though few \npredictive modeling software packages support the creation or use of PMML \ncode for data preparation. However, most tools support encoding models into \nPMML. One simple example of a model in PMML is the logistic regression \nmodel shown in the following code:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<PMML version=\"4.1\" xmlns=\"http://www.dmg.org/PMML-4_1\">\n<DataDictionary numberOfFields=\"2\">\n\n \nChapter 12 \u25a0 Model Deployment \n361\n    <DataField dataType=\"string\" name=\"TARGET_B\" optype=\"categorical\">\n      <Value value=\"1\"/>\n      <Value value=\"0\"/>\n    </DataField>\n    <DataField dataType=\"integer\" name=\"LASTGIFT\" optype=\"continuous\">\n      <Interval closure=\"close", "TARGET_B\" optype=\"categorical\">\n      <Value value=\"1\"/>\n      <Value value=\"0\"/>\n    </DataField>\n    <DataField dataType=\"integer\" name=\"LASTGIFT\" optype=\"continuous\">\n      <Interval closure=\"closedClosed\" leftMargin=\"0.0\" \n                                       rightMargin=\"1000.0\"/>\n    </DataField>\n  </DataDictionary>\n  <GeneralRegressionModel modelType=\"multinomialLogistic\"\n              functionName=\"classification\"\n              algorithmName=\"LogisticRegression\"\n              modelName=\" TARGET_B Logistic Regression\">\n    <MiningSchema>\n      <MiningField name=\"LASTGIFT\" invalidValueTreatment=\"asIs\"/>\n      <MiningField name=\"TARGET_B\" invalidValueTreatment=\"asIs\" \n                   usageType=\"predicted\"/>\n    </MiningSchema>\n    <ParameterList>\n      <Parameter name=\"p0\" label=\"Intercept\"/>\n      <Parameter name=\"p1\" label=\"LASTGIFT\"/>\n    </ParameterList>\n    <FactorList/>\n    <CovariateList>\n      <Predictor name=\"LASTGIFT\"/>\n    </CovariateList>\n    <PPMatrix>\n      <PPC", "\"Intercept\"/>\n      <Parameter name=\"p1\" label=\"LASTGIFT\"/>\n    </ParameterList>\n    <FactorList/>\n    <CovariateList>\n      <Predictor name=\"LASTGIFT\"/>\n    </CovariateList>\n    <PPMatrix>\n      <PPCell value=\"1\" predictorName=\"LASTGIFT\" parameterName=\"p1\"/>\n    </PPMatrix>\n    <ParamMatrix>\n<PCell targetCategory=\"1\" parameterName=\"p0\" beta=\"-2.5401948236443457\" \n       df=\"1\"/>\n<PCell targetCategory=\"1\" parameterName=\"p1\" beta=\"-0.02410922320457224\" \n       df=\"1\"/>\n    </ParamMatrix>\n  </GeneralRegressionModel>\n</PMML>\nComparing the Costs\nWhich of these methods is most cost effective is specifi c to an organization. \nCost tradeoffs depend not only on hardware and software license costs for \nmodel deployment but also on human resources needed to build and support \ninfrastructure around each of these methods. For example, deploying models \nin the predictive analytics software may not require any additional software \ncosts, but will require an analyst to be dedicated to running the mod", "ach of these methods. For example, deploying models \nin the predictive analytics software may not require any additional software \ncosts, but will require an analyst to be dedicated to running the models each \ntime scores are needed. The analyst will also have to develop custom reports \nthat describe the model scores to the extent the organization needs them. \n\n362 \nChapter 12 \u25a0 Model Deployment\nOn the other hand, purchasing software add-ons to score and manage models \nmay seem like a one-time, up-front purchase. However, they often require mainte-\nnance and monitoring themselves by an analyst. The human resource cost depends \non the organization\u2019s in-house capabilities and the complexity of the software. \nPost-Processing Model Scores\nThe last step in the deployment process is post-processing the models scores. \nPredictive models create scores that, in of themselves, don\u2019t do anything; they \nmust be acted on for the scores to be useful. The way they are used is defi ned \nas business ob", "ing the models scores. \nPredictive models create scores that, in of themselves, don\u2019t do anything; they \nmust be acted on for the scores to be useful. The way they are used is defi ned \nas business objectives created during the Business Understanding stage of \nthe project.\nSeveral options for post-processing scores can be specifi ed during Business \nUnderstanding, including the following:\n \n\u25a0Threshold the score by value into two or more groups: two groups (\u201cselect\u201d \nand \u201cnot select\u201d), three groups (\u201clow,\u201d \u201cmedium,\u201d and \u201chigh\u201d), or more. \nThese groups are sometimes described as segments. Some typical meth-\nods for building the segments is described in more detail in the section \nthat follows.\n \n\u25a0Combine segments with suppression rules to remove records from treat-\nment, or business rules to tune the segments based on non-analytic criteria. \nThe business rules add context to segments determined by the scores alone \nand add business value that the scores did not include.\n \n\u25a0Route segments", " tune the segments based on non-analytic criteria. \nThe business rules add context to segments determined by the scores alone \nand add business value that the scores did not include.\n \n\u25a0Route segments to other humans for further review.\n \n\u25a0Incorporate model scores into an additional analytic system to combine \nthe scores and other rules to improve decisions. This is often an optimi-\nzation system that uses the scores along with other costs and constraints \nbefore making a fi nal decision. For example, in claims fraud, the model \nscore may be one component along with additional business rules and \nrules derived from regulatory constraints, all combined to generate the \nnext best case to investigate.\nAfter applying one or more of these post-processing strategies, decisions \nare generated that result in an action: for example, a triggered contact to the \ncustomer, an investigation by an agent (a phone call, a letter, or a visit, depend-\ning on the model score and other rules), or even the", "t result in an action: for example, a triggered contact to the \ncustomer, an investigation by an agent (a phone call, a letter, or a visit, depend-\ning on the model score and other rules), or even the routing of the transaction \nto another model.\nCreating Score Segments\nSome models are applied to the entire population of records; every record is \nscored and acted on. Cross-sell and up-sell models in a call center are examples \nof this: Every customer is scored and product recommendations are made. \n\n \nChapter 12 \u25a0 Model Deployment \n363\nHowever, if the predictive model is used to select a population to act on, and \nby extension, select another population to not act on, what threshold should be \nused to decide which to select? Generally, there are two approaches to selecting \nthe population: Pick the population that exceeds a model score directly, or pick \nthe population based on cumulative statistics of the rank-ordered population.\nPicking the \u201cSelect\u201d Population Directly from Model Sco", " Pick the population that exceeds a model score directly, or pick \nthe population based on cumulative statistics of the rank-ordered population.\nPicking the \u201cSelect\u201d Population Directly from Model Score\nSome models generate predictions that are immediately interpretable and directly \nusable. For example, the model may predict the actual probability an outcome \nwill occur, and the business objective intends to select every customer that has \nmore than an x percent likelihood of responding. Or the model may predict \nthe customer lifetime value (CLV), and if it is accurate, the action prescribed \nby the business objective may be to contact all customers with CLV > $1,000. In \nthese cases, deployment consists of using the model score directly to select the \nrecords to be included in the population to treat. \nFigure 12-4 shows one such case. For this histogram, the scores are segmented \ninto two groups: a select segment and a non-select segment. The upper bound \nof Bin04 (and lower bound of", "tion to treat. \nFigure 12-4 shows one such case. For this histogram, the scores are segmented \ninto two groups: a select segment and a non-select segment. The upper bound \nof Bin04 (and lower bound of Bin05) is the score 0.0535\u2014the threshold that was \nselected to divide the scores into the two groups. Once these labels are applied, \none of the prescribed post-processing strategies then takes place: direct action \nof the select group, combining the select group with other business rules before \nmaking a decision on the best action to take, or incorporating the select and \nnon-select groups into an optimization procedure.\nBin01\n0\n1034\n2068\n3102\n4136\n5170\n6204\n7238\n8272\n9306\n10340\n11374\nBin02\nBin03\nBin04\nBin05\nBin06\nBin07\nBin08\nBin09\nBin10\nSelect\nP(Target_B = 1)\ncutoff: 0.05350\nNon-Select\nFigure 12-4:  Histogram with a score cutoff\n\n364 \nChapter 12 \u25a0 Model Deployment\nPicking the \u201cSelect\u201d Population Based on Rank-Ordered Statistics\nOften, however, the selection is not based on a specifi c ", "ure 12-4:  Histogram with a score cutoff\n\n364 \nChapter 12 \u25a0 Model Deployment\nPicking the \u201cSelect\u201d Population Based on Rank-Ordered Statistics\nOften, however, the selection is not based on a specifi c score, but rather the \nrank of the score or a cumulative statistic. The most common rank-ordered \nmethods measure gain and lift, and therefore these will be described in more \ndetail here. However, any rank-ordered method can apply the same steps to \nidentify a threshold for creating the select and non-select populations. Two other \ncommon rank-ordered metrics are profi t charts and ROC curves.\nAs an example, consider models built from the KDD Cup 1998 data set. The \ny-axis shows the gain found in the model, meaning the percentage of lapsed \ndonors found using the model after rank-ordering by the model score. The \nupper horizontal line is drawn to show where the 80 percent gain appears in \nthe gains curve. If the model were to be deployed so that the expectation is \nthat 80 percent of the ", "the model score. The \nupper horizontal line is drawn to show where the 80 percent gain appears in \nthe gains curve. If the model were to be deployed so that the expectation is \nthat 80 percent of the lapsed donors would be found by the model, you need \nto determine the model score to use as the cut point or threshold. Figure 12-5 \nshows a gains chart depicting this scenario. \nP(Target_B = 1)\ncutoff: 0.03579\nP(Target_B = 1)\ncutoff: 0.05350\n0\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nFigure 12-5:  Gains chart used for creating a select population\nTable 12-3 shows what the data looks like as this decision is made. The data \nis sorted by model score, from highest score to lowest score (descending). \n\n \nChapter 12 \u25a0 Model Deployment \n365\nThe fi le depth in number of records and percentage of records in the valida-\ntion data is shown in the table as well. If the criterion is to select a population \nsuch that you expect to fi nd 80 percent of lapsed donor responders, you", "s and percentage of records in the valida-\ntion data is shown in the table as well. If the criterion is to select a population \nsuch that you expect to fi nd 80 percent of lapsed donor responders, you need \nto fi nd the record where the 80 percent gain percentage appears. The model \nscore associated with that record is the model score threshold. In this case, that \nscore is 0.03579; whenever the model score exceeds 0.03579, the record is a select, \notherwise it is a non-select.\nTable 12-3: Model Scores Corresponding to Gain Percent and Lift\nMODEL \nSCORE\nGAIN \nPERCENT\nLIFT\nVALIDATION \nDATA FILE \nDEPTH\nVALIDATION \nDATA FILE DEPTH \nPERCENTAGE\n0.10661\n0.00\n0.0\n1\n  0.002\n0.10661\n0.00\n0.0\n2\n  0.004\n0.10661\n0.00\n0.0\n3\n  0.006\n...\n...\n...\n...\n...\n0.03579\n79.96\n1.1416\n33,413\n70.039\n0.03579\n79.96\n1.1416\n33,414\n70.042\n0.03579\n79.96\n1.1416\n33,415\n70.044\n0.03579\n79.96\n1.1415\n33,416\n70.046\n0.03579\n80.00\n1.1421\n33,417\n70.048\n0.03579\n80.04\n1.1426\n33,418\n70.050\nIf the threshold is to be set so that onl", "1416\n33,414\n70.042\n0.03579\n79.96\n1.1416\n33,415\n70.044\n0.03579\n79.96\n1.1415\n33,416\n70.046\n0.03579\n80.00\n1.1421\n33,417\n70.048\n0.03579\n80.04\n1.1426\n33,418\n70.050\nIf the threshold is to be set so that only 50 percent of the recoverable lapsed \ndonors are found rather than 80 percent, the model score cut point is now \n0.0535 as shown in Figure 12-5. Note that the depth of lapsed donors mailed to \nin this scenario is only about 35 percent of the population (the x-axis value for \n50 percent gain).\nIf the criterion is to select all lapsed donors such that the expected model lift \nis greater than or equal to 1.5, Figure 12-6 shows the lift chart for the model. \nThe model score corresponding to the lift value is equal to 1.5 0.06089. Finally, \nif the criterion is to select all lapsed donors such that the expected response rate \nof the model is greater than or equal to 7.5 percent, a model score cutoff value \nof 0.05881 is to be used, as shown in Figure 12-7.\n\n366 \nChapter 12 \u25a0 Model Deployment\nP", "t the expected response rate \nof the model is greater than or equal to 7.5 percent, a model score cutoff value \nof 0.05881 is to be used, as shown in Figure 12-7.\n\n366 \nChapter 12 \u25a0 Model Deployment\nP(Target_B = 1)\ncutoff: 0.06089\nCumulative Lift\nLift = 1.5\nDecile Lift\n10\n20\n30\n40\n0.621\n0.721\n0.821\n0.921\n1.021\n1.121\n1.221\n1.321\n1.421\n1.521\n1.621\n1.721\n1.821\n1.888\n50\n60\n70\n80\n90\n100\nFigure 12-6:  Lift equal to 1.5 as the metric for selecting the population to contact\nP(Target_B = 1)\ncutoff: 0.05881\nResponse\nRate = 7.5%\nResponse Rate\nDecile 1\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nDecile 2\nDecile 3\nDecile 4\nDecile 5\nDecile 6\nDecile 7\nDecile 8\nDecile 9 Decile 10\nFigure 12-7:  Response Rate equal to 7.5 percent as the metric for selecting the population to \ncontact\n\n \nChapter 12 \u25a0 Model Deployment \n367\nWhen Should Models Be Rebuilt?\nEven the most accurate and effective models don\u2019t stay effective indefi nitely. \nChanges in behavior due to new trends, fads, incentives, or disincentives should \nbe expected.", "hould Models Be Rebuilt?\nEven the most accurate and effective models don\u2019t stay effective indefi nitely. \nChanges in behavior due to new trends, fads, incentives, or disincentives should \nbe expected. Common reasons for models to lose effectiveness include:\n \n\u25a0Fraud models: If patterns related to fraud found by models result in the \nidentifi cation of perpetrators, the pool of those using these patterns will \ndecrease as a result of the models, forcing a change in tactics and behavior \nfor those who wish to perpetrate fraud.\n \n\u25a0Customer acquisition models: Some campaigns target products that \nhave a limited supply or tap into a short-term preference or fad. Once \nindividuals have purchased the product, or once the product has had its \nrun, response rates will decrease.\n \n\u25a0Manufacturing defect identifi cation: Once defects have been corrected \nand problems leading to the defect have been corrected, fewer cases will \nresult in high model scores.\nNevertheless, uncovering the reason for ch", "ect identifi cation: Once defects have been corrected \nand problems leading to the defect have been corrected, fewer cases will \nresult in high model scores.\nNevertheless, uncovering the reason for changes in behavior isn\u2019t needed to \nidentify that the models need to be updated; one only needs to measure whether \nor not model accuracy has worsened. The most common way organizations \nmeasure model degradation is by examining average model performance over \nregular time intervals. \nThe rest of this section describes one approach to determine when models \nshould be rebuilt based on model effectiveness. Our discussion will primarily \nrefer to the KDD Cup 1998 data\u2014the lapsed donor problem\u2014but can be extended \nto any other type of classifi cation or regression problem.\nFor example, you can measure the response rate for the population the model \nselected on a weekly basis, such as what is shown in Figure 12-8. In this fi gure, \nin the fi rst four weeks the model was used, an average response", "ure the response rate for the population the model \nselected on a weekly basis, such as what is shown in Figure 12-8. In this fi gure, \nin the fi rst four weeks the model was used, an average response rate of 7.5 \npercent was observed for the lapsed donors selected by the model. However, \nbeginning in week 5, the rate began to decline to the point that after 12 weeks \nthe rate had settled at 6 percent. \n\n368 \nChapter 12 \u25a0 Model Deployment\n0\n0.00%\n1.00%\n2.00%\n3.00%\n4.00%\n5.00%\n6.00%\n7.00%\n8.00%\n2\n4\n6\nResponse Rate\n8\n10\n12\n14\nResponse Rate\nFigure 12-8:  Decaying response rate from model\nHow much reduction in model performance is too much? Or, stated another \nway, is a reduction from 7.5 to 7.3 percent a signifi cant reduction in performance, \nindicating the model needs to be rebuilt? The answer requires consideration of \ntwo additional pieces of information. First, recall that any measure of amounts \nor rates from a sample is an estimate with some uncertainty: The response rate \n7.5 perc", "wer requires consideration of \ntwo additional pieces of information. First, recall that any measure of amounts \nor rates from a sample is an estimate with some uncertainty: The response rate \n7.5 percent is actually 7.5 percent plus or minus some amount. The error depends \non the size of the sample. \nFor two-valued categorical target variables (binomial or binary variables), \na binomial calculator can identify the size of a population needed to identify a \ndifference in response rates that is of interest to an organization operationally. \nWithout addressing the theoretic justifi cation for the formula, the assumptions \nbehind the formula, or the specifi c formula, you can compute the sample size \nneeded if you know the proportion (response rate for the lapsed donor problem), \nthe error that is acceptable or observed, and your desired confi dence that the \nestimate is within the error bounds . Most online calculators begin with sample \nsize, proportion, and confi dence, and return the c", "t is acceptable or observed, and your desired confi dence that the \nestimate is within the error bounds . Most online calculators begin with sample \nsize, proportion, and confi dence, and return the confi dence interval, but this \nis reversed easily by solving for sample size from the error rather than solving \nfor error from the sample size. \nTable 12-4 shows some sample size values for the 7.5 percent response rate. \nThe interpretation of the fi rst row of the table in the context of model deploy-\nment of the lapsed donor model is as follows. Suppose you are expecting a 7.5 \npercent response rate from the campaign based on modeling data. If the cam-\npaign contacted approximately 29,000 individuals, then there is approximately \nan 80 percent confi dence that the observed response rate will fall between 7.3 \nand 7.7 percent, as shown in the third row. So if we return to the question \u201cIs \nthe reduction from 7.5 to 7.3 percent a signifi cant reduction in response rate?\u201d \n\n \nChapter 12 \u25a0 ", "l fall between 7.3 \nand 7.7 percent, as shown in the third row. So if we return to the question \u201cIs \nthe reduction from 7.5 to 7.3 percent a signifi cant reduction in response rate?\u201d \n\n \nChapter 12 \u25a0 Model Deployment \n369\nwe have a complicated answer: According to the table, we expected a 7.3 percent \nresponse rate to be possible, but only with an 80 percent confi dence, so the 7.3 \nresponse rate is on the edge of our expectations.\nIf, on the other hand, the response rate was 6.5 percent, as it was 8 weeks after \nthe model was deployed (see Figure 12-8), the fourth row of Table 12-4 shows \nthat for 28,862 records, this is outside the range 7.1 to 7.9 percent at the 99 percent \nconfi dence level, so it is very unlikely that a 6.5 percent response rate would \nhave been observed by chance; at this point, once the response rate dips below \n7.1 percent, the model should be rebuilt based on the statistical signifi cance test. \nTable 12-4: Binomial Signi\ufb01 cance\nPERCENT \nRESPONSE \nRATE \n\uf6aePROPO", " this point, once the response rate dips below \n7.1 percent, the model should be rebuilt based on the statistical signifi cance test. \nTable 12-4: Binomial Signi\ufb01 cance\nPERCENT \nRESPONSE \nRATE \n\uf6aePROPORTION\uf6af\n+/\uf6bb \nPERCENT \nERROR\nEXPECTED \nRESPONSE \nRATE RANGE \n\uf6aePERCENT\uf6af\nPERCENT \nCONFIDENCE \nSAMPLE SIZE \nNEEDED FOR \nRANGE AND \nCONFIDENCE\n7.5\n0.2\n7.3\u20137.7\n99\n115,447\n7.5\n0.2\n7.3\u20137.7\n95\n66,628\n7.5\n0.2\n7.3\u20137.7\n80\n28,416\n7.5\n0.4\n7.1\u20137.9\n99\n28,862\n7.5\n0.4\n7.1\u20137.9\n95\n16,657\n7.5\n0.4\n7.1\u20137.9\n80\n7,104\n7.5\n0.1\n6.0\u20138.5\n99\n461,788\n7.5\n0.1\n6.0\u20138.5\n95\n266,511\n7.5\n0.1\n6.0\u20138.5\n80\n113,664\nIf the target variable is instead continuous, you could apply a difference of \nmeans test instead as measured by the F-statistic to provide an indication if \nthe outcome of the response from the model is different from what is expected.\nThese kinds of tests\u2014the binomial test and the F test\u2014assume distributions \nthat may or may not be observed in the data. They are useful barometers for \npredictive modelers to use to gauge ", " expected.\nThese kinds of tests\u2014the binomial test and the F test\u2014assume distributions \nthat may or may not be observed in the data. They are useful barometers for \npredictive modelers to use to gauge if differences in rates or mean values are \ntruly different, though modelers should be careful to not interpret them too \nstrictly without a deeper understanding of statistics, the assumptions, and the \nmeaning of the tests.\nModel Assessment by Comparing Observed vs. Expected\nSo far, the observed results after the model is deployed have been compared with \nthe expected rates or mean values based on what was observed during modeling, \n\n370 \nChapter 12 \u25a0 Model Deployment\nand in particular, based on the validation data. However, model deployment \ndoes not necessarily apply to a representative sample. For example, the model \ndeployment could be done for records from just one geographical region or a \nparticular segment of the population being used for a custom deployment. In \nthese cases, the ", "ple. For example, the model \ndeployment could be done for records from just one geographical region or a \nparticular segment of the population being used for a custom deployment. In \nthese cases, the observed response rates may be much lower or higher than the \naverage value found in validation data, not because the models aren\u2019t work-\ning properly, but rather because the input data itself is different than the full \npopulation used in modeling, and therefore response rates for the particular \nsubpopulation are expected to be different.\nFortunately, you know exactly what the model performance should be. Each \nrecord generates a score, and you already know how the scores relate to the \nmetric being used: You identifi ed this relationship during the Model Evaluation \nstage. You therefore know what response rate is expected from the popula-\ntion used for deployment. This is the better number to use as the baseline for \nidentifying when the model is behaving differently than expected becau", "w what response rate is expected from the popula-\ntion used for deployment. This is the better number to use as the baseline for \nidentifying when the model is behaving differently than expected because it \ntakes into account any population used for deployment.\nSampling Considerations for Rebuilding Models\nWhen it is time to rebuild models, care should be taken to ensure the data used \nto rebuild the models is representative of the entire population the models are \nto be applied to, not just the recently treated population. Supervised learning \nrequires a target variable\u2014a measure of what happens when a treatment occurs, \nwhether that treatment is a display ad, an e-mail, an audit, or some other action. \nWhen a model is deployed and its scores are used to select a sub-population of \nrecords, only the sub-population that has been selected will generate responses; \nthe untreated population, obviously, cannot and will not generate responses.\nConsider again the KDD Cup 1998 data set. Assum", "s, only the sub-population that has been selected will generate responses; \nthe untreated population, obviously, cannot and will not generate responses.\nConsider again the KDD Cup 1998 data set. Assume that the select crite-\nrion chose all lapsed donors with an expected response rate greater than or \nequal to 7.5 percent. This translated into selecting all donors whose model \nscore exceeded a cut point of 0.05881 (from Figure 12-7). Now assume that the \nnon-profi t organization has a population of 96,367 lapsed donors to score, and of \nthese, 29,769 lapsed donors have scores that exceed the threshold score of 0.05881 \nand will therefore be selected to be contacted. After the contact has occurred, \nyou will have 29,769 new outcomes: the response of the lapsed donors to the \ncampaign. Each of these 29,769 lapsed donors will have either responded or not \nresponded to the campaign, giving you a new target value for each. This new \nvalue can be used for future modeling in the same way TARGE", "f these 29,769 lapsed donors will have either responded or not \nresponded to the campaign, giving you a new target value for each. This new \nvalue can be used for future modeling in the same way TARGET_B was used \nin the original model. \nHowever, the 66,598 lapsed donors who weren\u2019t contacted have no outcome \nassociated with them; you don\u2019t know whether or not they would have responded \nhad they received the contact. \n\n \nChapter 12 \u25a0 Model Deployment \n371\nThe fi rst fi ve rows of Table 12-5 show a summary of the populations after \napplying the model, including the selects and non-selects if a threshold is used. \nOf course, as the table shows, our expectation is that the response rate of the \nnon-selected population will be only 3.97 percent, just over half the rate of the \npopulation selected by the mode, but we can\u2019t verify this unless the 66,598 \nindividuals are contacted. The last three rows of Table 12-5 show the mailing \ncounts and expected response rate when we incorporate the st", "ted by the mode, but we can\u2019t verify this unless the 66,598 \nindividuals are contacted. The last three rows of Table 12-5 show the mailing \ncounts and expected response rate when we incorporate the strategy that adds \na random subset of non-selects to the mailing.\nTable 12-5: Expected count and response rates from the sampling strategy\nMEASURE\nSELECT\nNON\uf6baSELECT\nMAILING\n# Lapsed donors\n29,769\n66,598\nNA\nMin score\n0.05881\n0.0208\nNA\nMax score\n0.1066\n0.05881\nNA\nExpected response rate\n7.50%\n3.97%\nNA\nNumber of expected responders (if \neveryone mailed)\n2,232\n2,641\nNA\nMail sample (selects and random \nnon-selects)\n29,769\n2,556\n32,325\nExpected responders\n2,232\n101\n2,333\nExpected rate: Select + non-select\n7.5%\n3.97%\n7.2%\nOn the surface, mailing to those who are unlikely to respond is counter-\nintuitive and an unnecessary cost. However, there is a different cost associated \nwith forgoing this additional mailing cost when it is time to rebuild models. \nThere are only a few possible behavioral differ", "e and an unnecessary cost. However, there is a different cost associated \nwith forgoing this additional mailing cost when it is time to rebuild models. \nThere are only a few possible behavioral differences that could cause the lower \nperformance in model scores that lead to rebuilding the models. It could be \nthat the entire population (select and non-select) responds at a lower rate due to \na fundamental behavioral shift. However, it could also be that the entire \npopulation would have responded at approximately the same rate, but who \nresponds has shifted. Some who were not likely to respond before have now \nbecome likely to respond and some who were likely to respond before have \nnow become unlikely to respond.\nHowever, you can\u2019t identify any of these effects unless you measure responses \nfrom the entire population; to understand the shifts in behavior you need to \ncontact those in the non-select population. But isn\u2019t this the very reason you \nbuilt models in the fi rst place, so yo", "s \nfrom the entire population; to understand the shifts in behavior you need to \ncontact those in the non-select population. But isn\u2019t this the very reason you \nbuilt models in the fi rst place, so you wouldn\u2019t need to contact the entire popula-\ntion? The answer is \u201cyes,\u201d but there is an alternative. If you only contact a subset \nof the non-select population that is large enough to measure the response of \n\n372 \nChapter 12 \u25a0 Model Deployment\nnon-responders but small enough that it doesn\u2019t infl uence the cost signifi cantly \nfor the organization, you can satisfy both objectives. \nThe size of the sample needed can be computed in the same way you computed \nthe sample size. Table 12-6 shows the tradeoffs you can make to ensure the sample \nsize is big enough to avoid clashing with the 7.1 percent response rate from the \nselect population. To identify the difference between a cumulative response of \n7.1 percent (the lower bound expected from the model according to Table 12-4 \nif you use the ", "nt response rate from the \nselect population. To identify the difference between a cumulative response of \n7.1 percent (the lower bound expected from the model according to Table 12-4 \nif you use the 99 percent confi dence level) and 3.97 percent, you only need 266 \nrecords. However, this sample size is too small to characterize the non-selects \nwell; only 10 responses would be expected from the non-select group from the \nsample size of 266. A better sample size is 2,000, using the rule-of-thumb from \nChapter 4, which considers the curse of dimensionality. \nIf 2,556 non-selects are included in the population to contact, then a total of \n32,325 (29,769 + 2,556 ) lapsed donors will be mailed. The expected response \nrate for the total population has now dropped from 7.5 percent to 7.2 percent, \nbut you gain two important pieces of information:\n \n\u25a0You can now compare the response rate from the select population to \nthe sample of the non-select population and ensure there is a signifi cant ", ", \nbut you gain two important pieces of information:\n \n\u25a0You can now compare the response rate from the select population to \nthe sample of the non-select population and ensure there is a signifi cant \ndifference in response rates.\n \n\u25a0You now have a signifi cantly sized population of non-selects to include in \nthe training data for new models when they have to be rebuilt.\nTable 12-6: Sample Sizes for Non-Select Population\nPERCENT \nEXPECTED \nRESPONSE \nRATE\nPERCENT \nCONFIDENCE\nPERCENT \nERROR\nMAXIMUM \nEXPECTED \nRESPONSE RATE\n\uf6aePERCENT\uf6af\nNUMBER OF \nSAMPLES \nNEEDED\n4\n99\n1\n5\n2,556\n4\n99\n1.5\n5.5\n1,113\n4\n99\n2\n6\n639\n4\n99\n3.1\n7.1\n266\nWhat Is Champion-Challenger?\nThe champion model is the model being deployed currently. We have already \ndescribed why models need to be rebuilt and what data to use to rebuild models. \nHowever, rather than waiting until a model needs to be rebuilt before building a \nnew one, the champion-challenger approach builds a challenger model as often as \n\n \nChapter 12 \u25a0 Model De", "ild models. \nHowever, rather than waiting until a model needs to be rebuilt before building a \nnew one, the champion-challenger approach builds a challenger model as often as \n\n \nChapter 12 \u25a0 Model Deployment \n373\nnew data is available to improve the existing model. Once the challenger model \nperforms better than the champion model, the challenger model replaces the \nchampion model and the process of building a new challenger model begins. \nBuilding the challenger model is usually an automated background process \nrather than a manual, analyst-driven process. However, any time models are \nbuilt automatically, care must be taken to ensure the new model is not only \nmore accurate, but also appears to be stable and continues to make sense. In \nother words, the analyst should still ensure the new model is acceptable before \ndeploying the challenger model.\nConsider Figure 12-9. The champion model is the same one shown in Figure \n12-8, which shows the declining response rate. Each new challen", "ew model is acceptable before \ndeploying the challenger model.\nConsider Figure 12-9. The champion model is the same one shown in Figure \n12-8, which shows the declining response rate. Each new challenger model \nincludes data used in building the initial models, but is augmented by the new \ndata collected from the treatments from weeks 1 and 2 (the two left-most data \npoints in the fi gure). Its performance is remaining stable, in part because it is \nincorporating new data the original model had not seen. The proportion of data \nto use from the initial modeling data compared to the new data depends on \nhow much new data is available from weeks 1 and 2; if there is suffi cient data \nto build stable models, only newer data can be used. This approach is good to \nidentify new behavioral patterns that emerge over time. If insuffi cient data is \navailable from weeks 1 and 2, the data can be augmented with older data.\n0\n0.00%\n1.00%\n2.00%\n3.00%\n4.00%\n5.00%\n6.00%\n7.00%\n8.00%\n2\n4\n6\n8\n10\n12\n14\nCha", "s that emerge over time. If insuffi cient data is \navailable from weeks 1 and 2, the data can be augmented with older data.\n0\n0.00%\n1.00%\n2.00%\n3.00%\n4.00%\n5.00%\n6.00%\n7.00%\n8.00%\n2\n4\n6\n8\n10\n12\n14\nChallenger\nChampion\nFigure 12-9:  Champion vs. Challenger average response rate\nAssessing the accuracy of the champion model is straightforward for the \nlapsed donor problem: Compute the response rate for the lapsed donors selected \nto treat. How is the challenger model assessed? After all, this model isn\u2019t being \ndeployed. \n\n374 \nChapter 12 \u25a0 Model Deployment\nThe simplest method is to compare the accuracy of the challenger model on \nits own validation data with the accuracy of the champion model on its own \nvalidation data. The two values can then be tested for signifi cance; when the \nchallenger model accuracy on validation data is signifi cantly better than the \nchampion model accuracy on its validation data, replace the champion model \nwith the challenger model.\nHowever, if the characteri", "r model accuracy on validation data is signifi cantly better than the \nchampion model accuracy on its validation data, replace the champion model \nwith the challenger model.\nHowever, if the characteristics of the model inputs are changing, comparisons \nbetween the two models are not based on the same data and therefore could \nbe misleading.\nOne approach compares the two models based on identical data, taking \nadvantage of the similarity in the population of records the models select. In \nthis approach, let\u2019s compare the accuracy of the two models using only records \nthat were selected by the champion model and would have been selected by \nthe challenger model. The champion model selects were treated\u2014they were \ncontacted in the lapsed donor model example\u2014so you have actual responses for \nthese records. The model with the higher response rate indicates a better model.\nFigure 12-10 shows this approach visually. The champion model and chal-\nlenger model were scored on 25,500 records. The n", "ese records. The model with the higher response rate indicates a better model.\nFigure 12-10 shows this approach visually. The champion model and chal-\nlenger model were scored on 25,500 records. The number of records the two \nmodels have in common is 20,000, and therefore the response rates on the actual \ntreatment will be computed based on the 20,000 records the two models have \nin common. \nSelect\nSelect\nChampion\nChallenger\nSelect\nNon-\nSelect\nNon-\nSelect\nNon-\nSelect\n23,000\n2,500\n20,000\n3,000\n2,000\n500\nFigure 12-10:  Champion-Challenger sampling\nThe determination of when the challenger model is better than the cham-\npion model, or more precisely, how much better the challenger model needs \nto be before it is considered defi nitively better can be made in the same way \nthe determination was made whether or not a new model is needed: When the \ndifference between the champion model accuracy and the challenger model \n\n \nChapter 12 \u25a0 Model Deployment \n375\naccuracy is unlikely to have occurr", " made whether or not a new model is needed: When the \ndifference between the champion model accuracy and the challenger model \n\n \nChapter 12 \u25a0 Model Deployment \n375\naccuracy is unlikely to have occurred by chance as measured by the binomial \nsignifi cance test or an F test, the challenger model is considered better and will \nreplace the champion model for deployment.\nSummary\n Deployment of predictive models is the most underappreciated stage of \nthe CRISP-DM process. Planning for deployment begins during Business \nUnderstanding and should incorporate not only how to generate model scores, \nbut also how to convert the scores to decisions, and how to incorporate the deci-\nsions into an operational system. \nGreatly increased effi ciency in generating model scores can be achieved by \neliminating processing no longer needed to generate the model scores and by \npushing as much of the data preparation steps up to the database as possible. \nFinally, a good deployment system plan recognizes tha", "ng processing no longer needed to generate the model scores and by \npushing as much of the data preparation steps up to the database as possible. \nFinally, a good deployment system plan recognizes that models are not \nstatic: They were built from data that represented the history at a particular \nmoment. But that moment will ultimately pass. Models should be monitored and \nultimately be replaced by an improved model when needed. \n\n\n377\nThese case studies are included to describe real projects that use principles \ndescribed in this book to create predictive models. Throughout any project, \ndecisions are made at key points of the analysis that infl uence the quality of the \nfi nal solution; this is the art of predictive modeling. Rarely does an analyst have \ntime to consider all of the potential approaches to a solution, and therefore deci-\nsions must be made during Data Preparation, Modeling, and Model Evaluation. \nThe case studies should be used as motivational rather than as recipes f", "tial approaches to a solution, and therefore deci-\nsions must be made during Data Preparation, Modeling, and Model Evaluation. \nThe case studies should be used as motivational rather than as recipes for \npredictive modeling. The analyses in these case studies don\u2019t present perfect \nsolutions, but they were successful. The survey analysis case study had the luxury \nof trying two approaches, each with pros and cons. The help desk case study \nsucceeded because of the perseverance of the analysts after the fi rst modeling \napproach failed. In both cases, the fi nal solution used the science of predictive \nanalytics plus creative twists that were unconventional, but productive. \nSurvey Analysis Case Study: Overview\nThis case study describes a survey analysis project for the YMCA, a cause-driven \ncharity whose core mission is to improve the lives of its members and build \ncommunities. The YMCA achieves these objectives primarily through facilities \nand programs that promote the physical well", " \ncharity whose core mission is to improve the lives of its members and build \ncommunities. The YMCA achieves these objectives primarily through facilities \nand programs that promote the physical well-being of its members. \n C H A P T E R \n13\nCase Studies\n\n378 \nChapter 13 \u25a0 Case Studies\nThe YMCA expends considerable time and effort to understand how to help \nmembers achieve their fi tness goals and build community. These analyses are \ngrounded in sound social science and must be diagnostic and predictive so \nthey are understandable by decision makers. Moreover, decisions are made at \nthe branch level\u2014the individual YMCA facility.\nOne source of data for achieving the analytics objectives is the annual member \nsurvey. The YMCA, as a national organization with more than 2,500 branches \nacross the country, developed a member survey for use by its branches. Tens \nof thousands of these surveys are completed each year. \nSEER Analytics (http://www.seeranalytics.com) was, and continues to be, \n", " the country, developed a member survey for use by its branches. Tens \nof thousands of these surveys are completed each year. \nSEER Analytics (http://www.seeranalytics.com) was, and continues to be, \nthe leader in analyzing YMCA surveys and creating actionable insights based \non the analyses. I was a subcontractor to Seer Analytics for the work described \nin the case study.\nBusiness Understanding: De\ufb01 ning the Problem\nFor the project, 32,811 surveys with responses were made available for the year \n2001. Modeling data from the survey contained 48 multiple choice questions \ncoded with values between 1 and 5, where 1 was the most positive response \nand 5 the most negative. Questions were primarily attitudinal, related to the \nmember\u2019s experience with the Y, though four questions were demographic \nvariables. There were two free-form text questions and two related fi elds that \ncategorized the text into buckets, but these were not used in the analysis.\nThroughout this case study, questions ", "hic \nvariables. There were two free-form text questions and two related fi elds that \ncategorized the text into buckets, but these were not used in the analysis.\nThroughout this case study, questions will be identifi ed by a Q followed by \nthe question number, so Q1 for question 1. Sometimes an explanation of the key \nidea of the question will be provided in parentheses, such as Q1 (satisfaction), \nindicating that the key idea in question 1 is member satisfaction.\nDe\ufb01 ning the Target Variable\nThree questions in the survey were identifi ed as key questions related to attitudes \nand behaviors in members that are indicative of how well the Y is meeting the \nneeds and expectations of the members. These three questions were Q1, Q32, \nand Q48, with the full text of the questions shown in Table 13-1. \n\n \nChapter 13 \u25a0 Case Studies \n379\nTable 13-1: Three Questions for Target Variables\nQUESTION\nFULL QUESTION\nSUMMARY OF IDEA \nREPRESENTED IN \nQUESTION\nPERCENT WITH \nRESPONSE = 1\nQ1\nOverall, how wou", "3-1. \n\n \nChapter 13 \u25a0 Case Studies \n379\nTable 13-1: Three Questions for Target Variables\nQUESTION\nFULL QUESTION\nSUMMARY OF IDEA \nREPRESENTED IN \nQUESTION\nPERCENT WITH \nRESPONSE = 1\nQ1\nOverall, how would you \nrate the [Branch Name] \nYMCA?\nSatisfaction\n31%\nQ32\nAll things considered, \ndo you think you will \nbelong to this Club a \nyear from now?\nIntend to renew\n46%\nQ48\nWould you recommend \nthe Club to your friends?\nRecommend to a friend\n54%\nThe three target variables had high association with each other. Of the 31 \npercent of Q1 = 1 responders, 86 percent of them also had Q48 = 1, a lift of 1.6 \nover the 54 percent baseline for Q48 = 1. Conversely, of the 54 percent with \nQ48 = 1, 49 percent have Q1 = 1, a lift of 1.6 as well. \nBecause of this phenomenon and the desire to create a single model to char-\nacterize members with a highly positive opinion of their Y branch, we created \na new derived target variable: a simple linear combination of the three ques-\ntions, called the Index of Excell", " to char-\nacterize members with a highly positive opinion of their Y branch, we created \na new derived target variable: a simple linear combination of the three ques-\ntions, called the Index of Excellence (IOE). In order to have larger values of IOE \nconsidered better than smaller values and have its maximum value equal to \n10, we reversed the scale by subtracting the sum from the maximum possible \nvalue (15), or\nIOE = 10 \u00d7 [(15 \u2013 Q1 \u2013 Q32 \u2013 Q48) \u00f7 12]\nThe three target variables are all skewed toward the lower end of their respec-\ntive distributions, and therefore IOE, with its distribution reversed, was skewed \ntoward the upper end of its range; the vast majority of values exceed 7, as can \nbe seen in Figure 13-1.\n\n380 \nChapter 13 \u25a0 Case Studies\n0\n[0\u20130.716]\n(0.716\u20131.432]\n(1.432\u20132.148]\n(2.148\u20132.864]\n(2.864\u20133.58]\n(3.58\u20134.296]\n(4.296\u20135.012]\n(5.012\u20135.728]\n(5.728\u20136.444]\n(6.444\u20137.36]\n(7.36\u20137.876]\n(7.876\u20138.592]\n(8.592\u20139.308]\n(9.308\u201310]\n645\n1290\n1935\n2580\n3225\n3870\n4515\n5160\n5805\n6450\n7098\nFi", "148\u20132.864]\n(2.864\u20133.58]\n(3.58\u20134.296]\n(4.296\u20135.012]\n(5.012\u20135.728]\n(5.728\u20136.444]\n(6.444\u20137.36]\n(7.36\u20137.876]\n(7.876\u20138.592]\n(8.592\u20139.308]\n(9.308\u201310]\n645\n1290\n1935\n2580\n3225\n3870\n4515\n5160\n5805\n6450\n7098\nFigure 13-1: Index of Excellence distribution\nData Understanding\nThe candidate input questions are shown in Table 13-2, along with their category \nlabels as determined by the analysts. \nTable 13-2: Questions and Categories for Model Inputs and Outputs\nCATEGORY\nQUESTIONS\nSta\ufb00 \nQ2, Q3, Q4, Q5, Q6, Q7, Q8, Q9, Q10, Q11, Q12\nBuilding\nQ13, Q14, Q15, Q16, Q17, Q42\nEquipment\nQ18, Q19, Q20, Q37, Q38, Q39, Q40, Q41\nPrograms\nQ21, Q23, Q43\nValue\nQ22\nGoals\nQ44\nRelationships\nQ24, Q25, Q26, Q27\nOther\nQ28, Q29, Q30, Q31, Q45, Q46, Q47\nTargets\nQ1, Q32, Q48\nText\nQ33, Q34, Q35, Q36\nMembership\nQ49, Q50, Q51\nDemographics\nQ52, Q53, Q54, Q55\nFirst, two of the quirks and potential problems in the data are as follows:\n \n\u25a0In the full data set, 232 surveys had no responses (0.7 percent of the total \nsurvey population", "aphics\nQ52, Q53, Q54, Q55\nFirst, two of the quirks and potential problems in the data are as follows:\n \n\u25a0In the full data set, 232 surveys had no responses (0.7 percent of the total \nsurvey population); all the response values were zero. However, these \n\n \nChapter 13 \u25a0 Case Studies \n381\n232 surveys had text comments, indicating the members still wanted to \ncommunicate a viewpoint. These surveys were included in the analysis \n(an oversight), though had little effect on the patterns found.\n \n\u25a0One question that arose was whether there was a signifi cant number of \nmembers who merely checked the same box for all the questions: all 1s, \nall 2s, all 3s, and so on. The good news was that the worst instance of \nthis pattern was in 274 surveys that had all responses equal to 1 (only 0.8 \npercent of the population), more than the number of surveys with all 2s \n(58), all 3s (108). Therefore, the responses to the questions were believed \nto be an accurate refl ection of the true member response.\nB", " the population), more than the number of surveys with all 2s \n(58), all 3s (108). Therefore, the responses to the questions were believed \nto be an accurate refl ection of the true member response.\nBecause neither of these problems cause signifi cant numerical issues, no cor-\nrective action was taken.\nData Preparation\nVery little data preparation was needed for the data set. The responses were \ncoded with values 0 through 5, so there were no outliers. There were few NULL \nvalues. Any NULL values were recoded as 0.\nThe most signifi cant decision about the data was determining how to use the \nquestions in the algorithms. The data was ordinal and not truly continuous. \nHowever, for regression models, it was far more convenient to use the data as \ncontinuous because in this case, there is only one column in the data per question. \nIf the data were assumed to be categorical, the questions would be exploded to \ndummy variables, leading to up to fi ve or six columns per question if the zeros", "y one column in the data per question. \nIf the data were assumed to be categorical, the questions would be exploded to \ndummy variables, leading to up to fi ve or six columns per question if the zeros \nwere included as dummy columns. \nMissing Data Imputation\nThe next most signifi cant issue was the problem with responses coded with \n0\u2014the code for no response. If the data were treated as categorical, these could \nbe left as the value 0 and treated as just another level. However, if the variables \nare treated as continuous or ordinal, leaving these as 0 communicates to the \nalgorithms that 0 is the best answer of all (smaller than the \u201ctop\u201d response \nvalue of 0). These values therefore were similar in function to missing values \nand needed to be corrected in some way.\nOn the surface, mean imputation seemed to be a misleading way to recode \nthese values; coding non-responses as 3 didn\u2019t necessarily convey the relation-\nship between the non-responses and the target variables. Instead, the", "mputation seemed to be a misleading way to recode \nthese values; coding non-responses as 3 didn\u2019t necessarily convey the relation-\nship between the non-responses and the target variables. Instead, the approach \nwas to identify the relationship between the 0s and the target variable fi rst, and \nthen recode the 0s to values that did the least harm. \n\n382 \nChapter 13 \u25a0 Case Studies\nFigure 13-2 shows two plots. The plots show the relationship between the \nmean value of the target Q1 = 1 and Q2. Q2 = 5 was omitted because for this \nquestion and most others, the counts for the worst response code, 5, was very \nsmall and did not follow the trend of the other responses. Note that the relation-\nship between the percentage of responses for Q1 = 1 decreases monotonically \nwith Q2. The percentage of Q1 = 1 responses for non-responsive Q2 was 16.24 \npercent, shown at the far left of both plots.\nThe plot at the right shows a linear curve fi t of the data points and the plot \nat the left an exponent", "Q1 = 1 responses for non-responsive Q2 was 16.24 \npercent, shown at the far left of both plots.\nThe plot at the right shows a linear curve fi t of the data points and the plot \nat the left an exponential curve fi t of the percentage of Q1 = 1 vs. Q2, and the \nresulting formulas for the curve fi ts are shown on the plots. The equations were \nsolved for \u201cx\u201d (Q2), which yielded Q2 = 2.2 for the exponential equation and \nQ2 = 2.8 for the linear fi t. Replacing the 0s with either of these values would bias \nthe models less than using the mean (1.8) or the median (2). The fi nal formula for \nimputation places the 0s at a value in between these, 2.5 in the case of question Q2. \nPct Q1 = 1 for vs. Q2\nPct Q1 = 1 for vs. Q2\n0\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n1\n2\n3\n4\n5\ny = 1.3214e\u22120.946x\nR2 = 0.99621\nQ1_1_Mean\nExpon.\n(Q1_1_Mean)\n0\n0.1\n\u22120.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0\n1\n2\n3\n4\n5\ny = \u22120.1664x + 0.6256\nR2 = 0.82838\nQ1_1_Mean\nLinear\n(Q1_1_Mean)\nFigure 13-2: Missing value imputation\nWas this cheating, to use the ta", "ean\nExpon.\n(Q1_1_Mean)\n0\n0.1\n\u22120.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0\n1\n2\n3\n4\n5\ny = \u22120.1664x + 0.6256\nR2 = 0.82838\nQ1_1_Mean\nLinear\n(Q1_1_Mean)\nFigure 13-2: Missing value imputation\nWas this cheating, to use the target variable in the imputation? The answer \ntechnically was \u201cyes\u201d; using the target variable in decisions for how to recode \nand transform variables is inherently dangerous, possibly leading to biased \ndecisions that will increase accuracy on training data but lessen accuracy on \nnew data. On the other hand, from a practical standpoint, the answer is clearly \n\u201cno.\u201d Because the values used in imputation were nearly always 2.5, they were \nstable and not prone to overfi tting data based on the imputation value. \nThis process of imputing missing values using a linear fi t of a question to the \ntarget variable was repeated for every question. Very often, the value 2.5 was a \ngood value to use like Q2, though this wasn\u2019t always the case. \nFeature Creation and Selection through Factor Analysis\nT", "rget variable was repeated for every question. Very often, the value 2.5 was a \ngood value to use like Q2, though this wasn\u2019t always the case. \nFeature Creation and Selection through Factor Analysis\nThe questions in a survey were intended to capture attitudes of the respond-\ners. Often, multiple questions were asked to uncover an attitude or opinion in \ndifferent ways, which led to a high correlation between questions. One approach \noften used by social scientists to uncover the underlying ideas represented by the \n\n \nChapter 13 \u25a0 Case Studies \n383\nquestions is Factor Analysis, a technique closely related to Principal Component \nAnalysis, discussed in Chapter 6. Even though the algorithm has some differ-\nence with PCA, the use of Factor Analysis in this case study mirrors the use of \nPCA described in Chapter 6. \nAfter applying Factor Analysis using default settings, we determined that the \ntop ten factors were interesting and comprised enough of the variance to be a \ngood cutoff. As is", "escribed in Chapter 6. \nAfter applying Factor Analysis using default settings, we determined that the \ntop ten factors were interesting and comprised enough of the variance to be a \ngood cutoff. As is the usual practice, each of the factors was named according to \nthe questions that loaded highest on the factors. The factor loadings for fi ve of \nthe factors are shown in Table 13-3. The highest loading questions on the factors \nappear in bold. The highest loading questions generally tend to be questions \nneighboring each other because of the way the survey was laid out, though it \nisn\u2019t always the case. In addition, a few questions loaded high on multiple fac-\ntors, indicating a clean set of ideas identifi ed by Factor Analysis.\nTable 13-3: Factor Loadings for Five of Six Top Factors\nFACTOR \nDESCRIPTION\nSTAFF \nCARES\nFACILITIES \nCLEAN/SAFE\nEQUIPMENT\nREGISTRATION\nFRIENDLY \nSTAFF\nFactor \nNumber\n1\n2\n3\n4\n6\nQ2\n0.295\n0.238\n0.115\n0.458\n0.380 \nQ3\n0.217\n0.143\n0.093\n0.708\n0.077 \nQ4\n0.298\n0.174\n0.", "R \nDESCRIPTION\nSTAFF \nCARES\nFACILITIES \nCLEAN/SAFE\nEQUIPMENT\nREGISTRATION\nFRIENDLY \nSTAFF\nFactor \nNumber\n1\n2\n3\n4\n6\nQ2\n0.295\n0.238\n0.115\n0.458\n0.380 \nQ3\n0.217\n0.143\n0.093\n0.708\n0.077 \nQ4\n0.298\n0.174\n0.106\n0.601\n0.266 \nQ5\n0.442\n0.198\n0.087\n0.173\n0.613 \nQ6\n0.417\n0.254\n0.142\n0.318\n0.584 \nQ7\n0.406\n0.277\n0.167\n0.252\n0.461 \nQ8\n0.774\n0.058\n0.041\n0.093\n0.113 \nQ9\n0.733\n0.175\n0.108\n0.145\n0.260 \nQ10\n0.786\n0.139\n0.079\n0.110\n0.218 \nQ11\n0.765\n0.120\n0.101\n0.132\n0.015 \nQ12\n0.776\n0.090\n0.049\n0.087\n0.014 \nQ13\n0.145\n0.728\n0.174\n0.112\n0.110 \nQ14\n0.191\n0.683\n0.163\n0.151\n0.124 \nQ15\n0.102\n0.598\n0.141\n0.090\n0.070 \nQ16\n0.100\n0.370\n0.133\n0.082\n0.035 \nQ17\n0.128\n0.567\n0.229\n0.102\n0.080 \nQ18\n0.148\n0.449\n0.562\n0.116\n0.114 \nQ19\n0.129\n0.315\n0.811\n0.101\n0.103 \nQ20\n0.171\n0.250\n0.702\n0.086\n0.078 \n\n384 \nChapter 13 \u25a0 Case Studies\nTable 13-4 rearranges the information in Table 13-3, showing a summary of \nthe factors, the top loading questions, and the percent variance explained. It is \nperhaps more clear from this table tha", "ies\nTable 13-4 rearranges the information in Table 13-3, showing a summary of \nthe factors, the top loading questions, and the percent variance explained. It is \nperhaps more clear from this table that questions did not overlap between factors.\nTable 13-4: Summary of Factors, Questions, and Variance Explained\nFACTOR\nFACTOR DESCRIPTION\nTOP LOADING \nQUESTIONS\nCUMULATIVE PERCENT \nVARIANCE EXPLAINED\nFactor 1\nSta\ufb00  cares\nQ8, Q9, Q10, Q11, \nQ12\n12.2\nFactor 2\nFacilities clean/safe\nQ13, Q14, Q16\n20.1\nFactor 3\nEquipment\nQ18, Q19, Q20\n25.3\nFactor 4\nRegistration\nQ3, Q4\n29.7\nFactor 5\nCondition of locker \nrooms, gym, swimming \npool\nQ39, Q40, Q41\n34.1\nFactor 6\nFriendly/competent sta\ufb00 \nQ5, Q6, Q7\n38.2\nFactor 7\nFinancial assistance\nQ28, Q29\n42.2\nFactor 8\nParking\nQ16, Q42\n46.0\nFactor 9\nGood place for families\nQ26, Q30\n49.1\nFactor 10\nCan relate to members / \nfeel welcome\nQ23, Q24, Q25\n52.1\nTo help understand why the factors were labeled as they were, Tables 13-5, \n13-6, 13-7, and 13-8 show the question ", "ies\nQ26, Q30\n49.1\nFactor 10\nCan relate to members / \nfeel welcome\nQ23, Q24, Q25\n52.1\nTo help understand why the factors were labeled as they were, Tables 13-5, \n13-6, 13-7, and 13-8 show the question numbers and corresponding descriptions \nof the questions that loaded highest for the top four factors. \nTable 13-5: Top Loading Questions for Factor 1, Sta\ufb00  Cares\nQ8\nKnow your name\nQ9\nCare about your well-being\nQ10\nTake the initiative to talk to members\nQ11\nCheck on your progress & discuss it with you\nQ12\nWould notice if you stop coming\nTable 13-6: Top Loading Questions for Factor 2, Facilities Clean/Safe\nQ13\nOverall cleanliness\nQ14\nSecurity and safety\nQ16\nAdequate parking\n\n \nChapter 13 \u25a0 Case Studies \n385\nTable 13-7: Top Loading Questions for Factor 3, Equipment\nQ18\nMaintenance of equipment\nQ19\nHas the right equipment\nQ20\nHas enough equipment\nTable 13-8: Top Loading Questions for Factor 4, Registration\nQ3\nEase of program or class registration\nQ4\nSta\ufb00  can answer questions about schedules", "Q19\nHas the right equipment\nQ20\nHas enough equipment\nTable 13-8: Top Loading Questions for Factor 4, Registration\nQ3\nEase of program or class registration\nQ4\nSta\ufb00  can answer questions about schedules, classes, \netc.\nModeling\nSampling for building the predictive models was standard: 50 percent of the data \nwas used for training, 50 percent for out-of-sample testing. The fi rst models used \na traditional approach of stepwise linear regression to predict IOE with a few \nkey questions and the factor analysis projections as inputs. Some descriptions \nof Factor Analysis and Principal Component Analysis (like Wikipedia) describe \nthese techniques as performing data reduction or dimensionality reduction. \nA more accurate description is that these techniques perform candidate input \nvariable reduction. All 48 of the original questions are still needed to compute \nthe factors or principal components, so all of the data is still needed even if the \nFactor Analysis or Principal Component Analysis", "tion. All 48 of the original questions are still needed to compute \nthe factors or principal components, so all of the data is still needed even if the \nFactor Analysis or Principal Component Analysis reduces the questions down \nto ten factors like the ones shown in Table 13-4. \nFor this project, two questions were identifi ed as key questions on their own: \nQ22 (Value for the money) and Q44 (How has the YMCA helped meet your fi t-\nness goals) based on the recommendations of domain experts and the strong \npredictive relationship of these questions in preliminary modeling. These were \nincluded in addition to the ten factors so that there were 12 candidate inputs. \nA stepwise linear regression variable selection approach was taken in building \nthe model summarized in Table 13-9.\nTable 13-9: Regression Model with Factors as Inputs\nVARIABLE\nVALUE\nSTD. ERROR\nT VALUE\nPR\uf6ae>|T|\uf6af\n(Intercept)\n10.2566\n0.0172\n597.0967\n0.0000\nQ44\n\u20130.5185\n0.0074\n\u201370.4438\n0.0000\nQ22\n\u20130.4893\n0.0068\n\u201371.5139\n0.0000\nCont", "Regression Model with Factors as Inputs\nVARIABLE\nVALUE\nSTD. ERROR\nT VALUE\nPR\uf6ae>|T|\uf6af\n(Intercept)\n10.2566\n0.0172\n597.0967\n0.0000\nQ44\n\u20130.5185\n0.0074\n\u201370.4438\n0.0000\nQ22\n\u20130.4893\n0.0068\n\u201371.5139\n0.0000\nContinues\n\n386 \nChapter 13 \u25a0 Case Studies\nVARIABLE\nVALUE\nSTD. ERROR\nT VALUE\nPR\uf6ae>|T|\uf6af\nFactor2\n\u20130.2761\n0.0055\n\u201350.5849\n0.0000\nFactor1\n\u20130.2397\n0.0051\n\u201347.0156\n0.0000\nFactor6\n\u20130.2242\n0.0056\n\u201339.9239\n0.0000\nFactor9\n\u20130.2158\n0.0054\n\u201340.0539\n0.0000\nFactor10\n\u20130.1917\n0.0057\n\u201333.4452\n0.0000\nFactor3\n\u20130.1512\n0.0051\n\u201329.472\n0.0000\nFactor4\n\u20130.1068\n0.0055\n\u201319.2649\n0.0000\nFactor5\n\u20130.0798\n0.0054\n\u201314.846\n0.0000\nAll of the factors included in the model were signifi cant predictors. Factors 7 \nand 8 were removed as a result of the stepwise procedure, as their reduction in \naccuracy did not justify their inclusion per the Akaike information criterion (AIC). \nHowever, the two key questions, Q44 and Q22, had a much larger infl uence on \nthe model as evidenced by their coeffi cients and t values. \nAn alternative appro", "the Akaike information criterion (AIC). \nHowever, the two key questions, Q44 and Q22, had a much larger infl uence on \nthe model as evidenced by their coeffi cients and t values. \nAn alternative approach was tried as well. Rather than using the factors as \ninputs to the models, the question that loaded the highest on each factor was \nselected as the representative of the factor. This approach has an advantage over \nusing the factors as inputs: It is more transparent. \nThe factors, while representing an idea, still require all of the inputs so the \nfactors can be computed; each factor is a linear combination of all the survey \nquestions. However, if you use just one representative question for each factor, \nthe question that loaded the highest on the factor can be used instead of the entire \nfactor. In this approach, rather than needing all the questions to run the model, \nonly 12 questions (at most, if no variable selection took place) are candidate inputs. \nAfter further assessment of", " \nfactor. In this approach, rather than needing all the questions to run the model, \nonly 12 questions (at most, if no variable selection took place) are candidate inputs. \nAfter further assessment of the data, a third key variable, Q25 (Feel welcome at \nthe YMCA) was added, making 13 candidate inputs to the model. That model, \nwith seven inputs found by a stepwise regression procedure, is summarized in \nTable 13-10. The four factors represented in the model are 1, 2, 3, and 6. \nTable 13-10: Regression Model with Representative Questions as Inputs\nINPUT \nQUESTION\nQUESTION DESCRIPTION\nFACTOR\n1\nQ25\nFeel Welcome\nNA\n2\nQ44\nY Helps Meet Fitness Goals\nNA\n3\nQ22\nValue for the Dollar\nNA\n4\nQ13\nFacilities clean\nFactor 2\n5\nQ18\nEquipment Maintained\nFactor 3\nTable 13-9 (continued)\n\n \nChapter 13 \u25a0 Case Studies \n387\n6\nQ9\nSta\ufb00  Cares about Well-Being\nFactor 1\n7\nQ6\nCompetent Sta\ufb00 \nFactor 6\nAfter a comparison of the two approaches, using individual questions as \ninputs rather than the factors, generated h", "udies \n387\n6\nQ9\nSta\ufb00  Cares about Well-Being\nFactor 1\n7\nQ6\nCompetent Sta\ufb00 \nFactor 6\nAfter a comparison of the two approaches, using individual questions as \ninputs rather than the factors, generated higher R-squared, and therefore was \nthe model selected for use. \nModel Interpretation\nThe regression model was not built so that the three target questions or the \ncombined form of the three questions, IOE, could be predicted. They were \nbuilt to identify member attitudes related to these target questions. The models \nidentifi ed the key questions (inputs) related to IOE nationally. However, the \nmodels needed to be able to inform individual branches how well they were \nachieving their own IOE, and in what areas they could improve their Y so that \nIOE could be increased. \nTo explain the models, Seer Analysis focused on the questions included as \ninputs to the models. If a YMCA branch was doing well with those questions, \nthey were necessarily doing well on the IOE scale. The branches then ", ", Seer Analysis focused on the questions included as \ninputs to the models. If a YMCA branch was doing well with those questions, \nthey were necessarily doing well on the IOE scale. The branches then could \nincorporate changes in their staff, programs, facilities, or equipment to improve \nthe responses to the inputs of the model, thus increasing their IOE. The desire \nwas to show these relationships in a concise and informative way through data \nvisualization. \nThe key visual shown for each branch was like the one shown in Figure 13-3. \nThe seven key questions found by the regression model were shown along the \nx axis, with the importance as found by the regression equation increasing as \none goes to the right; the most important question nationally was at the far right. \nAll of the questions were important though, not just the ones at the right. The \nsame visualization could be created for each branch.\nOn the y axis, relative measures of the impact of these questions on IOE were \nshow", "ons were important though, not just the ones at the right. The \nsame visualization could be created for each branch.\nOn the y axis, relative measures of the impact of these questions on IOE were \nshown. Several key ideas were incorporated in the visualization. First, the order \nof the balls on the x axis refl ected the level of relative importance of the question \nin the model. Feel welcome was the most important question and Staff cares \nwas the least important of the top seven questions.\nSecond, Figure 13-3 compares the effectiveness of a YMCA branch to its \npeers. The average value of the peer group the branch belonged to appeared as \na horizontal line across the middle of the plot. Peer groups were branches whose \nmembers are from similar socio-economic status, level of education, and ethnic \nbackground. A ball whose center was above the line was performing better for \nthat question than its peers, whereas if the ball center fell below the line, the \nbranch was performing worse tha", "ethnic \nbackground. A ball whose center was above the line was performing better for \nthat question than its peers, whereas if the ball center fell below the line, the \nbranch was performing worse than its peers.\n\n388 \nChapter 13 \u25a0 Case Studies\nStaff cares\nStaff competence\nEquipment\nValue\nFacilities\nMeet \ufb01tness goals\nImportance\nFeel welcome\nPeer\nAverage\ndrivers of\nexcellence\nCurrent Year\nvs. Prior Year\nSample YMCA\nBetter\nSame\nWorse\nPrior Year\nRelative Performance\nFigure 13-3: Visualization of Drivers of Excellence\nThird, Figure 13-3 compares the effectiveness of the branch compared to the \nvalues of each factor for the prior year. If the ball was medium gray, the branch \nwas improving compared to the year prior; if it was dark gray, the branch was \ndoing worse; and if the ball was light gray, it was approximately the same as \nthe year prior. To provide a measure of how much better or worse the branch \nwas performing, a small round dot was placed on the plot to indicate the prior \nyear ", "ay, it was approximately the same as \nthe year prior. To provide a measure of how much better or worse the branch \nwas performing, a small round dot was placed on the plot to indicate the prior \nyear value for each question.\nFourth, the size of the balls indicated the relative importance of the question to \nIOE for that particular branch; larger balls indicated higher importance, smaller \nballs lower importance. The order of the balls from left to right was kept the \nsame for every branch, indicating the national trend relating the questions to \nIOE. The individual branch could therefore identify if it was behaving in ways \nsimilar to or different from the national trends.\nFor the branch shown in Figure 13-3, Staff competence improved compared \nto the prior year, but most of the other factors were worse than the prior year. \nFeel welcome had the most signifi cant infl uence on IOE (just like the average \ninfl uence nationally). The Facilities question is the smallest ball, and therefor", "ors were worse than the prior year. \nFeel welcome had the most signifi cant infl uence on IOE (just like the average \ninfl uence nationally). The Facilities question is the smallest ball, and therefore, \nfor this branch, has the least infl uence on IOE, although it was the fourth most \ninfl uence nationally.\n\n \nChapter 13 \u25a0 Case Studies \n389\nThe second example, Figure 13-4, shows a very successful YMCA branch. All \nseven of the key factors show that this branch performs better than its peer group, \nand fi ve of the seven questions show an improvement for this branch compared \nto its prior year measures. Six of the seven questions were worse than their \npeer group the year before (the dots are below the peer average line), indicating \nthis branch took steps in the past year to improve the attributes. Why is this so \nimportant? If responses to the questions were improved, IOE would improve; \nthis is what the predictive model demonstrated empirically from the data.\nStaff competence\nEquipm", "ttributes. Why is this so \nimportant? If responses to the questions were improved, IOE would improve; \nthis is what the predictive model demonstrated empirically from the data.\nStaff competence\nEquipment\nValue\nFacilities\nMeet \ufb01tness goals\nImportance\nFeel welcome\nPeer\nAverage\nRelative Performance\nStaff cares\nFigure 13-4: Drivers of Excellence, example 2\nIn the example shown in Figure 13-5, the branch performed at or below its peer \ngroup average for every factor, and its performance was fl at or mildly improving \nfrom the prior year, indicating that the branch is making some improvements \nto close its gap with other branches. The biggest areas of defi cit were related to \nthe facilities and equipment, good candidates for improvement in the next year.\nIn the example shown in Figure 13-6, the branch was performing worse in \nsix of the seven factors compared to its prior year surveys, and was below its \npeer group average in the current year. Clearly this branch was heading in the \nwrong d", " branch was performing worse in \nsix of the seven factors compared to its prior year surveys, and was below its \npeer group average in the current year. Clearly this branch was heading in the \nwrong direction and needed extensive improvements.\n\n390 \nChapter 13 \u25a0 Case Studies\nStaff competence\nEquipment\nValue\nFacilities\nMeet \ufb01tness goals\nImportance\nFeel welcome\nPeer\nAverage\nRelative Performance\nStaff cares\nFigure 13-5: Drivers of Excellence, example 3\nStaff cares\nStaff competence\nEquipment\nValue\nFacilities\nMeet \ufb01tness goals\nImportance\nFeel welcome\nPeer\nAverage\nRelative Performance\nFigure 13-6: Drivers of Excellence, example 4\nFinally, in Figure 13-7, the branch performance is mixed. On some dimensions, \nsuch as Staff competence, the branch did well: It was higher than its peers and \ndoing better than it was in the prior year. The Staff cares question was also much \n\n \nChapter 13 \u25a0 Case Studies \n391\nbetter than its peers. On the other hand, the Facilities and Value of the branch \nwere wel", "tter than it was in the prior year. The Staff cares question was also much \n\n \nChapter 13 \u25a0 Case Studies \n391\nbetter than its peers. On the other hand, the Facilities and Value of the branch \nwere well below the values of its peers and the branch was doing worse than \nthe prior year. The factors Facilities and Value clearly needed improvement.\nStaff cares\nStaff competence\nEquipment\nValue\nFacilities\nMeet \ufb01tness goals\nImportance\nFeel welcome\nPeer\nAverage\nRelative Performance\nFigure 13-7:  Drivers of Excellence, example 5\nThe idea behind the visualization, therefore, was to provide a report and an \nexplanation of the report to every branch; each branch would only see its own \nreport and could focus on factors where the branch was performing worse than \nits peers or worse than its performance in the prior year. \nDeployment: \u201cWhat-If\u201d Analysis\nOne additional deployment strategy considered was incorporating a \u201cwhat-if\u201d \nanalysis based on model predictions. The premise was that if the branch ", "in the prior year. \nDeployment: \u201cWhat-If\u201d Analysis\nOne additional deployment strategy considered was incorporating a \u201cwhat-if\u201d \nanalysis based on model predictions. The premise was that if the branch could \nchange a signifi cant percentage of members that checked the \u201c2\u201d box on a survey \nquestion to a \u201c1,\u201d how would that change IOE? Figure 13-8 shows the results of \nsome of the analysis. The entire simulation took place in a spreadsheet, where \na random proportion of a single question changed in value from 2 to 1\u201410 per-\ncent, 20 percent, and 50 percent\u2014and IOE was recalculated based on the new \nproportions of 1s and 2s. The percent change in IOE was recorded in the fi gure. \nRecall that the regression models showed that \u201cFeel Welcome\u201d was the most \nimportant factor in predicting IOE. However, this factor did not have the highest \n\n392 \nChapter 13 \u25a0 Case Studies\nsensitivity to change from 2 to 1; that honor belonged to Value for the money. \nIn other words, nationally, on average for al", "ver, this factor did not have the highest \n\n392 \nChapter 13 \u25a0 Case Studies\nsensitivity to change from 2 to 1; that honor belonged to Value for the money. \nIn other words, nationally, on average for all branches, the best way to improve \nIOE was to do something at the branch that caused a signifi cant percentage of \nmembers to change their survey score for Value for the money. In second place \nwas \u201cStaff Cares.\u201d Therefore, getting 10 percent or more of the members to believe \nthe staff cares about their well being would result in a 4.2 percent increase in \nIOE, thus increasing Satisfaction, Intend to renew, and Recommend to a friend.\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nCurrent\n10%\n25%\n50%\nCurrent\n10%\n25%\n50%\nCurrent\n10%\n25%\n50%\nCurrent\n10%\n25%\n50%\nCurrent\n10%\n25%\n50%\nCurrent\n10%\n25%\n50%\nCurrent\n10%\n25%\n50%\nCurrent\nResponses\n10%\n25%\n50%\n2.7%\n2.6%\nRegistration\nFriendly\nCompetent\nstaff\nStaff Cares\nFacilities\nclean/safe\nParking\nImprovement Over All Responses\nPercentile\nEquipment\nValue", "0%\nCurrent\n10%\n25%\n50%\nCurrent\nResponses\n10%\n25%\n50%\n2.7%\n2.6%\nRegistration\nFriendly\nCompetent\nstaff\nStaff Cares\nFacilities\nclean/safe\nParking\nImprovement Over All Responses\nPercentile\nEquipment\nValue for\nMoney\nFeel\nwelcome\n3.5%\n3.4%\n4.2%\n3.1%\n3.1%\n3.1%\n3.2%\n4.3%\n2.4%\n3.9%\n4.9%\n4.1%\n3.9%\n5.1%\n5.0%\n5.4%\n6.3%\n4.8%\n4.8%\n5.8%\n5.7%\n7.3%\nFigure 13-8: What-if scenarios for key questions\nThis simulation was never delivered to the YMCA but provides a way to use \nthe models that is not directly included in the predictive modeling business \nobjective. In fact, the predictive models didn\u2019t directly infl uence the simulation \nat all; the role of the regression models was to identify which questions to focus \non in the simulation, and therefore was intended to complement the visualiza-\ntions shown in Figures 13-3 to 13-7.\nRevisit Models\nUnfortunately, decision-makers found the visualization too complex; it wasn\u2019t \nclear to them exactly what the information meant for their individual branch. \nThere a", "gures 13-3 to 13-7.\nRevisit Models\nUnfortunately, decision-makers found the visualization too complex; it wasn\u2019t \nclear to them exactly what the information meant for their individual branch. \nThere are other predictive modeling approaches that are more accessible, how-\never, including decision trees. Therefore, decision trees were created to uncover \nkey questions in the survey related to the three target variables that would \nhopefully provide a more transparent interpretation of the survey.\n\n \nChapter 13 \u25a0 Case Studies \n393\nBusiness Understanding\nThe business objective remained the same: Identify survey questions related \nto the three target questions so that individual YMCA branches can evaluate \nhow they can improve member satisfaction, likelihood to renew, and recom-\nmendations to friends. However, this time, a key part of the business objectives \nis making the insights transparent.\nDecision trees are often used to gain insights into data because rules are easier \nto interpret th", "iends. However, this time, a key part of the business objectives \nis making the insights transparent.\nDecision trees are often used to gain insights into data because rules are easier \nto interpret than mathematical equations, especially if the rules are simple. The \noriginal three target variables were modeled directly instead of modeling IOE, \nin keeping with the desire to make the models as easy to understand as possible.\nData Preparation\nBecause the decision tree algorithm used in the analysis could handle missing \nvalues, no data preparation was done to recode NULLs or 0s. However, for each \nquestion, dummy variables were created to indicate if the responder checked \nthe \u201c1\u201d box or not; these were the only inputs used in the models.\nModeling and Model Interpretation\nWe built decision trees using a CART-styled algorithm and determined complex-\nity using cross-validation, a standard CART algorithm practice. The algorithm \nidentifi ed surrogate splits in the tree which we used to hel", "trees using a CART-styled algorithm and determined complex-\nity using cross-validation, a standard CART algorithm practice. The algorithm \nidentifi ed surrogate splits in the tree which we used to help understand the \nvariables that were the most important to predict to target variables. Models \nwere built for each of the three target variables: Satisfaction, Intend to renew, \nand Recommend to a friend.\nSatisfaction Model\nThe tree for target variable Q1 = 1 (Satisfaction) is shown in Figure 13-9. If the \nquestion response was 1, the path of the tree goes down the left side of the split. \nThe  number at the bottom of a branch labels the terminal node and indicates \nit is one of the most signifi cant terminal nodes. For example, terminal node 1 \nfollows the rule Q25 = 1 and Q13 = 1, whereas terminal node 2 follows the rule \nQ25 = 1 and Q13 \u22601 and Q22 = 1. Table 13-11 shows a list of variables included \nin the tree.\n\n394 \nChapter 13 \u25a0 Case Studies\nQ25\nQ13\nQ22\n1\n2\n3\n8\n9\n10\n15\nQ13\nQ31\nQ31\nQ", "terminal node 2 follows the rule \nQ25 = 1 and Q13 \u22601 and Q22 = 1. Table 13-11 shows a list of variables included \nin the tree.\n\n394 \nChapter 13 \u25a0 Case Studies\nQ25\nQ13\nQ22\n1\n2\n3\n8\n9\n10\n15\nQ13\nQ31\nQ31\nQ19\nQ36\nQ36\nQ6\nQ2\nQ13\nFigure 13-9: Member satisfaction tree\nTable 13-11: Key Variables Included in the Satisfaction Tree\nQUESTION\nDESCRIPTION\nQ2\nE\ufb03  ciency of front desk procedures\nQ6\nSta\ufb00  competence\nQ13\nOverall cleanliness\nQ19\nHas the right equipment\nQ22\nValue for the money\nQ25\nYou feel welcome at the Club.\nQ31\nCompared to other organizations in your community or companies you \ndeal with, please rate your loyalty to the Club.\nCharacterizing the rules in English rather than in mathematical symbols is \nnecessary to facilitate interpretation. Decision tree rules can be read as sequences \nof \u201cand\u201d conditions. The rules in the tree indicate the best way to predict the \ntarget variable according to the algorithm, though they aren\u2019t necessarily the \nonly way to achieve comparable predictive accu", "\u201d conditions. The rules in the tree indicate the best way to predict the \ntarget variable according to the algorithm, though they aren\u2019t necessarily the \nonly way to achieve comparable predictive accuracy; sometimes other combina-\ntions of splits in the tree can yield nearly identical accuracy. \n\n \nChapter 13 \u25a0 Case Studies \n395\nTable 13-12 shows a summary report based on the tree in Figure 13-9. The \nkey terminal nodes were defi ned as those terminal nodes with much higher \nthan average satisfaction proportions and those with much lower than average \nsatisfaction proportions. The rules defi ned by the branch of the tree were put \ninto English to make it easier for decision-makers to understand what the rules \nwere communicating about the member attitudes represented by the branch.\nTable 13-12: Rule Descriptions for the Satisfaction Model\nTERMINAL \nNODE\nRULE\n1\nIf strongly agree that facilities are clean and strongly agree that mem-\nber feels welcome, then highly satis\ufb01 ed.\n9\nIf strongl", "3-12: Rule Descriptions for the Satisfaction Model\nTERMINAL \nNODE\nRULE\n1\nIf strongly agree that facilities are clean and strongly agree that mem-\nber feels welcome, then highly satis\ufb01 ed.\n9\nIf strongly agree that facilities are clean, and strongly agree that sta\ufb00  \nis competent, even if don\u2019t strongly agree feel welcome, then highly \nsatis\ufb01 ed.\n2\nIf strongly agree that feel welcome and strongly agree Y is value for \nmoney, even if don\u2019t strongly agree facilities are clean, then highly \nsatis\ufb01 ed.\n3\nIf strongly agree that Y has the right equipment and strongly agree \nthat feel welcome, and somewhat agree that facilities are clean, even \nthough don\u2019t strongly feel Y is good value for the money, then highly \nsatis\ufb01 ed.\n10\nIf strongly agree that loyal to Y and strongly agree that facilities \nare clean, even though don\u2019t strongly agree that feel welcome nor \nstrongly agree that sta\ufb00  is competent, then highly satis\ufb01 ed.\n8\nIf don\u2019t strongly agree that facilities are clean and don\u2019t strongly ", "re clean, even though don\u2019t strongly agree that feel welcome nor \nstrongly agree that sta\ufb00  is competent, then highly satis\ufb01 ed.\n8\nIf don\u2019t strongly agree that facilities are clean and don\u2019t strongly agree \nthat the Y is good value for the money, even though strongly agree \nthat feel welcome, member isn\u2019t highly satis\ufb01 ed.\n15\nIf don\u2019t strongly agree that sta\ufb00  is e\ufb03  cient and don\u2019t strongly agree \nthat feel welcome, and don\u2019t strongly agree that the facilities are clean, \nthen member isn\u2019t highly satis\ufb01 ed.\nFinally, Table 13-13 shows several key terminal nodes from the satisfaction \nmodel, including two key statistics. The fi rst key statistic is the proportion of \nthe population with high satisfaction. The highest value is found in terminal \nnode 1 (72.8 percent). The second key statistic is also important, however: the \nproportion of all highly satisfi ed members identifi ed by the rule. The top ter-\nminal node by percent satisfaction is terminal node 1, but also important is \nthat ", "c is also important, however: the \nproportion of all highly satisfi ed members identifi ed by the rule. The top ter-\nminal node by percent satisfaction is terminal node 1, but also important is \nthat the rule fi nds nearly half of all highly satisfi ed members (49.1 percent). In \nfact, the top three rules, terminal nodes 1, 2, and 9, comprise over 70 percent \nof all highly satisfi ed members. The key questions included in these rules are \nsummarized in Table 13-14, with a \u201cyes\u201d if the question has the value 1 in the \n\n396 \nChapter 13 \u25a0 Case Studies\nbranch, \u201cno\u201d if the question has a value greater than 1 in the branch, and \u201cNA\u201d \nif the question is not in the branch.\nNote as well that terminal node 15, the terminal node with the lowest satis-\nfaction, is primarily the converse of the best rule: If the member doesn\u2019t agree \nthat they feel welcome, the facilities are clean and the staff is effi cient, only 6 \npercent of the members were highly satisfi ed with the branch; it makes one \nwond", "le: If the member doesn\u2019t agree \nthat they feel welcome, the facilities are clean and the staff is effi cient, only 6 \npercent of the members were highly satisfi ed with the branch; it makes one \nwonder why these 6 percent were still highly satisfi ed!\nTable 13-13: Key Terminal Nodes in the Satisfaction Model\nTERMINAL \nNODE\nNUMBER \nOF \nSURVEYS \nIN NODE\nPERCENT \nOF ALL \nSURVEYS \nFALLING \nINTO \nNODE\nNUMBER \nOF \nHIGHLY \nSATISFIED \nIN \nTERMINAL \nNODE\nPERCENT \nOF \nHIGHLY \nSATISFIED \nIN \nTERMINAL \nNODE\nPERCENT OF \nALL HIGHLY \nSATISFIED IN \nTERMINAL \nNODE\n1\n10,014\n20.80\n7,289\n72.8%\n49.1\n9\n1,739\n  3.60\n904\n52.0%\n  6.1\n2\n4,578\n   9.50\n2,317\n50.6%\n15.5\n3\n1,014\n  2.11\n471\n46.5%\n  3.2\n10\n998\n  2.08\n431\n43.2%\n  2.9\n8\n1,364\n  2.80\n141\n10.3%\n  1.0\n15\n19,323\n40.20\n1,231\n6.4%\n  8.3\nThe highest satisfaction comes from clean facilities and a feeling of being \nwelcome at the Y. However, the branch can overcome a lack of cleanliness by \ndemonstrating value in being a member of the Y (terminal node 2), and ", " comes from clean facilities and a feeling of being \nwelcome at the Y. However, the branch can overcome a lack of cleanliness by \ndemonstrating value in being a member of the Y (terminal node 2), and the \nbranch can overcome a lack of feeling welcome with high staff competence. \nThese \u201ceven if\u201d insights from the decision tree provide nuance to the picture \nof why members are satisfi ed with their Y branch. These interaction effects \ncannot be learned or inferred from the regression models without introducing \nthe interaction effects directly.\nTable 13-14: Key Questions in Top Terminal Nodes\nTERMINAL \nNODE\nFEEL \nWELCOME\nOVERALL \nCLEANLINESS\nSTAFF \nCOMPETENCE\nVALUE FOR \nMONEY\n1\nyes\nyes\nNA\nNA\n2\nyes\nno\nNA\nyes\n9\nno\nyes\nyes\nNA\n\n \nChapter 13 \u25a0 Case Studies \n397\nRecommend to a Friend Model\nThe Recommend to a friend model appears in Figure 13-10, which shows that \nthe key questions are Feel welcome (Q25) and Loyal to the Y (Q31), with terminal \nnode summary statistics shown in Table 13-15. The ", "ommend to a friend model appears in Figure 13-10, which shows that \nthe key questions are Feel welcome (Q25) and Loyal to the Y (Q31), with terminal \nnode summary statistics shown in Table 13-15. The loyalty question was inter-\nesting because the other two models did not use this question at all, nor was \nit a competitor or surrogate split in the other models. The top rule\u2014terminal \nnode 1\u2014found a population with 88.6 percent highly likely to recommend to a \nfriend, comprising nearly half of all members who strongly recommend the Y. \nOnce again, the top three terminal nodes represented more than 70 percent of \nthe target variable population. These three terminal nodes (1, 2, and 4) included \nthe questions related to Loyalty to the Y, Feel welcome, and Value for the money \n(Q31, Q25, and Q22). The descriptions of top rules for Recommend to a friend \nare shown in Table 13-16.\n1\n2\n4\n5\n7\nQ22\nQ25\nQ44\nQ31\nQ25\nQ22\nFigure 13-10:  Recommend to a Friend decision tree\nTable 13-15: Terminal Node P", "2). The descriptions of top rules for Recommend to a friend \nare shown in Table 13-16.\n1\n2\n4\n5\n7\nQ22\nQ25\nQ44\nQ31\nQ25\nQ22\nFigure 13-10:  Recommend to a Friend decision tree\nTable 13-15: Terminal Node Populations for the Recommend to a Friend Model\nTERMINAL \nNODE\nNUMBER \nOF \nSURVEYS \nIN NODE\nPERCENT \nOF ALL \nSURVEYS \nFALLING \nINTO \nNODE\nNUMBER \nOF HIGHLY \nRECOMMEND \nTO FRIEND \nIN TERMINAL \nNODE\nPERCENT \nOF HIGHLY \nRECOMMEND \nTO FRIEND \nIN TERMINAL \nNODE\nPERCENT OF \nALL HIGHLY \nRECOMMEND \nTO FRIEND \nIN TERMINAL \nNODE\n1\n13678\n28.46\n12122\n88.60\n47.0\n2\n6637\n13.80\n4744\n71.50\n18.4\nContinues\n\n398 \nChapter 13 \u25a0 Case Studies\nTERMINAL \nNODE\nNUMBER \nOF \nSURVEYS \nIN NODE\nPERCENT \nOF ALL \nSURVEYS \nFALLING \nINTO \nNODE\nNUMBER \nOF HIGHLY \nRECOMMEND \nTO FRIEND \nIN TERMINAL \nNODE\nPERCENT \nOF HIGHLY \nRECOMMEND \nTO FRIEND \nIN TERMINAL \nNODE\nPERCENT OF \nALL HIGHLY \nRECOMMEND \nTO FRIEND \nIN TERMINAL \nNODE\n4\n2628\n5.50\n1932\n73.50\n6.1\n7\n21865\n45.50\n5461\n25.00\n21.2\n5\n814\n1.70\n509\n62.50\n2.0\nTable 13-16: Rule Descr", " \nTO FRIEND \nIN TERMINAL \nNODE\nPERCENT OF \nALL HIGHLY \nRECOMMEND \nTO FRIEND \nIN TERMINAL \nNODE\n4\n2628\n5.50\n1932\n73.50\n6.1\n7\n21865\n45.50\n5461\n25.00\n21.2\n5\n814\n1.70\n509\n62.50\n2.0\nTable 13-16: Rule Descriptions for the Recommend to a Friend Model\nTERMINAL \nNODE\nRULE\n1\nIf strongly agree that loyal to Y and strongly agree that feel welcome, then \nstrongly agree that will recommend to a friend.\n2\nIf strongly agree that loyal to Y and agree that Y is a good value for the \nmoney, even though don\u2019t strongly agree feel welcome, strongly agree will \nrecommend to a friend.\n4\nIf strongly agree that Y is a good value for the money and strongly agree \nthat feel welcome, even though not strongly loyal to Y, strongly agree will \nrecommend to a friend.\n7\nIf don\u2019t strongly agree that loyal to Y and don\u2019t strongly agree that Y is \nvalue for the money, then will not highly recommend to a friend.\n5\nIf strongly agree that Y is good value for the money, and strongly agree \nthat Y helps meet \ufb01 tness goals, eve", "ongly agree that Y is \nvalue for the money, then will not highly recommend to a friend.\n5\nIf strongly agree that Y is good value for the money, and strongly agree \nthat Y helps meet \ufb01 tness goals, even though not strongly loyal to the Y and \ndon\u2019t strongly feel welcome, will highly recommend to a friend.\nThe interesting contrast of the Recommend to the Friend model compared \nto the satisfaction model is the inclusion of loyalty here; when a member feels \na sense of loyalty to their branch, they are more likely to recommend it to a \nfriend. If a member is loyal, if the member feels welcome or believes the Y is a \ngood value for the dollar, they are likely to recommend it. But even without the \nloyalty, meeting fi tness goals and good value for the dollar is enough for the \nmember to recommend the Y to a friend. \nIntend to Renew Model\nThe Intend to Renew model appears in Figure 13-11, which shows that the key \nquestions were Feel welcome (Q25), Y helps to meet fi tness goals (Q44), and V", "nd the Y to a friend. \nIntend to Renew Model\nThe Intend to Renew model appears in Figure 13-11, which shows that the key \nquestions were Feel welcome (Q25), Y helps to meet fi tness goals (Q44), and Value \nfor the money (Q22), with terminal node summary statistics shown in Table \n13-17. Rules using the fi tness goals question were interesting because the other \nTable 13-15 (continued)\n\n \nChapter 13 \u25a0 Case Studies \n399\ntwo models did not use this question in their top three terminal nodes, though \nit was in the fourth best terminal in the Recommend to a friend model, and in \nthat same model was a surrogate for the Loyal to the Y split. As a reminder, \nsurrogate splits identify a variable that splits the most like the winning split \nfor a node in the tree. \nThe top rule\u2014terminal node 1\u2014found a population with 73.9 percent highly \nlikely to intend to renew, comprising nearly half of all members who intended \nto renew. Once again, the top three terminal nodes represented more than 70 \nperc", " a population with 73.9 percent highly \nlikely to intend to renew, comprising nearly half of all members who intended \nto renew. Once again, the top three terminal nodes represented more than 70 \npercent of the target variable population. The description of top rules for Intend \nto renew are shown in Table 13-18.\n1\n2\n3\n5\n7\n8\nQ22\nQ22\nQ47\nQ27\nQ25\nQ44\nQ44\nFigure 13-11: Intend to Renew decision tree\nTable 13-17: Terminal Node Populations for the Intend to Renew Model\nTERMINAL \nNODE\nNUMBER OF \nSURVEYS IN \nNODE\nPERCENT \nOF ALL \nSURVEYS \nFALLING \nINTO NODE\nNUMBER \nOF HIGH \nINTEND TO \nRENEW IN \nTERMINAL \nNODE\nPERCENT \nOF HIGH \nINTEND TO \nRENEW IN \nTERMINAL \nNODE\nPERCENT OF \nALL HIGH \nINTEND TO \nRENEW IN \nTERMINAL \nNODE\n1\n13397\n27.90\n9903\n73.90\n48.40\n2\n3051\n6.30\n1823\n59.80\n8.90\n5\n5704\n11.90\n3201\n56.10\n15.60\n8\n18547\n38.60\n3130\n16.90\n15.30\n7\n2178\n4.50\n578\n26.50\n2.80\n\n400 \nChapter 13 \u25a0 Case Studies\nTable 13-18: Rule Descriptions for the Intend to Renew Model\nTERMINAL \nNODE\nRULE\n1\nIf strongly agree", ".10\n15.60\n8\n18547\n38.60\n3130\n16.90\n15.30\n7\n2178\n4.50\n578\n26.50\n2.80\n\n400 \nChapter 13 \u25a0 Case Studies\nTable 13-18: Rule Descriptions for the Intend to Renew Model\nTERMINAL \nNODE\nRULE\n1\nIf strongly agree that feel welcome and strongly agree that Y helps \nmeet \ufb01 tness goals, then strongly agree that intend to renew.\n2\nIf strongly agree Y is good value for the money and strongly agree that \nfeel welcome, even if don\u2019t strongly agree that Y helps meet \ufb01 tness \ngoals, then strongly agree that intend to renew.\n5\nIf strongly agree that feel sense of belonging, and agree that Y is value \nfor the money, and strongly agree that Y helps meet \ufb01 tness goals, even \nif don\u2019t feel welcome, then strongly agree intend to renew.\n8\nIf don\u2019t strongly agree that feel welcome and don\u2019t strongly agree that \nY helps meet \ufb01 tness goals, then don\u2019t strongly agree that intend to \nrenew.\n7\nIf don\u2019t strongly agree that Y is good value for money and don\u2019t \nstrongly agree that feel welcome, even if strongly agree Y hel", " meet \ufb01 tness goals, then don\u2019t strongly agree that intend to \nrenew.\n7\nIf don\u2019t strongly agree that Y is good value for money and don\u2019t \nstrongly agree that feel welcome, even if strongly agree Y helps meet \n\ufb01 tness goals, don\u2019t strongly agree that intend to renew.\nFitness goals fi gured strongly in the Intend to renew models when coupled \nwith feeling welcome (73.9 percent). Conversely, negating both of these reduced \nthe Intent to renew to 16.9 percent, a ratio of more than four to one.  Value for \nthe money can help overcome fi tness goals not being met, but only partially. \nTerminal nodes 2 and 5 had Intent to renew percentages of 59.8 and 56.1, respec-\ntively, well below the top terminal node.\nSummary of Models\nThe Satisfaction model was more complex than the other two models, which \nimplied that the reasons for satisfaction are more complex. Each of the models \ncaptured different characteristics of the members. The differences also highlight \nthe importance of defi ning the targ", "\nimplied that the reasons for satisfaction are more complex. Each of the models \ncaptured different characteristics of the members. The differences also highlight \nthe importance of defi ning the target variable well. For this project, all three of \nthe target variables provided insights into the attitudes of members.\nFeel welcome was a top question for all three models and provided an impor-\ntant insight into the mindset of the members; the connection to the Y was \nmore important than the conditions of the facility, parking, programs, or staff \ncompetence. Value for the money was also a key question in all three models, \nshowing that member costs were signifi cant contributors to the attitudes of \nmembers toward their branch.\nHowever, each model also had one or two questions that differed from the \nother models, summarized in Table 13-19. These differences could be useful \nin helping decision-makers tune the changes to the target they are most inter-\nested in. For example, if the bran", " from the \nother models, summarized in Table 13-19. These differences could be useful \nin helping decision-makers tune the changes to the target they are most inter-\nested in. For example, if the branch wishes to improve member satisfaction, in \naddition to making members feel welcome and revisiting the value of the Y, \n\n \nChapter 13 \u25a0 Case Studies \n401\nthey can make sure staff is trained well (Staff competence) and the facilities \nare kept clean.\nTable 13-19: Key Questions That Di\ufb00 er between Target Variables.\nMODEL\nKEY QUESTION\uf6aeS\uf6af\nSatisfaction\nFacility cleanliness, Sta\ufb00  competence\nRecommend to a friend\nLoyalty to the Y\nIntend to renew\nY helps to meet \ufb01 tness goals\nDeployment\nThe models were not deployed nationwide; individual branches decided to what \ndegree the models were used to infl uence what changes were made within their \nbranches. After years of using models, a 32 percent improvement in satisfac-\ntion (Q1 = 1) was measured, which clearly indicates improvement at the branch \n", " uence what changes were made within their \nbranches. After years of using models, a 32 percent improvement in satisfac-\ntion (Q1 = 1) was measured, which clearly indicates improvement at the branch \nlevel in meeting the expectations of members. Additionally, the Recommend \nto a friend (Q48 = 1) improved by 6 percent, easily a statistically signifi cant \nimprovement based on the number of surveys, though operationally not much \nhigher than the original value.\nSummary and Conclusions\nTwo predictive modeling approaches were described in this chapter, the fi rst \na mostly traditional approach to modeling, including the use of factor analy-\nsis and stepwise regression. The model visualization reports provided a rich, \nbranch-level summary of the status of key questions on the survey that infl uence \nsatisfaction. It ultimately was not used because of its complexity.\nThe second took a machine learning approach, building decision trees to \npredict the target variable. Simple rules were built", "uence \nsatisfaction. It ultimately was not used because of its complexity.\nThe second took a machine learning approach, building decision trees to \npredict the target variable. Simple rules were built that provided a more trans-\nparent view of which questions infl uence Satisfaction, Intend to renew, and \nRecommend to a friend, including how much interactions in key questions \nrelate to these three target questions.\nNeither modeling approach is right or wrong; they provide different ways to \nunderstand the surveys in complementary ways. A more sophisticated approach \ncould even incorporate both to fi nd main effects and interactions related to IOE \nand its components. Given more time and resources, more avenues of analysis \ncould have been attempted, but in most projects, time and resources are limited, \nrequiring the analysts to make decisions that may not be optimum, but hope-\nfully are reasonable and will help the company improve decisions.\n\n402 \nChapter 13 \u25a0 Case Studies\nThis case ", "s are limited, \nrequiring the analysts to make decisions that may not be optimum, but hope-\nfully are reasonable and will help the company improve decisions.\n\n402 \nChapter 13 \u25a0 Case Studies\nThis case study also demonstrates the iterative nature of predictive modeling \nsolutions. The data was plentiful and relatively clean, and the target variables \nwere well defi ned. Nevertheless, translating the insights from the models was \nnot straightforward and required considerable thought before communicating \nthese insights to the decision makers. Success in most predictive modeling \nprojects hinge on how well the information gleaned from the models\u2014both \npredictions and interpretations\u2014can ultimately be leveraged. \nSince the completion of work described in this case study, Seer Analytics \nhas progressed well beyond these models and has developed a framework that \naddresses the target variable from a different perspective; they readdressed the \nobjectives of modeling to better match the busine", "gressed well beyond these models and has developed a framework that \naddresses the target variable from a different perspective; they readdressed the \nobjectives of modeling to better match the business needs. Instead of predicting \ninfl uencers for factors that drive satisfaction, they now defi ne six key components \nof the member experience and create a pyramid of factors that are hierarchi-\ncal. These six factors are, in order from bottom to top, Facility, Value, Service \n(of staff), Engagement (with staff and members), Health (meeting goals), and \nInvolvement. This approach has resonated much better with YMCA branches \nand is in use today.\nHelp Desk Case Study\nIn the second case study, a combination of text mining and predictive model-\ning approaches were used to improve the effi ciency of the help desk of a large \nU.S. corporation responsible for hardware services and repairs of devices it \nmanufactured, sold, and supported.\nThe problem the company needed to address is this: Could", "y of the help desk of a large \nU.S. corporation responsible for hardware services and repairs of devices it \nmanufactured, sold, and supported.\nThe problem the company needed to address is this: Could the company use \nthe description of problems transcribed from help desk calls to predict if a part \nwould be needed to resolve the problem. After receiving a help desk phone \ncall and after the call was processed, a support engineer was assigned to the \nticket to try to resolve it. The engineer fi rst tried to resolve the problem over \nthe phone, and if that wasn\u2019t successful, the engineer went to the customer site \nto resolve the ticket. \nIn particular, the problem was effi ciency. Knowing whether a part was needed \nfor a repair or not before going to the customer site was helpful. Even more \nimportant was predicting which parts were most likely to be needed in the \nrepair. These models were built for the modeling project but will not be described \nin this case study. A second set of mod", " \nimportant was predicting which parts were most likely to be needed in the \nrepair. These models were built for the modeling project but will not be described \nin this case study. A second set of models were built to predict the actual part \nthat was needed to complete the repair, though that part of the modeling is not \ndescribed here.\nConcerns over revealing too much to competitors prevents me from reveal-\ning the client. Additionally, specifi cs about the project are not provided, such \nas actual performance numbers, precise defi nitions of derived variables, and \n\n \nChapter 13 \u25a0 Case Studies \n403\nthe identities of the most predictive variables in the models. The project was so \nsuccessful that the details about the models and how much the models improved \neffi ciency became a strategic corporate asset. Nevertheless, even without the \ndetails, the principles used to solve the problem can be applied broadly.\nThe accuracy of the model was paramount for successful deployment. The \ndec", "c corporate asset. Nevertheless, even without the \ndetails, the principles used to solve the problem can be applied broadly.\nThe accuracy of the model was paramount for successful deployment. The \ndecision makers determined the minimum accuracy of the \u201cparts needed\u201d \nmodel for the model to be successful, a number that cannot be revealed here. \nThe entire data set did not need to be classifi ed at this rate. If even 20 percent \nof the tickets that needed a part to complete the repair could be identifi ed cor-\nrectly, the model could be successfully deployed. The remaining tickets would \nthen be processed as they had always been processed. \nData Understanding: De\ufb01 ning the Data \nThe unit of analysis was a support ticket, so each row contained a unique ticket \nID and columns contained descriptions of the ticket. More than 1,000,000 tickets \nwere available for modeling. For each support ticket, the following variables \nwere available as candidate inputs for models: \n \n\u25a0Date and time of the", "tions of the ticket. More than 1,000,000 tickets \nwere available for modeling. For each support ticket, the following variables \nwere available as candidate inputs for models: \n \n\u25a0Date and time of the call\n \n\u25a0Business/individual name that generated the ticket\n \n\u25a0Country of origin\n \n\u25a0Device error codes for the hardware\n \n\u25a0The reason for the call that generated the ticket: regularly scheduled \nmaintenance call or a hardware problem\n \n\u25a0Warranty information\n \n\u25a0The problem description (text transcribed from the call)\n \n\u25a0The outcome of the ticket: ticket closed, solution found, and so on\n \n\u25a0The part(s) needed to close the ticket\nThe target variable had two parts: Did the ticket require a part to close, and \nwhich part or parts were needed to close the ticket. Only the PartsUsed target \nvariable models are described in this case study.\nData Preparation\nExtensive preparation was done on the data to examine all codes, correcting \nincorrect codes when they were identifi ed as incorrect. The date", "models are described in this case study.\nData Preparation\nExtensive preparation was done on the data to examine all codes, correcting \nincorrect codes when they were identifi ed as incorrect. The date of the call was \ntransformed into a single column: day of week. The business name was not \nused in building models, but helped in interpreting them.\n\n404 \nChapter 13 \u25a0 Case Studies\nProblems with the Target Variable\nThe target variable defi nition was clear: Create a 1/0 dummy variable indicat-\ning if the ticket required a part to complete the repair. The target variable was \nlabeled the PartsUsed fl ag. There were, however, ambiguities even with this \ndefi nition. First, if a part was not necessary for the repair, although it helped \nthe repair, it was still coded as a part being used.  \nIn other cases, a part was used to fi x a problem that was not the one speci-\nfi ed on the ticket. If this situation was discovered, the PartsUsed fl ag was set \nto 0 because a part the engineer specifi e", "ther cases, a part was used to fi x a problem that was not the one speci-\nfi ed on the ticket. If this situation was discovered, the PartsUsed fl ag was set \nto 0 because a part the engineer specifi ed to close the ticket was not actually \nneeded to fi x the problem on the ticket, and the engineer should have opened a \nnew ticket for the second problem. These ambiguities were diffi cult to discover, \nhowever, and it is presumed that these problems persisted in the data.  \nA third problem occurred when a fi x was made using a part and the ticket \nwas closed, only to reopen later when the problem reappeared. If the problem \nwas subsequently solved using a different part, the PartsUsed fl ag would still \nbe coded as a 1 (though a different part was used to fi x the problem). However, \nif a part was not needed to fi x the problem, the PartsUsed fl ag would have to \nbe changed to a 0. If the ticket was coded properly, this would work itself out. \nFeature Creation for Text\nThe company realiz", "rt was not needed to fi x the problem, the PartsUsed fl ag would have to \nbe changed to a 0. If the ticket was coded properly, this would work itself out. \nFeature Creation for Text\nThe company realized early on in the project that extracting features from the \ntranscribed text was critical to the success of the models. Initially, text was \nextracted using SQL text matching rather than a text mining software package, \nand it was done quite well. After fi nding some success using SQL to extract text, \nthey initiated a search for text mining software and technology to give it the \nability to leverage the transcription of help desk calls. \nProblems with the text were those problems common in text mining including \nmisspellings, synonyms, and abbreviations. Fortunately, the client had an excel-\nlent database programmer who spent considerable time fi xing these problems. \nTable 13-20 shows a sample of the kinds of misspellings found in the data. \nThe database programmer not only recoded the", "-\nlent database programmer who spent considerable time fi xing these problems. \nTable 13-20 shows a sample of the kinds of misspellings found in the data. \nThe database programmer not only recoded these to a common spelling, but \neven without any knowledge of stemming, he essentially stemmed these words, \ncapturing the concepts into one keyword. The keyword STICK from the table \ntherefore included all of the variants listed in the table.\nTable 13-20: Data Preparation for Help Desk Text\nID\nWORD\nKEYWORD\nCOUNT\n1220\nSTCK\nSTICK\n19\n1221\nSTIC\nSTICK\n16\n\n \nChapter 13 \u25a0 Case Studies \n405\n1222\nSTICK\nSTICK\n61\n1223\nSTICKIN\nSTICK\n15\n1224\nSTICKING\nSTICK\n1,141\n1225\nSTICKS\nSTICK\n179\n1226\nSTICKY\nSTICK\n176\nAs a second example, the keyword FAILURE included words FAILURE, FAIL, \nFAILURES, FAILED, and FA found in the text. These words were combinations \nof infl ected words and abbreviations.\nDomain experts and database programmers worked together to build a list \nof keywords deemed to be potentially useful ", "in the text. These words were combinations \nof infl ected words and abbreviations.\nDomain experts and database programmers worked together to build a list \nof keywords deemed to be potentially useful in building the predictive models. \nAfter combining synonyms, stemming, and converting abbreviations to the \nappropriate keyword, the team identifi ed more than 600 keywords and phrases. \nModeling\nSeveral algorithms could have been built to achieve the business objectives, \nincluding neural networks, support vector machines, and decision trees, but \ntwo considerations pushed the team to select decision trees. First, the data was \nlargely categorical and the number of candidate inputs was very large, two \nareas decision trees handle well. Second, the company needed to build models \nthat were easy to interpret so support engineers could understand why a part \nwas predicted to be needed for the repair. For these reasons, decision trees were \nused for modeling.\nHundreds of decision trees were ", "sy to interpret so support engineers could understand why a part \nwas predicted to be needed for the repair. For these reasons, decision trees were \nused for modeling.\nHundreds of decision trees were built from the modeling data using differ-\nent partitions of training and testing data records, different variable subsets of \nkeywords, and different tree settings for priors and complexity penalties. The \ntree in Figure 13-12 is typical of the trees that were built. The column name for \nthe PartsUsed target variable in the tree is Parts. Terminal nodes had the per-\ncentage of records needing a part (Class equal to Parts) ranging from a low of \n1.5 percent in Terminal Node 2 to 50.5 percent in Terminal Node 8. The nodes in \nFigure 13-12 are color coded by percentage of records needing a part to facilitate \nseeing where the percentages are highest.\nHowever, 50.5 percent fell far short of the minimum value required in the \nbusiness objectives, even though the 50.5 percent Parts rate represe", "to facilitate \nseeing where the percentages are highest.\nHowever, 50.5 percent fell far short of the minimum value required in the \nbusiness objectives, even though the 50.5 percent Parts rate represented a lift of \nmore than two over the baseline rate of 23.2 percent. This tree and the hundreds \nof others were clearly insuffi cient to achieve the goals of the model. \nWhat was the problem? The trees were able to handle the large number of \nkeywords but struggled to fi nd combinations that produced high percentages \nof tickets with parts use. Decision trees sometimes struggle with sparse data \n\n406 \nChapter 13 \u25a0 Case Studies\nbecause of the greedy search strategy: Each keyword dummy variable was a \nsparse variable, populated in a small minority of tickets, and individual keywords \ndidn\u2019t necessarily provide enough information on their own to create good splits. \nCallType\nClass \n# \n%\nNoParts \n6762 76.8\nParts \n2038 23.2\nTotal \n8800 100\nTerminal Node 1\nClass \n# \n%\nNoParts \n3060 92.3\nParts \n", "essarily provide enough information on their own to create good splits. \nCallType\nClass \n# \n%\nNoParts \n6762 76.8\nParts \n2038 23.2\nTotal \n8800 100\nTerminal Node 1\nClass \n# \n%\nNoParts \n3060 92.3\nParts \n  255 7.7\nTotal \n3315 100\nTerminal Node 2\nClass \n# \n%\nNoParts \n193 \n98.5\nParts \n  3 \n1.5\nTotal \n196 \n100\nSTATE\nClass \n# \n%\nNoParts \n1128 77.6\nParts \n  325 22.4\nTotal \n1453 100\nTerminal Node 3\nClass \n# \n%\nNoParts \n926 \n81.2\nParts \n215 \n18.8\nTotal \n1141 100\nTerminal Node 4\nClass \n# \n%\nNoParts \n202 \n64.7\nParts \n110 \n35.3\nTotal \n312 \n100\nTerminal Node 7\nClass \n# \n%\nNoParts \n583 \n83.3\nParts \n117 \n16.7\nTotal \n700 \n100\nTerminal Node 8\nClass \n# \n%\nNoParts \n684 \n49.5\nParts \n699 \n50.5\nTotal \n1383 100\nTerminal Node 5\nClass \n# \n%\nNoParts \n232 \n53.5\nParts \n202 \n46.5\nTotal \n434 \n100\nTerminal Node 6\nClass \n# \n%\nNoParts \n882 \n66.9\nParts \n437 \n33.1\nTotal \n1319 100\nKeywordGroup\nClass \n# \n%\nNoParts \n1267 60.6\nParts \n816 \n39.2\nTotal \n2083 100\nCallType\nClass \n# \n%\nNoParts \n3509 66.3\nParts \n1780 33.7\nTotal \n528", " \n%\nNoParts \n882 \n66.9\nParts \n437 \n33.1\nTotal \n1319 100\nKeywordGroup\nClass \n# \n%\nNoParts \n1267 60.6\nParts \n816 \n39.2\nTotal \n2083 100\nCallType\nClass \n# \n%\nNoParts \n3509 66.3\nParts \n1780 33.7\nTotal \n5289 100\nKeyword 1\nClass \n# \n%\nNoParts \n3702 67.5\nParts \n1783 32.5\nTotal \n5485 100\nKeyword 2\nClass \n# \n%\nNoParts \n2149 63.2\nParts \n1253 36.8\nTotal \n3402 100\nWARRANTY TYPE\nClass \n# \n%\nNoParts \n1360 72.1\nParts \n  527 27.9\nTotal \n1887 100\nFigure 13-12: Typical parts prediction decision tree\nModel ensembles, such as Random Forests, helped the accuracy some, but \nstill not enough to achieve the business objective. Random Forests, in particular, \nrepresent a good strategy for this kind of problem because of the random vari-\nable subsets that are part of the algorithm, forcing the trees to fi nd many ways \nto achieve high accuracy. But the modest improvement in accuracy also came at \nthe expense of transparency of the rules, an important factor to communicate \nto the support engineer before going to", "ys \nto achieve high accuracy. But the modest improvement in accuracy also came at \nthe expense of transparency of the rules, an important factor to communicate \nto the support engineer before going to the company site; even if the Random \nForests solution was accurate enough, it is unlikely the company would have \nadopted it as the model to deploy.\nSome trees were very interesting because they did not determine if parts \nwere needed from the keywords, but rather from the business making the call, \n\n \nChapter 13 \u25a0 Case Studies \n407\nthe level of expertise of their internal support within the company making the \nsupport call, or specifi c devices needing repair. Trees, to some degree, were \ntherefore measuring which companies had better internal support (if they had \ngood support, parts were more likely to be needed because they fi xed the easy \nproblems themselves).\nThe best branches in the trees were actually ones that predicted a part not \nbeing needed. For example, in one tree, when b", "more likely to be needed because they fi xed the easy \nproblems themselves).\nThe best branches in the trees were actually ones that predicted a part not \nbeing needed. For example, in one tree, when both keywords \u201cmachine\u201d and \n\u201cdown\u201d existed in the ticket, parts were rarely needed for the repair. Rules like \nthis one were valuable, but not the kind of rule that the company needed for \nsuccessful deployment.\nRevisit Business Understanding\nThe research team went back to the drawing board to try to fi nd a different \nway to solve the problem. They asked good questions, such as \u201cWhat additional \ninformation is known about problems at the time tickets are submitted?\u201d and \n\u201cHow would an informed person think about the data to solve the problem?\u201d \nThe answers to these questions were in the historical context of the tickets \nthemselves.\nWhen a ticket came in, a support engineer who was trying to predict what \nthe problem might be and whether a part was needed for a repair would refi ne \ntheir", "rical context of the tickets \nthemselves.\nWhen a ticket came in, a support engineer who was trying to predict what \nthe problem might be and whether a part was needed for a repair would refi ne \ntheir conclusion by eliminating options that were not likely, just as decision trees \ndo. But the engineer would also group together problems naturally, using an \n\u201cor\u201d condition, an operation that trees do not do in a single split. But even these \nconditions did something subtle in the mind of the support engineer. \nThe keywords and other features represented a historical idea: the experience \nof the engineer. The experiences of the engineer also included temporal informa-\ntion; one combination of keywords may have been a problem for particular kinds \nof devices at one time, but two years prior to that time the problem didn\u2019t exist. \nThe analysts and decision makers then determined the following. Rather \nthan using codes and keywords directly, they would use historic information \nabout parts be", "that time the problem didn\u2019t exist. \nThe analysts and decision makers then determined the following. Rather \nthan using codes and keywords directly, they would use historic information \nabout parts being needed when those codes and keywords appeared in the \nticket as inputs. Let\u2019s say \u201cpaper jam\u201d was a keyword in the data. Rather than \nusing this as a dummy variable, use the percentage of times this pair needed \na part in the past year. \nFigure 13-13 is a visual representation of the thought process. Ticket 1 was \nresolved at some timestamp prior to \u201cNow,\u201d where \u201cNow\u201d represents the date \nthe data for modeling was created. The date of resolution for Ticket 1, called \n\u201cTicket 1 Timestamp\u201d in the fi gure, means Ticket 1 has a target variable. The input \nvariables for Ticket 1, PartsUsed percentages for keywords and codes in Ticket \n1, were generated for the time period range represented by the curly bracket \nwith the label \u201cTime Period to Compute PartsUsed for Keywords in Ticket 1.\u201d \n\n40", "entages for keywords and codes in Ticket \n1, were generated for the time period range represented by the curly bracket \nwith the label \u201cTime Period to Compute PartsUsed for Keywords in Ticket 1.\u201d \n\n408 \nChapter 13 \u25a0 Case Studies\nTime\nTicket 2\nTimestamp\nTicket 1\nTimestamp\nTime Period to\nCompute PartsUsed\nfor Keywords in\nTicket 2\nTime Period to\nCompute PartsUsed\nfor Keywords in\nTicket 1\n\u201cNow\u201d\nTicket 1\nResolution\nTicket 2\nResolution\nFigure 13-13:  Temporal framework for new features\nThe same process was done for each ticket, including Ticket 2 shown in the \nfi gure, and all other tickets (more than one million).\nWhat\u2019s the difference between this and just using the fl ag? What is the dif-\nference between this data and the data used in the fi rst modeling pass? After \nall, doesn\u2019t the tree compute the average PartsUsed rate for each keyword in the \ndata and show us those in the nodes and terminal nodes of the tree? \nFirst, the new representation contains richer information: instead of a 1/", "pute the average PartsUsed rate for each keyword in the \ndata and show us those in the nodes and terminal nodes of the tree? \nFirst, the new representation contains richer information: instead of a 1/0 \ndummy, you have a percentage. Second, there is additional temporal informa-\ntion in the new features missing from the dummy variables. When models \nwere built from the dummy variables, all occurrences of the keywords and \nerror codes were included in the tree regardless of when the ticket occurred. \nIn the new representation, tickets occurring after the date for Ticket 1 (\u201cTicket \n1 Resolution\u201d) are not included in the computation of the PartsUsed percentage. \nTherefore, the new variables take trends into account in ways the fi rst data set \ncouldn\u2019t; the fi rst models included all tickets in the modeling data regardless \nof when the tickets occurred.\nNew features included not only historic PartsUsed, but also counts of how \nmany tickets contained the keyword or code. \nModeling and Mode", "in the modeling data regardless \nof when the tickets occurred.\nNew features included not only historic PartsUsed, but also counts of how \nmany tickets contained the keyword or code. \nModeling and Model Interpretation\nDecision trees were used once again for building the predictive models because \nthey could scale well. In addition, many decision trees could be built easily by \nmodifying parameters, such as priors or misclassifi cation costs and the set of \ninputs that could be included in the models. \n\n \nChapter 13 \u25a0 Case Studies \n409\nNevertheless, the trees still did not achieve the classifi cation accuracy required \nby the business objectives if measured by Percent Correct Classifi cation (PCC) \nor by using a confusion matrix. What was new was that there were fi nally at \nleast some terminal nodes in most of the trees that did meet the business objec-\ntive PartsUsed rate, even if only for a small percentage of the overall records. \nWhen examining these terminal nodes, the analysts dis", "nal nodes in most of the trees that did meet the business objec-\ntive PartsUsed rate, even if only for a small percentage of the overall records. \nWhen examining these terminal nodes, the analysts discovered that the rules \ndescribing the path to the terminal nodes differed from one another, as one \nwould expect from decision trees. Moreover, these terminal nodes were not \nidentifying the same populations: There were more than a few ways to predict \na high percent of PartsUsed for different groups of tickets. Interestingly, some \nof the rules contained six or more conditions (the trees were six or more levels \ndeep), but even in these situations, the rules made sense to domain experts.\nAt this point, an astute analyst may see that this kind of data is ideal for \nensembles. Random Forests (RF), in particular, is a good match for generat-\ning trees from different input variables and achieving higher accuracy than \nindividual trees can achieve. However, even though the RF models had highe", "RF), in particular, is a good match for generat-\ning trees from different input variables and achieving higher accuracy than \nindividual trees can achieve. However, even though the RF models had higher \naccuracy than individual trees, the RF models had two problems. First, the full \ntrees did not achieve overall accuracy that was high enough to deploy. Second, \nthe individual branches of RF trees were (purposefully) overfi t and unsuitable \nto use as a way to interpret why a part was needed. \nTherefore, the rules that showed the most potential had to be plucked from \nthousands of trees. The most interesting rules were those that related to high \npercentages of PartsUsed, but the rules matching very low PartsUsed percent-\nages were also interesting as cases that could be solved without parts being \nneeded and therefore were good candidates for phone support. \nAn alternative to picking the best rules from decision trees is to use associa-\ntion rules. Advantages of association rules are t", "s being \nneeded and therefore were good candidates for phone support. \nAn alternative to picking the best rules from decision trees is to use associa-\ntion rules. Advantages of association rules are that they fi nd (exhaustively) all \ncombinations of rules rather than just the best paths found by decision trees. \nA disadvantage is that the inputs must all be categorical, so the PartsUsed per-\ncentages would have to be binned to create the categorical representation for \nevery keyword and code. \nDeployment\nMore than 10,000 decision trees were built, creating more than 20,000 terminal \nnodes with a high percentage of PartsUsed to complete the repair. Each termi-\nnal represents a rule\u2014a series of \u201cand\u201d conditions found by the decision tree. \nThese rules were collected and sorted by the predicted percent PartsUsed for \nthe terminal node. The sorted list of rules then was treated as a sequence of \nrules to fi re, from highest PartsUsed likelihood to lowest, though even the low-\nest containe", "ted percent PartsUsed for \nthe terminal node. The sorted list of rules then was treated as a sequence of \nrules to fi re, from highest PartsUsed likelihood to lowest, though even the low-\nest contained a high percentage of PartsUsed. \n\n410 \nChapter 13 \u25a0 Case Studies\nThe algorithm for applying rules in this way can be thought of in this way:\n \n1. Sort the rules by percent PartsUsed on training data in descending order. \nEach rule contains not only the PartsUsed percentage, but also the variables \nand splits (the rules) used to generate the PartsUsed percentage.\n \n2. Choose one or more tickets to apply the rules to.\n \n3. Run the fi rst ticket through the rules, stopping when a rule matches (or \n\u201cfi res\u201d) with the highest PartsUsed percentage. \n \n4. Repeat for all tickets in the set. \nTable 13-21 contains a sample list of rules. As a reminder, these numbers are \nnot real and are used for illustration purposes only; they should not be used to \ninfer the PartsUsed percentages found and used", "3-21 contains a sample list of rules. As a reminder, these numbers are \nnot real and are used for illustration purposes only; they should not be used to \ninfer the PartsUsed percentages found and used by the company. The PartsUsed \npercentage found during training is shown in the table, but the more important \nnumber is the PartsUsed Percentage in Sequence, a number that can be signifi -\ncantly different than the Training PartsUsed Percentage. \nTable 13-21: Sample List of Rules to Fire\nRULE ID\nRULE \nSEQUENCE \nNUMBER\nNUMBER \nTICKETS \nMATCHING \nRULE\nTRAINING \nPARTSUSED\nPERCENTAGE\nPARTSUSED \nPERCENTAGE \nIN SEQUENCE\nCUMULATIVE \nPARTSUSED \nPERCENTAGE\n278\n1\n11809\n93.6\n93.6\n93.56\n2255\n2\n7215\n93.5\n91.7\n92.85\n1693\n3\n1927\n93.5\n93.0\n92.85\n2258\n4\n16\n93.5\n70.5\n92.84\n1337\n5\n20\n93.5\n89.3\n92.83\n993\n6\n2727\n93.5\n84.1\n91.82\n977\n7\n2\n93.5\n94.0\n91.82\n2134\n8\n2516\n93.5\n92.8\n91.91\n1783\n9\n4670\n93.5\n93.1\n92.09\n984\n10\n6\n93.5\n94.0\n92.09\nFor example, consider Rule ID 2258. This rule matched only 16 tickets because ", "\n93.5\n84.1\n91.82\n977\n7\n2\n93.5\n94.0\n91.82\n2134\n8\n2516\n93.5\n92.8\n91.91\n1783\n9\n4670\n93.5\n93.1\n92.09\n984\n10\n6\n93.5\n94.0\n92.09\nFor example, consider Rule ID 2258. This rule matched only 16 tickets because \nthe thousands of tickets it matched during training already matched one of the \nprior rules in the sequence: 278, 2255, or 1693. Moreover, the best tickets from \nrule 2258 were gone: The remaining 16 tickets had only a 70.5 percent PartsUsed \nrate. This rule should therefore be pruned from the list. The same applies to \nrules 1337, 977, and 984.\n\n \nChapter 13 \u25a0 Case Studies \n411\nFor the company in this case study, these rules were particularly attractive \nbecause they could be converted to SQL so easily, making deployment simple. \nSummary and Conclusions\n Several aspects of this case study were critical to the success of the project. \nFirst, deriving new, innovative features to use for modeling made the differ-\nence between failure and success. Sometimes one hears that more data will \nove", "re critical to the success of the project. \nFirst, deriving new, innovative features to use for modeling made the differ-\nence between failure and success. Sometimes one hears that more data will \novercome algorithms and domain expertise. In this case, this was clearly false. \nIt wouldn\u2019t matter how many tickets were included in the models. If the new \nfeatures weren\u2019t created, the performance would still be below the requirements. \nSubject matter experts were essential to defi ning the new features.\nSecond, using decision trees allows the modeling to proceed quickly because \ntrees handle wide data so easily and effi ciently. Moreover, adjusting learning \nparameters for the trees produced thousands of trees and interesting terminal \nnodes. Third, the decision trees were not used blindly and as black boxes. When \nthey were considered feature creators themselves, fi nding interesting business \nrules, the fact that no tree solved the entire problem was irrelevant. Finally, using \nthe deci", "d as black boxes. When \nthey were considered feature creators themselves, fi nding interesting business \nrules, the fact that no tree solved the entire problem was irrelevant. Finally, using \nthe decision trees to fi nd business rules meant the list of rules was intuitive and \neasy to communicate to the support engineers. \nThe models went into production and were very successful for the company, \nand so much so that the modeling was expanded to try to include more tickets \nin the models and to produce models that matched more tickets with high \nPartsUsed scores. Cost savings for the company as a result of the models were \nsignifi cant. \n\n\n413\n A\naccuracy, 150\nensembles and, 308\nADALINE (Adaptive Linear \nNeuron), 241\nAIC (Akaike information criterion), \n277, 323\nAID (Automatic Interaction Detection) \nalgorithm, 215\nalgorithms, 3\u20135\nAID (Automatic Interaction \nDetection), 215\nbagging, 311\u2013316\nRF (Random Forests), 320\u2013321\nboosting, 316\u2013320\nSGB (Stochastic Gradient Boosting), \n321\nCART, 215", "etection) \nalgorithm, 215\nalgorithms, 3\u20135\nAID (Automatic Interaction \nDetection), 215\nbagging, 311\u2013316\nRF (Random Forests), 320\u2013321\nboosting, 316\u2013320\nSGB (Stochastic Gradient Boosting), \n321\nCART, 215\nCHAID (Chisquare Automatic \nInteraction Detection), 215, 222\nclustering, 177\nhard partitioning, 178\nheterogeneous ensembles, 321\u2013233\nID3, 215\nk-NN learning algorithm, 254\u2013258\nKohonen SOM, 192\u2013194\nlearning algorithms, 191\nMART (Multiple Additive \nRegression Trees), 321\nmultidimensional features and, 114\nNetfl ix error, 308\nPCA (Principal Component \nAnalysis), 165\u2013169\nQUEST (Quick, Unbiased and \nEffi cient Statistical Tree), 216\nrecursive partitioning algorithms, \n218\nregression, 280\nanalytics\ndescription, 3\nstatistics and, 11\u201312\nANN (Artifi cial Neural Networks), \n240\u2013241. See also neural networks\nhidden layers, 243\u2013244\ntraining, 244\u2013247\ntransfer functions, 242\nanomalies, 210\nANOVA, 204\u2013205\nAnscombe\u2019s Quartet, 71\u201375\nantecedents, 148\u2013149\narctangent, 242\nassociation rules, 145, 146\u2013147\naccur", "orks\nhidden layers, 243\u2013244\ntraining, 244\u2013247\ntransfer functions, 242\nanomalies, 210\nANOVA, 204\u2013205\nAnscombe\u2019s Quartet, 71\u201375\nantecedents, 148\u2013149\narctangent, 242\nassociation rules, 145, 146\u2013147\naccuracy, 150\nantecedents, 148\u2013149\nIndex\n\n414 \nIndex \u25a0 B\u2013C\nbeer and diaper association, 146\nclassifi cation rules from, 159\u2013160\nconclusion, 148\nconditional statement, 147\u2013148\nconditions, 147\u2013148\nconfi dence, 150\nconsequents, 148\ndeployment, 156\u2013157\ninteraction variables, 157\u2013158\nvariable selection, 157\nitem set, 148\nLHS (left-hand-side), 148\nlift, 150\nnumber of, 158\u2013159\noutput, 148\nparameter settings, 151\nredundant, 158\nRHS (right-hand-side), 148\nrules, 148\nsupport, 149\nsorting\nby confi dence, 154\u2013155\nby support, 156\nsupport, 149\nattributes, predictive modeling and, \n26\u201327\nAUC (Area under the Curve), 33, 292\nB\nbackward selection, variables, 277\nbagging algorithm, 311\u2013316\nheterogeneous ensembles and, 322\nRF (Random Forests), 320\u2013321\nbatch approach to model assessment\nconfusion matrices, 286\u2013291\n", "), 33, 292\nB\nbackward selection, variables, 277\nbagging algorithm, 311\u2013316\nheterogeneous ensembles and, 322\nRF (Random Forests), 320\u2013321\nbatch approach to model assessment\nconfusion matrices, 286\u2013291\nmulti-class classifi cation, 291\nROC curves, 291\u2013293\nPCC (percent correct classifi cation), \n284\u2013286\nBayes\u2019 Theorem, 264\nprobability and, 265\u2013266\nbeer and diaper association, 146\nbell curve, 45\u201346\nbias, 98\nbias-variance tradeoff, ensembles and, \n309\u2013311\nBIC (Bayesian information criterion), \n323\nbinary classifi cation, misclassifi cation \ncosts and, 226\nbinary fi les, 25\nbins, continuous variables, 103\u2013104\nBonferroni correction, 222\nboosting algorithm, 316\u2013320\nSGB (Stochastic Gradient Boosting), \n321\nBootstrap Aggregating (bagging) \nalgorithm. See bagging algorithm\nBoW (bag of words), text mining, 336\nbox plots, histograms, 61\nbusiness intelligence, 6\u20137\ncustomer analytics, 7\nfraud detection, 6\u20137\nversus predictive analytics, 8\u201310\npredictive analytics similarities, 9\u201310\nbusiness understandin", "ing, 336\nbox plots, histograms, 61\nbusiness intelligence, 6\u20137\ncustomer analytics, 7\nfraud detection, 6\u20137\nversus predictive analytics, 8\u201310\npredictive analytics similarities, 9\u201310\nbusiness understanding, 21\nCRISP-DM sequence, 20\nhelp desk case study, 407\u2013409\nobjectives, 23\u201325\ntarget variables, 29\u201332\nthree-legged stool analogy, 22\u201323\nC\nCART algorithm, 215, 223\ncase studies\nfraud detection, 39\u201342\nhelp desk, 402\u2013411\nlapsed donors, 35\u201339\nsurvey analysis, 377\u2013402\ncategorical variables, 55\u201358\ncleaning and, 85\nhistograms, 59\nimputation, 97\nk-NN (nearest neighbor), 262\u2013263\n\n \nIndex \u25a0 C\u2013C \n415\nCHAID (Chisquare Automatic \nInteraction Detection), 215, 223\nBonferroni correction, 222\nchampion-challenger approach to \ndeployment, 372\u2013375\ncharacter length, text mining, 337\nchi-square test, 215\nCI (Confi dence Interval), logistic \nregression and, 233\nclassifi cation, 5\nconfusion matrices and, 286\u2013291\nmulti-class classifi cation, 291\ndocument, text mining and, 332\nlinear regression and, 279\nmisclassifi c", "nce Interval), logistic \nregression and, 233\nclassifi cation, 5\nconfusion matrices and, 286\u2013291\nmulti-class classifi cation, 291\ndocument, text mining and, 332\nlinear regression and, 279\nmisclassifi cation costs, 224\u2013229\nmulti-class, logistic regression and, \n239\u2013240\nneural networks, 244\nPCA and, 173\nPCC (percent correct classifi cation), \n284\u2013286\nsuccess measurement, 32\u201333\nclassifi cation rules from association \nrules, 159\u2013160\ncloud deployment, 359\ncluster models, 199\u2013201\ninterpretation methods, 202\u2013203\nvariables, 203\u2013204\nANOVA, 204\u2013205\nclustering algorithms, 177\ncluster types, 187\u2013189\ninputs, 181\u2013183\nK-Means\ndata distributions, 183\u2013184\nhard partitioning, 178\nhyper-spherical clusters, 184\ninputs, 181\u2013183\ninter-cluster distances, 178\u2013179\nirrelevant variables, 184\nKohonen Maps, 190\u2013192\nnumber of clusters, 185\u2013192\nSSE (Sum of Squared Error) metric, \n181\nvalue of \u201ck,\u201d 185\u2013192\nclustering features, 114\nclusters\ndocument, text mining and, 332\nhierarchical, 205\u2013206\nK-Means, normalization and,", "er of clusters, 185\u2013192\nSSE (Sum of Squared Error) metric, \n181\nvalue of \u201ck,\u201d 185\u2013192\nclustering features, 114\nclusters\ndocument, text mining and, 332\nhierarchical, 205\u2013206\nK-Means, normalization and, 139\u2013143\noutliers, 210\u2013212\nprototypes, 209\u2013210\nCLV (customer lifetime value), 163\ncoeffi cient, logistic regression models, \n233\ncolumns\ndeleting, 92\npredictive modeling and, 26\u201327\ncombinatorial explosion, variable \ninteractions, 65\u201366\nconclusion, 148\nconditional statement, 147\u2013148\nconditions, 147\u2013148\nconfi dence, 150\nassociation rules, sorting, 154\u2013155\nconfusion matrices, 286\u2013291\nmulti-class classifi cation, 291\nROC curves, 291\u2013293\nconsequents, 148\nconsistency, data format, 85\nconstants, imputation and, 92\u201393\ncontinuous values, cleaning and, 85\ncontinuous variables\nbinning, 103\u2013104\nimputation, 93\u201394\ncorrelations\nspurious, 66\u201367\nvariables, 66\u201368\ncosine similarity, text mining, 346\ncost comparison, deployment and, \n361\u2013362\nCRISP-DM (Cross-Industry Standard \nProcess Model for Data Mining), \n", ", 93\u201394\ncorrelations\nspurious, 66\u201367\nvariables, 66\u201368\ncosine similarity, text mining, 346\ncost comparison, deployment and, \n361\u2013362\nCRISP-DM (Cross-Industry Standard \nProcess Model for Data Mining), \n19\u201321\ndeployment and, 354\ncrosstabs\ncross-validation, 130\u2013132\n\n416 \nIndex \u25a0 D\u2013D\ncrowdsourcing, 308\u2013309\ncurse of dimensionality, 126\u2013128\nK-Means, 183\nk-NN (nearest neighbor), 263\ncustomer analytics\nbusiness intelligence, 7\npredictive analytics versus business \nintelligence, 8\ncybernetics, 3\nD\ndata, 43\u201344\nexperts, three-legged stool analogy, \n22\nformat, 151\u2013152\nconsistency, 85\ntransactional, 152\u2013153\nnon-stationary, 134\nsize determination, 128\u2013129\ntext mining sources, 333\nunit of analysis, 25\ndata analysis, 3\ndata audit, 81\u201382\ndata interpretation, PCA and, 171\u2013172\ndata mining, 3\nCRISP-DM, 19\u201321\nversus predictive analytics, 13\ndata preparation, 83\nCRISP-DM sequence, 20\ndescriptive modeling and, 164\u2013165\nhelp desk case study, 403\u2013405\nmodel scoring and, 356\nsurvey analysis case study, 381\u2013385\nval", "ersus predictive analytics, 13\ndata preparation, 83\nCRISP-DM sequence, 20\ndescriptive modeling and, 164\u2013165\nhelp desk case study, 403\u2013405\nmodel scoring and, 356\nsurvey analysis case study, 381\u2013385\nvalues, missing, 90\u201398\nvariable cleaning\nformat consistency, 85\nincorrect values, 84\u201385\noutliers, 85\u201389\ndata science, 3\ndata understanding, 44\nCRISP-DM sequence, 20\ndata audit, 81\u201382\ndoubles, 44\nhistograms, 59\u201363\nintegers, 44\nints, 44\nreal variables, 44\nstatistics, 47\u201349\nIQR (Inter-Quartile Range), 54\u201355\nrank-ordered, 52\u201355\nrobust, 53\nsurvey analysis case study, 380\u2013381\ndata visualization\nnormalization and, 106\nscatterplots, 69\u201371\nAnscombe\u2019s Quartet, 71\u201375\nmatrices, 75\u201376\nmultiple dimensions, 78\u201380\nsingle dimension, 58\u201359\ndatabases, three-legged stool analogy, \n22\ndate and time variables, 109\u2013110\ndecision boundaries, neural networks, \n253\ndecision stump, 7\ndecision trees, 206\u2013208, 214\u2013215\nAID (Automatic Interaction \nDetection) algorithm, 215\nboosted models, 318\u2013319\nbuilding, 218\u2013221\nCART algo", "\ndecision boundaries, neural networks, \n253\ndecision stump, 7\ndecision trees, 206\u2013208, 214\u2013215\nAID (Automatic Interaction \nDetection) algorithm, 215\nboosted models, 318\u2013319\nbuilding, 218\u2013221\nCART algorithm, 215\nCHAID (Chisquare Automatic \nInteraction Detection), 215\nchi-square test, 215\ndepth, 222\nnonlinearities, 219\nQUEST (Quick, Unbiased and \nEffi cient Statistical Tree) \nalgorithm, 216\nrecords\nBonferroni correction, 222\nmisclassifi cation costs, 224\u2013229\nparent node, 222\nprior probabilities, 224\nterminal nodes, 222\n\n \nIndex \u25a0 D\u2013D \n417\nrecursive partitioning algorithms, \n218\nsplits, 216\nsplitting metrics, 221\u2013222\nvariables, 229\nweak learners, 229\ndeletion\ncolumn, 92\nlistwise, 92\ndelimited fl at fi les, 25\ndeployment, 353\u2013355\nassociation rules, 156\u2013157\ninteraction variables, 157\u2013158\nvariable selection, 157\nchampion-challenger approach, \n372\u2013375\ncloud, 359\ncost comparison, 361\u2013362\nCRISP-DM sequence, 20, 354\nencoding in other language, 359\u2013361\nheadless predictive modeling \nsoftware, 358\n", "le selection, 157\nchampion-challenger approach, \n372\u2013375\ncloud, 359\ncost comparison, 361\u2013362\nCRISP-DM sequence, 20, 354\nencoding in other language, 359\u2013361\nheadless predictive modeling \nsoftware, 358\nhelp desk case study, 409\u2013411\nin-database, 358\u2013359\nlocation, 356\u2013362\nmodel rebuild, 367\u2013370\nsampling and, 370\u2013372\npost-processing scores, 362\npredictive modeling software, 358\npredictive modeling software \ndeployment add-on, 358\nscore segments, 362\u2013366\nsteps, 355\nsurvey analysis case study, 401\nwhat-if analysis, survey analysis case \nstudy, 391\u2013392\ndescriptive modeling, 5, 163\u2013164\ncluster models, 199\u2013201\ninterpretation methods, 202\u2013203\nclustering algorithms, 177\ninter-cluster distances, 178\u2013179\nK-Means, 178\u2013184\nK-Means data preparation, 183\u2013184\nnumber of clusters, 185\u2013192\nSSE (Sum of Squared Error) metric, \n181\nKohonen Maps, 190\u2013192\nPCA (Principal Component \nAnalysis)\nalgorithm, 165\u2013169\nclassifi cation, 173\ndata interpretation, 171\u2013172\nnew data, 169\u2013170\nPCs (principal components), 165\u2013\n166", "rror) metric, \n181\nKohonen Maps, 190\u2013192\nPCA (Principal Component \nAnalysis)\nalgorithm, 165\u2013169\nclassifi cation, 173\ndata interpretation, 171\u2013172\nnew data, 169\u2013170\nPCs (principal components), 165\u2013\n166\nregression, 173\nvariable magnitude, 174\u2013177\nvalue of \u201ck,\u201d 185\u2013192\ndescriptors, predictive modeling and, \n26\u201327\ndestructive collinearity, 276\nDF (document frequency), text \nmining, 344\ndictionaries, text mining, 338\u2013339\ndimensions\ncurse of dimensionality, 126\u2013128\nK-Means, 183\nk-NN (nearest neighbor), 263\ndata points, distance between, \n126\u2013128\ndistance\nbetween data points, 126\u2013128\nmetrics, 190\ndistance metrics, k-NN (nearest \nneighbor), 258\u2013262\ndistribution\nimputation and, 94\u201395\nrandom, 96\nK-Means, 183\u2013184\nhyper-spherical clusters, 184\nkurtosis, 51\u201352\nnormal distribution, 45\u201346\nskewness, 49\u201350\nuniform, 46\u201347\ndocuments, text mining and, 332\ndomain experts\n\n418 \nIndex \u25a0 E\u2013G\nmultidimensional features, 112\u2013113\nthree-legged stool analogy, 22\ndoubles, 44\ndummy variables, 97\nlogistic regression a", "\nuniform, 46\u201347\ndocuments, text mining and, 332\ndomain experts\n\n418 \nIndex \u25a0 E\u2013G\nmultidimensional features, 112\u2013113\nthree-legged stool analogy, 22\ndoubles, 44\ndummy variables, 97\nlogistic regression and, 238\u2013239\nE\neigenvalues, 168\neigenvectors, 168\nencoding in other language, \ndeployment and, 359\u2013361\nensembles, 307\u2013308\nbagging algorithm, 311\u2013316\nbias-variance tradeoff, 309\u2013311\nboosting algorithm, 316\u2013320\nGDF (Generalized Degrees of \nFreedom), 323\nheterogeneous, 321\u2013233\ninterpretation, 323\u2013325\nOccam\u2019s razor and, 323\nepoch\nneural networks, 246\nnumber, 250\nerrors\nfalse alarms, 287\nfalse dismissals, 287\nresiduals, 321\ntype I/II, 287\nestimation, success measurement, 33\nEuclidean Distances, 104\nk-NN (nearest neighbor), 258\u2013259\nevaluation, CRISP-DM sequence, 20\nF\nfalse alarms, 287\nfalse dismissals, 287\nfeatures\ncontinuous variables, binning, 103\u2013104\ndate and time variables, 109\u2013110\nirrelevant variables, removing, 118\nmultidimensional\nalgorithms, 114\nclustering, 114\ndomain experts, 112\u2013113\ntim", "s, 287\nfeatures\ncontinuous variables, binning, 103\u2013104\ndate and time variables, 109\u2013110\nirrelevant variables, removing, 118\nmultidimensional\nalgorithms, 114\nclustering, 114\ndomain experts, 112\u2013113\ntime series, 116\u2013117\ntying records together, 115\u2013116\nnumeric variables, scaling, 104\u2013107\npredictive modeling and, 26\u201327\nreducing variables prior to selection, \n117\u2013122\nredundant variables, removing, 119\nsampling, 123\u2013124\ncross-validation, 130\u2013132\ndata size determination, 128\u2013129\ndistance between data points, \n126\u2013128\npartitioning, 124\u2013126, 130\nstratifi ed, 136\u2013139\ntemporal sequencing, 134\u2013136\nskew correction, 99\u2013103\ntransformations, 98\u201399\nnominal variables, 107\u2013108\nordinal variables, 108\u2013109\nversions of variables, 110\u2013112\nZIP codes, 110\nfi elds, predictive modeling and, \n26\u201327\nfi xed-width fl at fi les, 25\nfl at fi les\ncustomized, 25\ndelimited, 25\nfi xed-width, 25\nformats, 151\u2013152\nconsistency, 85\ntransactional, 152\u2013153\nforward selection, variables, 276\u2013277\nfraud detection\nbusiness intelligenc", "fi les, 25\nfl at fi les\ncustomized, 25\ndelimited, 25\nfi xed-width, 25\nformats, 151\u2013152\nconsistency, 85\ntransactional, 152\u2013153\nforward selection, variables, 276\u2013277\nfraud detection\nbusiness intelligence, 6\u20137\ncase study, 39\u201342\npredictive analytics versus business \nintelligence, 8\nfrequent item set mining, 145\nG\nGain Ratio, 215\ngains chart, 294\u2013298\nGaussian distribution, 45\u201346\n\n \nIndex \u25a0 H\u2013K \n419\nGDF (Generalized Degrees of \nFreedom), 323\nH\nhard partitioning algorithms, 178\nheadless predictive modeling \nsoftware, deployment and, 358\nhelp desk case study, 402\u2013403\nbusiness understanding, 407\u2013409\ndata preparation, 403\u2013405\ndefi ning data, 403\ndeployment, 409\u2013411\nmodeling, 405\u2013407\nheterogeneous ensembles, 321\u2013233\nhierarchical clustering, 205\u2013206\nhistograms\naxes, 59\nbins, 60\nbox plots, 61\ncategorical variables, multiple, 60\ncustomizing, 60\nK-Means clustering, 59\ny-axis, scale, 60\nhyperbolic tangent, 242\nhyper-spherical clusters, 184\nI\nID3 algorithm, 215\nIDF (inverse document frequency), \ntext m", "al variables, multiple, 60\ncustomizing, 60\nK-Means clustering, 59\ny-axis, scale, 60\nhyperbolic tangent, 242\nhyper-spherical clusters, 184\nI\nID3 algorithm, 215\nIDF (inverse document frequency), \ntext mining, 344\nimputation\nwith constant, 92\u201393\ndistributions and, 94\u201395\nrandom, 96\nmean, continuous variables, 93\u201394\nmedian, continuous variables, \n93\u201394\nfrom model, 96\u201397\nsoftware dependence and, 97\u201398\nvariables, categorical, 97\nin-database deployment, 358\u2013359\ninformation extraction, text mining \nand, 333\nInformation Gain, 215\ninput variables, 4\ninputs, K-Means, 181\u2013183\nintegers, 44\ninteractions\nlogistic regression and, 236\u2013237\nselecting, 122\nvariables, association rule \ndeployment, 157\u2013158\ninter-cluster distances, 178\u2013179\ninterpretation\ncluster outliers, 210\u2013212\ncluster prototypes, 209\u2013210\ndecision trees, 206\u2013208\nensembles, 323\u2013325\nhierarchical clustering, 205\u2013206\nirrelevant variables, 208\nlinear regression models, 278\u2013279\nlogistic regression models, 233\u2013235\nNa\u00efve Bayes classifi ers, 268\u2013260", "ion trees, 206\u2013208\nensembles, 323\u2013325\nhierarchical clustering, 205\u2013206\nirrelevant variables, 208\nlinear regression models, 278\u2013279\nlogistic regression models, 233\u2013235\nNa\u00efve Bayes classifi ers, 268\u2013260\nneural networks, 252\nnormalized data and, 202\nwithin-cluster descriptions, 203\nints, 44\nIQR (Inter-Quartile Range), 54\u201355\nirrelevant variables\ninterpretation and, 208\nK-Means, 184\nremoving, 118\nitem set mining, 145\nitem sets, 148\niterations, neural networks, 250\nK\nK-Means\ncluster types, 187\u2013189\nclustering, normalization and, \n139\u2013143\ncurse of dimensionality, 183\ndistribution, 183\u2013184\nhard partitioning algorithms, 178\nhistograms, 59\nhyper-spherical clusters, 184\ninputs, 181\u2013183\n\n420 \nIndex \u25a0 L\u2013M\ninter-cluster distances, 178\u2013179\nKohonen Maps comparison, 195\u2013197\nvariables, irrelevant, 184\nkeyword features, reducing, 347\nknees, 169\nk-NN (nearest neighbor)\ncategorical variables, 262\u2013263\ncurse of dimensionality, 263\ndistance metrics, 258\u2013262\nlearning algorithm, 254\u2013258\nweighted votes, 263\nknowl", "d features, reducing, 347\nknees, 169\nk-NN (nearest neighbor)\ncategorical variables, 262\u2013263\ncurse of dimensionality, 263\ndistance metrics, 258\u2013262\nlearning algorithm, 254\u2013258\nweighted votes, 263\nknowledge discovery, 3\nKohonen Maps, 190\u2013192\nK-Means similarities, 195\u2013197\nparameters, 193\u2013194\nvisualization, 194\u2013195\nKohonen Self-Organizing Map (SOM), \n190\nalgorithm, 192\u2013194\nKPIs (Key Performance Indicators), 6\nkurtosis, 51\u201352\nL\nlayers, neural networks, hidden, 250\nlearning\nalgorithms, 191\nsupervised versus unsupervised, 5\nlearning rate of neural networks, \n249\u2013250\nLHS (left-hand-side), 148\nlift, 150, 284\nbaseline class rates, 285\ncharts, 294\u2013298\nlinear methods, PCA, 173\nlinear regression, 271\u2013274\nassumptions, 274\u2013276\nclassifi cation and, 279\ndestructive collinearity, 276\ninterpreting models, 278\u2013279\nresiduals, 272\u2013273\nskewed distribution and, 104\nvariable selection, 276\u2013277\nlistwise deletion, 92\nlog transform, skew correction, 100\u2013103\nlogistic curve, 232\nlogistic regression, 230\u2013233\nCI (Con", "8\u2013279\nresiduals, 272\u2013273\nskewed distribution and, 104\nvariable selection, 276\u2013277\nlistwise deletion, 92\nlog transform, skew correction, 100\u2013103\nlogistic curve, 232\nlogistic regression, 230\u2013233\nCI (Confi dence Interval), 233\ncoeffi cient and, 233\ndummy variables, 238\u2013239\ninteractions, 236\u2013237\nlogistic curve, 232\nmissing values, 238\nmodel interpretation, 233\u2013235\nmulti-class classifi cation, 239\u2013240\nodds ratio, 230\u2013231\nPr(>|z|), 234\nSE (standard error of the coeffi cient), \n233\nvariable selection options, 234\nvariables, selection options, 234\nWald test, 234\nz statistic, 234\nM\nMahalanobis distance, k-NN (nearest \nneighbor), 259\nMAR (missing at random), 91\nmarket basket analysis, 145, 146\u2013147\naccuracy, 150\nantecedents, 148\nsupport, 149\nconclusion, 148\nconditional statement, 147\u2013148\nconditions, 147\u2013148\nconfi dence, 150\nconsequents, 148\nitem set, 148\nLHS (left-hand-side), 148\nlift, 150\noutput, 148\nRHS (right-hand-side), 148\nrule support, 149\nrules, 148\nsupport, 149\n\n \nIndex \u25a0 N\u2013N \n421\nMART (M", "147\u2013148\nconfi dence, 150\nconsequents, 148\nitem set, 148\nLHS (left-hand-side), 148\nlift, 150\noutput, 148\nRHS (right-hand-side), 148\nrule support, 149\nrules, 148\nsupport, 149\n\n \nIndex \u25a0 N\u2013N \n421\nMART (Multiple Additive Regression \nTrees) algorithm, 321\nmatrices\nconfusion matrices, 286\u2013291\nscatterplots, 75\u201376\nMAXRAMNT variable, 88\nMCAR (missing completely at \nrandom), 91\nMDL (minimum description length), \n277, 323\nmean, 45\nimputation for continuous variables, \n93\u201394\nmedian, imputation for continuous \nvariables, 93\u201394\nmisclassifi cation costs, 224\u2013229\nmissing values\ncodes, 90\nfi xing\ndefault dependence, 97\u201398\ndummy variables, 97\nimputation for categorical values, \n97\nimputation from model, 97\nimputation with constant, \n92\u201393\nimputation with distributions, \n94\u201395\nlistwise deletion, 92\nmean imputation for continuous \nvariables, 93\u201394\nmedian imputation for continuous \nvariables, 93\u201394\nrandom imputation from own \ndistributions, 96\nsoftware dependence, 97\u201398\ntoo much, 97\nlogistic regression and", "tation for continuous \nvariables, 93\u201394\nmedian imputation for continuous \nvariables, 93\u201394\nrandom imputation from own \ndistributions, 96\nsoftware dependence, 97\u201398\ntoo much, 97\nlogistic regression and, 238\nMAR, 91\nMCAR, 91\nMNAR, 91\nnull, 90\nMK82 glide bomb, 2\nMLP (multi-layer perceptron), neural \nnetworks, 241\nMNAR (missing not at random), 91\nmodel assessment, 283\u2013284\nbatch approach\nconfusion matrices, 286\u2013293\nPCC (percent correct classifi cation), \n284\u2013286\nbinary classifi cation, 285\u2013286\nrank-ordered approach, 293\u2013294\ncustom, 298\u2013300\ngains chart, 294\u2013298\nlift charts, 294\u2013298\nregression models, 301\u2013304\nmodel deployment. See deployment\nmodel ensembles. See ensembles\nmodeling\nCRISP-DM sequence, 20\nhelp desk case study, 405\u2013407\nsurvey analysis case study, \n385\u2013401\ntext mining features and, 347\u2013349\nvariables, reducing prior to selection, \n117\u2013122\nMSE (Mean Squared Error), 33\nmulti-class classifi cation\nconfusion matrices and, 291\nlogistic regression and, 239\u2013240\nmultidimensional features\na", "7\u2013349\nvariables, reducing prior to selection, \n117\u2013122\nMSE (Mean Squared Error), 33\nmulti-class classifi cation\nconfusion matrices and, 291\nlogistic regression and, 239\u2013240\nmultidimensional features\nalgorithms, 114\nclustering, 114\ndomain experts, 112\u2013113\nPCA (Principal Component \nAnalysis) features, 113\u2013114\ntime series, 116\u2013117\ntying records together, 115\u2013116\nN\nNa\u00efve Bayes algorithm, 264\ncategorical inputs, 269\ncorrelated variables, 270\ninteractions, 270\n\n422 \nIndex \u25a0 O\u2013P\nNa\u00efve Bayes classifi er, 268\ninterpretation, 268\u2013270\nprobability, 264\u2013265\nNa\u00efve Bayes classifi er, 268\ninterpretation, 268\u2013270\nnegative skew, 101\u2013102\nNetfl ix algorithm, 308\nneural networks, 3. See also ANN \n(Artifi cial Neural Networks)\nactivation functions, 242\u2013243\nADALINE (Adaptive Linear \nNeuron), 241\narctangent, 242\nclassifi cation modeling, 244\ncost function, 244\ncriteria, stopping, 250\ndecision boundaries, 253\nepoch, 246\nnumber, 250\nfl exibility, 247\u2013249\nhyperbolic tangent, 242\ninterpretation, 252\niteration num", "classifi cation modeling, 244\ncost function, 244\ncriteria, stopping, 250\ndecision boundaries, 253\nepoch, 246\nnumber, 250\nfl exibility, 247\u2013249\nhyperbolic tangent, 242\ninterpretation, 252\niteration number, 250\nlayers, hidden, 250\nlearning rate, 249\u2013250\nMLP (multi-layer perceptron), \n241\nmomentum, 250\nneurons, 242\u2013244\nsquashing function, 242\npass, 246\nPDP (Parallel Distributed \nProcessing), 241\npropagation, 251\npruning algorithms, 251\u2013252\nRprop (resilient propagation), 251\nsecond-order methods, 251\ntraining, 244\u2013247\nvariable infl uence, 252\nweight updates, 250\nN-grams, text mining, 346\nnominal variables, transformations, \n107\u2013108\nnon-parametric models, 6\nnon-stationary data, 134\nnormal distribution, 45\u201346\nskewness, 49\u201350\nnormalization\ndata visualization and, 106\ninterpretation and, 203\nK-Means clustering, 139\u2013143\nmethods, 105\nnull value, 90\nnumber fi lters, text mining, 337\nnumeric variables, scaling, 104\u2013107\nO\nOccam\u2019s razor and model ensembles, \n323\nodds ratio, 230\u2013231\nOptimal Path-to-G", "ns clustering, 139\u2013143\nmethods, 105\nnull value, 90\nnumber fi lters, text mining, 337\nnumeric variables, scaling, 104\u2013107\nO\nOccam\u2019s razor and model ensembles, \n323\nodds ratio, 230\u2013231\nOptimal Path-to-Go guidance, 2\nordinal variables, transformations, \n108\u2013109\noutlier variables, 85\u201386\nbins, 87\nclusters and, 210\u2013212\nmodifi cation, 87\u201388\nmultidimensional, 89\nremoving from modeling data, 86\nseparate models, 86\u201387\ntransforming, 87\noutput, 148\noverfi t, 123\nP\nparameter settings, association rules, \n151\nparametric models, 6\nparent node, records, 222\npartitioning\nrecursive partitioning algorithms, 218\nsubsets, 130\npattern extraction, text mining and, 329\npattern recognition, 3\nPCA (Principal Component Analysis)\nalgorithm, 165\u2013169\nclassifi cation, 173\ndata interpretation, 171\u2013172\n\n \nIndex \u25a0 P\u2013P \n423\nfeatures, 113\u2013114\nlinear methods, 173\nnew data, 169\u2013170\nPCs (principal components), 165\u2013166\nregression, 173\nvariables, magnitude, 174\u2013177\nPCC (percent correct classifi cation), 32, \n284\u2013286\nPCs (prin", "\nfeatures, 113\u2013114\nlinear methods, 173\nnew data, 169\u2013170\nPCs (principal components), 165\u2013166\nregression, 173\nvariables, magnitude, 174\u2013177\nPCC (percent correct classifi cation), 32, \n284\u2013286\nPCs (principal components), 165\u2013166\nPDP (Parallel Distributed Processing), \n241\nplotting. See also scatterplots\nknees, 169\nparallel coordinates, 75\nscree plot, 169\npopulation, selected population, 32\u201333\nPOS (Part of Speech) tagging, text \nmining, 333\u2013336\nposterior probability, 264\u2013265\nprecision, 287\npredictive analytics, 1\u20132\nalgorithms, 3\u20135\nversus business intelligence, 8\u201310\nbusiness intelligence similarities, \n9\u201310\nchallenges\nin data, 14\u201315\nin deployment, 16\nin management, 14\nin modeling, 15\nCRISP-DM, 19\u201321\nversus data mining, 13\nmodeler requirements, 16\u201317\nnon-parametric models, 6\nparametric models, 6\nstatistics comparison, 10\u201311\nstatistics contrasted, 12\nusers of, 13\u201314\npredictive modeling, 5, 9, 213\u2013214\nANN (Artifi cial Neural Networks), \n240\u2013241\nBayes\u2019 theorem, 264\ncolumns, 26\u201327\ndata defi nit", "6\nstatistics comparison, 10\u201311\nstatistics contrasted, 12\nusers of, 13\u201314\npredictive modeling, 5, 9, 213\u2013214\nANN (Artifi cial Neural Networks), \n240\u2013241\nBayes\u2019 theorem, 264\ncolumns, 26\u201327\ndata defi nition, 25\u201326\ndata format, 151\u2013152\ntransactional, 152\u2013153\ndecision trees, 214\u2013218\nexperts, three-legged stool analogy, \n22\nk-NN (nearest neighbor)\ndistance metrics, 258\u2013259\nlearning algorithm, 254\u2013258\nlinear regression, 271\u2013274\nlogistic regression, 230\u2013233\ndummy variables, 238\u2013239\ninteractions, 236\u2013237\nlogistic curve, 232\nmissing values, 238\nmodel interpretation, 233\u2013235\nmulti-class classifi cation, 239\u2013240\nodds ratio, 230\u2013231\nvariable selection options, 234\nmeasuring success, 32\u201334\nNa\u00efve Bayes algorithm, 264\nBayes\u2019 Theorem, 264\ncategorical inputs, 269\ncorrelated variables and, 270\ninteractions, 270\nprobability, 264\u2013265\nNa\u00efve Bayes classifi er, 268\nnearest neighbor, 254\nout of order, 34\u201335\nregression models, 270\nthree-legged stool analogy, 22\u201323\nunit of analysis, 27\u201329\npredictive modeling sof", "270\nprobability, 264\u2013265\nNa\u00efve Bayes classifi er, 268\nnearest neighbor, 254\nout of order, 34\u201335\nregression models, 270\nthree-legged stool analogy, 22\u201323\nunit of analysis, 27\u201329\npredictive modeling software, \ndeployment and, 358\npredictive modeling software \ndeployment add-on, deployment \nand, 358\nprior probabilities, 224, 264\u2013265\nprobability\nBayes\u2019 Theorem and, 265\u2013266\nposterior, 264\u2013265\nprior, 264\u2013265\npropagation, neural networks, 251\nPr(>|z|), logistic regression and, \n234\npunctuation fi lters, text mining, \n336\u2013337\n\n424 \nIndex \u25a0 Q\u2013S\nQ\nquantiles, 53\u201354\nquartiles, 53\u201354\nIQR (Inter-Quartile Range), 54\u201355\nQUEST (Quick, Unbiased and Effi cient \nStatistical Tree) algorithm, 216\nR\nrank-ordered approach to model \nassessment, 293\u2013294\ncustom, 298\u2013300\ngains chart, 294\u2013298\nlift charts, 294\u2013298\nrank-ordered statistics, 52\u201355\nreal variables, 44\nrecall, 287\nrecords, decision trees\nBonferroni correction, 222\nmisclassifi cation costs, 224\u2013229\nparent nodes, 222\nprior probabilities, 224\nterminal nodes", "-ordered statistics, 52\u201355\nreal variables, 44\nrecall, 287\nrecords, decision trees\nBonferroni correction, 222\nmisclassifi cation costs, 224\u2013229\nparent nodes, 222\nprior probabilities, 224\nterminal nodes, 222\nrecursive partitioning algorithms, 218\nredundant association rules, 158\nredundant variables, removing, 119\nregression, 5\nalgorithms, 280\nlinear, 271\u2013274\nassumptions, 274\u2013276\nclassifi cation and, 279\ndestructive collinearity, 276\ninterpreting models, 278\u2013279\nresiduals, 272\u2013273\nvariable selection, 276\u2013277\nlogistic, 230\u2013233\nlogistic curve, 232\nmodel interpretation, \n233\u2013235\nodds ratio, 230\u2013231\nmodels, 270\nassessment, 301\u2013304\nPCA and, 173\nREs (regular expressions), text mining \nand, 349\u2013352\nresiduals, 321\nlinear regression, 272\u2013273\nRF (Random Forests), bagging \nalgorithm, 320\u2013321\nRHS (right-hand-side), 148\nRMSE (root means squared error) of \nNetfl ix algorithm, 308\nrobust statistics, 53\nROC (Receiver Operating \nCharacteristic) curves, 291\u2013293\nRprop (resilient propagation), neural \nnetwor", "hand-side), 148\nRMSE (root means squared error) of \nNetfl ix algorithm, 308\nrobust statistics, 53\nROC (Receiver Operating \nCharacteristic) curves, 291\u2013293\nRprop (resilient propagation), neural \nnetworks, 251\nrule support, 149\nrules, 148\nlift, 150\nS\nsampling, 123\u2013124\ncross-validation, 130\u2013132\ndata size determination, 128\u2013129\ndistance between data points, \n126\u2013128\nmodel rebuild and, 370\u2013372\npartitioning, 124\u2013126\nsubsets, 130\nstratifi ed, 136\u2013139\ntemporal sequencing, 134\u2013136\nscaling\nmethods, 105\nvariables, numeric, 104\u2013107\nscatterplots, 69\u201371\nAnscombe\u2019s Quartet, 71\u201375\nmatrices, 75\u201376\nmultiple dimensions, 78\u201380\nparallel coordinates, 75\nscore segments, deployment and, \n362\u2013366\nscree plot, 169\nSE (standard error of the coeffi cient), \nlogistic regression and, 233\nsecond-order methods, neural \nnetworks, 251\nsegments, 163\ndeployment and, 362\u2013366\n\n \nIndex \u25a0 T\u2013T \n425\nsensitivity, 287\nsentiment analysis, text mining and, \n333\nSentiment Polarity Dataset, text \nmining, 339\u2013340\nSGB (Stochastic Gradi", "tworks, 251\nsegments, 163\ndeployment and, 362\u2013366\n\n \nIndex \u25a0 T\u2013T \n425\nsensitivity, 287\nsentiment analysis, text mining and, \n333\nSentiment Polarity Dataset, text \nmining, 339\u2013340\nSGB (Stochastic Gradient Boosting), \n321\nSimpson\u2019s paradox, 64\u201365\nskew, 49\u201350\ncorrection, 99\u2013103\nlinear regression and, 104\nnegative, 101\u2013102\nsoftware, dependence, imputation \nand, 97\u201398\nspecifi city, 287\nsplits, 216\ndecision trees\nsplitting metrics, 221\u2013222\nvariables, 229\nspurious correlations, 66\u201367\nsquashing function, neurons, 242\nSSE (Sum of Squared Error) metric, \n181, 186\u2013187\nStandard Deviation, 45\nstandard deviation, mean imputation \nand, 94\nstatistics, 3\nanalytics and, 11\u201312\nIQR (Inter-Quartile Range), \n54\u201355\npredictive analytics comparison, \n10\u201311\npredictive analytics contrasted, 12\nrank-ordered, 52\u201355\nrobust, 53\nsignifi cance, 80\u201381\nstemming, text mining, 337\u2013338\nstepwise selection, variables, 277\nstop words, text mining, 336\u2013337\nstratifi ed sampling, 136\u2013139\nstructured data, text mining and, \n329\u201333", "bust, 53\nsignifi cance, 80\u201381\nstemming, text mining, 337\u2013338\nstepwise selection, variables, 277\nstop words, text mining, 336\u2013337\nstratifi ed sampling, 136\u2013139\nstructured data, text mining and, \n329\u2013330\nsubsets, partitioning and, 130\nsuccess measurements, 32\u201334\nsupervised learning, 5\nSurowiecki, James, The Wisdom of \nCrowds, 308\u2013309\nsurvey analysis case study, 377\u2013378\ndata preparation, 381\u2013385\ndata understanding, 380\u2013381\ndefi ning problem, 378\u2013380\ndeployment, 401\nwhat-if analysis, 391\u2013392\nmodeling, 385\u2013401\nsyntax, regular expressions, 350\nT\ntarget variables, 5\ndefi ning, 29\u201332\noverlaying, 76\u201378\ntemporal sequencing, sampling and, \n134\u2013136\nterm grouping, text mining, 347\nterminal nodes, records, 222\ntext mining, 327\u2013328\nBoW (bag of words), 336\ncharacter length, 337\ncosine similarity, 346\ndata sources, 333\nDF (document frequency), 344\ndictionaries, 338\u2013339\ndiffi culties in, 330\u2013332\ndocument classifi cation and, 332\ndocument clustering and, 332\nIDF (inverse document frequency), \n344\ninforma", " sources, 333\nDF (document frequency), 344\ndictionaries, 338\u2013339\ndiffi culties in, 330\u2013332\ndocument classifi cation and, 332\ndocument clustering and, 332\nIDF (inverse document frequency), \n344\ninformation extraction, 333\ninformation retrieval and, 332\nkeyword features, reducing, 347\nmodeling and, 347\u2013349\nN-grams, 346\nnumber fi lters, 337\npattern extraction and, 329\nPOS (Part of Speech) tagging, \n333\u2013336\npunctuation fi lters, 336\u2013337\nREs (regular expressions) and, \n349\u2013352\nsentiment analysis, 333\n\n426 \nIndex \u25a0 U\u2013V\nSentiment Polarity Dataset, 339\u2013340\nstemming, 337\u2013338\nstop words, 336\u2013337\nstructured data, 329\u2013330\nterm grouping, 347\nTF (term frequency), 341\u2013344\nTF-IDF, 344\u2013345\ntokenization, 336\nunstructured data, 329\u2013330\nTF (term frequency), text mining, \n341\u2013344\nTF-IDF, text mining, 344\u2013345\nthree-legged stool analogy, 22\u201323\ntime series features, 116\u2013117\ntokenization, text mining, 336\ntransactional format, 152\u2013153\ntransfer functions, ANN, 242\ntransformations, 98\u201399\nskew correction, 100\u2013101", "hree-legged stool analogy, 22\u201323\ntime series features, 116\u2013117\ntokenization, text mining, 336\ntransactional format, 152\u2013153\ntransfer functions, ANN, 242\ntransformations, 98\u201399\nskew correction, 100\u2013101\ntrue regression line, 272\ntype I errors, 287\ntype II errors, 287\nU\nuniform distribution, 46\u201347\nunit of analysis, 25\npredictive modeling and, 27\u201329\nunstructured data\nexamples, 330\ntext mining and, 329\u2013330\nunsupervised learning, 5\nV\nvalidation, cross-validation, 130\u2013132\nvalues\neigenvalues, 168\nmissing\ncodes, 90\ndefault dependence, 97\u201398\ndummy variables, 97\nimputation for categorical values, \n97\nimputation from model, 97\nimputation with constant, 92\u201393\nimputation with distributions, \n94\u201395\nlistwise deletion, 92\nMAR, 91\nMCAR, 91\nmean imputation for continuous \nvariables, 93\u201394\nmedian imputation for continuous \nvariables, 93\u201394\nMNAR, 91\nnull value, 90\nrandom imputation from own \ndistributions, 96\nsoftware dependence, 97\u201398\ntoo much, 97\nnull value, 90\nvariable cleaning and, 84\u201385\nvariables\nasso", "or continuous \nvariables, 93\u201394\nMNAR, 91\nnull value, 90\nrandom imputation from own \ndistributions, 96\nsoftware dependence, 97\u201398\ntoo much, 97\nnull value, 90\nvariable cleaning and, 84\u201385\nvariables\nassociation rules, deployment, 157\nbackward selection, 277\ncategorical, 55\u201358\nimputation, 97\nk-NN (nearest neighbor), 262\u2013263\ncleaning\nformat consistency, 85\nincorrect values, 84\u201385\ncluster models, 203\u2013204\nANOVA, 204\u2013205\ncontinuous\nbinning, 103\u2013104\nimputation, 93\u201394\ncorrelations, 66\u201368\nspurious, 66\u201367\ncrosstabs, 68\u201369\ndate and time, 109\u2013110\ndecision trees, splits, 229\ndummy, 97\nlogistic regression and, 238\u2013239\nfeatures\nbinning continuous variables, \n103\u2013104\ndate and time, 109\u2013110\nmultidimensional, 112\u2013117\nnominal, transformation, 107\u2013108\n\n \nIndex \u25a0 W\u2013X Y Z \n427\nnumber variable scaling, 104\u2013107\nordinal, transformations, 108\u2013109\nskew correction, 99\u2013103\ntransformations, 98\u201399\nZIP codes, 110\nforward selection, 276\u2013277\ninfl uence, neural networks, 252\ninteractions\nassociation rule deployment, \n157\u2013", "inal, transformations, 108\u2013109\nskew correction, 99\u2013103\ntransformations, 98\u201399\nZIP codes, 110\nforward selection, 276\u2013277\ninfl uence, neural networks, 252\ninteractions\nassociation rule deployment, \n157\u2013158\ncombinatorial explosion, 65\u201366\nselecting, 122\nSimpson\u2019s Paradox, 64\u201365\nirrelevant\ninterpretation and, 208\nK-Means, 184\nremoving, 118\nlinear regression, 276\u2013277\nlogistic regression, selection options, \n234\nMAXRAMNT, 88\nnominal, transformations, \n107\u2013108\nnumeric, scaling, 104\u2013107\nordinal, transformations, 108\u2013109\noutliers, 85\u201386\nbins, 87\nclusters and, 210\u2013212\nmodifi cation, 87\u201388\nmultidimensional, 89\nremoving from modeling data, \n86\nseparate models, 86\u201387\ntransforming, 87\nPCA (Principal Component \nAnalysis), 174\u2013177\npredictive modeling and, 26\u201327\nreducing, prior to selection, 117\u2013122\nredundant, removing, 119\nSimpson\u2019s paradox, 64\u201365\nstepwise selection, 277\ntarget variable, 76\u201378\ndefi ning, 29\u201332\ntoo many, 119\u2013121\nversions, 110\u2013112\nZIP codes, 110\nvariance reduction, bagging \nalgorithm, 31", "nt, removing, 119\nSimpson\u2019s paradox, 64\u201365\nstepwise selection, 277\ntarget variable, 76\u201378\ndefi ning, 29\u201332\ntoo many, 119\u2013121\nversions, 110\u2013112\nZIP codes, 110\nvariance reduction, bagging \nalgorithm, 312\nvisualization\nKohonen Maps, 194\u2013195\nsingle dimension, 58\u201359\nW\nWald test, logistic regression and, 234\nweight updates, neural networks, 250\nweighted votes, k-NN (nearest \nneighbor), 263\nThe Wisdom of Crowds (Surowiecki), \n308\u2013309\nX Y Z\n y-axis, histograms, 59\nz statistic, logistic regression and, 234\nZIP codes, 110\nz-scores, 105 \n", "Essentials of Cloud Computing 1st Edition K.\nChandrasekaran - PDF Download (2025)\nhttps://ebookultra.com/download/essentials-of-cloud-\ncomputing-1st-edition-k-chandrasekaran/\nVisit ebookultra.com today to download the complete set of\nebooks or textbooks\n\nWe have selected some products that you may be interested in\nClick the link to download now or visit ebookultra.com\nfor more options!.\nCloud Computing Nayan Ruparelia\nhttps://ebookultra.com/download/cloud-computing-nayan-ruparelia/\nCloud Computing Strategies 1st Edition Dimitris N.\nChorafas\nhttps://ebookultra.com/download/cloud-computing-strategies-1st-\nedition-dimitris-n-chorafas/\nCloud Computing Tricks And Tips 4th Edition\nhttps://ebookultra.com/download/cloud-computing-tricks-and-tips-4th-\nedition/\nCloudonomics Website The Business Value of Cloud Computing\n1st Edition Weinman\nhttps://ebookultra.com/download/cloudonomics-website-the-business-\nvalue-of-cloud-computing-1st-edition-weinman/\n\nCloud Security A Comprehensive Guide to Secur", "e of Cloud Computing\n1st Edition Weinman\nhttps://ebookultra.com/download/cloudonomics-website-the-business-\nvalue-of-cloud-computing-1st-edition-weinman/\n\nCloud Security A Comprehensive Guide to Secure Cloud\nComputing 1st Edition Ronald L. Krutz\nhttps://ebookultra.com/download/cloud-security-a-comprehensive-guide-\nto-secure-cloud-computing-1st-edition-ronald-l-krutz/\nCloud Computing and Big Data 1st Edition C. Catlett\nhttps://ebookultra.com/download/cloud-computing-and-big-data-1st-\nedition-c-catlett/\nOpenStack Cloud Computing Cookbook Second Edition Kevin\nJackson\nhttps://ebookultra.com/download/openstack-cloud-computing-cookbook-\nsecond-edition-kevin-jackson/\nResource management of mobile cloud computing networks and\nenvironments 1st Edition Mastorakis\nhttps://ebookultra.com/download/resource-management-of-mobile-cloud-\ncomputing-networks-and-environments-1st-edition-mastorakis/\nEnterprise Cloud Computing Technology Architecture\nApplications 1st Edition Gautam Shroff\nhttps://ebookultr", "urce-management-of-mobile-cloud-\ncomputing-networks-and-environments-1st-edition-mastorakis/\nEnterprise Cloud Computing Technology Architecture\nApplications 1st Edition Gautam Shroff\nhttps://ebookultra.com/download/enterprise-cloud-computing-technology-\narchitecture-applications-1st-edition-gautam-shroff/\n\n\nEssentials of Cloud Computing 1st Edition K.\nChandrasekaran Digital Instant Download\nAuthor(s): K. Chandrasekaran\nISBN(s): 9781482205435, 1482205432\nEdition: 1\nFile Details: PDF, 7.67 MB\nYear: 2014\nLanguage: english\n\nEssentials of CLOUD \nCOMPUTING\nK. CHANDRASEKARAN\n\n\nEssentials of CLOUD \nCOMPUTING\n\n\nEssentials of CLOUD \nCOMPUTING\nK. Chandrasekaran\n\nCRC Press\nTaylor & Francis Group\n6000 Broken Sound Parkway NW, Suite 300\nBoca Raton, FL 33487-2742\n\u00a9 2015 by Taylor & Francis Group, LLC\nCRC Press is an imprint of Taylor & Francis Group, an Informa business\nNo claim to original U.S. Government works\nVersion Date: 20141014\nInternational Standard Book Number-13: 978-1-4822-0544-2 (eBook - ", " Press is an imprint of Taylor & Francis Group, an Informa business\nNo claim to original U.S. Government works\nVersion Date: 20141014\nInternational Standard Book Number-13: 978-1-4822-0544-2 (eBook - PDF)\nThis book contains information obtained from authentic and highly regarded sources. Reasonable \nefforts have been made to publish reliable data and information, but the author and publisher cannot \nassume responsibility for the validity of all materials or the consequences of their use. The authors and \npublishers have attempted to trace the copyright holders of all material reproduced in this publication \nand apologize to copyright holders if permission to publish in this form has not been obtained. If any \ncopyright material has not been acknowledged please write and let us know so we may rectify in any \nfuture reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical,", "ay rectify in any \nfuture reprint.\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, \ntransmitted, or utilized in any form by any electronic, mechanical, or other means, now known or \nhereafter invented, including photocopying, microfilming, and recording, or in any information stor-\nage or retrieval system, without written permission from the publishers.\nFor permission to photocopy or use material electronically from this work, please access www.copy-\nright.com (http://www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222 \nRosewood Drive, Danvers, MA 01923, 978-750-8400. CCC is a not-for-profit organization that pro-\nvides licenses and registration for a variety of users. For organizations that have been granted a photo-\ncopy license by the CCC, a separate system of payment has been arranged.\nTrademark Notice: Product or corporate names may be trademarks or registered trademarks, and are \nused only for identificat", "to-\ncopy license by the CCC, a separate system of payment has been arranged.\nTrademark Notice: Product or corporate names may be trademarks or registered trademarks, and are \nused only for identification and explanation without intent to infringe.\nVisit the Taylor & Francis Web site at\nhttp://www.taylorandfrancis.com\nand the CRC Press Web site at\nhttp://www.crcpress.com\n\nv\nContents\nForeword..............................................................................................................xvii\nPreface................................................................................................................... xix\n\t 1.\t Computing Paradigms...................................................................................1\nLearning Objectives..........................................................................................1\nPreamble............................................................................................................1\n1.1\t\nHigh-Performance Comp", "......................................................1\nPreamble............................................................................................................1\n1.1\t\nHigh-Performance Computing............................................................1\n1.2\t\nParallel Computing...............................................................................2\n1.3\t\nDistributed Computing........................................................................3\n1.4\t\nCluster Computing................................................................................3\n1.5\t\nGrid Computing.....................................................................................4\n1.6\t\nCloud Computing..................................................................................5\n1.7\t\nBiocomputing.........................................................................................5\n1.8\t\nMobile Computing................................................................................6\n1.9\t", "ng.........................................................................................5\n1.8\t\nMobile Computing................................................................................6\n1.9\t\nQuantum Computing...........................................................................6\n1.10\t Optical Computing................................................................................7\n1.11\t Nanocomputing.....................................................................................7\n1.12\t Network Computing.............................................................................7\n1.13\t Summary................................................................................................8\nKey Points..........................................................................................................8\nReview Questions.............................................................................................8\nFurther Reading.....................................", "...................................8\nReview Questions.............................................................................................8\nFurther Reading................................................................................................8\n\t 2.\t Cloud Computing Fundamentals................................................................9\nLearning Objectives..........................................................................................9\nPreamble............................................................................................................9\n2.1\t\nMotivation for Cloud Computing..................................................... 10\n2.1.1\t\nThe Need for Cloud Computing.................................................11\n2.2\t\nDefining Cloud Computing...............................................................12\n2.2.1\t\nNIST Definition of Cloud Computing.................................12\n2.2.2\t\nCloud Computing Is a Service........................", ".............................................................12\n2.2.1\t\nNIST Definition of Cloud Computing.................................12\n2.2.2\t\nCloud Computing Is a Service..............................................13\n2.2.3\t\nCloud Computing Is a Platform...........................................13\n2.3\t\n5-4-3 Principles of Cloud computing................................................ 14\n2.3.1\t\nFive Essential Characteristics............................................... 14\n2.3.2\t\nFour Cloud Deployment Models..........................................15\n2.3.3\t\nThree Service Offering Models............................................ 16\n2.4\t\nCloud Ecosystem.................................................................................17\n2.5\t\nRequirements for Cloud Services......................................................19\n2.6\t\nCloud Application...............................................................................21\n2.7\t\nBenefits and Drawbacks.....................", ".............................................19\n2.6\t\nCloud Application...............................................................................21\n2.7\t\nBenefits and Drawbacks.....................................................................22\n\nvi\nContents\n2.8\t\nSummary..............................................................................................24\nReview Points..................................................................................................24\nReview Questions...........................................................................................25\nReference..........................................................................................................25\nFurther Reading..............................................................................................26\n\t 3.\t Cloud Computing Architecture and Management................................27\nLearning Objectives......................................................................", "........................26\n\t 3.\t Cloud Computing Architecture and Management................................27\nLearning Objectives........................................................................................27\nPreamble..........................................................................................................27\n3.1\t\nIntroduction..........................................................................................28\n3.2\t\nCloud Architecture..............................................................................28\n3.2.1\t\nLayer 1 (User/Client Layer)...................................................28\n3.2.2\t\nLayer 2 (Network Layer)........................................................29\n3.2.3\t\nLayer 3 (Cloud Management Layer)....................................30\n3.2.4\t\nLayer 4 (Hardware Resource Layer)....................................30\n3.3\t\nAnatomy of the Cloud.........................................................................30\n3.4\t\nNetwork ", "....30\n3.2.4\t\nLayer 4 (Hardware Resource Layer)....................................30\n3.3\t\nAnatomy of the Cloud.........................................................................30\n3.4\t\nNetwork Connectivity in Cloud Computing...................................32\n3.4.1\t\nPublic Cloud Access Networking........................................32\n3.4.2\t\nPrivate Cloud Access Networking.......................................32\n3.4.3\t\nIntracloud Networking for Public Cloud Services............32\n3.4.4\t\nPrivate Intracloud Networking............................................33\n3.4.5\t\nNew Facets in Private Networks..........................................33\n3.4.6\t\nPath for Internet Traffic..........................................................34\n3.5\t\nApplications on the Cloud..................................................................34\n3.6\t\nManaging the Cloud...........................................................................37\n3.6.1\t\nManaging the Cloud Infrastructure....", "....................................................34\n3.6\t\nManaging the Cloud...........................................................................37\n3.6.1\t\nManaging the Cloud Infrastructure....................................37\n3.6.2\t\nManaging the Cloud Application........................................39\n3.7\t\nMigrating Application to Cloud........................................................40\n3.7.1\t\nPhases of Cloud Migration....................................................40\n3.7.2\t\nApproaches for Cloud Migration.........................................41\n3.8\t\nSummary..............................................................................................41\nReview Points..................................................................................................42\nReview Questions...........................................................................................42\nReferences................................................................................", "Review Questions...........................................................................................42\nReferences........................................................................................................43\nFurther Reading..............................................................................................43\n\t 4.\t Cloud Deployment Models.........................................................................45\nLearning Objectives........................................................................................45\nPreamble..........................................................................................................45\n4.1\t\nIntroduction..........................................................................................46\n4.2\t\nPrivate Cloud........................................................................................47\n4.2.1\t\nCharacteristics.........................................................................47\n4.2.2\t\nSuita", ".......................................................................................47\n4.2.1\t\nCharacteristics.........................................................................47\n4.2.2\t\nSuitability.................................................................................48\n4.2.3\t\nOn-Premise Private Cloud....................................................49\n4.2.3.1\t\nIssues........................................................................49\n\nvii\nContents\n4.2.4\t\nOutsourced Private Cloud.....................................................51\n4.2.4.1\t\nIssues........................................................................51\n4.2.5\t\nAdvantages..............................................................................52\n4.2.6\t\nDisadvantages.........................................................................52\n4.3\t\nPublic Cloud.........................................................................................53\n4.3.1\t\nCharacteristics..................", "................................................52\n4.3\t\nPublic Cloud.........................................................................................53\n4.3.1\t\nCharacteristics.........................................................................53\n4.3.2\t\nSuitability.................................................................................54\n4.3.3\t\nIssues........................................................................................54\n4.3.4\t\nAdvantages..............................................................................56\n4.3.5\t\nDisadvantages.........................................................................56\n4.4\t\nCommunity Cloud...............................................................................56\n4.4.1\t\nCharacteristics.........................................................................57\n4.4.2\t\nSuitability.................................................................................58\n4.4.3\t\nOn-Premise Community Cloud.......", ".......................................................57\n4.4.2\t\nSuitability.................................................................................58\n4.4.3\t\nOn-Premise Community Cloud...........................................58\n4.4.3.1\t\nIssues........................................................................58\n4.4.4\t\nOutsourced Community Cloud...........................................59\n4.4.4.1\t\nIssues........................................................................60\n4.4.5\t\nAdvantages..............................................................................60\n4.4.6\t\nDisadvantages......................................................................... 61\n4.5\t\nHybrid Cloud....................................................................................... 61\n4.5.1\t\nCharacteristics.........................................................................62\n4.5.2\t\nSuitability.................................................................................62", "\n4.5.1\t\nCharacteristics.........................................................................62\n4.5.2\t\nSuitability.................................................................................62\n4.5.3\t\nIssues........................................................................................62\n4.5.4\t\nAdvantages..............................................................................63\n4.5.5\t\nDisadvantages.........................................................................64\n4.6\t\nSummary..............................................................................................64\nReview Points..................................................................................................64\nReview Questions...........................................................................................65\nReferences........................................................................................................65\n\t 5.\t Cloud Service Models.........................", ".............................65\nReferences........................................................................................................65\n\t 5.\t Cloud Service Models..................................................................................67\nLearning Objectives........................................................................................67\nPreamble..........................................................................................................67\n5.1\t\nIntroduction..........................................................................................68\n5.2\t\nInfrastructure as a Service.................................................................71\n5.2.1\t\nCharacteristics of IaaS............................................................72\n5.2.2\t\nSuitability of IaaS....................................................................73\n5.2.3\t\nPros and Cons of IaaS............................................................ 74\n5.2.4\t\nSummary of Iaa", "tability of IaaS....................................................................73\n5.2.3\t\nPros and Cons of IaaS............................................................ 74\n5.2.4\t\nSummary of IaaS Providers..................................................75\n5.3\t\nPlatform as a Service...........................................................................77\n5.3.1\t\nCharacteristics of PaaS...........................................................79\n5.3.2\t\nSuitability of PaaS...................................................................80\n5.3.3\t\nPros and Cons of PaaS...........................................................81\n5.3.4\t\nSummary of PaaS Providers.................................................83\n\nviii\nContents\n5.4\t\nSoftware as a Service...........................................................................83\n5.4.1\t\nCharacteristics of SaaS...........................................................86\n5.4.2\t\nSuitability of SaaS.................................", "..............................................83\n5.4.1\t\nCharacteristics of SaaS...........................................................86\n5.4.2\t\nSuitability of SaaS...................................................................87\n5.4.3\t\nPros and Cons of SaaS...........................................................88\n5.4.4\t\nSummary of SaaS Providers.................................................90\n5.5\t\nOther Cloud Service Models..............................................................90\n5.6\t\nSummary..............................................................................................93\nReview Points..................................................................................................94\nReview Questions...........................................................................................94\nFurther Reading..............................................................................................95\n\t 6.\t Technological Drivers for Cloud Computing.", ".....................................94\nFurther Reading..............................................................................................95\n\t 6.\t Technological Drivers for Cloud Computing..........................................97\nLearning Objectives........................................................................................97\nPreamble..........................................................................................................97\n6.1\t\nIntroduction..........................................................................................98\n6.2\t\nSOA and Cloud....................................................................................98\n6.2.1\t\nSOA and SOC..........................................................................99\n6.2.2\t\nBenefits of SOA.....................................................................100\n6.2.3\t\nTechnologies Used by SOA................................................. 101\n6.2.4\t\nSimilarities and Differences bet", "SOA.....................................................................100\n6.2.3\t\nTechnologies Used by SOA................................................. 101\n6.2.4\t\nSimilarities and Differences between SOA and \nCloud Computing................................................................. 101\n6.2.4.1\t\nSimilarities............................................................. 102\n6.2.4.2\t\nDifferences............................................................. 102\n6.2.5\t\nHow SOA Meets Cloud Computing.................................. 103\n6.2.6\t\nCCOA..................................................................................... 104\n6.3\t\nVirtualization..................................................................................... 105\n6.3.1\t\nApproaches in Virtualization............................................. 106\n6.3.1.1\t\nFull Virtualization................................................ 106\n6.3.1.2\t\nParavirtualization................................................. ", "..................................... 106\n6.3.1.1\t\nFull Virtualization................................................ 106\n6.3.1.2\t\nParavirtualization................................................. 106\n6.3.1.3\t\nHardware-Assisted Virtualization..................... 107\n6.3.2\t\nHypervisor and Its Role...................................................... 107\n6.3.3\t\nTypes of Virtualization........................................................108\n6.3.3.1\t\nOS Virtualization.................................................. 108\n6.3.3.2\t\nServer Virtualization............................................108\n6.3.3.3\t\nMemory Virtualization........................................ 108\n6.3.3.4\t\nStorage Virtualization..........................................108\n6.3.3.5\t\nNetwork Virtualization........................................ 109\n6.3.3.6\t\nApplication Virtualization................................... 109\n6.4\t\nMulticore Technology...............................................................", "................................ 109\n6.3.3.6\t\nApplication Virtualization................................... 109\n6.4\t\nMulticore Technology....................................................................... 109\n6.4.1\t\nMulticore Processors and VM Scalability......................... 110\n6.4.2\t\nMulticore Technology and the Parallelism in Cloud...... 110\n6.4.3\t\nCase Study............................................................................. 110\n6.5\t\nMemory and Storage Technologies................................................. 111\n6.5.1\t\nCloud Storage Requirements.............................................. 111\n\nix\nContents\n6.5.2\t\nVirtualization Support......................................................... 111\n6.5.3\t\nStorage as a Service (STaaS)................................................ 112\n6.5.4\t\nEmerging Trends and Technologies in Cloud Storage.......112\n6.6\t\nNetworking Technologies................................................................ 113\n6.6.1\t\nNetwork ", "................ 112\n6.5.4\t\nEmerging Trends and Technologies in Cloud Storage.......112\n6.6\t\nNetworking Technologies................................................................ 113\n6.6.1\t\nNetwork Requirements for Cloud..................................... 113\n6.6.2\t\nVirtualization Support......................................................... 114\n6.6.3\t\nUsage of Virtual Networks................................................. 115\n6.6.4\t\nDCs and VPLS....................................................................... 115\n6.6.5\t\nSDN......................................................................................... 115\n6.6.6\t\nMPLS...................................................................................... 116\n6.6.7\t\nOther Emerging Networking Trends and \nTechnologies in Cloud......................................................... 116\n6.7\t\nWeb 2.0................................................................................................ 117\n6.7.1\t\nCharacteri", "in Cloud......................................................... 116\n6.7\t\nWeb 2.0................................................................................................ 117\n6.7.1\t\nCharacteristics of Web 2.0................................................... 118\n6.7.2\t\nDifference between Web 1.0 and Web 2.0.........................120\n6.7.3\t\nApplications of Web 2.0.......................................................120\n6.7.3.1\t\nSocial Media..........................................................120\n6.7.3.2\t\nMarketing...............................................................120\n6.7.3.3\t\nEducation............................................................... 121\n6.7.4\t\nWeb 2.0 and Cloud Computing.......................................... 121\n6.8\t\nWeb 3.0................................................................................................122\n6.8.1\t\nComponents of Web 3.0.......................................................123\n6.8.1.1\t\nSemantic Web..........", "..............................................................................122\n6.8.1\t\nComponents of Web 3.0.......................................................123\n6.8.1.1\t\nSemantic Web........................................................123\n6.8.1.2\t\nWeb Services..........................................................124\n6.8.2\t\nCharacteristics of Web 3.0...................................................124\n6.8.3\t\nConvergence of Cloud and Web 3.0...................................125\n6.8.4\t\nCase Studies in Cloud and Web 3.0...................................126\n6.8.4.1\t\nConnecting Information: Facebook....................126\n6.8.4.2\t\nSearch Optimization and Web Commerce: \nBest Buy..................................................................126\n6.8.4.3\t\nUnderstanding Text: Millward Brown..............127\n6.9\t\nSoftware Process Models for Cloud................................................127\n6.9.1\t\nTypes of Software Models.................................................", "llward Brown..............127\n6.9\t\nSoftware Process Models for Cloud................................................127\n6.9.1\t\nTypes of Software Models...................................................127\n6.9.1.1\t\nWaterfall Model.....................................................127\n6.9.1.2\t\nV Model..................................................................128\n6.9.1.3\t\nIncremental Model................................................128\n6.9.1.4\t\nRAD Model............................................................128\n6.9.1.5\t\nAgile Model...........................................................128\n6.9.1.6\t\nIterative Model......................................................128\n6.9.1.7\t\nSpiral Model..........................................................129\n6.9.2\t\nAgile SDLC for Cloud Computing.....................................129\n6.9.2.1\t\nFeatures of Cloud SDLC.......................................130\n6.9.3\t\nAgile Software Development Process..........................", "or Cloud Computing.....................................129\n6.9.2.1\t\nFeatures of Cloud SDLC.......................................130\n6.9.3\t\nAgile Software Development Process...............................130\n6.9.4\t\nAdvantages of Agile Model................................................ 131\n\nx\nContents\n6.9.5\t\nHow Cloud Meets Agile Process?...................................... 132\n6.9.5.1\t\nSix Ways the Cloud Enhances Agile \nSoftware Development......................................... 132\n6.9.5.2\t\nCase Study of Agile Development......................133\n6.10\t Programming Models.......................................................................134\n6.10.1\t Programming Models in Cloud.........................................135\n6.10.1.1\t BSP Model..............................................................135\n6.10.1.2\t MapReduce Model................................................136\n6.10.1.3\t SAGA...................................................................... 137\n6.10.1.4", "......................135\n6.10.1.2\t MapReduce Model................................................136\n6.10.1.3\t SAGA...................................................................... 137\n6.10.1.4\t Transformer........................................................... 137\n6.10.1.5\t Grid Batch Framework......................................... 139\n6.11\t Pervasive Computing........................................................................ 139\n6.11.1\t How Pervasive Computing Works?................................... 140\n6.11.2\t How Pervasive Computing Helps Cloud \nComputing?.....................................................................141\n6.12\t Operating System.............................................................................. 142\n6.12.1\t Types of Operating Systems............................................... 143\n6.12.2\t Role of OS in Cloud Computing......................................... 144\n6.12.3\t Features of Cloud OS.....................................", "............................................... 143\n6.12.2\t Role of OS in Cloud Computing......................................... 144\n6.12.3\t Features of Cloud OS........................................................... 146\n6.12.3.1\t Well-Defined and Abstracted Interfaces........... 146\n6.12.3.2\t Support for Security at the Core......................... 146\n6.12.3.3\t Managing Virtualized Workloads..................... 147\n6.12.3.4\t Management of Workloads................................. 147\n6.12.4\t Cloud OS Requirements...................................................... 147\n6.12.5\t Cloud-Based OS.................................................................... 148\n6.13\t Application Environment................................................................. 149\n6.13.1\t Need for Effective ADE....................................................... 149\n6.13.2\t Application Development Methodologies........................150\n6.13.2.1\t Distributed Development......................", "ffective ADE....................................................... 149\n6.13.2\t Application Development Methodologies........................150\n6.13.2.1\t Distributed Development....................................150\n6.13.2.2\t Agile Development...............................................150\n6.13.3\t Power of Cloud Computing in Application \nDevelopment.........................................................................150\n6.13.3.1\t Disadvantages of Desktop Development.......... 151\n6.13.3.2\t Advantages of Application Development \nin\u00a0the Cloud........................................................... 152\n6.13.4\t Cloud Application Development Platforms..................... 152\n6.13.4.1\t Windows Azure....................................................153\n6.13.4.2\t Google App Engine..............................................153\n6.13.4.3\t Force.com...............................................................153\n6.13.4.4\t Manjrasoft Aneka........................................", "............................................153\n6.13.4.3\t Force.com...............................................................153\n6.13.4.4\t Manjrasoft Aneka.................................................154\n6.13.5\t Cloud Computing APIs.......................................................154\n6.13.5.1\t Rackspace...............................................................154\n6.13.5.2\t IBM..........................................................................154\n6.13.5.3\t Intel.........................................................................154\n\nxi\nContents\n6.14\t Summary............................................................................................155\nReview Points................................................................................................155\nReview Questions.........................................................................................156\nReferences.............................................................................", "155\nReview Questions.........................................................................................156\nReferences...................................................................................................... 157\nFurther Reading............................................................................................ 160\n\t 7.\t Virtualization............................................................................................... 161\nLearning Objectives...................................................................................... 161\nPreamble........................................................................................................ 161\n7.1\t\nIntroduction........................................................................................ 162\n7.2\t\nVirtualization Opportunities........................................................... 164\n7.2.1\t\nProcessor Virtualization......................................................164\n7.2.2\t\nMemory V", ".2\t\nVirtualization Opportunities........................................................... 164\n7.2.1\t\nProcessor Virtualization......................................................164\n7.2.2\t\nMemory Virtualization........................................................ 165\n7.2.3\t\nStorage Virtualization.......................................................... 165\n7.2.4\t\nNetwork Virtualization....................................................... 166\n7.2.5\t\nData Virtualization............................................................... 167\n7.2.6\t\nApplication Virtualization.................................................. 169\n7.3\t\nApproaches to Virtualization.......................................................... 169\n7.3.1\t\nFull Virtualization................................................................ 170\n7.3.2\t\nParavirtualization.................................................................172\n7.3.3\t\nHardware-Assisted Virtualization....................................173", "....................... 170\n7.3.2\t\nParavirtualization.................................................................172\n7.3.3\t\nHardware-Assisted Virtualization....................................173\n7.4\t\nHypervisors........................................................................................ 174\n7.4.1\t\nTypes of Hypervisors...........................................................175\n7.4.2\t\nSecurity Issues and Recommendations............................177\n7.5\t\nFrom Virtualization to Cloud Computing..................................... 179\n7.5.1\t\nIaaS......................................................................................... 180\n7.5.2\t\nPaaS........................................................................................ 182\n7.5.3\t\nSaaS......................................................................................... 182\n7.6\t\nSummary............................................................................................184\nReview Points......", ".................................................................... 182\n7.6\t\nSummary............................................................................................184\nReview Points................................................................................................185\nReview Questions......................................................................................... 186\nFurther Reading............................................................................................ 187\n\t 8.\t Programming Models for Cloud Computing........................................ 189\nLearning Objectives...................................................................................... 189\nPreamble........................................................................................................ 189\n8.1\t\nIntroduction........................................................................................190\n8.2\t\nExtended Programming Models for Cloud.................", "........................... 189\n8.1\t\nIntroduction........................................................................................190\n8.2\t\nExtended Programming Models for Cloud................................... 191\n8.2.1\t\nMapReduce............................................................................ 191\n8.2.1.1\t\nMap Function........................................................ 192\n8.2.1.2\t\nReduce Function................................................... 192\n8.2.2\t\nCGL-MapReduce.................................................................. 193\n\nxii\nContents\n8.2.3\t\nCloud Haskell: Functional Programming........................ 195\n8.2.4\t\nMultiMLton: Functional Programming............................ 197\n8.2.5\t\nErlang: Functional Programming...................................... 197\n8.2.5.1\t\nCloudI..................................................................... 198\n8.2.6\t\nSORCER: Object-Oriented Programming.........................199\n8.2.7\t\nProgramming Models ", "......... 197\n8.2.5.1\t\nCloudI..................................................................... 198\n8.2.6\t\nSORCER: Object-Oriented Programming.........................199\n8.2.7\t\nProgramming Models in Aneka........................................202\n8.2.7.1\t\nTask Execution Model..........................................202\n8.2.7.2\t\nThread Execution Model.....................................203\n8.2.7.3\t\nMap Reduce Model...............................................203\n8.3\t\nNew Programming Models Proposed for Cloud.........................203\n8.3.1\t\nOrleans...................................................................................204\n8.3.2\t\nBOOM and Bloom................................................................206\n8.3.3\t\nGridBatch...............................................................................207\n8.3.4\t\nSimple API for Grid Applications......................................207\n8.4\t\nSummary....................................................................", "...................................207\n8.3.4\t\nSimple API for Grid Applications......................................207\n8.4\t\nSummary............................................................................................ 210\nReview Points................................................................................................ 210\nReview Questions......................................................................................... 211\nReferences...................................................................................................... 212\nFurther Reading............................................................................................ 213\n\t 9.\t Software Development in Cloud............................................................. 215\nLearning Objectives...................................................................................... 215\nPreamble...................................................................................................", "es...................................................................................... 215\nPreamble........................................................................................................ 215\n9.1\t\nIntroduction........................................................................................ 216\n9.1.1\t\nSaaS Is Different from Traditional Software.................... 217\n9.1.2\t\nSaaS Benefits.......................................................................... 217\n9.1.3\t\nSuitability of SaaS................................................................. 218\n9.2\t\nDifferent Perspectives on SaaS Development................................ 219\n9.2.1\t\nSaaS from Managed Infrastructure and Platform........... 219\n9.2.2\t\nSaaS from IaaS and Managed Platform............................220\n9.2.3\t\nSaaS from Managed Infrastructure and PaaS..................221\n9.2.4\t\nSaaS from IaaS and PaaS.....................................................222\n9.3\t\nNew Challenges........", "............220\n9.2.3\t\nSaaS from Managed Infrastructure and PaaS..................221\n9.2.4\t\nSaaS from IaaS and PaaS.....................................................222\n9.3\t\nNew Challenges.................................................................................224\n9.3.1\t\nMultitenancy.........................................................................224\n9.3.2\t\nSecurity..................................................................................224\n9.3.3\t\nScalability..............................................................................225\n9.3.4\t\nAvailability............................................................................225\n9.3.5\t\nUsability.................................................................................225\n9.3.6\t\nSelf-Service Sign-Up.............................................................226\n9.3.7\t\nAutomated Billing................................................................226\n9.3.8\t\nNondisruptive Updates..............", "p.............................................................226\n9.3.7\t\nAutomated Billing................................................................226\n9.3.8\t\nNondisruptive Updates.......................................................226\n9.3.9\t\nService Integration...............................................................226\n9.3.10\t Vendor Lock-In.....................................................................227\n\nxiii\nContents\n9.4\t\nCloud-Aware Software Development Using PaaS Technology....... 227\n9.4.1\t\nRequirements Analysis........................................................230\n9.4.2\t\nMultitenant Architecture....................................................230\n9.4.3\t\nHighly Scalable and Available Architecture.................... 231\n9.4.4\t\nDatabase Design...................................................................233\n9.4.5\t\nSaaS Development................................................................234\n9.4.6\t\nMonitoring and SLA Maintenance...........", ".........................................................233\n9.4.5\t\nSaaS Development................................................................234\n9.4.6\t\nMonitoring and SLA Maintenance....................................235\n9.5\t\nSummary............................................................................................236\nReview Points................................................................................................236\nReview Questions.........................................................................................237\nFurther Reading............................................................................................238\n\t10.\t Networking for Cloud Computing.......................................................... 241\nLearning Objectives...................................................................................... 241\nPreamble........................................................................................................ 241\n10.1", ".......................................................................... 241\nPreamble........................................................................................................ 241\n10.1\t Introduction........................................................................................ 241\n10.2\t Overview of Data Center Environment.........................................243\n10.2.1\t Architecture of Classical Data Centers.............................244\n10.2.2\t CEDCs....................................................................................245\n10.2.3\t Physical Organization..........................................................245\n10.2.4\t Storage and Networking Infrastructure...........................246\n10.2.5\t Cooling Infrastructure......................................................... 247\n10.2.6\t Nature of Traffic in Data Centers.......................................248\n10.3\t Networking Issues in Data Centers.............................................", "........................... 247\n10.2.6\t Nature of Traffic in Data Centers.......................................248\n10.3\t Networking Issues in Data Centers................................................ 249\n10.3.1\t Availability............................................................................ 249\n10.3.2\t Poor Network Performance................................................250\n10.3.3\t Security..................................................................................250\n10.4\t Transport Layer Issues in DCNs.....................................................250\n10.4.1\t TCP Impairments in DCNs.................................................250\n10.4.1.1\t TCP Incast..............................................................251\n10.4.1.2\t TCP Outcast...........................................................252\n10.4.1.3\t Queue Buildup......................................................254\n10.4.1.4\t Buffer Pressure....................................................", "......................................252\n10.4.1.3\t Queue Buildup......................................................254\n10.4.1.4\t Buffer Pressure......................................................254\n10.4.1.5\t Pseudocongestion Effect......................................254\n10.4.2\t Summary: TCP Impairments and Causes........................255\n10.5\t TCP Enhancements for DCNs.........................................................255\n10.5.1\t TCP with Fine-Grained RTO (FG-RTO)............................256\n10.5.2\t TCP with FG-RTO + Delayed ACKs Disabled.................256\n10.5.3\t DCTCP...................................................................................257\n10.5.3.1\t ECN.........................................................................257\n10.5.4\t ICTCP..................................................................................... 261\n10.5.5\t IA-TCP.................................................................................... 262\n10.5.6\t D2TCP.....", ".......................................................................... 261\n10.5.5\t IA-TCP.................................................................................... 262\n10.5.6\t D2TCP.....................................................................................263\n10.5.7\t TCP-FITDC............................................................................263\n\nxiv\nContents\n10.5.8\t TDCTCP.................................................................................264\n10.5.9\t TCP with Guarantee Important Packets (GIP).................265\n10.5.10\tPVTCP....................................................................................266\n10.5.11\tSummary: TCP Enhancements for DCNs.........................266\n10.6\t Summary............................................................................................268\nReview Points................................................................................................269\nReview Questions.........................", "..........................................268\nReview Points................................................................................................269\nReview Questions.........................................................................................269\nReferences......................................................................................................269\n\t11.\t Cloud Service Providers............................................................................273\nLearning Objectives......................................................................................273\nPreamble........................................................................................................273\n11.1\t Introduction........................................................................................ 274\n11.2\t EMC..................................................................................................... 274\n11.2.1\t EMC IT..........................................", "........................ 274\n11.2\t EMC..................................................................................................... 274\n11.2.1\t EMC IT................................................................................... 274\n11.2.2\t Captiva Cloud Toolkit.......................................................... 276\n11.3\t Google.................................................................................................277\n11.3.1\t Cloud Platform......................................................................277\n11.3.2\t Cloud Storage........................................................................278\n11.3.3\t Google Cloud Connect........................................................278\n11.3.4\t Google Cloud Print..............................................................278\n11.3.5\t Google App Engine..............................................................279\n11.4\t Amazon Web Services............................................................", "....................278\n11.3.5\t Google App Engine..............................................................279\n11.4\t Amazon Web Services......................................................................280\n11.4.1\t Amazon Elastic Compute Cloud........................................281\n11.4.2\t Amazon Simple Storage Service........................................283\n11.4.3\t Amazon Simple Queue Service..........................................283\n11.5\t Microsoft.............................................................................................284\n11.5.1\t Windows Azure....................................................................284\n11.5.2\t Microsoft Assessment and Planning Toolkit...................285\n11.5.3\t SharePoint..............................................................................285\n11.6\t IBM.......................................................................................................285\n11.6.1\t Cloud Models...............................", ".............................285\n11.6\t IBM.......................................................................................................285\n11.6.1\t Cloud Models........................................................................286\n11.6.2\t IBM SmartCloud...................................................................287\n11.7\t SAP Labs.............................................................................................289\n11.7.1\t SAP HANA Cloud Platform...............................................289\n11.7.2\t Virtualization Services Provided by SAP.........................289\n11.8\t Salesforce............................................................................................290\n11.8.1\t Sales Cloud............................................................................290\n11.8.2\t Service Cloud: Knowledge as a Service............................290\n11.9\t Rackspace..........................................................................................", "..............290\n11.8.2\t Service Cloud: Knowledge as a Service............................290\n11.9\t Rackspace............................................................................................ 291\n11.10\t VMware...............................................................................................292\n11.11\t Manjrasoft...........................................................................................294\n11.11.1\t Aneka Platform.....................................................................294\n11.12\t Summary............................................................................................296\nReview Points................................................................................................297\n\nxv\nContents\nReview Questions.........................................................................................297\nReferences......................................................................................................298\nFurther Read", "....................................................................297\nReferences......................................................................................................298\nFurther Reading............................................................................................298\n\t12.\t Open Source Support for Cloud..............................................................299\nLearning Objectives......................................................................................299\nPreamble........................................................................................................299\n12.1\t Introduction........................................................................................300\n12.1.1\t Open Source in Cloud Computing: An Overview..........300\n12.1.2\t Difference between Open Source and Closed Source....300\n12.1.3\t Advantages of Having an Open Source............................301\n12.2\t Open Source Tools for IaaS.............................", "12.1.2\t Difference between Open Source and Closed Source....300\n12.1.3\t Advantages of Having an Open Source............................301\n12.2\t Open Source Tools for IaaS..............................................................301\n12.2.1\t OpenNebula..........................................................................302\n12.2.2\t Eucalyptus.............................................................................303\n12.2.3\t OpenStack..............................................................................304\n12.2.4\t Apache CloudStack..............................................................306\n12.2.5\t Nimbus...................................................................................306\n12.2.6\t GoGrid Cloud........................................................................307\n12.3\t Open Source Tools for PaaS.............................................................308\n12.3.1\t Paasmaker...................................................................", "...............307\n12.3\t Open Source Tools for PaaS.............................................................308\n12.3.1\t Paasmaker..............................................................................308\n12.3.2\t Red Hat OpenShift Origin..................................................308\n12.3.3\t Xen Cloud Platform..............................................................309\n12.3.4\t Cloudify.................................................................................309\n12.4\t Open Source Tools for SaaS.............................................................. 310\n12.4.1\t Apache VCL........................................................................... 310\n12.4.2\t Google Drive......................................................................... 311\n12.4.3\t Google Docs.......................................................................... 312\n12.4.4\t Dropbox................................................................................. 313\n12.5\t Open S", "gle Docs.......................................................................... 312\n12.4.4\t Dropbox................................................................................. 313\n12.5\t Open Source Tools for Research...................................................... 314\n12.5.1\t CloudSim............................................................................... 314\n12.5.2\t SimMapReduce..................................................................... 314\n12.5.3\t Cloud Analyst....................................................................... 315\n12.5.4\t GreenCloud........................................................................... 316\n12.6\t Distributed Computing Tools for Management of \nDistributed Systems.......................................................................... 318\n12.6.1\t Cassandra.............................................................................. 318\n12.6.2\t Hadoop...............................................................", ".................. 318\n12.6.1\t Cassandra.............................................................................. 318\n12.6.2\t Hadoop.................................................................................. 318\n12.6.3\t MongoDB...............................................................................320\n12.6.4\t NGrid.....................................................................................320\n12.6.5\t Ganglia................................................................................... 321\n12.7\t Summary............................................................................................ 321\nReview Points................................................................................................322\nReview Questions.........................................................................................323\nReferences......................................................................................................323\nFurther Reading..........", ".......................................................323\nReferences......................................................................................................323\nFurther Reading............................................................................................ 324\n\nxvi\nContents\n\t13.\t Security in Cloud Computing..................................................................325\nLearning Objectives......................................................................................325\nPreamble........................................................................................................325\n13.1\t Introduction........................................................................................325\n13.1.1\t Cloud in Information Technology.....................................326\n13.1.2\t Cloud General Challenges..................................................327\n13.2\t Security Aspects.........................................................................", "...............326\n13.1.2\t Cloud General Challenges..................................................327\n13.2\t Security Aspects................................................................................328\n13.2.1\t Data Security.........................................................................329\n13.2.1.1\t Data Center Security............................................330\n13.2.1.2\t Access Control.......................................................333\n13.2.1.3\t Encryption and Decryption................................333\n13.2.2\t Virtualization Security........................................................334\n13.2.3\t Network Security.................................................................336\n13.3\t Platform-Related Security................................................................337\n13.3.1\t Security Issues in Cloud Service Models..........................337\n13.3.2\t Software-as-a-Service Security Issues...............................338\n13.3.3\t Platform-as", "....................337\n13.3.1\t Security Issues in Cloud Service Models..........................337\n13.3.2\t Software-as-a-Service Security Issues...............................338\n13.3.3\t Platform-as-a-Service Security Issues...............................340\n13.3.4\t Infrastructure-as-a-Service Security Issues.....................340\n13.4\t Audit and Compliance......................................................................341\n13.4.1\t Disaster Recovery.................................................................342\n13.4.2\t Privacy and Integrity...........................................................343\n13.5\t Summary............................................................................................344\nReview Points................................................................................................344\nReview Questions.........................................................................................345\nFurther Reading...............................", ".........................................344\nReview Questions.........................................................................................345\nFurther Reading............................................................................................345\n\t14.\t Advanced Concepts in Cloud Computing.............................................347\nLearning Objectives......................................................................................347\nPreamble........................................................................................................347\n14.1\t Intercloud............................................................................................348\n14.2\t Cloud Management...........................................................................351\n14.3\t Mobile Cloud......................................................................................353\n14.4\t Media Cloud.................................................................................", "Mobile Cloud......................................................................................353\n14.4\t Media Cloud.......................................................................................355\n14.5\t Interoperability and Standards.......................................................357\n14.6\t Cloud Governance.............................................................................358\n14.7\t Computational Intelligence in Cloud.............................................360\n14.8\t Green Cloud....................................................................................... 361\n14.9\t Cloud Analytics.................................................................................364\n14.10\t Summary............................................................................................366\nReview Points................................................................................................366\nReview Questions................................................", "...................366\nReview Points................................................................................................366\nReview Questions.........................................................................................367\nReferences......................................................................................................368\nFurther Reading............................................................................................369\n\nxvii\nForeword\nCloud computing is sprawling the IT landscape. Driven by several converg-\ning and complementary factors, cloud computing is advancing as a viable \nIT service delivery model at an incredible pace. It has caused a paradigm \nshift in how we deliver, use, and harness the variety of IT services it offers. \nIt also offers several benefits compared to traditional on-premise comput-\ning models, including reduced costs and increased agility and flexibility. Its \ntransformational potential is huge and impressive, and ", "fers several benefits compared to traditional on-premise comput-\ning models, including reduced costs and increased agility and flexibility. Its \ntransformational potential is huge and impressive, and consequently cloud \ncomputing is being adopted by individual users, businesses, educational \ninstitutions, governments, and community organizations. It helps close the \ndigital (information) divide. It might even help save our planet by providing \nan overall greener computing environment.\nHence, corporations are eagerly investing in promising cloud comput-\ning technologies and services not only in developed economies but also \nincreasingly in emerging economies\u2014including India, China, Taiwan, the \nPhilippines, and South Africa\u2014to address a region\u2019s specific needs. Cloud \ncomputing is receiving considerable interest among several \u00adstakeholders\u2014\nbusinesses, the IT industry, application developers, IT administrators \nand managers, researchers, and students who aspire to be successful IT \nprof", " considerable interest among several \u00adstakeholders\u2014\nbusinesses, the IT industry, application developers, IT administrators \nand managers, researchers, and students who aspire to be successful IT \nprofessionals. \nTo successfully embrace this new computing paradigm, however, they \nneed\u00a0to acquire new cloud computing knowledge and skills. In answer to this, \nuniversities have begun to offer new courses on cloud computing. Though \nthere are several books on cloud computing\u2014from basic books intended for \ngeneral readers to advanced compendium for researchers\u2014there are few \nbooks that comprehensively cover a range of cloud computing topics and \nare particularly intended as an entry-level textbook for university students. \nThis book, Essentials of Cloud Computing, fills this void and is a timely and \nvaluable addition by Professor K. Chandrasekaran, a well-recognized aca-\ndemic and researcher in cloud computing.\nThe book, beginning with a brief overview on different computing par-\nadigms and ", "and \nvaluable addition by Professor K. Chandrasekaran, a well-recognized aca-\ndemic and researcher in cloud computing.\nThe book, beginning with a brief overview on different computing par-\nadigms and potentials of those paradigms, outlines the fundamentals of \ncloud computing. Then, it deals with cloud services types, cloud deploy-\nment models, technologies supporting and driving the cloud, software \nprocess models and programming models for cloud, and development of \nsoftware application that runs the cloud. It also gives an overview of ser-\nvices available from major cloud providers, highlights currently available \nopen source software and tools for cloud deployment, and discusses secu-\nrity concerns and issues in cloud computing. Finally, it outlines advances \nin cloud computing such as mobile cloud and green cloud. The book\u2019s \npresentation style supports ease of reading and comprehension. Further, \n\nxviii\nForeword\neach chapter is supplemented with review questions that help the rea", " mobile cloud and green cloud. The book\u2019s \npresentation style supports ease of reading and comprehension. Further, \n\nxviii\nForeword\neach chapter is supplemented with review questions that help the readers \nto check their understanding of topics and issues explored in the chapter.\nCloud computing is here to stay, and its adoption will be widespread. \nIt will transform not only the IT industry but also every sector of society. \nA\u00a0 wide range of people\u2014application developers, enterprise IT architects \nand administrators, and future IT professionals and managers\u2014will need \nto learn about cloud computing and how it can be deployed for a variety of \napplications. This concise and comprehensive book will help readers under-\nstand several key aspects of cloud computing\u2014technologies, models, cloud \nservices currently available, applications that are better suited for cloud, and \nmore. It will also help them examine the issues and challenges and develop \nand deploy applications in clouds. \nI bel", "\nservices currently available, applications that are better suited for cloud, and \nmore. It will also help them examine the issues and challenges and develop \nand deploy applications in clouds. \nI believe you will find the book informative, concise, comprehensive, and \nhelpful to gain cloud knowledge.\nSan Murugesan\nAdjunct Professor\nUniversity of Western Sydney\nRichmond, New South Wales, Australia\nEditor in Chief\nIEEE IT Professional\nEditor\nIEEE Computer\nDirector\nBRITE Professional Services\nAustralia\n\nOther documents randomly have \ndifferent content \n\n\n\nI\nVII\nT would be cruel to dwell upon the sufferings of Norah. She came\nto consciousness while being carried bodily through the streets by\nhalf a dozen of \u201cthe finest\u201d in Japan. But she retained consciousness\nonly long enough to give vent to another terrific shriek and then\nfaint again. When next she came to, she was in the \u201cdhirty haythen\ndoongeon,\u201d as she termed it. There Mr. Kurukawa found her, secured\nher release, and took her home.\n", " another terrific shriek and then\nfaint again. When next she came to, she was in the \u201cdhirty haythen\ndoongeon,\u201d as she termed it. There Mr. Kurukawa found her, secured\nher release, and took her home.\nBut the baby! It was only a little after nine when Norah had gone\nforth so bravely. By five in the afternoon the search for the baby had\nnot ended. Everybody in the village appeared to have had the baby\nat one time or another through the day. The little one had been\npassed from house to house as an object of curiosity. Its clothing\nwas a marvel to all Japanese eyes; its blue eyes were extraordinary;\nits little wisps of yellow hair the most amazing of sights ever seen in\nthe little town; and its milk-white skin positively unreal. Japanese\nmothers brought their own brown offspring and put them side by\nside with the little white baby. They patted its little, chubby hands,\nand put their fingers into its mouth. The latter never failed to please\nthe Kurukawa baby, which immediately fell to sucki", "ide by\nside with the little white baby. They patted its little, chubby hands,\nand put their fingers into its mouth. The latter never failed to please\nthe Kurukawa baby, which immediately fell to sucking the finger\ngreedily. After a time, however, as no milk was forthcoming from the\nnumberless fingers thus offered, the baby became cross.\nThen nobody wanted it any longer.\nMr. and Mrs. Kurukawa and a policeman went about the town\nhunting for the child. The mother was almost prostrated, but insisted\non accompanying her husband. As they turned away from each\nhouse the mother grew paler and more fearful. Finally the policeman\n\nsuggested that they abandon the search until the following morning.\nIt was getting towards night, and the Japanese retire early.\nThe parents would not hear of this. They would search all night if\nnecessary. The policeman shrugged his shoulders. Very well, he had\nother duties. As the honorable excellencies could see for themselves,\nthe streets were already almost desert", "ould search all night if\nnecessary. The policeman shrugged his shoulders. Very well, he had\nother duties. As the honorable excellencies could see for themselves,\nthe streets were already almost deserted. Indeed, there were only a\nfew children left yonder in the street. The father and mother turned\nalmost aimlessly towards the place where a number of children were\nplaying skip rope. One little girl after another would jump back and\nforth over the swinging rope. One girl seemed less nimble than the\nothers. She slipped once, and trod on the rope often. As the\nKurukawas came nearer to the group they noticed her because she\nseemed humpbacked. But the hump upon her back bobbed and\nmoved up and down. When she stopped skipping and came to their\nside of the rope the hump upon her back moved a bit higher, until it\nrested against her neck. It was a little baby\u2019s head!\nMrs. Kurukawa uttered a faint cry and rushed upon the little girl,\npitifully trying to drag the baby from her back. It was sound a", "igher, until it\nrested against her neck. It was a little baby\u2019s head!\nMrs. Kurukawa uttered a faint cry and rushed upon the little girl,\npitifully trying to drag the baby from her back. It was sound asleep\nand seemed perfectly comfortable and none the worse for its late\nadventures. Mrs. Kurukawa hugged it wildly.\n\u201cOh, my little, little baby!\u201d she sobbed. It opened its sleepy blue\neyes and gooed and gurgled softly.\nFrom this time forth the baby became the centre of attraction to\nall the family. Even Juji seemed to be conscious of its enviable\nposition. Was it not surrounded at all times by the little girls? Was it\nnot hugged and petted in a way he had considered due only to him\nfrom his sisters?\nHe had watched with wonder the queer little plaything ever since\nit had come into the house. It was no larger than some dolls his\nsisters had; but when it opened its mouth it could make a noise\nalmost as loud as Juji himself. In fact, its noises and its limbs and\neverything about it had an absor", "was no larger than some dolls his\nsisters had; but when it opened its mouth it could make a noise\nalmost as loud as Juji himself. In fact, its noises and its limbs and\neverything about it had an absorbing interest for Juji. He began to\nhang about its vicinity. Norah would discover him pressed up close to\n\nher knee, his little, serious slits of eyes intent upon every movement\nof the baby.\n\u201cBless his heart,\u201d she would say. \u201cShure the little lamb loves his\nwee brother. Then give him a nice kiss,\u201d whereupon she would put\nthe baby\u2019s face close to Juji. The latter would rub his nose against\nthe fat, soft, baby cheek. He must have pondered over his little step-\nbrother, for one night Norah was awakened by strange little sounds\nin the vicinity of the baby\u2019s bed. She reached over in the dark, found\nand enclosed a little hand in her large one. Then she saw a little\nfigure in bed with the baby. Juji was sitting up and leaning over the\nbaby. In his hand was a bottle, the end of which was thrust in", "\nand enclosed a little hand in her large one. Then she saw a little\nfigure in bed with the baby. Juji was sitting up and leaning over the\nbaby. In his hand was a bottle, the end of which was thrust into the\nbaby\u2019s mouth!\nNorah was too astonished at first to do anything but watch the\nchild. Then she seized him.\n\u201cYou lamb!\u201d said she. \u201cIf you aren\u2019t the swatest haythen, shure I\ndon\u2019t know who is!\u201d\n\u201cOpey mouth,\u201d said little Juji, in English, and pushed the bottle\ntowards Norah\u2019s lips.\nHe had seen the nurse-maid do this with the baby, and had heard\nher say:\n\u201cOpey mouthie, lovey!\u201d\nHe had found the bottle, and while all were asleep and there was\nno one to interfere with him, he had sought to feed his baby step-\nbrother.\n\n\nM\nVIII\nARION came flying into the garden, her cheeks aglow, her\nbright eyes dancing.\n\u201cIris\u2014Blossom!\u201d she called, excitedly.\nShe could hardly get her breath to tell them the great news. In\nher hand she waved aloft a sheet of paper.\n\u201cWhat ees\u2019t?\u201d asked Plum Blossom, puzzled.\n\u201c", " dancing.\n\u201cIris\u2014Blossom!\u201d she called, excitedly.\nShe could hardly get her breath to tell them the great news. In\nher hand she waved aloft a sheet of paper.\n\u201cWhat ees\u2019t?\u201d asked Plum Blossom, puzzled.\n\u201cA letter,\u201d cried Marion. \u201cGuess who from?\u201d\n\u201cGozo,\u201d both answered at once.\nMarion nodded.\n\u201cRight,\u201d she said, \u201cand to me!\u2014me!\u201d She began dancing airily\nabout, waving the letter triumphantly and then caressing it.\nIris shrieked the news across the garden to Taro, pirouetting on\nhis beloved pole. He leaped down and came running to join them.\n\u201cWhy he ride unto you?\u201d demanded Plum Blossom, enviously.\n\u201cWell, now, I\u2019ll tell you,\u201d confided Marion, sweetly. \u201cYou know ever\nsince we\u2019ve been here I\u2019ve heard nothing but Gozo, Gozo, Gozo,\nfrom you all. Goodness! you never speak a sentence without \u2018Gozo\u2019\nin it. Well, I began to think him a real hero, and I just longed to\nknow him. Besides\u201d\u2014she lowered her voice\u2014\u201dI did think he ought to\nbe warned about that\u2014about Summer!\u201d\n\u201cAbout Summer?\u201d repeated Plum Blos", "t. Well, I began to think him a real hero, and I just longed to\nknow him. Besides\u201d\u2014she lowered her voice\u2014\u201dI did think he ought to\nbe warned about that\u2014about Summer!\u201d\n\u201cAbout Summer?\u201d repeated Plum Blossom, hazily.\n\u201cWe kinno understan\u2019. You spik so fast.\u201d\n\n\u201cOh, dear, don\u2019t you see? Why, she\u2019s not good enough for a hero\u2014\nnow is she?\u201d\n\u201cWha\u2019s \u2018hero\u2019?\u201d asked Taro, disgustedly. Had they brought him\nfrom his favorite sport merely to bother him with words he could not\nunderstand.\n\u201cA hero is\u2014is\u2014well, he\u2019s something grand!\u201d\nIris yawned sleepily. She had forgotten all about the letter and\nnow was lying on the grass blinking sleepily at the blue sky\noverhead.\n\u201cYou\u2019re not listening, Iris,\u201d said Marion, frowning upon her and\nforcing her to get up.\n\u201cDon\u2019t you want to hear Gozo\u2019s letter?\u201d\n\u201cYes, yes\u2014spik it,\u201d urged Plum Blossom.\n\u201cBut I didn\u2019t finish what I was saying\u2014explaining why he wrote\nme. Don\u2019t you see, I wrote to him first. Yes, I did, too, I wrote him\nthe longest letter, and I told him about you", ",\u201d urged Plum Blossom.\n\u201cBut I didn\u2019t finish what I was saying\u2014explaining why he wrote\nme. Don\u2019t you see, I wrote to him first. Yes, I did, too, I wrote him\nthe longest letter, and I told him about you all\u2014and\u2014and\u2014can he\nread English?\u201d\nBilly had joined the group, and he spoke up now:\n\u201cAh, sis, go on now\u2014read his answer. What\u2019s he say?\u201d\n\u201cBut I can\u2019t read it. See, it\u2019s in Japanese.\u201d\n\u201cYou read it, Taro.\u201d\n\u201cMe?\u201d Taro seized the letter, and began laboriously reading it in\nJapanese.\n\u201cWell, well, what does he say?\u201d asked Marion, excitedly.\nPlum Blossom looked over her brother\u2019s shoulder and translated in\nthis wise:\n\u201cM-m-Madame,\u2014Your letter got\u2014\n\u201cYours truly forever,\n\n\u201cKurukawa Gozo.\u201d\n\u201cIs that all?\u201d inquired Marion, blankly, her blue eyes filling with\ntears.\n\u201cPostscript,\u201d shouted Taro, then read it: \u201cWrite agin, thangs!\u201d\nMarion pouted and sat down in deep dejection.\n\u201cWell, I won\u2019t do it, if that\u2019s the way he answers my letters.\u201d\nShe took the letter and went to her mother.\n\n\n\n\n\nO\nIX\nN the 15th of", " \u201cWrite agin, thangs!\u201d\nMarion pouted and sat down in deep dejection.\n\u201cWell, I won\u2019t do it, if that\u2019s the way he answers my letters.\u201d\nShe took the letter and went to her mother.\n\n\n\n\n\nO\nIX\nN the 15th of April the children dressed themselves in pink-and-\nwhite kimonos, simulating cherry blossoms, and strolled abroad\nfor hanami (flower picnic). They had been looking forward to this\ndelightful occasion for weeks. The costumes had been prepared by\ntheir grandmother some days in advance of the festival. Even Marion\nhad a little, white cr\u00eape kimono embroidered with the pale pink\nflower, and with the sash or obi of the same shade. She made quite\na picture, as with her eyes dancing and shining she came running\ninto the garden to join her step-sisters. The wings of the dainty\nsleeves of her dress fluttered back and forth. Her cheeks were the\ncolor of the cherry blossom, and the golden crown of her hair, drawn\nup into the Japanese fashion, glistened in the sun. Plum Blossom\nwore a cr\u00eape silk gown ", "ttered back and forth. Her cheeks were the\ncolor of the cherry blossom, and the golden crown of her hair, drawn\nup into the Japanese fashion, glistened in the sun. Plum Blossom\nwore a cr\u00eape silk gown of deep pink, shading at the ends to white.\nThe sash was white with pale green leaves and stalks embroidered\non it. Iris, too, was in pink, and the bow of her obi was tied to\nimitate a cherry blossom. The three little girls had flowers in their\nhair\u2014cherry blossoms, of course. They waited now in the garden for\ntheir brothers and parents. As the festival was new to Marion, she\nwas the most eager of the girls.\nFrom above their heads a voice rang out:\n\u201cHere, you, girls! get your masks and petals ready.\u201d\n\u201cWhere are you, Billy?\u201d called Marion, looking everywhere about\nthem.\n\u201cHere\u2014up in the tree.\u201d\n\nHe was perched in an old cherry-tree, where with vandal hand he\nwas plucking the blossoms.\n\u201cO-o-oo!\u201d exclaimed Plum Blossom. \u201cYou ba\u2019 boy! No can pig\nflower. Tha\u2019s nod ride!\u201d\n\u201cWhy, father said we were", "He was perched in an old cherry-tree, where with vandal hand he\nwas plucking the blossoms.\n\u201cO-o-oo!\u201d exclaimed Plum Blossom. \u201cYou ba\u2019 boy! No can pig\nflower. Tha\u2019s nod ride!\u201d\n\u201cWhy, father said we were to fill our sleeves\u2014get all we could,\u201d\ncalled down Billy.\n\u201cYes, pig from ground,\u201d said Plum Blossom; \u201cnever mus\u2019 pig from\ntree.\u201d\n\u201cBilly, you vandal, what are you doing up there?\u201d\nMr. Kurukawa had joined the children in the garden. He, too, was\nin Japanese dress.\n\u201cWhy,\u201d said Billy, \u201cyou said\u2014\u201d\n\u201cNow, my boy, come down.\u201d\nVery promptly Billy obeyed.\nTaking his step-son by the hand, Mr. Kurukawa taught him a\nlesson known to all Japanese children.\n\u201cNever pluck the flowers wantonly, least of all the sacred cherry\nblossom. When you wish the flower in your house, pluck out one\nbranch, one flower. See, you have filled the front of your kimono,\nyour sleeves, and your obi with the blossoms. Look at them!\u201d\nHe held up the crushed branches to view. They drooped almost\nreproachfully at Billy.\n\u201cBut, fathe", " you have filled the front of your kimono,\nyour sleeves, and your obi with the blossoms. Look at them!\u201d\nHe held up the crushed branches to view. They drooped almost\nreproachfully at Billy.\n\u201cBut, father,\u201d he began again. \u201cYou did tell me\u2014\u201d\n\u201cTo gather all the cherry-blossom petals you could. See, the\nground is thick with them.\u201d\n\u201cBut they are all apart. They have no stalks.\u201d\nMr. Kurukawa stooped and filled his hands full of petals. He held\nthem a moment and then lightly tossed them into the air.\n\n\u201cThat is how we want them, boy. We use them like confetti. Now\nfill all your sleeves, children. Get as many as you can, and then we\u2019ll\nstart.\u201d\nSoon the long sleeves of their dresses were filled with the petals,\nand hung like little pillows. Mrs. Kurukawa was the last to join the\nmerry party. All the children helped her to fill her sleeves, for she,\ntoo, wore the national kimono.\n\u201cHere are your masks, children,\u201d said the father. With laughing\nchatter they fastened on the grotesque masks and clambe", "children helped her to fill her sleeves, for she,\ntoo, wore the national kimono.\n\u201cHere are your masks, children,\u201d said the father. With laughing\nchatter they fastened on the grotesque masks and clambered into\nthe jinrikishas. It was a joyful day.\nThey passed numbers of picnickers, and exchanged showers of\ncherry-blossom petals with them.\nThey ate a delicious luncheon under a tree fairly weighted down\nwith the heavenly flower. While they were in the midst of their\nrepast, Taro and Billy mounted into the tree and shook it till the\nlunch was almost hidden under the petals, and the heads of all were\ncrowned in cherry pink.\nThe petals they slipped into their food purposely, declaring that it\nadded a delicious taste. Then the children played battledore and\nshuttlecock. Later, there being a pleasant wind, Mr. Kurukawa sent\nup a kite. Billy was permitted to hold the string. This was great fun,\nespecially when Taro\u2019s kite had a race with Billy\u2019s, and finally won. By\nfour in the afternoon they w", "ant wind, Mr. Kurukawa sent\nup a kite. Billy was permitted to hold the string. This was great fun,\nespecially when Taro\u2019s kite had a race with Billy\u2019s, and finally won. By\nfour in the afternoon they were all so refreshingly tired that nobody\nwanted to go home, and soon \u201cfather\u201d was besieged for a story.\n\u201cMake it modern, father,\u201d said Billy, \u201cfor we like that kind best.\u201d\n\u201cWell, let\u2019s see. What shall it be about?\u201d\n\u201cWar,\u201d shouted Taro.\nFor a while there was silence, and Mr. Kurukawa looked very\ngrave. He was thinking of Gozo.\n\u201cVery well,\u201d said he, after a moment\u2019s thought. \u201cI will tell you a\ntrue story of to-day which has to do with a war.\u201d\n\n\u201cMake it very, very long, father,\u201d said Plum Blossom.\n\u201cAnd exciting,\u201d said Taro.\n\u201cWith a little girl in it,\u201d said Iris.\n\u201cNo, no, a liddle boy,\u201d growled Juji.\n\u201cIt\u2019s about a little woman,\u201d said Mr. Kurukawa, \u201cand she was called\n\u2018The Widow of Sanyo.\u2019\u201d\n\n\n\n\nWelcome to our website \u2013 the ideal destination for book lovers and\nknowledge seekers. With a mission", "uji.\n\u201cIt\u2019s about a little woman,\u201d said Mr. Kurukawa, \u201cand she was called\n\u2018The Widow of Sanyo.\u2019\u201d\n\n\n\n\nWelcome to our website \u2013 the ideal destination for book lovers and\nknowledge seekers. With a mission to inspire endlessly, we offer a\nvast collection of books, ranging from classic literary works to\nspecialized publications, self-development books, and children's\nliterature. Each book is a new journey of discovery, expanding\nknowledge and enriching the soul of the reade\nOur website is not just a platform for buying books, but a bridge\nconnecting readers to the timeless values of culture and wisdom. With\nan elegant, user-friendly interface and an intelligent search system,\nwe are committed to providing a quick and convenient shopping\nexperience. Additionally, our special promotions and home delivery\nservices ensure that you save time and fully enjoy the joy of reading.\nLet us accompany you on the journey of exploring knowledge and\npersonal growth!\nebookultra.com\n", "elivery\nservices ensure that you save time and fully enjoy the joy of reading.\nLet us accompany you on the journey of exploring knowledge and\npersonal growth!\nebookultra.com\n", "\nRegister Your Book\nat \nibmpressbooks.com/ibmregister\nUpon registration, we will send you electronic sample chapters from two of our popular\nIBM Press books. In addition, you will be automatically entered into a monthly drawing\nfor a free IBM Press book.\nRegistration also entitles you to:\n\u2022 \nNotices and reminders about author appearances, conferences, and online chats\nwith special guests\n\u2022 \nAccess to supplemental material that may be available\n\u2022 \nAdvance notice ol forthcoming editions\n\u2022 \nRelated book recommendations\n\u2022 \nInformation about special contests and promotions throughout the year\n\u2022 \nChapter excerpts and supplements of forthcoming books\nContact us\nIf you are interested in writing a book or reviewing manuscripts prior to publication,\nplease write to us at:\nEditorial Director, IBM Press\nc/o Pearson Education\n800 East 96\nlh Street\nIndianapolis, IN 46240\ne-mail: IBMPress@pearsoned.com\nVisit us on the Web: ibmpressbooks.com\n\nRelated Books of Interest\nSign up for the monthly IBM Press", "ss\nc/o Pearson Education\n800 East 96\nlh Street\nIndianapolis, IN 46240\ne-mail: IBMPress@pearsoned.com\nVisit us on the Web: ibmpressbooks.com\n\nRelated Books of Interest\nSign up for the monthly IBM Press newsletter at \nibmpressbooks/newsletters\nDITA Best Practices\nBy Laura Bellamy, Michelle Carey, \nand Jenifer Schlotfeldt\nISBN: 0-13-248052-2\nDarwin Information Typing Architecture \n(DITA) is today\u2019s most powerful toolbox for \nconstructing information. By implementing \nDITA, organizations can gain more value \nfrom their technical documentation than \never before. In DITA Best Practices, three \nDITA pioneers offer the \ufb01 rst complete \nroadmap for successful DITA adoption, \nimplementation, and usage. Drawing \non years of experience helping large \norganizations adopt DITA, the authors \nanswer crucial questions the \u201cof\ufb01 cial\u201d DITA \ndocuments ignore. An indispensable re-\nsource for every writer, editor, information \narchitect, manager, or consultant involved \nwith evaluating, deploying, or using D", "l questions the \u201cof\ufb01 cial\u201d DITA \ndocuments ignore. An indispensable re-\nsource for every writer, editor, information \narchitect, manager, or consultant involved \nwith evaluating, deploying, or using DITA.\nThe IBM Style Guide\nConventions for Writers \nand Editors\nby Francis DeRespinis, Peter Hayward, \nJana Jenkins, Amy Laird, Leslie McDonald, \nEric Radzinski\nISBN: 0-13-210130-0\nThe IBM Style Guide distills IBM wis-\ndom for developing superior content: \ninformation that is consistent, clear, \nconcise, and easy to translate. This ex-\npert guide contains practical guidance \non topic-based writing, writing content \nfor different media types, and writing \nfor global audiences and can help \nany organization improve and \nstandardize content across authors, \ndelivery mechanisms, and geographic \nlocations.\nThe IBM Style Guide can help any \norganization or individual create and \nmanage content more effectively. The \nguidelines are especially valuable for \nbusinesses that have not previously \nadopt", "tions.\nThe IBM Style Guide can help any \norganization or individual create and \nmanage content more effectively. The \nguidelines are especially valuable for \nbusinesses that have not previously \nadopted a corporate style guide, for \nanyone who writes or edits for IBM \nas an employee or outside contractor, \nand for anyone who uses modern ap-\nproaches to information architecture.\n\nVisit ibmpressbooks.com \nfor all product information\nRelated Books of Interest\nData Integration \nBlueprint and Modeling\nTechniques for a Scalable and \nSustainable Architecture\nBy Anthony David Giordano\nISBN: 0-13-708493-5\nMaking Data Integration Work: How to \nSystematically Reduce Cost, Improve Quality, \nand Enhance Effectiveness\nThis book presents the solution: a clear, \nconsistent approach to de\ufb01 ning, designing, \nand building data integration components to \nreduce cost, simplify management, enhance \nquality, and improve effectiveness. Leading \nIBM data management expert Tony Giordano \nbrings together best pr", " \nand building data integration components to \nreduce cost, simplify management, enhance \nquality, and improve effectiveness. Leading \nIBM data management expert Tony Giordano \nbrings together best practices for architec-\nture, design, and methodology and shows \nhow to do the disciplined work of getting data \nintegration right.\nMr. Giordano begins with an overview of the \n\u201cpatterns\u201d of data integration, showing how \nto build blueprints that smoothly handle both \noperational and analytic data integration. \nNext, he walks through the entire project \nlifecycle, explaining each phase, activity, task, \nand deliverable through a complete case \nstudy. Finally, he shows how to integrate data \nintegration with other information manage-\nment disciplines, from data governance \nto metadata. The book\u2019s appendices bring \ntogether key principles, detailed models, and \na complete data integration glossary.\nDeveloping Quality \nTechnical Information, \nSecond Edition\nBy Gretchen Hargis, Michelle Carey, A", " appendices bring \ntogether key principles, detailed models, and \na complete data integration glossary.\nDeveloping Quality \nTechnical Information, \nSecond Edition\nBy Gretchen Hargis, Michelle Carey, Ann Kilty \nHernandez, Polly Hughes, Deirdre Longo, \nShannon Rouiller, and Elizabeth Wilde\nISBN: 0-13-147749-8\nDirect from IBM\u2019s own documentation \nexperts, this is the de\ufb01 nitive guide \nto developing outstanding technical \ndocumentation\u2014for the Web and for \nprint. Using extensive before-and-after \nexamples, illustrations, and checklists, \nthe authors show exactly how to create \ndocumentation that\u2019s easy to \ufb01 nd, \nunderstand, and use. This edition includes \nextensive new coverage of topic-based \ninformation, simplifying search and \nretrievability, internationalization, visual \neffectiveness, and much more.\n\nRelated Books of Interest\nSign up for the monthly IBM Press newsletter at \nibmpressbooks/newsletters\nDo It Wrong Quickly\nHow the Web Changes the \nOld Marketing Rules\nMoran\nISBN: 0-13-2255", " much more.\n\nRelated Books of Interest\nSign up for the monthly IBM Press newsletter at \nibmpressbooks/newsletters\nDo It Wrong Quickly\nHow the Web Changes the \nOld Marketing Rules\nMoran\nISBN: 0-13-225596-0\nGet Bold\nUsing Social Media to Create a \nNew Type of Social Business\nCarter\nISBN: 0-13-261831-1\nThe Social Factor\nInnovate, Ignite, and Win \nthrough Mass Collaboration \nand Social Networking\nAzua\nISBN: 0-13-701890-8\nSearch Engine \nMarketing, Inc.\nBy Mike Moran and Bill Hunt\nISBN: 0-13-606868-5\nThe #1 Step-by-Step Guide to Search Mar-\nketing Success...Now Completely Updated \nwith New Techniques, Tools, Best Practices, \nand Value-Packed Bonus DVD!\nIn this book, two world-class experts pres-\nent today\u2019s best practices, step-by-step \ntechniques, and hard-won tips for using \nsearch engine marketing to achieve your \nsales and marketing goals, whatever they are. \nMike Moran and Bill Hunt thoroughly cover \nboth the business and technical aspects \nof contemporary search engine marketing, \nwalk", "eting to achieve your \nsales and marketing goals, whatever they are. \nMike Moran and Bill Hunt thoroughly cover \nboth the business and technical aspects \nof contemporary search engine marketing, \nwalking beginners through all the basics while \nproviding reliable, up-to-the-minute insights \nfor experienced professionals.\nThoroughly updated to fully re\ufb02 ect today\u2019s \nlatest search engine marketing opportunities, \nthis book guides you through pro\ufb01 ting from \nsocial media marketing, site search, advanced \nkeyword tools, hybrid paid search auctions, \nand much more.\nAudience, Relevance, \nand Search\nTargeting Web Audiences with \nRelevant Content\nMathewson, Donatone, Fishel \nISBN: 0-13-700420-6\nMaking the World \nWork Better\nThe Ideas That Shaped a \nCentury and a Company\nManey, Hamm, O\u2019Brien\nISBN: 0-13-275510-6\nListen to the author\u2019s podcast at:\nibmpressbooks.com/podcasts\n\nMultilingual\nNatural\nLanguage\nProcessing\nApplications\n\nThis page intentionally left blank \n\nMultilingual\nNatural\nLanguage\nPr", "-13-275510-6\nListen to the author\u2019s podcast at:\nibmpressbooks.com/podcasts\n\nMultilingual\nNatural\nLanguage\nProcessing\nApplications\n\nThis page intentionally left blank \n\nMultilingual\nNatural\nLanguage\nProcessing\nApplications\nFrom Theory to Practice\nEdited by  Daniel M. Bikel  Imed Zitouni\nIBM Press\nPearson plc\nUpper Saddle River, NJ \u2022 Boston \u2022 Indianapolis \u2022 San Francisco\nNew York \u2022 Toronto \u2022 Montreal \u2022 London \u2022 Munich \u2022 Paris \u2022 Madrid\nCapetown \u2022 Sydney \u2022 Tokyo \u2022 Singapore \u2022 Mexico City\nibmpressbooks.com\n\nThe authors and publisher have taken care in the preparation of this book, but make no expressed or\nimplied warranty of any kind and assume no responsibility for errors or omissions. No liability is assumed\nfor incidental or consequential damages in connection with or arising out of the use of the information or\nprograms contained herein.\nc\u20ddCopyright 2012 by International Business Machines Corporation. All rights reserved.\nNote to U.S. Government Users: Documentation related to restricte", "of the information or\nprograms contained herein.\nc\u20ddCopyright 2012 by International Business Machines Corporation. All rights reserved.\nNote to U.S. Government Users: Documentation related to restricted right. Use, duplication, or disclosure\nis subject to restrictions set forth in GSA ADP Schedule Contract with IBM Corporation.\nIBM Press Program Managers: Steven M. Stansel, Ellice U\ufb00er\nCover design: IBM Corporation\nExecutive Editor: Bernard Goodwin\nMarketing Manager: Stephane Nakib\nPublicist: Heather Fox\nManaging Editor: John Fuller\nDesigner: Alan Clements\nProject Editor: Elizabeth Ryan\nCopy Editor: Carol Lallier\nIndexer: Jack Lewis\nCompositor: LaurelTech\nProofreader: Kelli M. Brooks\nManufacturing Buyer: Dan Uhrig\nPublished by Pearson plc\nPublishing as IBM Press\nIBM Press o\ufb00ers excellent discounts on this book when ordered in quantity for bulk purchases or special\nsales, which may include electronic versions and/or custom covers and content particular to your business,\ntraining goals, m", "scounts on this book when ordered in quantity for bulk purchases or special\nsales, which may include electronic versions and/or custom covers and content particular to your business,\ntraining goals, marketing focus, and branding interests. For more information, please contact\nU.S. Corporate and Government Sales\n1-800-382-3419\ncorpsales@pearsontechgroup.com\nFor sales outside the United States, please contact\nInternational Sales\ninternational@pearson.com\n\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed as\ntrademarks. Where those designations appear in this book, and the publisher was aware of a trademark\nclaim, the designations have been printed with initial capital letters or in all capitals.\nThe following terms are trademarks or registered trademarks of International Business Machines Corpora-\ntion in the United States, other countries, or both: IBM, the IBM press logo, IBM Watson, ThinkPlace,\nWebSphere, and InfoSphere. A current lis", "ed trademarks of International Business Machines Corpora-\ntion in the United States, other countries, or both: IBM, the IBM press logo, IBM Watson, ThinkPlace,\nWebSphere, and InfoSphere. A current list of IBM trademarks is available on the web at \u201ccopyright and\ntrademark information\u201d as www.ibm.com/legal/copytrade.shtml. Microsoft, Windows, Windows NT, and\nthe Windows logo are trademarks of the Microsoft Corporation in the United States, other countries, or\nboth. Java and all Java-based trademarks and logos are trademarks of Oracle and/or its a\ufb03liates. Linux\nis a registered trademark of Linus Torvalds in the United States, other countries, or both. Other company,\nproduct, or service names may be trademarks or service marks of others.\nLibrary of Congress Cataloging-in-Publication Data is on \ufb01le with the Library of Congress.\nAll rights reserved. This publication is protected by copyright, and permission must be obtained from\nthe publisher prior to any prohibited reproduction, storage in ", " \ufb01le with the Library of Congress.\nAll rights reserved. This publication is protected by copyright, and permission must be obtained from\nthe publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any\nform or by any means, electronic, mechanical, photocopying, recording, or likewise. To obtain permission\nto use material from this work, please submit a written request to Pearson Education, Inc., Permissions\nDepartment, One Lake Street, Upper Saddle River, New Jersey 07458, or you may fax your request to (201)\n236-3290.\nISBN-13: 978-0-13-715144-8\nISBN-10:\n0-13-715144-6\nText printed in the United States on recycled paper at Courier in Westford, Massachusetts.\nFirst printing, May 2012\n\nI dedicate this book to\nmy mother Rita, my brother Robert, my sister-in-law Judi,\nmy nephew Wol\ufb01e, and my niece Freya\u2014Bikels all.\nI also dedicate it to Science.\nDMB\nI dedicate this book to\nmy parents Ali and Radhia, who taught me the love of science,\nmy wife Barbara,", "r-in-law Judi,\nmy nephew Wol\ufb01e, and my niece Freya\u2014Bikels all.\nI also dedicate it to Science.\nDMB\nI dedicate this book to\nmy parents Ali and Radhia, who taught me the love of science,\nmy wife Barbara, for her support and encouragement,\nmy kids Nassim and Ines, for the joy they give me.\nI also dedicate it to my grandmother Zohra,\nmy brother Issam, my sister-in-law Chahnez,\nas well as my parents-in-law Alain and Pilar.\nIZ\n\nContents\nPreface\nxxi\nAcknowledgments\nxxv\nAbout the Authors\nxxvii\nPart I\nIn Theory\n1\nChapter 1\nFinding the Structure of Words\n3\n1.1\nWords and Their Components\n4\n1.1.1\nTokens\n4\n1.1.2\nLexemes\n5\n1.1.3\nMorphemes\n5\n1.1.4\nTypology\n7\n1.2\nIssues and Challenges\n8\n1.2.1\nIrregularity\n8\n1.2.2\nAmbiguity\n10\n1.2.3\nProductivity\n13\n1.3\nMorphological Models\n15\n1.3.1\nDictionary Lookup\n15\n1.3.2\nFinite-State Morphology\n16\n1.3.3\nUni\ufb01cation-Based Morphology\n18\n1.3.4\nFunctional Morphology\n19\n1.3.5\nMorphology Induction\n21\n1.4\nSummary\n22\nChapter 2\nFinding the Structure of Documents\n29\n2.1\nIntrod", "2\nFinite-State Morphology\n16\n1.3.3\nUni\ufb01cation-Based Morphology\n18\n1.3.4\nFunctional Morphology\n19\n1.3.5\nMorphology Induction\n21\n1.4\nSummary\n22\nChapter 2\nFinding the Structure of Documents\n29\n2.1\nIntroduction\n29\n2.1.1\nSentence Boundary Detection\n30\n2.1.2\nTopic Boundary Detection\n32\n2.2\nMethods\n33\n2.2.1\nGenerative Sequence Classi\ufb01cation Methods\n34\n2.2.2\nDiscriminative Local Classi\ufb01cation Methods\n36\nxi\n\nxii\nContents\n2.2.3\nDiscriminative Sequence Classi\ufb01cation Methods\n38\n2.2.4\nHybrid Approaches\n39\n2.2.5\nExtensions for Global Modeling for Sentence Segmentation\n40\n2.3\nComplexity of the Approaches\n40\n2.4\nPerformances of the Approaches\n41\n2.5\nFeatures\n41\n2.5.1\nFeatures for Both Text and Speech\n42\n2.5.2\nFeatures Only for Text\n44\n2.5.3\nFeatures for Speech\n45\n2.6\nProcessing Stages\n48\n2.7\nDiscussion\n48\n2.8\nSummary\n49\nChapter 3\nSyntax\n57\n3.1\nParsing Natural Language\n57\n3.2\nTreebanks: A Data-Driven Approach to Syntax\n59\n3.3\nRepresentation of Syntactic Structure\n63\n3.3.1\nSyntax Analysis Using Dependen", "\n2.8\nSummary\n49\nChapter 3\nSyntax\n57\n3.1\nParsing Natural Language\n57\n3.2\nTreebanks: A Data-Driven Approach to Syntax\n59\n3.3\nRepresentation of Syntactic Structure\n63\n3.3.1\nSyntax Analysis Using Dependency Graphs\n63\n3.3.2\nSyntax Analysis Using Phrase Structure Trees\n67\n3.4\nParsing Algorithms\n70\n3.4.1\nShift-Reduce Parsing\n72\n3.4.2\nHypergraphs and Chart Parsing\n74\n3.4.3\nMinimum Spanning Trees and Dependency Parsing\n79\n3.5\nModels for Ambiguity Resolution in Parsing\n80\n3.5.1\nProbabilistic Context-Free Grammars\n80\n3.5.2\nGenerative Models for Parsing\n83\n3.5.3\nDiscriminative Models for Parsing\n84\n3.6\nMultilingual Issues: What Is a Token?\n87\n3.6.1\nTokenization, Case, and Encoding\n87\n3.6.2\nWord Segmentation\n89\n3.6.3\nMorphology\n90\n3.7\nSummary\n92\nChapter 4\nSemantic Parsing\n97\n4.1\nIntroduction\n97\n4.2\nSemantic Interpretation\n98\n4.2.1\nStructural Ambiguity\n99\n4.2.2\nWord Sense\n99\n4.2.3\nEntity and Event Resolution\n100\n4.2.4\nPredicate-Argument Structure\n100\n4.2.5\nMeaning Representation\n101\n4.3\nSystem Parad", "antic Interpretation\n98\n4.2.1\nStructural Ambiguity\n99\n4.2.2\nWord Sense\n99\n4.2.3\nEntity and Event Resolution\n100\n4.2.4\nPredicate-Argument Structure\n100\n4.2.5\nMeaning Representation\n101\n4.3\nSystem Paradigms\n101\n4.4\nWord Sense\n102\n4.4.1\nResources\n104\n\nContents\nxiii\n4.4.2\nSystems\n105\n4.4.3\nSoftware\n116\n4.5\nPredicate-Argument Structure\n118\n4.5.1\nResources\n118\n4.5.2\nSystems\n122\n4.5.3\nSoftware\n147\n4.6\nMeaning Representation\n147\n4.6.1\nResources\n148\n4.6.2\nSystems\n149\n4.6.3\nSoftware\n151\n4.7\nSummary\n152\n4.7.1\nWord Sense Disambiguation\n152\n4.7.2\nPredicate-Argument Structure\n153\n4.7.3\nMeaning Representation\n153\nChapter 5\nLanguage Modeling\n169\n5.1\nIntroduction\n169\n5.2\nn-Gram Models\n170\n5.3\nLanguage Model Evaluation\n170\n5.4\nParameter Estimation\n171\n5.4.1\nMaximum-Likelihood Estimation and Smoothing\n171\n5.4.2\nBayesian Parameter Estimation\n173\n5.4.3\nLarge-Scale Language Models\n174\n5.5\nLanguage Model Adaptation\n176\n5.6\nTypes of Language Models\n178\n5.6.1\nClass-Based Language Models\n178\n5.6.2\nVariable-Leng", "4.2\nBayesian Parameter Estimation\n173\n5.4.3\nLarge-Scale Language Models\n174\n5.5\nLanguage Model Adaptation\n176\n5.6\nTypes of Language Models\n178\n5.6.1\nClass-Based Language Models\n178\n5.6.2\nVariable-Length Language Models\n179\n5.6.3\nDiscriminative Language Models\n179\n5.6.4\nSyntax-Based Language Models\n180\n5.6.5\nMaxEnt Language Models\n181\n5.6.6\nFactored Language Models\n183\n5.6.7\nOther Tree-Based Language Models\n185\n5.6.8\nBayesian Topic-Based Language Models\n186\n5.6.9\nNeural Network Language Models\n187\n5.7\nLanguage-Speci\ufb01c Modeling Problems\n188\n5.7.1\nLanguage Modeling for Morphologically Rich Languages\n189\n5.7.2\nSelection of Subword Units\n191\n5.7.3\nModeling with Morphological Categories\n192\n5.7.4\nLanguages without Word Segmentation\n193\n5.7.5\nSpoken versus Written Languages\n194\n5.8\nMultilingual and Crosslingual Language Modeling\n195\n5.8.1\nMultilingual Language Modeling\n195\n5.8.2\nCrosslingual Language Modeling\n196\n5.9\nSummary\n198\n\nxiv\nContents\nChapter 6\nRecognizing Textual Entailment\n209\n6.1\nI", "nd Crosslingual Language Modeling\n195\n5.8.1\nMultilingual Language Modeling\n195\n5.8.2\nCrosslingual Language Modeling\n196\n5.9\nSummary\n198\n\nxiv\nContents\nChapter 6\nRecognizing Textual Entailment\n209\n6.1\nIntroduction\n209\n6.2\nThe Recognizing Textual Entailment Task\n210\n6.2.1\nProblem De\ufb01nition\n210\n6.2.2\nThe Challenge of RTE\n212\n6.2.3\nEvaluating Textual Entailment System Performance\n213\n6.2.4\nApplications of Textual Entailment Solutions\n214\n6.2.5\nRTE in Other Languages\n218\n6.3\nA Framework for Recognizing Textual Entailment\n219\n6.3.1\nRequirements\n219\n6.3.2\nAnalysis\n220\n6.3.3\nUseful Components\n220\n6.3.4\nA General Model\n224\n6.3.5\nImplementation\n227\n6.3.6\nAlignment\n233\n6.3.7\nInference\n236\n6.3.8\nTraining\n238\n6.4\nCase Studies\n238\n6.4.1\nExtracting Discourse Commitments\n239\n6.4.2\nEdit Distance-Based RTE\n240\n6.4.3\nTransformation-Based Approaches\n241\n6.4.4\nLogical Representation and Inference\n242\n6.4.5\nLearning Alignment Independently of Entailment\n244\n6.4.6\nLeveraging Multiple Alignments for RTE\n245\n6.", "40\n6.4.3\nTransformation-Based Approaches\n241\n6.4.4\nLogical Representation and Inference\n242\n6.4.5\nLearning Alignment Independently of Entailment\n244\n6.4.6\nLeveraging Multiple Alignments for RTE\n245\n6.4.7\nNatural Logic\n245\n6.4.8\nSyntactic Tree Kernels\n246\n6.4.9\nGlobal Similarity Using Limited Dependency Context\n247\n6.4.10 Latent Alignment Inference for RTE\n247\n6.5\nTaking RTE Further\n248\n6.5.1\nImprove Analytics\n248\n6.5.2\nInvent/Tackle New Problems\n249\n6.5.3\nDevelop Knowledge Resources\n249\n6.5.4\nBetter RTE Evaluation\n251\n6.6\nUseful Resources\n252\n6.6.1\nPublications\n252\n6.6.2\nKnowledge Resources\n252\n6.6.3\nNatural Language Processing Packages\n253\n6.7\nSummary\n253\nChapter 7\nMultilingual Sentiment and Subjectivity Analysis\n259\n7.1\nIntroduction\n259\n7.2\nDe\ufb01nitions\n260\n7.3\nSentiment and Subjectivity Analysis on English\n262\n7.3.1\nLexicons\n262\n\nContents\nxv\n7.3.2\nCorpora\n262\n7.3.3\nTools\n263\n7.4\nWord- and Phrase-Level Annotations\n264\n7.4.1\nDictionary-Based\n264\n7.4.2\nCorpus-Based\n267\n7.5\nSentence-Level", "nalysis on English\n262\n7.3.1\nLexicons\n262\n\nContents\nxv\n7.3.2\nCorpora\n262\n7.3.3\nTools\n263\n7.4\nWord- and Phrase-Level Annotations\n264\n7.4.1\nDictionary-Based\n264\n7.4.2\nCorpus-Based\n267\n7.5\nSentence-Level Annotations\n270\n7.5.1\nDictionary-Based\n270\n7.5.2\nCorpus-Based\n271\n7.6\nDocument-Level Annotations\n272\n7.6.1\nDictionary-Based\n272\n7.6.2\nCorpus-Based\n274\n7.7\nWhat Works, What Doesn\u2019t\n274\n7.7.1\nBest Scenario: Manually Annotated Corpora\n274\n7.7.2\nSecond Best: Corpus-Based Cross-Lingual Projections\n275\n7.7.3\nThird Best: Bootstrapping a Lexicon\n275\n7.7.4\nFourth Best: Translating a Lexicon\n276\n7.7.5\nComparing the Alternatives\n276\n7.8\nSummary\n277\nPart II\nIn Practice\n283\nChapter 8\nEntity Detection and Tracking\n285\n8.1\nIntroduction\n285\n8.2\nMention Detection\n287\n8.2.1\nData-Driven Classi\ufb01cation\n287\n8.2.2\nSearch for Mentions\n289\n8.2.3\nMention Detection Features\n291\n8.2.4\nMention Detection Experiments\n294\n8.3\nCoreference Resolution\n296\n8.3.1\nThe Construction of Bell Tree\n297\n8.3.2\nCoreference Models: Li", "Search for Mentions\n289\n8.2.3\nMention Detection Features\n291\n8.2.4\nMention Detection Experiments\n294\n8.3\nCoreference Resolution\n296\n8.3.1\nThe Construction of Bell Tree\n297\n8.3.2\nCoreference Models: Linking and Starting Model\n298\n8.3.3\nA Maximum Entropy Linking Model\n300\n8.3.4\nCoreference Resolution Experiments\n302\n8.4\nSummary\n303\nChapter 9\nRelations and Events\n309\n9.1\nIntroduction\n309\n9.2\nRelations and Events\n310\n9.3\nTypes of Relations\n311\n9.4\nRelation Extraction as Classi\ufb01cation\n312\n9.4.1\nAlgorithm\n312\n9.4.2\nFeatures\n313\n9.4.3\nClassi\ufb01ers\n316\n\nxvi\nContents\n9.5\nOther Approaches to Relation Extraction\n317\n9.5.1\nUnsupervised and Semisupervised Approaches\n317\n9.5.2\nKernel Methods\n319\n9.5.3\nJoint Entity and Relation Detection\n320\n9.6\nEvents\n320\n9.7\nEvent Extraction Approaches\n320\n9.8\nMoving Beyond the Sentence\n323\n9.9\nEvent Matching\n323\n9.10 Future Directions for Event Extraction\n326\n9.11 Summary\n326\nChapter 10\nMachine Translation\n331\n10.1 Machine Translation Today\n331\n10.2 Machine Translat", "ond the Sentence\n323\n9.9\nEvent Matching\n323\n9.10 Future Directions for Event Extraction\n326\n9.11 Summary\n326\nChapter 10\nMachine Translation\n331\n10.1 Machine Translation Today\n331\n10.2 Machine Translation Evaluation\n332\n10.2.1 Human Assessment\n332\n10.2.2 Automatic Evaluation Metrics\n334\n10.2.3 WER, BLEU, METEOR, . . .\n335\n10.3 Word Alignment\n337\n10.3.1 Co-occurrence\n337\n10.3.2 IBM Model 1\n338\n10.3.3 Expectation Maximization\n339\n10.3.4 Alignment Model\n340\n10.3.5 Symmetrization\n340\n10.3.6 Word Alignment as Machine Learning Problem\n341\n10.4 Phrase-Based Models\n343\n10.4.1 Model\n343\n10.4.2 Training\n344\n10.4.3 Decoding\n345\n10.4.4 Cube Pruning\n347\n10.4.5 Log-Linear Models and Parameter Tuning\n348\n10.4.6 Coping with Model Size\n349\n10.5 Tree-Based Models\n350\n10.5.1 Hierarchical Phrase-Based Models\n350\n10.5.2 Chart Decoding\n351\n10.5.3 Syntactic Models\n352\n10.6 Linguistic Challenges\n354\n10.6.1 Lexical Choice\n354\n10.6.2 Morphology\n355\n10.6.3 Word Order\n356\n10.7 Tools and Data Resources\n356\n10.7.1 B", "50\n10.5.2 Chart Decoding\n351\n10.5.3 Syntactic Models\n352\n10.6 Linguistic Challenges\n354\n10.6.1 Lexical Choice\n354\n10.6.2 Morphology\n355\n10.6.3 Word Order\n356\n10.7 Tools and Data Resources\n356\n10.7.1 Basic Tools\n357\n10.7.2 Machine Translation Systems\n357\n10.7.3 Parallel Corpora\n358\n\nContents\nxvii\n10.8 Future Directions\n358\n10.9 Summary\n359\nChapter 11\nMultilingual Information Retrieval\n365\n11.1 Introduction\n366\n11.2 Document Preprocessing\n366\n11.2.1 Document Syntax and Encoding\n367\n11.2.2 Tokenization\n369\n11.2.3 Normalization\n370\n11.2.4 Best Practices for Preprocessing\n371\n11.3 Monolingual Information Retrieval\n372\n11.3.1 Document Representation\n372\n11.3.2 Index Structures\n373\n11.3.3 Retrieval Models\n374\n11.3.4 Query Expansion\n376\n11.3.5 Document A Priori Models\n377\n11.3.6 Best Practices for Model Selection\n377\n11.4 CLIR\n378\n11.4.1 Translation-Based Approaches\n378\n11.4.2 Machine Translation\n380\n11.4.3 Interlingual Document Representations\n381\n11.4.4 Best Practices\n382\n11.5 MLIR\n382\n11.5.", " Model Selection\n377\n11.4 CLIR\n378\n11.4.1 Translation-Based Approaches\n378\n11.4.2 Machine Translation\n380\n11.4.3 Interlingual Document Representations\n381\n11.4.4 Best Practices\n382\n11.5 MLIR\n382\n11.5.1 Language Identi\ufb01cation\n383\n11.5.2 Index Construction for MLIR\n383\n11.5.3 Query Translation\n384\n11.5.4 Aggregation Models\n385\n11.5.5 Best Practices\n385\n11.6 Evaluation in Information Retrieval\n386\n11.6.1 Experimental Setup\n387\n11.6.2 Relevance Assessments\n387\n11.6.3 Evaluation Measures\n388\n11.6.4 Established Data Sets\n389\n11.6.5 Best Practices\n391\n11.7 Tools, Software, and Resources\n391\n11.8 Summary\n393\nChapter 12\nMultilingual Automatic Summarization\n397\n12.1 Introduction\n397\n12.2 Approaches to Summarization\n399\n12.2.1 The Classics\n399\n12.2.2 Graph-Based Approaches\n401\n12.2.3 Learning How to Summarize\n406\n12.2.4 Multilingual Summarization\n409\n\nxviii\nContents\n12.3 Evaluation\n412\n12.3.1 Manual Evaluation Methodologies\n413\n12.3.2 Automated Evaluation Methods\n415\n12.3.3 Recent Development in ", "Summarize\n406\n12.2.4 Multilingual Summarization\n409\n\nxviii\nContents\n12.3 Evaluation\n412\n12.3.1 Manual Evaluation Methodologies\n413\n12.3.2 Automated Evaluation Methods\n415\n12.3.3 Recent Development in Evaluating Summarization Systems\n418\n12.3.4 Automatic Metrics for Multilingual Summarization\n419\n12.4 How to Build a Summarizer\n420\n12.4.1 Ingredients\n422\n12.4.2 Devices\n423\n12.4.3 Instructions\n423\n12.5 Competitions and Datasets\n424\n12.5.1 Competitions\n424\n12.5.2 Data Sets\n425\n12.6 Summary\n426\nChapter 13\nQuestion Answering\n433\n13.1 Introduction and History\n433\n13.2 Architectures\n435\n13.3 Source Acquisition and Preprocessing\n437\n13.4 Question Analysis\n440\n13.5 Search and Candidate Extraction\n443\n13.5.1 Search over Unstructured Sources\n443\n13.5.2 Candidate Extraction from Unstructured Sources\n445\n13.5.3 Candidate Extraction from Structured Sources\n449\n13.6 Answer Scoring\n450\n13.6.1 Overview of Approaches\n450\n13.6.2 Combining Evidence\n452\n13.6.3 Extension to List Questions\n453\n13.7 Crosslingu", "5\n13.5.3 Candidate Extraction from Structured Sources\n449\n13.6 Answer Scoring\n450\n13.6.1 Overview of Approaches\n450\n13.6.2 Combining Evidence\n452\n13.6.3 Extension to List Questions\n453\n13.7 Crosslingual Question Answering\n454\n13.8 A Case Study\n455\n13.9 Evaluation\n460\n13.9.1 Evaluation Tasks\n460\n13.9.2 Judging Answer Correctness\n461\n13.9.3 Performance Metrics\n462\n13.10 Current and Future Challenges\n464\n13.11 Summary and Further Reading\n465\nChapter 14\nDistillation\n475\n14.1 Introduction\n475\n14.2 An Example\n476\n14.3 Relevance and Redundancy\n477\n14.4 The Rosetta Consortium Distillation System\n479\n14.4.1 Document and Corpus Preparation\n480\n14.4.2 Indexing\n483\n14.4.3 Query Answering\n483\n\nContents\nxix\n14.5 Other Distillation Approaches\n488\n14.5.1 System Architectures\n488\n14.5.2 Relevance\n488\n14.5.3 Redundancy\n489\n14.5.4 Multimodal Distillation\n490\n14.5.5 Crosslingual Distillation\n490\n14.6 Evaluation and Metrics\n491\n14.6.1 Evaluation Metrics in the GALE Program\n492\n14.7 Summary\n495\nChapter 15\nS", "3 Redundancy\n489\n14.5.4 Multimodal Distillation\n490\n14.5.5 Crosslingual Distillation\n490\n14.6 Evaluation and Metrics\n491\n14.6.1 Evaluation Metrics in the GALE Program\n492\n14.7 Summary\n495\nChapter 15\nSpoken Dialog Systems\n499\n15.1 Introduction\n499\n15.2 Spoken Dialog Systems\n499\n15.2.1 Speech Recognition and Understanding\n500\n15.2.2 Speech Generation\n503\n15.2.3 Dialog Manager\n504\n15.2.4 Voice User Interface\n505\n15.3 Forms of Dialog\n509\n15.4 Natural Language Call Routing\n510\n15.5 Three Generations of Dialog Applications\n510\n15.6 Continuous Improvement Cycle\n512\n15.7 Transcription and Annotation of Utterances\n513\n15.8 Localization of Spoken Dialog Systems\n513\n15.8.1 Call-Flow Localization\n514\n15.8.2 Prompt Localization\n514\n15.8.3 Localization of Grammars\n516\n15.8.4 The Source Data\n516\n15.8.5 Training\n517\n15.8.6 Test\n519\n15.9 Summary\n520\nChapter 16\nCombining Natural Language Processing Engines\n523\n16.1 Introduction\n523\n16.2 Desired Attributes of Architectures for Aggregating Speech and\nNLP ", "raining\n517\n15.8.6 Test\n519\n15.9 Summary\n520\nChapter 16\nCombining Natural Language Processing Engines\n523\n16.1 Introduction\n523\n16.2 Desired Attributes of Architectures for Aggregating Speech and\nNLP Engines\n524\n16.2.1 Flexible, Distributed Componentization\n524\n16.2.2 Computational E\ufb03ciency\n525\n16.2.3 Data-Manipulation Capabilities\n526\n16.2.4 Robust Processing\n526\n16.3 Architectures for Aggregation\n527\n16.3.1 UIMA\n527\n16.3.2 GATE: General Architecture for Text Engineering\n529\n16.3.3 InfoSphere Streams\n530\n\nxx\nContents\n16.4 Case Studies\n531\n16.4.1 The GALE Interoperability Demo System\n531\n16.4.2 Translingual Automated Language Exploitation\nSystem (TALES)\n538\n16.4.3 Real-Time Translation Services (RTTS)\n538\n16.5 Lessons Learned\n540\n16.5.1 Segmentation Involves a Trade-o\ufb00between Latency and\nAccuracy\n540\n16.5.2 Joint Optimization versus Interoperability\n540\n16.5.3 Data Models Need Usage Conventions\n540\n16.5.4 Challenges of Performance Evaluation\n541\n16.5.5 Ripple-Forward Training of Engine", "ccuracy\n540\n16.5.2 Joint Optimization versus Interoperability\n540\n16.5.3 Data Models Need Usage Conventions\n540\n16.5.4 Challenges of Performance Evaluation\n541\n16.5.5 Ripple-Forward Training of Engines\n541\n16.6 Summary\n542\n16.7 Sample UIMA Code\n542\nIndex\n551\n\nPreface\nAlmost everyone on the planet, it seems, has been touched in some way by advances in\ninformation technology and the proliferation of the Internet. Recently, multimedia infor-\nmation sources have become increasingly popular. Nevertheless, the sheer volume of raw\nnatural language text keeps increasing, and this text is being generated in all the major\nlanguages on Earth. For example, the English Wikipedia reports that 101 language-speci\ufb01c\nWikipedias exist with at least 10,000 articles each. There is therefore a pressing need for\ncountries, companies, and individuals to analyze this massive amount of text, translate it,\nand synthesize and distill it.\nPreviously, to build robust and accurate multilingual natural language proce", "\ncountries, companies, and individuals to analyze this massive amount of text, translate it,\nand synthesize and distill it.\nPreviously, to build robust and accurate multilingual natural language processing (NLP)\napplications, a researcher or developer had to consult several reference books and dozens,\nif not hundreds, of journal and conference papers. Our aim for this book is to provide a\n\u201cone-stop shop\u201d that o\ufb00ers all the requisite background and practical advice for building\nsuch applications. Although it is quite a tall order, we hope that, at a minimum, you \ufb01nd\nthis book a useful resource.\nIn the last two decades, NLP researchers have developed exciting algorithms for process-\ning large amounts of text in many di\ufb00erent languages. By far, the dominant approach has\nbeen to build a statistical model that can learn from examples. In this way, a model can be\nrobust to changes in the type of text and even the language of text on which it operates.\nWith the right design choices, the same ", "tical model that can learn from examples. In this way, a model can be\nrobust to changes in the type of text and even the language of text on which it operates.\nWith the right design choices, the same model can be trained to work in a new domain or\nnew language simply by providing new examples in that domain. This approach also obvi-\nates the need for researchers to lay out, in a painstaking fashion, all the rules that govern\nthe problem at hand and the manner in which those rules must be combined. Rather, a sta-\ntistical system typically allows for researchers to provide an abstract expression of possible\nfeatures of the input, where the relative importance of those features can be learned during\nthe training phase and can be applied to new text during the decoding, or inference, phase.\nThe \ufb01eld of statistical NLP is rapidly changing. Part of the change is due to the \ufb01eld\u2019s\ngrowth. For example, one of the main conferences in the \ufb01eld is that of the Association of\nComputational Linguist", "e \ufb01eld of statistical NLP is rapidly changing. Part of the change is due to the \ufb01eld\u2019s\ngrowth. For example, one of the main conferences in the \ufb01eld is that of the Association of\nComputational Linguistics, where conference attendance has doubled in the last \ufb01ve years.\nAlso, the share of NLP papers in the IEEE speech and language processing conferences and\njournals more than doubled in the last decade; IEEE constitutes one of the world\u2019s largest\nprofessional associations for the advancement of technology. Not only are NLP researchers\nmaking inherent progress on the various subproblems of the \ufb01eld, but NLP continues to ben-\ne\ufb01t (and borrow) heavily from progress in the machine learning community and linguistics\nalike. This book devotes some attention to cutting-edge algorithms and techniques, but its\nprimary purpose is to be a thorough explication of best practices in the \ufb01eld. Furthermore,\nevery chapter describes how the techniques discussed apply in a multilingual setting.\nThis book is ", " but its\nprimary purpose is to be a thorough explication of best practices in the \ufb01eld. Furthermore,\nevery chapter describes how the techniques discussed apply in a multilingual setting.\nThis book is divided into two parts. Part I, In Theory, includes the \ufb01rst seven chapters\nand lays out the various core NLP problems and algorithms to attack those problems. The\nxxi\n\nxxii\nPreface\n\ufb01rst three chapters focus on \ufb01nding structure in language at various levels of granularity.\nChapter 1 introduces the important concept of morphology, the study of the structure of\nwords, and ways to process the diverse array of morphologies present in the world\u2019s lan-\nguages. Chapter 2 discusses the methods by which documents may be decomposed into\nmore manageable parts, such as sentences and larger units related by topic. Finally, in this\ninitial trio of chapters, Chapter 3 investigates the various methods of uncovering a sentence\u2019s\ninternal structure, or syntax. Syntax has long been a dominant area of researc", "y topic. Finally, in this\ninitial trio of chapters, Chapter 3 investigates the various methods of uncovering a sentence\u2019s\ninternal structure, or syntax. Syntax has long been a dominant area of research in linguistics,\nand that dominance has been mirrored in the \ufb01eld of NLP as well. The dominance, in part,\nstems from the fact that the structure of a sentence bears relation to the sentence\u2019s meaning,\nso uncovering syntactic structure can serve as a \ufb01rst step toward a full \u201cunderstanding\u201d of\na sentence.\nFinding a structured meaning representation for a sentence, or for some other unit of\ntext, is often called semantic parsing, which is the concern of Chapter 4. That chapter covers,\ninter alia, a related subproblem that has garnered much attention in recent years known\nas semantic role labeling, which attempts to \ufb01nd the syntactic phrases that constitute the\narguments to some verb or predicate. By identifying and classifying a verb\u2019s arguments,\nwe come one step closer to producing a logica", "g, which attempts to \ufb01nd the syntactic phrases that constitute the\narguments to some verb or predicate. By identifying and classifying a verb\u2019s arguments,\nwe come one step closer to producing a logical form for a sentence, which is one way to\nrepresent a sentence\u2019s meaning in such a way as to be readily processed by machine, using\nthe rich array of tools available from logic that mankind has been developing since ancient\ntimes.\nBut what if we do not want or need the deep syntactico-semantic structure that seman-\ntic parsing would provide? What if our problem is simply to decide which among many\ncandidate sentences is the most likely sentence a human would write or speak? One way to\ndo so would be to develop a model that could score each sentence according to its gram-\nmaticality and pick the sentence with the highest score. The problem of producing a score\nor probability estimate for a sequence of word tokens is known as language modeling and is\nthe subject of Chapter 5.\nRepresenting m", "he sentence with the highest score. The problem of producing a score\nor probability estimate for a sequence of word tokens is known as language modeling and is\nthe subject of Chapter 5.\nRepresenting meaning and judging a sentence\u2019s grammaticality are only two of many\npossible \ufb01rst steps toward processing language. Moving further toward some sense of under-\nstanding, we might wish to have an algorithm make inferences about facts expressed in\na piece of text. For example, we might want to know if a fact mentioned in one sentence\nis entailed by some previous sentence in a document. This sort of inference is known as\nrecognizing textual entailment and is the subject of Chapter 6.\nFinding which facts or statements are entailed by others is clearly important to the\nautomatic understanding of text, but there is also the nature of those statements. Under-\nstanding which statements are subjective and the polarity of the opinion expressed is the\nsubject matter of Chapter 7. Given how often peopl", "t, but there is also the nature of those statements. Under-\nstanding which statements are subjective and the polarity of the opinion expressed is the\nsubject matter of Chapter 7. Given how often people express opinions, this is clearly an\nimportant problem area, all the more so in an age when social networks are fast becoming\nthe dominant form of person-to-person communication on the Internet. This chapter rounds\nout Part I of our book.\nPart II, In Practice, takes the various core areas of NLP described in Part I and explains\nhow to apply them to the diverse array of real-world NLP applications. Engineering is often\nabout trade-o\ufb00s, say, between time and space, and so the chapters in this applied part of our\nbook explore the trade-o\ufb00s in making various algorithmic and design choices when building\na robust, multilingual NLP application.\n\nPreface\nxxiii\nChapter 8 describes ways to identify and classify named entities and other mentions\nof those entities in text, as well as methods to iden", "uilding\na robust, multilingual NLP application.\n\nPreface\nxxiii\nChapter 8 describes ways to identify and classify named entities and other mentions\nof those entities in text, as well as methods to identify when two or more entity mentions\ncorefer. These two problems are typically known as mention detection and coreference res-\nolution; they are two of the core parts of a larger application area known as information\nextraction.\nChapter 9 continues the information extraction discussion, exploring techniques for \ufb01nd-\ning out how two entities are related to each other, known as relation extraction, and identi-\nfying and classifying events, or event extraction. An event, in this case, is when something\nhappens involving multiple entities, and we would like a machine to uncover who the par-\nticipants are and what their roles are. In this way, event extraction is closely related to the\ncore NLP problem of semantic role labeling.\nChapter 10 describes one of the oldest problems in the \ufb01eld, and ", "ants are and what their roles are. In this way, event extraction is closely related to the\ncore NLP problem of semantic role labeling.\nChapter 10 describes one of the oldest problems in the \ufb01eld, and one of the few that\nis an inherently multilingual NLP problem: machine translation, or MT. Automatically\ntranslating from one language to another has long been a holy grail of NLP research, and in\nrecent years the community has developed techniques and can obtain hardware that make\nMT a practical reality, reaping rewards after decades of e\ufb00ort.\nIt is one thing to translate text, but how do we make sense of all the text out there\nin seemingly limitless quantity? Chapters 8 and 9 make some headway in this regard by\nhelping us automatically produce structured records of information in text. Another way to\ntackle the quantity problem is to narrow down the scope by \ufb01nding the few documents,\nor subparts of documents, that are relevant based on a search query. This problem is\nknown as information", "er way to\ntackle the quantity problem is to narrow down the scope by \ufb01nding the few documents,\nor subparts of documents, that are relevant based on a search query. This problem is\nknown as information retrieval and is the subject of Chapter 11. In many ways, com-\nmercial search engines such as Google are large-scale information retrieval systems. Given\nthe popularity of search engines, this is clearly an important NLP problem\u2014all the more\nso given the number of corpora that are not public and therefore searchable by commercial\nengines.\nAnother way we might tackle the sheer quantity of text is by automatically summarizing\nit, which is the topic of Chapter 12. This very di\ufb03cult problem involves either \ufb01nding\nthe sentences, or bits of sentences, that contribute to providing a relevant summary of a\nlarger quantity of text or else ingesting the text summarizing its meaning in some internal\nrepresentation, and then generating the text that constitutes a summary, much as a human\nmight do.\nOft", " of a\nlarger quantity of text or else ingesting the text summarizing its meaning in some internal\nrepresentation, and then generating the text that constitutes a summary, much as a human\nmight do.\nOften, humans would like machines to process text automatically because they have\nquestions they seek to answer. These questions can range from simple, factoid-like questions,\nsuch as \u201cWhen was John F. Kennedy born?\u201d to more complex questions such as \u201cWhat is\nthe largest city in Bavaria, Germany?\u201d Chapter 13 discusses ways to build systems to answer\nthese types of questions automatically.\nWhat if the types of questions we might like to answer are even more complex? Our\nqueries might have multiple answers, such as \u201cName all the foreign heads of state President\nBarack Obama met with in 2010.\u201d These types of queries are handled by a relatively new\nsubdiscipline within NLP known as distillation. In a very real way, distillation combines the\ntechniques of information retrieval with information ext", " types of queries are handled by a relatively new\nsubdiscipline within NLP known as distillation. In a very real way, distillation combines the\ntechniques of information retrieval with information extraction and adds a few of its own.\nIn many cases, we might like to have machines process language in an interactive way,\nmaking use of speech technology that both recognizes and synthesizes speech. Such systems\nare known as dialog systems and are covered in Chapter 15. Due to advances in speech\n\nxxiv\nPreface\nrecognition, dialog management, and speech synthesis, such systems are becoming increas-\ningly practical and are seeing widespread, real-world deployment.\nFinally, we, as NLP researchers and engineers, might like to build systems using diverse\narrays of components developed across the world. This aggregation of processing engines\nis described in Chapter 16. Although it is the \ufb01nal chapter of our book, in some ways it\nrepresents a beginning, not an end, to processing text, for it descri", "d. This aggregation of processing engines\nis described in Chapter 16. Although it is the \ufb01nal chapter of our book, in some ways it\nrepresents a beginning, not an end, to processing text, for it describes how a common\ninfrastructure can be used to produce a combinatorically diverse array of processing\npipelines.\nAs much as we hope this book is self-contained, we also hope that for you it serves as\nthe beginning and not an end. Each chapter has a long list of relevant work upon which it\nis based, allowing you to explore any subtopic in great detail. The large community of NLP\nresearchers is growing throughout the world, and we hope you join us in our exciting e\ufb00orts\nto process text automatically and that you interact with us at universities, at industrial\nresearch labs, at conferences, in blogs, on social networks, and elsewhere. The multilingual\nNLP systems of the future are going to be even more exciting than the ones we have now,\nand we look forward to all your contributions!\n\nAcknowl", "ogs, on social networks, and elsewhere. The multilingual\nNLP systems of the future are going to be even more exciting than the ones we have now,\nand we look forward to all your contributions!\n\nAcknowledgments\nThis book was, from its inception, designed as a highly collaborative e\ufb00ort. We are immensely\ngrateful for the encouraging support obtained from the beginning from IBM Press/Prentice\nHall, especially from Bernard Goodwin and all the others at IBM Press who helped us get\nthis project o\ufb00the ground and see it to completion. A book of this kind would also not have\nbeen possible without the generous time, e\ufb00ort, and technical acumen of our fellow chapter\nauthors, so we owe huge thanks to Otakar Smr\u02c7z, Hyun-Jo You, Dilek Hakkani-T\u00a8ur, Gokhan\nTur, Benoit Favre, Elizabeth Shriberg, Anoop Sarkar, Sameer Pradhan, Katrin Kirchho\ufb00,\nMark Sammons, V.G.Vinod Vydiswaran, Dan Roth, Carmen Banea, Rada Mihalcea, Janyce\nWiebe, Xiaqiang Luo, Philipp Koehn, Philipp Sorg, Philipp Cimiano, Frank Schilder", "ar, Sameer Pradhan, Katrin Kirchho\ufb00,\nMark Sammons, V.G.Vinod Vydiswaran, Dan Roth, Carmen Banea, Rada Mihalcea, Janyce\nWiebe, Xiaqiang Luo, Philipp Koehn, Philipp Sorg, Philipp Cimiano, Frank Schilder, Liang\nZhou, Nico Schlaefer, Jennifer Chu-Carroll, Vittorio Castelli, Radu Florian, Roberto Pierac-\ncini, David Suendermann, John F. Pitrelli, and Burn Lewis. Daniel M. Bikel is also grateful\nto Google Research, especially to Corinna Cortes, for her support during the \ufb01nal stages of\nthis project. Finally, we\u2014Daniel M. Bikel and Imed Zitouni\u2014would like to express our great\nappreciation for the backing of IBM Research, with special thanks to Ellen Yo\ufb00a, without\nwhom this project would not have been possible.\nxxv\n\nThis page intentionally left blank \n\nAbout the Authors\nDaniel M. Bikel (dbikel@google.com) is a senior research scientist\nat Google. He graduated with honors from Harvard in 1993 with a\ndegree in Classics\u2013Ancient Greek and Latin. From 1994 to 1997, he\nworked at BBN on several natur", "gle.com) is a senior research scientist\nat Google. He graduated with honors from Harvard in 1993 with a\ndegree in Classics\u2013Ancient Greek and Latin. From 1994 to 1997, he\nworked at BBN on several natural language processing problems,\nincluding development of the \ufb01rst high-accuracy stochastic name-\n\ufb01nder, for which he holds a patent. He received M.S. and Ph.D.\ndegrees in computer science from the University of Pennsylvania, in\n2000 and 2004 respectively, discovering new properties of statisti-\ncal parsing algorithms. From 2004 through 2010, he was a research\nsta\ufb00member at IBM Research, working on a wide variety of natu-\nral language processing problems, including parsing, semantic role\nlabeling, information extraction, machine translation, and question answering. Dr. Bikel\nhas been a reviewer for the Computational Linguistics journal, and has been on the pro-\ngram committees of the ACL, NAACL, EACL, and EMNLP conferences. He has published\nnumerous peer-reviewed papers in the leading conf", "r for the Computational Linguistics journal, and has been on the pro-\ngram committees of the ACL, NAACL, EACL, and EMNLP conferences. He has published\nnumerous peer-reviewed papers in the leading conferences and journals and has built soft-\nware tools that have seen widespread use in the natural language processing community.\nIn 2008, he won a Best Paper Award (Outstanding Short Paper) at the ACL-08: HLT\nconference. Since 2010, Dr. Bikel has been doing natural language processing and speech\nprocessing research at Google.\nImed Zitouni (izitouni@us.ibm.com) is a senior researcher working\nfor IBM since 2004. He received his M.Sc. and Ph.D. in computer\nscience with honors from University of Nancy, France in 1996 and\n2000 respectively. In 1995, he obtained an MEng degree in com-\nputer science from Ecole Nationale des Sciences de l\u2019Informatique,\na prestigious national computer institute in Tunisia. Before joining\nIBM, he was a principal scientist at a startup company, DIALOCA,\nin 1999 and 20", "rom Ecole Nationale des Sciences de l\u2019Informatique,\na prestigious national computer institute in Tunisia. Before joining\nIBM, he was a principal scientist at a startup company, DIALOCA,\nin 1999 and 2000. He then joined Bell Laboratories Lucent-Alcatel\nbetween 2000 and 2004 as a research sta\ufb00member. His research\ninterests include natural language processing, language modeling,\nspoken dialog systems, speech recognition, and machine learning.\nDr. Zitouni is a member of the IEEE Speech and Language Technical Committee in 2009\u2013\n2011. He is the associate editor of the ACM Transactions on Asian Language Information\nProcessing and the information o\ufb03cer of the Association for Computational Linguistics\n(ACL) Special Interest Group on Computational Approaches to Semitic Languages. He\nis a senior member of IEEE and member of ISCA and ACL. He served on the program\nxxvii\n\nxxviii\nAbout the Authors\ncommittee and as a chair for several peer-review conferences and journals. He holds several\npatents in t", "er of IEEE and member of ISCA and ACL. He served on the program\nxxvii\n\nxxviii\nAbout the Authors\ncommittee and as a chair for several peer-review conferences and journals. He holds several\npatents in the \ufb01eld and authored more than seventy-\ufb01ve papers in peer-review conferences\nand journals.\nCarmen Banea (carmen.banea@gmail.com) is a doctoral student\nin the Department of Computer Science and Engineering, Univer-\nsity of North Texas. She is working in the \ufb01eld of natural language\nprocessing. Her research work focuses primarily on multilingual app-\nroaches to subjectivity and sentiment analysis, where she developed\nboth dictionary- and corpus-based methods that leverage languages\nwith rich resources to create tools and data in other languages.\nCarmen has authored papers in major natural language processing\nconferences, including the Association for Computational Linguis-\ntics, Empirical Methods in Natural Language Processing, and the\nInternational Conference on Computational Linguistics. S", "ge processing\nconferences, including the Association for Computational Linguis-\ntics, Empirical Methods in Natural Language Processing, and the\nInternational Conference on Computational Linguistics. She served\nas a program committee member in numerous large conferences and was also a reviewer for\nthe Computational Linguistics Journal and the Journal of Natural Language Engineering.\nShe cochaired the TextGraphs 2010 Workshop collocated with ACL 2010 and was one of\nthe organizers of the University of North Texas site of the North American Computational\nLinguistics Olympiad in 2009 to 2011.\nVittorio Castelli (vittorio@us.ibm.com) received a Laurea degree\nin electrical engineering from Politecnico di Milano in 1988, an M.S.\nin electrical engineering in 1990, an M.S. in statistics in 1994, and\na Ph.D. in electrical engineering in 1995, with a dissertation on\ninformation theory and statistical classi\ufb01cation. In 1995 he joined\nthe IBM T. J. Watson Research Center. His recent work is in nat-\nu", "a Ph.D. in electrical engineering in 1995, with a dissertation on\ninformation theory and statistical classi\ufb01cation. In 1995 he joined\nthe IBM T. J. Watson Research Center. His recent work is in nat-\nural language processing, speci\ufb01cally in information extraction; he\nhas worked on the DARPA GALE and machine reading projects.\nVittorio previously started the Personal Wizards project, aimed at\ncapturing procedural knowledge from observation of experts per-\nforming a task. He has also done work on foundations of informa-\ntion theory, memory compression, time series prediction and indexing, performance analysis,\nmethods for improving the reliability and serviceability of computer systems, and digital\nlibraries for scienti\ufb01c imagery. From 1996 to 1998 he was coinvestigator of the NASA/CAN\nproject no. NCC5-101. His main research interests include information theory, probability\ntheory, statistics, and statistical pattern recognition. From 1998 to 2005 he was an adjunct\nassistant professor at C", " no. NCC5-101. His main research interests include information theory, probability\ntheory, statistics, and statistical pattern recognition. From 1998 to 2005 he was an adjunct\nassistant professor at Columbia University, teaching information theory and statistical pat-\ntern recognition. He is a member of Sigma Xi, of the IEEE IT Society, and of the Amer-\nican Statistical Association. Vittorio has published papers on natural language processing\ncomputer-assisted instruction, statistical classi\ufb01cation, data compression, image processing,\nmultimedia databases, database mining and multidimensional indexing structures, intelli-\ngent user interfactes, and foundational problems in information theory, and he coedited\nImage Databases: Search and Retrieval of Digital Imagery (Wiley, 2002).\n\nAbout the Authors\nxxix\nJennifer Chu-Carroll (jencc@us.ibm.com) is a research sta\ufb00\nmember in the Semantic Analysis and Integration Department at\nthe IBM T. J. Watson Research Center. Before joining IBM in 2001,", " Authors\nxxix\nJennifer Chu-Carroll (jencc@us.ibm.com) is a research sta\ufb00\nmember in the Semantic Analysis and Integration Department at\nthe IBM T. J. Watson Research Center. Before joining IBM in 2001,\nshe spent \ufb01ve years as a member of technical sta\ufb00at Lucent Tech-\nnologies Bell Labratories. Her research interests include question\nanswering, semantic search, discourse processing, and spoken dialog\nmanagement.\nPhilipp Cimiano (cimiano@cit-ec.uni-bielefeld.de) is professor in\ncomputer science at the University of Bielefeld, Germany. He leads\nthe Semantic Computing Group that is a\ufb03liated with the Cognitive\nInteraction Technology Excellence Center, funded by the Deutsche\nForschungsgemeinschaft in the framework of the excellence initia-\ntive. Philipp Cimiano graduated in computer science (major) and\ncomputational linguistics (minor) from the University of Stuttgart.\nHe obtained his doctoral degree (summa cum laude) from the Uni-\nversity of Karlsruhe. His main research interest lies in the c", ") and\ncomputational linguistics (minor) from the University of Stuttgart.\nHe obtained his doctoral degree (summa cum laude) from the Uni-\nversity of Karlsruhe. His main research interest lies in the combi-\nnation of natural language with semantic technologies. In the last\nseveral years, he has focused on multilingual information access. He\nhas been involved as main investigator in a number of European (Dot.Kom, X-Media, Mon-\nnet) as well as national research projects such as SmartWeb (BMBF) and Multipla (DFG).\nBenoit Favre (benoit.favre@lif.univ-mrs.fr) is an associate profes-\nsor at Aix-Marseille Universit\u00b4e, Marseille, France. He is a researcher\nin the \ufb01eld of natural language understanding. His research inter-\nests are in speech and text understanding with a focus on machine\nlearning approaches. He received his Ph.D. from the University of\nAvignon, France, in 2007 on the topic of automatic speech summa-\nrization. Benoit was a teaching assistant at University of Avignon\nbetween 2003 ", "proaches. He received his Ph.D. from the University of\nAvignon, France, in 2007 on the topic of automatic speech summa-\nrization. Benoit was a teaching assistant at University of Avignon\nbetween 2003 and 2007 and a research engineer at Thales Land &\nJoint Systems, Paris, during the same period. Between 2007 and\n2009, Benoit held a postdoctoral position at the International Com-\nputer Institute (Berkeley, CA) working with the speech group. From\n2009 to 2010, he held a postdoctoral position at University of Le Mans, France. Since\n2010, he is a tenured associate professor at Aix-Marseille Universit\u00b4e, member of Laboratoire\nd\u2019Informatique Fondamentale. Benoit is the coauthor of more than thirty refereed papers in\ninternational conferences and journals. He was a reviewer for major conferences in the \ufb01eld\n(ICASSP, Interspeech, ACL, EMNLP, Coling, NAACL) and for the IEEE Transactions on\nSpeech and Language Processing. He is a member of the International Speech Communica-\ntion Association and ", "n the \ufb01eld\n(ICASSP, Interspeech, ACL, EMNLP, Coling, NAACL) and for the IEEE Transactions on\nSpeech and Language Processing. He is a member of the International Speech Communica-\ntion Association and IEEE.\n\nxxx\nAbout the Authors\nRadu Florian (raduf@us.ibm.com) is the manager of the Statisti-\ncal Content Analytics (Information Extraction) group at IBM. He\nreceived his Ph.D. in 2002 from Johns Hopkins University, when\nhe joined the Multilingual NLP group at IBM. At IBM, he has\nworked on a variety of research projects in the area of informa-\ntion extraction: mention detection, coreference resolution, relation\nextraction, cross-document coreference, and targeted information\nretrieval. Radu led research groups participating in several DARPA\nprograms (GALE Distillation, MRP) and NIST-organized evalua-\ntions (ACE, TAC-KBP) and joint development programs with IBM\npartners for text mining in the medical domain (with Nuance), and\ncontributed to the Watson Jeopardy! project.\nDilek Hakkani-T\u00a8ur (D", "valua-\ntions (ACE, TAC-KBP) and joint development programs with IBM\npartners for text mining in the medical domain (with Nuance), and\ncontributed to the Watson Jeopardy! project.\nDilek Hakkani-T\u00a8ur (Dilek.Hakkani-Tur@microsoft.com) is a prin-\ncipal scientist at Microsoft. Before joining Microsoft, she was with\nthe International Computer Science Institute (ICSI) speech group\n(2006\u20132010) and AT&T Labs\u2013Research (2001\u20132005). She received\nher B.Sc. degree from Middle East Technical University in 1994,\nand M.Sc. and Ph.D. degrees from Bilkent University, department of\ncomputer engineering, in 1996 and 2000 respectively. Her Ph.D. the-\nsis is on statistical language modeling for agglutinative languages.\nShe worked on machine translation at Carnegie Mellon University,\nLanguage Technologies Institute, in 1997 and at Johns Hopkins\nUniversity in 1998. Between 1998 and 1999, Dilek worked on using\nlexical and prosodic information for information extraction from speech at SRI Interna-\ntional. Her re", "te, in 1997 and at Johns Hopkins\nUniversity in 1998. Between 1998 and 1999, Dilek worked on using\nlexical and prosodic information for information extraction from speech at SRI Interna-\ntional. Her research interests include natural language and speech processing, spoken dialog\nsystems, and active and unsupervised learning for language processing. She holds 13 patents\nand has coauthored over one hundred papers in natural language and speech processing. She\nwas an associate editor of IEEE Transactions on Audio, Speech and Language Processing\nbetween 2005 and 2008 and currently serves as an elected member of the IEEE Speech and\nLanguage Technical Committee (2009\u20132012).\nKatrin Kirchho\ufb00(kk2@u.washington.edu) is a research associate\nprofessor in electrical engineering at the University of Washington.\nHer main research interests are automatic speech recognition, natu-\nral language processing, and human\u2013computer interfaces, with par-\nticular emphasis on multilingual applications. She has auth", "ngton.\nHer main research interests are automatic speech recognition, natu-\nral language processing, and human\u2013computer interfaces, with par-\nticular emphasis on multilingual applications. She has authored over\nseventy peer-reviewed publications and is coeditor of Multilingual\nSpeech Processing. Katrin currently serves as a member of the IEEE\nSpeech Technical Committee and on the editorial boards of Com-\nputer, Speech and Language and Speech Communication.\n\nAbout the Authors\nxxxi\nPhilipp Koehn (pkoehn@inf.ed.ac.uk) is a reader at the University\nof Edinburgh. He received his Ph.D. from the University of South-\nern California, where he was a research assistant at the Information\nSciences Institute from 1997 to 2003. He was a postdoctoral research\nassociate at the Massachusetts Institute of Technology in 2004 and\njoined the University of Edinburgh as a lecturer in 2005. His research\ncenters on statistical machine translation, but he has also worked\non speech, text classi\ufb01cation, and inform", "chnology in 2004 and\njoined the University of Edinburgh as a lecturer in 2005. His research\ncenters on statistical machine translation, but he has also worked\non speech, text classi\ufb01cation, and information extraction. His major\ncontribution to the machine translation community are the prepara-\ntion and release of the Europarl corpus as well as the Pharaoh and\nMoses decoder. He is president of the ACL Special Interest Group on\nMachine Translation and author of Statistical Machine Translation (Cambridge University\nPress, 2010).\nBurn L. Lewis (burn@us.ibm.com) is a member of the computer\nscience department at the IBM Thomas J. Watson Research Center.\nHe received B.E. and M.E. degrees in electrical engineering from the\nUniversity of Auckland in 1967 and 1968, respectively, and a Ph.D.\nin electrical engineering and computer science from the University\nof California\u2013Berkeley in 1974. He subsequently joined IBM at the\nT. J. Watson Research Center, where he has worked on speech recog-\nnition a", "rical engineering and computer science from the University\nof California\u2013Berkeley in 1974. He subsequently joined IBM at the\nT. J. Watson Research Center, where he has worked on speech recog-\nnition and unstructured information management.\nXiaqiang Luo (xiaoluo@us.ibm.com) is a research sta\ufb00member\nat IBM T. J. Watson Research Center. He has extensive experi-\nences in human language technology, including speech recognition,\nspoken dialog systems, and natural language processing. He is a\nmajor contributor to IBM\u2019s success in many government-sponsored\nprojects in the area of speech and language technology. He received\nthe prestigious IBM Outstanding Technical Achievement Award in\n2007, IBM ThinkPlace Bravo Award in 2006, and numerous inven-\ntion achievement awards. Dr. Luo received his Ph.D. and M.S. in\nelectrical engineering from Johns Hopkins University in 1999 and\n1995, respectively, and B.A. in electrical engineering from Univer-\nsity of Science and Technology of China in 1990. Dr. Lu", "and M.S. in\nelectrical engineering from Johns Hopkins University in 1999 and\n1995, respectively, and B.A. in electrical engineering from Univer-\nsity of Science and Technology of China in 1990. Dr. Luo is a member of the Association of\nComputational Linguistics and has served as program committee member for major tech-\nnical conferences in the area of human language and arti\ufb01cial intelligence. He is a board\nmember of the Chinese Association for Science and Technology (Greater New York Chap-\nter). He served as an associate editor for ACM Transactions on Asian Language Information\nProcessing (TALIP) from 2007 to 2010.\n\nxxxii\nAbout the Authors\nRada Mihalcea (rada@cs.unt.edu) is associate professor in the\nDepartment of Computer Science and Engineering, University of\nNorth Texas. Her research interests are in computational linguis-\ntics, with a focus on lexical semantics, graph-based algorithms for\nnatural language processing, and multilingual natural language pro-\ncessing. She is currently", "nterests are in computational linguis-\ntics, with a focus on lexical semantics, graph-based algorithms for\nnatural language processing, and multilingual natural language pro-\ncessing. She is currently involved in a number of research projects,\nincluding word sense disambiguation, monolingual and crosslingual\nsemantic similarity, automatic keyword extraction and text summa-\nrization, emotion and sentiment analysis, and computational humor.\nRada serves or has served on the editorial boards of the Journals\nof Computational Linguistics, Language Resources and Evaluations,\nNatural Language Engineering, and Research in Language in Computation. Her research has\nbeen funded by the National Science Foundation, Google, the National Endowment for the\nHumanities, and the State of Texas. She is the recipient of a National Science Foundation\nCAREER award (2008) and a Presidential Early Career Award for Scientists and Engineers\n(PECASE, 2009).\nRoberto Pieraccini (www.robertopieraccini.com) is chief t", "ipient of a National Science Foundation\nCAREER award (2008) and a Presidential Early Career Award for Scientists and Engineers\n(PECASE, 2009).\nRoberto Pieraccini (www.robertopieraccini.com) is chief technol-\nogy o\ufb03cer of SpeechCycle Inc. Roberto graduated in electrical engi-\nneering at the University of Pisa, Italy, in 1980. In 1981 he started\nworking as a speech recognition researcher at CSELT, the research\ninstitution of the Italian telephone operating company. In 1990 he\njoined Bell Laboratories (Murray Hill, NJ) as a member of technical\nsta\ufb00where he was involved in speech recognition and spoken lan-\nguage understanding research. He then joined AT&T Labs in 1996,\nwhere he started working on spoken dialog research. In 1999 he was\ndirector of R&D for SpeechWorks International. In 2003 he joined\nIBM T. J. Watson Research where he managed the Advanced Con-\nversational Interaction Technology department, and then joined SpeechCycle in 2005 as their\nCTO. Roberto Pieraccini is the author of", "joined\nIBM T. J. Watson Research where he managed the Advanced Con-\nversational Interaction Technology department, and then joined SpeechCycle in 2005 as their\nCTO. Roberto Pieraccini is the author of more than one hundred twenty papers and articles\non speech recognition, language modeling, character recognition, language understanding,\nand automatic spoken dialog management. He is an ISCA and IEEE Fellow, a member of the\neditorial board of the IEEE Signal Processing Magazine and of the International Journal\nof Speech Technology. He is also a member of the Applied Voice Input Output Society and\nSpeech Technology Consortium boards.\n\nAbout the Authors\nxxxiii\nJohn\nF.\nPitrelli (pitrelli@us.ibm.com) is a member of the\nMultilingual Natural Language Processing department at the IBM\nT. J. Watson Research Center in Yorktown Heights, New York. He\nreceived S.B., S.M., and Ph.D. degrees in electrical engineering and\ncomputer science from the Massachusetts Institute of Technology\nin 1983, 1985, and", "esearch Center in Yorktown Heights, New York. He\nreceived S.B., S.M., and Ph.D. degrees in electrical engineering and\ncomputer science from the Massachusetts Institute of Technology\nin 1983, 1985, and 1990 respectively, with graduate work in speech\nrecognition and synthesis. Before his current position, he worked in\nthe Speech Technology Group at NYNEX Science & Technology,\nInc., in White Plains, New York; was a member of the IBM Pen\nTechnologies Group; and worked on speech synthesis and prosody\nin the Human Language Technologies group at Watson. John\u2019s\nresearch interests include natural language processing, speech synthesis, speech recognition,\nhandwriting recognition, statistical language modeling, prosody, unstructured information\nmanagement, and con\ufb01dence modeling for recognition. He has published forty papers and\nholds four patents.\nSameer Pradhan (sameer.pradhan@Colorado.edu) is a scientist at\nBBN Technologies in Cambridge, Massachusetts. He is the author\nof a number of widely ci", "as published forty papers and\nholds four patents.\nSameer Pradhan (sameer.pradhan@Colorado.edu) is a scientist at\nBBN Technologies in Cambridge, Massachusetts. He is the author\nof a number of widely cited articles and chapters in the \ufb01eld of com-\nputational semantics. He is currently creating the next generation\nof semantic analysis engines and their applications, through algo-\nrithmic innovation, wide distribution of research tools such as Au-\ntomatic Statistical SEmantic Role Tagger (ASSERT), and through\nthe generation of rich, multilayer, multilingual, integrated resources,\nsuch as OntoNotes, that serve as a platform. Eventually these mod-\nels of semantics should replace the currently impoverished, mostly\nword-based models, prevalent in most application domains, and help\ntake the area of language understanding to a new level of richness. Sameer received his Ph.D.\nfrom the University of Colorado in 2005, and since then has been working at BBN Technolo-\ngies developing the OntoNotes co", " language understanding to a new level of richness. Sameer received his Ph.D.\nfrom the University of Colorado in 2005, and since then has been working at BBN Technolo-\ngies developing the OntoNotes corpora as part of the DARPA Global Autonomus Language\nExploitation program. He is a member of ACL, and is a founding member of ACL\u2019s Special\nInterest Group for Annotation, promoting innovation in the area of annotation. He has reg-\nularly been on the program committees of various natural language processing conferences\nand workshops such as ACL, HLT, EMNLP, CoNLL, COLING, LREC, and LAW. He is\nalso an accomplished chef.\n\nxxxiv\nAbout the Authors\nDan Roth (danr@illinois.edu) is a professor in the department of\ncomputer science and the Beckman Institute at the University of\nIllinois at Urbana-Champaign. He is a Fellow of AAAI, a Univer-\nsity of Illinois Scholar, and holds faculty positions at the statistics\nand linguistics departments and at the Graduate School of Library\nand Information Scienc", "aign. He is a Fellow of AAAI, a Univer-\nsity of Illinois Scholar, and holds faculty positions at the statistics\nand linguistics departments and at the Graduate School of Library\nand Information Science. Professor Roth\u2019s research spans theoretical\nwork in machine learning and intelligent reasoning with a speci\ufb01c\nfocus on learning and inference in natural language processing and\nintelligent access to textual information. He has published over two\nhundred papers in these areas and his papers have received mul-\ntiple awards. He has developed advanced machine learning-based\ntools for natural language applications that are being used widely by the research commu-\nnity, including an award-winning semantic parser. He was the program chair of AAAI\u201911,\nCoNLL\u201902, and ACL\u201903, and is or has been on the editorial board of several journals in his\nresearch areas. He is currently an associate editor for the Journal of Arti\ufb01cial Intelligence\nResearch and the Machine Learning Journal. Professor Roth got ", " editorial board of several journals in his\nresearch areas. He is currently an associate editor for the Journal of Arti\ufb01cial Intelligence\nResearch and the Machine Learning Journal. Professor Roth got his B.A. summa cum laude\nin mathematics from the Technion, Israel, and his Ph.D. in computer science from Harvard\nUniversity.\nMark Sammons (mssammon@illinois.edu) is a principal research\nscientist working with the Cognitive Computation Group at the Uni-\nversity of Illinois at Urbana-Champaign. His primary interests are\nin natural language processing and machine learning, with a focus\non integrating diverse information sources in the context of textual\nentailment. His work has focused on developing a textual entail-\nment framework that can easily incorporate new resources, design-\ning appropriate inference procedures for recognizing entailment, and\nidentifying and developing automated approaches to recognize and\nrepresent implicit content in natural language text. Mark received\nhis M.Sc. in", "te inference procedures for recognizing entailment, and\nidentifying and developing automated approaches to recognize and\nrepresent implicit content in natural language text. Mark received\nhis M.Sc. in computer science from the University of Illinois in 2004\nand his Ph.D. in mechanical engineering from the University of Leeds, England, in 2000.\nAnoop Sarkar (www.cs.sfu.ca/\u223canoop) is an associate professor\nof computing science at Simon Fraser University in British\nColumbia, Canada, where he codirects the Natural Language Lab-\noratory (http://natlang.cs.sfu.ca). He received his Ph.D. from the\nDepartment of Computer and Information Sciences at the University\nof Pennsylvania under Professor Aravind Joshi for his work on semi-\nsupervised statistical parsing and parsing for tree-adjoining gram-\nmars. Anoop\u2019s current research is focused on statistical parsing and\nmachine translation (exploiting syntax or morphology, or both). His\ninterests also include formal language theory and stochastic gra", "ars. Anoop\u2019s current research is focused on statistical parsing and\nmachine translation (exploiting syntax or morphology, or both). His\ninterests also include formal language theory and stochastic gram-\nmars, in particular tree automata and tree-adjoining grammars.\n\nAbout the Authors\nxxxv\nFrank Schilder (frank.schilder@thomsonreuters.com) is a lead\nresearch scientist at the Research & Development department of\nThomson Reuters. He joined Thomson Reuters in 2004, where he\nhas been doing applied research on summarization technologies and\ninformation extraction systems. His summarization work has been\nimplemented as the snippet generator for search results of West-\nLawNext, the new legal research system produced by Thomson\nReuters. His current research activities involve the participation in\ndi\ufb00erent research competitions such as the Text Analysis Conference\ncarried out by the National Institute of Standards and Technology.\nHe obtained a Ph.D. in cognitive science from the University of\nEd", "i\ufb00erent research competitions such as the Text Analysis Conference\ncarried out by the National Institute of Standards and Technology.\nHe obtained a Ph.D. in cognitive science from the University of\nEdinburgh, Scotland, in 1997. From 1997 to 2003, he was employed by the Department for\nInformatics at the University of Hamburg, Germany, \ufb01rst as a postdoctoral researcher and\nlater as an assistant professor. Frank has authored several journal articles and book chapters,\nincluding \u201cNatural Language Processing: Overview\u201d from the Encyclopedia of Language and\nLinguistics (Elsevier, 2006), coauthored with Peter Jackson, the chief scientist of Thomson\nReuters. In 2011, he jointly won the Thomson Reuters Innovation challenge. He serves as\nreviewer for journals in computational linguistics and as program committee member of\nvarious conferences organized by the Association of Computational Linguistics.\nNico Schlaefer (nico@cs.cmu.edu) is a Ph.D. candidate in the\nSchool of Computer Science at Carneg", "ram committee member of\nvarious conferences organized by the Association of Computational Linguistics.\nNico Schlaefer (nico@cs.cmu.edu) is a Ph.D. candidate in the\nSchool of Computer Science at Carnegie Mellon University and an\nIBM Ph.D. Fellow. His research focus is the application of machine\nlearning techniques to natural language processing tasks. Schlae-\nfer developed algorithms that enable question-answering systems to\n\ufb01nd correct answers, even if the original information sources contain\nlittle relevant content, and a \ufb02exible architecture that supports the\nintegration of such algorithms. Schlaefer is the primary author of\nOpenEphyra, one of the most widely used open-source question-\nanswering systems. Nico also contributed a statistical source\nexpansion approach to Watson, the computer that won against\nhuman champions in the Jeopardy! quiz show. His approach automatically extends knowl-\nedge sources with related content from the Web and other large text corpora, making it\neasier f", "r that won against\nhuman champions in the Jeopardy! quiz show. His approach automatically extends knowl-\nedge sources with related content from the Web and other large text corpora, making it\neasier for Watson to \ufb01nd answers and supporting evidence.\n\nxxxvi\nAbout the Authors\nElizabeth Shriberg (elshribe@microsoft.com) is currently a prin-\ncipal scientist at Microsoft; previously she was at SRI International\n(Menlo Park, CA). She is also a\ufb03liated with the International\nComputer Science Institute (Berkeley, CA) and CASL (University\nof Maryland). She received a B.A. from Harvard (1987) and a\nPh.D. from the University of California\u2013Berkeley (1994). Elizabeth\u2019s\nmain interest is in modeling spontaneous speech using both lexi-\ncal and prosodic information. Her work aims to combine linguistic\nknowledge with corpora and techniques from automatic speech and\nspeaker recognition to advance both scienti\ufb01c understanding and\ntechnology. She has published roughly two hundred papers in speech\nscience an", "edge with corpora and techniques from automatic speech and\nspeaker recognition to advance both scienti\ufb01c understanding and\ntechnology. She has published roughly two hundred papers in speech\nscience and technology and has served as associate editor of language and speech, on the\nboards of Speech Communication and Computational Linguistics, on a range of conference\nand workshop boards, on the ISCA Advisory Council, and on the ICSLP Permanent Coun-\ncil. She has organized workshops and served on boards for the National Science Foundation,\nthe European Commission, NWO (Netherlands), and has reviewed for an interdisciplinary\nrange of conferences, workshops, and journals (e.g., IEEE Transactions on Speech and\nAudio Processing, Journal of the Acoustical Society of America, Nature, Journal of Pho-\nnetics, Computer Speech and Language, Journal of Memory and Language, Memory and\nCognition, Discourse Processes). In 2009 she received the ISCA Fellow Award. In 2010 she\nbecame a Fellow of SRI.\nOtakar", "netics, Computer Speech and Language, Journal of Memory and Language, Memory and\nCognition, Discourse Processes). In 2009 she received the ISCA Fellow Award. In 2010 she\nbecame a Fellow of SRI.\nOtakar Smr\u02c7z (otakar.smrz@cmu.edu) is a postdoctoral research\nassociate at Carnegie Mellon University in Qatar. He focuses on\nmethods of learning from comparable corpora to improve statisti-\ncal machine translation from and into Arabic. Otakar completed his\ndoctoral studies in mathematical linguistics at Charles University in\nPrague. He designed and implemented the ElixirFM computational\nmodel of Arabic morphology using functional programming and has\ndeveloped other open source software for natural language process-\ning. He has been the principal investigator of the Prague Arabic\nDependency Treebank. Otakar used to work as a research scientist\nat IBM Czech Republic, where he explored unsupervised semantic\nparsing as well as acoustic modeling for multiple languages. Otakar is a cofounder of the\nD", ". Otakar used to work as a research scientist\nat IBM Czech Republic, where he explored unsupervised semantic\nparsing as well as acoustic modeling for multiple languages. Otakar is a cofounder of the\nD\u02c7z\u00b4am-e D\u02c7zam Language Institute in Prague.\n\nAbout the Authors\nxxxvii\nPhilipp Sorg (philipp.sorg@kit.edu) is a Ph.D. student at the\nKarlsruhe Institute of Technology, Germany. He has a researcher\nposition at the Institute of Applied Informatics and Formal De-\nscription Methods. Philipp graduated in computer science at the\nUniversity of Karlsruhe. His main research interest lies in multilin-\ngual information retrieval. His special focus is the exploitation of\nsocial semantics in the context of the Web 2.0. He has been involved\nin the European research project Active, as well as in the national\nresearch project Multipla (DFG).\nDavid Suendermann (david@speechcycle.com) is the principal\nspeech scientist at SpeechCycle Labs (New York). Dr. Suendermann\nhas been working on various \ufb01elds of speech", "research project Multipla (DFG).\nDavid Suendermann (david@speechcycle.com) is the principal\nspeech scientist at SpeechCycle Labs (New York). Dr. Suendermann\nhas been working on various \ufb01elds of speech technology research for\nthe last ten years. He worked at multiple industrial and academic\ninstitutions including Siemens (Munich), Columbia University (New\nYork), University of Southern California (Los Angeles), Universitat\nPolit`ecnica de Catalunya (Barcelona), and Rheinisch Westf\u00a8alische\nTechnische Hochschule (Aachen, Germany). He has authored more\nthan sixty publications and patents, including a book and \ufb01ve book\nchapters, and holds a Ph.D. from the Bundeswehr University in\nMunich.\nGokhan Tur (gokhan.tur@ieee.org) is currently with Microsoft\nworking as a principal scientist. He received his B.S., M.S., and\nPh.D. from the Department of Computer Science, Bilkent Uni-\nversity, Turkey in 1994, 1996, and 2000 respectively. Between\n1997 and 1999, Tur visited the Center for Machine Translatio", "s B.S., M.S., and\nPh.D. from the Department of Computer Science, Bilkent Uni-\nversity, Turkey in 1994, 1996, and 2000 respectively. Between\n1997 and 1999, Tur visited the Center for Machine Translation of\nCarnegie Mellon University, then the Department of Computer Sci-\nence of Johns Hopkins University, and then the Speech Technol-\nogy and Research Lab of SRI International. He worked at AT&T\nLabs\u2013Research from 2001 to 2006 and at the Speech Technology\nand Research Lab of SRI International from 2006 to 2010. His\nresearch interests include spoken language understanding, speech\nand language processing, machine learning, and information retrieval and extraction. Tur\nhas coauthored more than one hundred papers published in refereed journals or books and\npresented at international conferences. He is the editor of Spoken Language Understanding:\nSystems for Extracting Semantic Information from Speech (Wiley, 2011). Dr. Tur is a se-\nnior member of IEEE, ACL, and ISCA, was a member of IEEE Signal", " is the editor of Spoken Language Understanding:\nSystems for Extracting Semantic Information from Speech (Wiley, 2011). Dr. Tur is a se-\nnior member of IEEE, ACL, and ISCA, was a member of IEEE Signal Processing Society\n(SPS), Speech and Language Technical Committee (SLTC) for 2006\u20132008, and is currently\nan associate editor for IEEE Transactions on Audio, Speech, and Language Processing.\n\nxxxviii\nAbout the Authors\nV. G. Vinod Vydiswaran (vgvinodv@illinois.edu) is currently\na Ph.D. student in the Department of Computer Science at the\nUniversity of Illinois, Urbana-Champaign. His thesis is on modeling\ninformation trustworthiness on the Web and is advised by profes-\nsors ChengXiang Zhai and Dan Roth. His research interests include\ntext informatics, natural language processing, machine learning, and\ninformation extraction. V. G. Vinod\u2019s work has included developing\na textual entailment system and applying textual entailment to rela-\ntion extraction and information retrieval. He received hi", "g, and\ninformation extraction. V. G. Vinod\u2019s work has included developing\na textual entailment system and applying textual entailment to rela-\ntion extraction and information retrieval. He received his M.S. from\nIndian Institute of Technology-Bombay in 2004, where he worked on\nconditional models for information extraction with Professor Sunita\nSarawagi. Later, he worked at Yahoo! Research & Development Center at Bangalore, India,\non scaling information extraction technologies over the Web.\nJanyce Wiebe (wiebe@cs.pitt.edu) is a professor of computer sci-\nence and codirector of the Intelligent Systems Program at the Uni-\nversity of Pittsburgh. Her research with students and colleagues has\nbeen in discourse processing, pragmatics, word-sense disambigua-\ntion, and probabilistic classi\ufb01cation in natural language processing.\nA major concentration of her research is subjectivity analysis, rec-\nognizing and interpretating expressions of opinions and sentiments\nin text, to support natural langu", "n natural language processing.\nA major concentration of her research is subjectivity analysis, rec-\nognizing and interpretating expressions of opinions and sentiments\nin text, to support natural language processing applications such as\nquestion answering, information extraction, text categorization, and\nsummarization. Janyce\u2019s current and past professional roles include\nACL program cochair, NAACL program chair, NAACL executive\nboard member, computational linguistics, and language resources and evaluation, editorial\nboard member, AAAI workshop cochair, ACM special interest group on arti\ufb01cial intelligence\n(SIGART) vice-chair, and ACM-SIGART/AAAI doctoral consortium chair.\nHyun-Jo You (youhyunjo@gmail.com) is currently a lecturer in the\nDepartment of Linguistics, Seoul National University. He received his\nPh.D. from Seoul National University. His research interests include\nquantitative linguistics, statistical language modeling, and comput-\nerized corpus analysis. He is especially interes", " received his\nPh.D. from Seoul National University. His research interests include\nquantitative linguistics, statistical language modeling, and comput-\nerized corpus analysis. He is especially interested in studying the\nmorpho-syntactic and discourse structure in morphologically rich,\nfree word order languages such as Korean, Czech, and Russian.\n\nAbout the Authors\nxxxix\nLiang Zhou (liangz@isi.edu) is a research scientist at Thomson\nReuters Corporation. She has extensive knowledge in natural lan-\nguage processing, including sentiment analysis, automated text sum-\nmarization, text understanding, information extraction, question\nanswering, and information distillation. During her graduate stud-\nies at the Information Sciences Institute, she was actively involved\nin various government-sponsored projects, such as NIST Document\nUnderstanding conferences and DARPA Global Autonomous Lan-\nguage Exploitation. Dr. Zhou received her Ph.D. from the Univer-\nsity of Southern California in 2006, M.S. ", "d projects, such as NIST Document\nUnderstanding conferences and DARPA Global Autonomous Lan-\nguage Exploitation. Dr. Zhou received her Ph.D. from the Univer-\nsity of Southern California in 2006, M.S. from Stanford University\nin 2001, and B.S. from the University of Tennessee in 1999, all in\ncomputer science.\n\nThis page intentionally left blank \n\nChapter 1\nFinding the Structure of Words\nOtakar Smr\u02c7z and Hyun-Jo You\nHuman language is a complicated thing. We use it to express our thoughts, and through\nlanguage, we receive information and infer its meaning. Linguistic expressions are not unor-\nganized, though. They show structure of di\ufb00erent kinds and complexity and consist of more\nelementary components whose co-occurrence in context re\ufb01nes the notions they refer to in\nisolation and implies further meaningful relations between them.\nTrying to understand language en bloc is not a viable approach. Linguists have developed\nwhole disciplines that look at language from di\ufb00erent perspectives and", "rther meaningful relations between them.\nTrying to understand language en bloc is not a viable approach. Linguists have developed\nwhole disciplines that look at language from di\ufb00erent perspectives and at di\ufb00erent levels of\ndetail. The point of morphology, for instance, is to study the variable forms and functions\nof words, while syntax is concerned with the arrangement of words into phrases, clauses,\nand sentences. Word structure constraints due to pronunciation are described by phonology,\nwhereas conventions for writing constitute the orthography of a language. The meaning of\na linguistic expression is its semantics, and etymology and lexicology cover especially the\nevolution of words and explain the semantic, morphological, and other links among them.\nWords are perhaps the most intuitive units of language, yet they are in general tricky to\nde\ufb01ne. Knowing how to work with them allows, in particular, the development of syntactic\nand semantic abstractions and simpli\ufb01es other advanced vi", " units of language, yet they are in general tricky to\nde\ufb01ne. Knowing how to work with them allows, in particular, the development of syntactic\nand semantic abstractions and simpli\ufb01es other advanced views on language. Morphology is\nan essential part of language processing, and in multilingual settings, it becomes even more\nimportant.\nIn this chapter, we explore how to identify words of distinct types in human languages,\nand how the internal structure of words can be modeled in connection with the grammatical\nproperties and lexical concepts the words should represent. The discovery of word structure\nis morphological parsing.\nHow di\ufb03cult can such tasks be? It depends. In many languages, words are delimited in\nthe orthography by whitespace and punctuation. But in many other languages, the writing\nsystem leaves it up to the reader to tell words apart or determine their exact phonologi-\ncal forms. Some languages use words whose form need not change much with the varying\ncontext; others are h", "ing\nsystem leaves it up to the reader to tell words apart or determine their exact phonologi-\ncal forms. Some languages use words whose form need not change much with the varying\ncontext; others are highly sensitive about the choice of word forms according to particular\nsyntactic and semantic constraints and restrictions.\n3\n\n4\nChapter 1\nFinding the Structure of Words\n1.1\nWords and Their Components\nWords are de\ufb01ned in most languages as the smallest linguistic units that can form a com-\nplete utterance by themselves. The minimal parts of words that deliver aspects of meaning\nto them are called morphemes. Depending on the means of communication, morphemes are\nspelled out via graphemes\u2014symbols of writing such as letters or characters\u2014or are realized\nthrough phonemes, the distinctive units of sound in spoken language.1 It is not always easy\nto decide and agree on the precise boundaries discriminating words from morphemes and\nfrom phrases [1, 2].\n1.1.1\nTokens\nSuppose, for a moment, that word", "ound in spoken language.1 It is not always easy\nto decide and agree on the precise boundaries discriminating words from morphemes and\nfrom phrases [1, 2].\n1.1.1\nTokens\nSuppose, for a moment, that words in English are delimited only by whitespace and punc-\ntuation [3], and consider Example 1\u20131:\nExample 1\u20131: Will you read the newspaper? Will you read it? I won\u2019t read it.\nIf we confront our assumption with insights from etymology and syntax, we notice two\nwords here: newspaper and won\u2019t. Being a compound word, newspaper has an interesting\nderivational structure. We might wish to describe it in more detail, once there is a lexicon or\nsome other linguistic evidence on which to build the possible hypotheses about the origins of\nthe word. In writing, newspaper and the associated concept is distinguished from the isolated\nnews and paper. In speech, however, the distinction is far from clear, and identi\ufb01cation of\nwords becomes an issue of its own.\nFor reasons of generality, linguists prefer to ", "guished from the isolated\nnews and paper. In speech, however, the distinction is far from clear, and identi\ufb01cation of\nwords becomes an issue of its own.\nFor reasons of generality, linguists prefer to analyze won\u2019t as two syntactic words, or\ntokens, each of which has its independent role and can be reverted to its normalized form.\nThe structure of won\u2019t could be parsed as will followed by not. In English, this kind of\ntokenization and normalization may apply to just a limited set of cases, but in other\nlanguages, these phenomena have to be treated in a less trivial manner.\nIn Arabic or Hebrew [4], certain tokens are concatenated in writing with the preceding or\nthe following ones, possibly changing their forms as well. The underlying lexical or syntactic\nunits are thereby blurred into one compact string of letters and no longer appear as distinct\nwords. Tokens behaving in this way can be found in various languages and are often called\nclitics.\nIn the writing systems of Chinese, Japanese", "ompact string of letters and no longer appear as distinct\nwords. Tokens behaving in this way can be found in various languages and are often called\nclitics.\nIn the writing systems of Chinese, Japanese [5], and Thai, whitespace is not used to\nseparate words. The units that are delimited graphically in some way are sentences or\nclauses. In Korean, character strings are called eojeol \u2018word segment\u2019 and roughly correspond\nto speech or cognitive units, which are usually larger than words and smaller than clauses [6],\nas shown in Example 1\u20132:\nExample 1\u20132: \ud559\uc0dd\ub4e4\uc5d0\uac8c\ub9cc\uc8fc\uc168\ub294\ub370\nhak.sayng.tul.ey.key.man cwu.syess.nun.te2\nhaksayng-tul-eykey-man cwu-si-ess-nunte\nstudent+plural+dative+only give+honori\ufb01c+past+while\nwhile (he/she) gave (it) only to the students\n1. Signs used in sign languages are composed of elements denoted as phonemes, too.\n2. We use the Yale romanization of the Korean script and indicate its original characters by dots. Hyphens\nmark morphological boundaries, and tokens are separated by plu", "ents denoted as phonemes, too.\n2. We use the Yale romanization of the Korean script and indicate its original characters by dots. Hyphens\nmark morphological boundaries, and tokens are separated by plus symbols.\n\n1.1\nWords and Their Components\n5\nNonetheless, the elementary morphological units are viewed as having their own syntactic\nstatus [7]. In such languages, tokenization, also known as word segmentation, is the\nfundamental step of morphological analysis and a prerequisite for most language processing\napplications.\n1.1.2\nLexemes\nBy the term word, we often denote not just the one linguistic form in the given context\nbut also the concept behind the form and the set of alternative forms that can express\nit. Such sets are called lexemes or lexical items, and they constitute the lexicon of a lan-\nguage. Lexemes can be divided by their behavior into the lexical categories of verbs, nouns,\nadjectives, conjunctions, particles, or other parts of speech. The citation form of a lexeme,\nby whic", " lan-\nguage. Lexemes can be divided by their behavior into the lexical categories of verbs, nouns,\nadjectives, conjunctions, particles, or other parts of speech. The citation form of a lexeme,\nby which it is commonly identi\ufb01ed, is also called its lemma.\nWhen we convert a word into its other forms, such as turning the singular mouse into\nthe plural mice or mouses, we say we in\ufb02ect the lexeme. When we transform a lexeme into\nanother one that is morphologically related, regardless of its lexical category, we say we\nderive the lexeme: for instance, the nouns receiver and reception are derived from the verb\nto receive.\nExample 1\u20133: Did you see him? I didn\u2019t see him. I didn\u2019t see anyone.\nExample 1\u20133 presents the problem of tokenization of didn\u2019t and the investigation of\nthe internal structure of anyone. In the paraphrase I saw no one, the lexeme to see would\nbe in\ufb02ected into the form saw to re\ufb02ect its grammatical function of expressing positive\npast tense. Likewise, him is the oblique case f", " anyone. In the paraphrase I saw no one, the lexeme to see would\nbe in\ufb02ected into the form saw to re\ufb02ect its grammatical function of expressing positive\npast tense. Likewise, him is the oblique case form of he or even of a more abstract lexeme\nrepresenting all personal pronouns. In the paraphrase, no one can be perceived as the\nminimal word synonymous with nobody. The di\ufb03culty with the de\ufb01nition of what counts as\na word need not pose a problem for the syntactic description if we understand no one as\ntwo closely connected tokens treated as one \ufb01xed element.\nIn the Czech translation of Example 1\u20133, the lexeme vid\u02c7et \u2018to see\u2019 is in\ufb02ected for past\ntense, in which forms comprising two tokens are produced in the second and \ufb01rst person\n(i.e., vid\u02c7ela jsi \u2018you-fem-sg saw\u2019 and nevid\u02c7ela jsem \u2018I-fem-sg did not see\u2019). Negation in\nCzech is an in\ufb02ectional parameter rather than just syntactic and is marked both in the verb\nand in the pronoun of the latter response, as in Example 1\u20134:\nExample 1\u20134: Vi", "-sg did not see\u2019). Negation in\nCzech is an in\ufb02ectional parameter rather than just syntactic and is marked both in the verb\nand in the pronoun of the latter response, as in Example 1\u20134:\nExample 1\u20134: Vid\u02c7elas ho? Nevid\u02c7ela jsem ho. Nevid\u02c7ela jsem nikoho.\nsaw+you-are him? not-saw I-am him. not-saw I-am no-one.\nHere, vid\u02c7elas is the contracted form of vid\u02c7ela jsi \u2018you-fem-sg saw\u2019. The s of jsi \u2018you are\u2019\nis a clitic, and due to free word order in Czech, it can be attached to virtually any part of\nspeech. We could thus ask a question like Nikohos nevid\u02c7ela? \u2018Did you see no one?\u2019 in which\nthe pronoun nikoho \u2018no one\u2019 is followed by this clitic.\n1.1.3\nMorphemes\nMorphological theories di\ufb00er on whether and how to associate the properties of word forms\nwith their structural components [8, 9, 10, 11]. These components are usually called seg-\nments or morphs. The morphs that by themselves represent some aspect of the meaning\nof a word are called morphemes of some function.\n\n6\nChapter 1\nFinding the S", "These components are usually called seg-\nments or morphs. The morphs that by themselves represent some aspect of the meaning\nof a word are called morphemes of some function.\n\n6\nChapter 1\nFinding the Structure of Words\nHuman languages employ a variety of devices by which morphs and morphemes are\ncombined into word forms. The simplest morphological process concatenates morphs one by\none, as in dis-agree-ment-s, where agree is a free lexical morpheme and the other elements\nare bound grammatical morphemes contributing some partial meaning to the whole word.\nIn a more complex scheme, morphs can interact with each other, and their forms may\nbecome subject to additional phonological and orthographic changes denoted as morpho-\nphonemic. The alternative forms of a morpheme are termed allomorphs.\nExamples of morphological alternation and phonologically dependent choice of the form\nof a morpheme are abundant in the Korean language. In Korean, many morphemes change\ntheir forms systematically with ", "amples of morphological alternation and phonologically dependent choice of the form\nof a morpheme are abundant in the Korean language. In Korean, many morphemes change\ntheir forms systematically with the phonological context. Example 1\u20135 lists the allomorphs\n-ess-, -ass-, -yess- of the temporal marker indicating past tense. The \ufb01rst two alter according\nto the phonological condition of the preceding verb stem; the last one is used especially for\nthe verb ha- \u2018do\u2019. The appropriate allomorph is merely concatenated after the stem, or it can\nbe further contracted with it, as was -si-ess- into -syess- in Example 1\u20132. During morpho-\nlogical parsing, normalization of allomorphs into some canonical form of the morpheme is\ndesirable, especially because the contraction of morphs interferes with simple segmentation:\nExample 1\u20135:\nconcatenated\ncontracted\n(a)\n\ubcf4\uc558-\npo-ass-\n\ubd24-\npwass-\n\u2018have seen\u2019\n(b)\n\uac00\uc9c0\uc5c8- ka.ci-ess-\n\uac00\uc84c- ka.cyess-\n\u2018have taken\u2019\n(c)\n\ud558\uc600-\nha-yess-\n\ud588-\nhayss-\n\u2018have done\u2019\n(d)\n\ub418\uc5c8-\ntoy-ess-\n\ub410-\ntwa", "e segmentation:\nExample 1\u20135:\nconcatenated\ncontracted\n(a)\n\ubcf4\uc558-\npo-ass-\n\ubd24-\npwass-\n\u2018have seen\u2019\n(b)\n\uac00\uc9c0\uc5c8- ka.ci-ess-\n\uac00\uc84c- ka.cyess-\n\u2018have taken\u2019\n(c)\n\ud558\uc600-\nha-yess-\n\ud588-\nhayss-\n\u2018have done\u2019\n(d)\n\ub418\uc5c8-\ntoy-ess-\n\ub410-\ntwayss-\n\u2018have become\u2019\n(e)\n\ub193\uc558-\nnoh-ass-\n\ub1a8-\nnwass-\n\u2018have put\u2019\nContractions (a, b) are ordinary but require attention because two characters are reduced\ninto one. Other types (c, d, e) are phonologically unpredictable, or lexically dependent. For\nexample, coh-ass- \u2018have been good\u2019 may never be contracted, whereas noh- and -ass- are\nmerged into nwass- in (e).\nThere are yet other linguistic devices of word formation to account for, as the morpho-\nlogical process itself can get less trivial. The concatenation operation can be complemented\nwith in\ufb01xation or intertwining of the morphs, which is common, for instance, in Arabic.\nNonconcatenative in\ufb02ection by modi\ufb01cation of the internal vowel of a word occurs even in\nEnglish: compare the sounds of mouse and mice, see and saw, read and read.\nNotably in A", "or instance, in Arabic.\nNonconcatenative in\ufb02ection by modi\ufb01cation of the internal vowel of a word occurs even in\nEnglish: compare the sounds of mouse and mice, see and saw, read and read.\nNotably in Arabic, internal in\ufb02ection takes place routinely and has a yet di\ufb00erent quality.\nThe internal parts of words, called stems, are modeled with root and pattern morphemes.\nWord structure is then described by templates abstracting away from the root but showing\nthe pattern and all the other morphs attached to either side of it.\nExample 1\u20136: hl stqrO h*h AljrA}d?3\n\u0002\u0003\u0004\u0005\u0006\u0007\b\t \n\u0006 \u000b \f\u0003\r \u0004\u0006\u0007\u000e\u000f\u000e\u0010\u0011 \u0012\r\nhal sa-taqra\u0002u h\u00afad\u00af ihi \u2019l-\u02c7gar\u00afa\u0002ida?\nwhether will+you-read this the-newspapers?\nhl stqrWhA? ln OqrOhA.\n\t\u0013\r\u0004\u0006\u0007\u000e\u0014\u0004\u0006 \f\u0015\n \u0002\u0013\r\u0004\u0016\u0007\u000e\u000f\u000e\u0010\u0011 \u0012\r\nhal sa-taqra\u0002uh\u00afa? lan \u0002aqra\u0002ah\u00afa.\nwhether will+you-read+it? not-will I-read+it.\n3. The original Arabic script is transliterated using Buckwalter notation. For readability, we also provide\nthe standard phonological transcription, which reduces ambiguity.\n\n1.1\nWords and The", "ead+it.\n3. The original Arabic script is transliterated using Buckwalter notation. For readability, we also provide\nthe standard phonological transcription, which reduces ambiguity.\n\n1.1\nWords and Their Components\n7\nThe meaning of Example 1\u20136 is similar to that of Example 1\u20131, only the phrase\nh\u00afad\u00af ihi \u2019l-\u02c7gar\u00afa\u0002ida refers to \u2018these newspapers\u2019. While sa-taqra\u0002u \u2018you will read\u2019 combines\nthe future marker sa- with the imperfective second-person masculine singular verb taqra\u0002u\nin the indicative mood and active voice, sa-taqra\u0002uh\u00afa \u2018you will read it\u2019 also adds the cliticized\nfeminine singular personal pronoun in the accusative case.4\nThe citation form of the lexeme to which taqra\u0002u \u2018you-masc-sg read\u2019 belongs is qara\u0002,\nroughly \u2018to read\u2019. This form is classi\ufb01ed by linguists as the basic verbal form represented\nby the template fa\u0003al merged with the consonantal root q r \u0002, where the f \u0003 l symbols of the\ntemplate are substituted by the respective root consonants. In\ufb02ections of this lexeme can\n", "form represented\nby the template fa\u0003al merged with the consonantal root q r \u0002, where the f \u0003 l symbols of the\ntemplate are substituted by the respective root consonants. In\ufb02ections of this lexeme can\nmodify the pattern fa\u0003al of the stem of the lemma into f\u0003al and concatenate it, under rules\nof morphophonemic changes, with further pre\ufb01xes and su\ufb03xes. The structure of taqra\u0002u is\nthus parsed into the template ta-f\u0003al-u and the invariant root.\nThe word al-\u02c7gar\u00afa\u0002ida \u2018the newspapers\u2019 in the accusative case and de\ufb01nite state is another\nexample of internal in\ufb02ection. Its structure follows the template al-fa\u0003\u00afa\u0002il-a with the root \u02c7g\nr d. This word is the plural of \u02c7gar\u00af\u0131dah \u2018newspaper\u2019 with the template fa\u0003\u00af\u0131l-ah. The links\nbetween singular and plural templates are subject to convention and have to be declared in\nthe lexicon.\nIrrespective of the morphological processes involved, some properties or features of a\nword need not be apparent explicitly in its morphological structure. Its existing s", "o be declared in\nthe lexicon.\nIrrespective of the morphological processes involved, some properties or features of a\nword need not be apparent explicitly in its morphological structure. Its existing structural\ncomponents may be paired with and depend on several functions simultaneously but may\nhave no particular grammatical interpretation or lexical meaning.\nThe -ah su\ufb03x of \u02c7gar\u00af\u0131dah \u2018newspaper\u2019 corresponds with the inherent feminine gender of\nthe lexeme. In fact, the -ah morpheme is commonly, though not exclusively, used to mark the\nfeminine singular forms of adjectives: for example, \u02c7gad\u00af\u0131d becomes \u02c7gad\u00af\u0131dah \u2018new\u2019. However,\nthe -ah su\ufb03x can be part of words that are not feminine, and there its function can be seen\nas either emptied or overridden [12]. In general, linguistic forms should be distinguished\nfrom functions, and not every morph can be assumed to be a morpheme.\n1.1.4\nTypology\nMorphological typology divides languages into groups by characterizing the prevalent mor-\nphologica", "be distinguished\nfrom functions, and not every morph can be assumed to be a morpheme.\n1.1.4\nTypology\nMorphological typology divides languages into groups by characterizing the prevalent mor-\nphological phenomena in those languages. It can consider various criteria, and during the\nhistory of linguistics, di\ufb00erent classi\ufb01cations have been proposed [13, 14]. Let us outline the\ntypology that is based on quantitative relations between words, their morphemes, and their\nfeatures:\nIsolating, or analytic, languages include no or relatively few words that would comprise\nmore than one morpheme (typical members are Chinese, Vietnamese, and Thai; ana-\nlytic tendencies are also found in English).\nSynthetic languages can combine more morphemes in one word and are further divided\ninto agglutinative and fusional languages.\nAgglutinative languages have morphemes associated with only a single function at a time\n(as in Korean, Japanese, Finnish, and Tamil, etc.).\n4. The logical plural of things is formall", "sional languages.\nAgglutinative languages have morphemes associated with only a single function at a time\n(as in Korean, Japanese, Finnish, and Tamil, etc.).\n4. The logical plural of things is formally treated as feminine singular in Arabic.\n\n8\nChapter 1\nFinding the Structure of Words\nFusional languages are de\ufb01ned by their feature-per-morpheme ratio higher than one (as in\nArabic, Czech, Latin, Sanskrit, German, etc.).\nIn accordance with the notions about word formation processes mentioned earlier, we\ncan also discern:\nConcatenative languages linking morphs and morphemes one after another.\nNonlinear languages allowing structural components to merge nonsequentially to apply\ntonal morphemes or change the consonantal or vocalic templates of words.\nWhile some morphological phenomena, such as orthographic collapsing, phonological\ncontraction, or complex in\ufb02ection and derivation, are more dominant in some languages\nthan in others, in principle, we can \ufb01nd, and should be able to deal with, ins", "thographic collapsing, phonological\ncontraction, or complex in\ufb02ection and derivation, are more dominant in some languages\nthan in others, in principle, we can \ufb01nd, and should be able to deal with, instances of these\nphenomena across di\ufb00erent language families and typological classes.\n1.2\nIssues and Challenges\nMorphological parsing tries to eliminate or alleviate the variability of word forms to provide\nhigher-level linguistic units whose lexical and morphological properties are explicit and well\nde\ufb01ned. It attempts to remove unnecessary irregularity and give limits to ambiguity, both\nof which are present inherently in human language.\nBy irregularity, we mean existence of such forms and structures that are not described\nappropriately by a prototypical linguistic model. Some irregularities can be understood by\nredesigning the model and improving its rules, but other lexically dependent irregularities\noften cannot be generalized.\nAmbiguity is indeterminacy in interpretation of expressions", "an be understood by\nredesigning the model and improving its rules, but other lexically dependent irregularities\noften cannot be generalized.\nAmbiguity is indeterminacy in interpretation of expressions of language. Next to acci-\ndental ambiguity and ambiguity due to lexemes having multiple senses, we note the issue of\nsyncretism, or systematic ambiguity.\nMorphological modeling also faces the problem of productivity and creativity in language,\nby which unconventional but perfectly meaningful new words or new senses are coined.\nUsually, though, words that are not licensed in some way by the lexicon of a morphological\nsystem will remain completely unparsed. This unknown word problem is particularly\nsevere in speech or writing that gets out of the expected domain of the linguistic model,\nsuch as when special terms or foreign names are involved in the discourse or when multiple\nlanguages or dialects are mixed together.\n1.2.1\nIrregularity\nMorphological parsing is motivated by the quest for ge", "s when special terms or foreign names are involved in the discourse or when multiple\nlanguages or dialects are mixed together.\n1.2.1\nIrregularity\nMorphological parsing is motivated by the quest for generalization and abstraction in the\nworld of words. Immediate descriptions of given linguistic data may not be the ultimate\nones, due to either their inadequate accuracy or inappropriate complexity, and better for-\nmulations may be needed. The design principles of the morphological model are therefore\nvery important.\nIn Arabic, the deeper study of the morphological processes that are in e\ufb00ect during\nin\ufb02ection and derivation, even for the so-called irregular words, is essential for mastering the\n\n1.2\nIssues and Challenges\n9\nwhole morphological and phonological system. With the proper abstractions made, irregular\nmorphology can be seen as merely enforcing some extended rules, the nature of which is\nphonological, over the underlying or prototypical regular word forms [15, 16].\nExample 1\u20137: hl", "ns made, irregular\nmorphology can be seen as merely enforcing some extended rules, the nature of which is\nphonological, over the underlying or prototypical regular word forms [15, 16].\nExample 1\u20137: hl rOyth? lm Orh. lm Or OHdA.\n\t\u0006\u0003\u0017\u0004\u0006 \u0018\u0004\u0006 \u0019\n \t \u000b\u0018\u0004\u0006 \u0019\n \u0002\u001a\u000e\u0010\u0005\u001b\u0004\u0006\u0018 \u0012\r\nhal ra\u0002aytihi? lam \u0002arahu. lam \u0002ara \u0002ah. adan.\nwhether you-saw+him? not-did I-see+him. not-did I-see anyone.\nIn Example 1\u20137, ra\u0002ayti is the second-person feminine singular perfective verb in active\nvoice, member of the ra\u0002\u00afa \u2018to see\u2019 lexeme of the r \u0002 y root. The prototypical, regularized\npattern for this citation form is fa\u0003al, as we saw with qara\u0002 in Example 1\u20136. Alternatively,\nwe could assume the pattern of ra\u0002\u00afa to be fa\u0003\u00afa, thereby asserting in a compact way that\nthe \ufb01nal root consonant and its vocalic context are subject to the particular phonological\nchange, resulting in ra\u0002\u00afa like fa\u0003\u00afa instead of ra\u0002ay like fa\u0003al. The occurrence of this change\nin the citation form may have possible implications for the morphological ", " the particular phonological\nchange, resulting in ra\u0002\u00afa like fa\u0003\u00afa instead of ra\u0002ay like fa\u0003al. The occurrence of this change\nin the citation form may have possible implications for the morphological behavior of the\nwhole lexeme.\nTable 1\u20131 illustrates di\ufb00erences between a naive model of word structure in Arabic and\nthe model proposed in Smr\u02c7z [12] and Smr\u02c7z and Bielick\u00b4y [17] where morphophonemic merge\nrules and templates are involved. Morphophonemic templates capture morphological pro-\ncesses by just organizing stem patterns and generic a\ufb03xes without any context-dependent\nvariation of the a\ufb03xes or ad hoc modi\ufb01cation of the stems. The merge rules, indeed very\nterse, then ensure that such structured representations can be converted into exactly the\nsurface forms, both orthographic and phonological, used in the natural language. Applying\nthe merge rules is independent of and irrespective of any grammatical parameters or infor-\nmation other than that contained in a template. Most morpholo", "logical, used in the natural language. Applying\nthe merge rules is independent of and irrespective of any grammatical parameters or infor-\nmation other than that contained in a template. Most morphological irregularities are thus\nsuccessfully removed.\nTable 1\u20131: Discovering the regularity of Arabic morphology using\nmorphophonemic templates, where uniform structural operations apply to\ndi\ufb00erent kinds of stems. In rows, surface forms S of qara\u0002 \u2018to read\u2019 and ra\u0002\u00afa\n\u2018to see\u2019 and their in\ufb02ections are analyzed into immediate I and\nmorphophonemic M templates, in which dashes mark the structural boundaries\nwhere merge rules are enforced. The outer columns of the table correspond to\nP perfective and I imperfective stems declared in the lexicon; the inner columns\ntreat active verb forms of the following morphosyntactic properties: I indicative,\nS subjunctive, J jussive mood; 1 \ufb01rst, 2 second, 3 third person; M masculine, F\nfeminine gender; S singular, P plural number\nP-stem\nP\u22123MS\nP\u22122FS\nP\u22123MP\nII2", "wing morphosyntactic properties: I indicative,\nS subjunctive, J jussive mood; 1 \ufb01rst, 2 second, 3 third person; M masculine, F\nfeminine gender; S singular, P plural number\nP-stem\nP\u22123MS\nP\u22122FS\nP\u22123MP\nII2MS\nIS1\u2212S\nIJ1\u2212S\nI-stem\nqara\u0002\nqara\u0002a\nqara\u0002ti\nqara\u0002\u00afu\ntaqra\u0002u\n\u0002aqra\u0002a\n\u0002aqra\u0002\nqra\u0002\nS\nfa\u0003al\nfa\u0003al-a\nfa\u0003al-ti\nfa\u0003al-\u00afu\nta-f\u0003al-u\n\u0002a-f\u0003al-a\n\u0002a-f\u0003al\nf\u0003al\nI\nfa\u0003al\nfa\u0003al-a\nfa\u0003al-ti\nfa\u0003al-\u00afu\nta-f\u0003al-u\n\u0002a-f\u0003al-a\n\u0002a-f\u0003al-\nf\u0003al\nM\n...\n...-a\n...-ti\n...-\u00afu\nta-...-u\n\u0002a-...-a\n\u0002a-...-\n...\nfa\u0003\u00afa\nfa\u0003\u00afa-a\nfa\u0003\u00afa-ti\nfa\u0003\u00afa-\u00afu\nta-f\u00afa-u\n\u0002a-f\u00afa-a\n\u0002a-f\u00afa-\nf\u00afa\nM\nfa\u0003\u00afa\nfa\u0003\u00afa\nfa\u0003al-ti\nfa\u0003-aw\nta-f\u00afa\n\u0002a-f\u00afa\n\u0002a-fa\nf\u00afa\nI\nra\u0002\u00afa\nra\u0002\u00afa\nra\u0002ayti\nra\u0002aw\ntar\u00afa\n\u0002ar\u00afa\n\u0002ara\nr\u00afa\nS\n\n10\nChapter 1\nFinding the Structure of Words\nTable 1\u20132: Examples of major Korean irregular verb classes compared\nwith regular verbs\nBase Form\n(-e)\nMeaning\nComment\n\uc9d1-\ncip-\n\uc9d1\uc5b4\ncip.e\n\u2018pick\u2019\nregular\n\uae41-\nkip-\n\uae30\uc6cc\nki.we\n\u2018sew\u2019\np-irregular\n\ubbff-\nmit-\n\ubbff\uc5b4\nmit.e\n\u2018believe\u2019\nregular\n\uc2e3-\nsit-\n\uc2e4\uc5b4\nsil.e\n\u2018load\u2019\nt-irregular\n\uc53b-\nssis-\n\uc53b\uc5b4\nssis.e\n\u2018wash\u2019\nregular\n\uc787-\nis-\n\uc774\uc5b4\ni.e\n\u2018link\u2019\ns-irreg", "\uc9d1-\ncip-\n\uc9d1\uc5b4\ncip.e\n\u2018pick\u2019\nregular\n\uae41-\nkip-\n\uae30\uc6cc\nki.we\n\u2018sew\u2019\np-irregular\n\ubbff-\nmit-\n\ubbff\uc5b4\nmit.e\n\u2018believe\u2019\nregular\n\uc2e3-\nsit-\n\uc2e4\uc5b4\nsil.e\n\u2018load\u2019\nt-irregular\n\uc53b-\nssis-\n\uc53b\uc5b4\nssis.e\n\u2018wash\u2019\nregular\n\uc787-\nis-\n\uc774\uc5b4\ni.e\n\u2018link\u2019\ns-irregular\n\ub0b3-\nnah-\n\ub0b3\uc544\nnah.a\n\u2018bear\u2019\nregular\n\uae4c\ub9e3- kka.mah-\n\uae4c\ub9e4\nkka.may\n\u2018be black\u2019\nh-irregular\n\uce58\ub974- chi.lu-\n\uce58\ub7ec\nchi.le\n\u2018pay\u2019\nregular u-ellipsis\n\uc774\ub974- i.lu-\n\uc774\ub974\ub7eci.lu.le\n\u2018reach\u2019\nle-irregular\n\ud750\ub974- hu.lu-\n\ud758\ub7ec\nhul.le\n\u2018\ufb02ow\u2019\nlu-irregular\nIn contrast, some irregularities are bound to particular lexemes or contexts, and can-\nnot be accounted for by general rules. Korean irregular verbs provide examples of such\nirregularities.\nKorean shows exceptional constraints on the selection of grammatical morphemes. It\nis hard to \ufb01nd irregular in\ufb02ection in other agglutinative languages: two irregular verbs\nin Japanese [18], one in Finnish [19]. These languages are abundant with morphological\nalternations that are formalized by precise phonological rules. Korean additionally features\nlexically dependent stem alternation. As in m", "Finnish [19]. These languages are abundant with morphological\nalternations that are formalized by precise phonological rules. Korean additionally features\nlexically dependent stem alternation. As in many other languages, i- \u2018be\u2019 and ha- \u2018do\u2019 have\nunique irregular endings. Other irregular verbs are classi\ufb01ed by the stem \ufb01nal phoneme.\nTable 1\u20132 compares major irregular verb classes with regular verbs in the same phonological\ncondition.\n1.2.2\nAmbiguity\nMorphological ambiguity is the possibility that word forms be understood in multiple ways\nout of the context of their discourse. Words forms that look the same but have distinct\nfunctions or meaning are called homonyms.\nAmbiguity is present in all aspects of morphological processing and language processing\nat large. Morphological parsing is not concerned with complete disambiguation of words in\ntheir context, however; it can e\ufb00ectively restrict the set of valid interpretations of a given\nword form [20, 21].\nIn Korean, homonyms are one of th", " concerned with complete disambiguation of words in\ntheir context, however; it can e\ufb00ectively restrict the set of valid interpretations of a given\nword form [20, 21].\nIn Korean, homonyms are one of the most problematic objects in morphological analysis\nbecause they prevail all around frequent lexical items. Table 1\u20133 arranges homonyms on\nthe basis of their behavior with di\ufb00erent endings. Example 1\u20138 is an example of homonyms\nthrough nouns and verbs.\n\n1.2\nIssues and Challenges\n11\nTable 1\u20133: Systematic homonyms arise as verbs combined with endings\nin Korean\n(-ko)\n(-e)\n(-un)\nMeaning\n\ubb3b\uace0\nmwut.ko\n\ubb3b\uc5b4\nmwut.e\n\ubb3b\uc740mwut.un\n\u2018bury\u2019\n\ubb3b\uace0\nmwut.ko\n\ubb3c\uc5b4\nmwul.e\n\ubb3c\uc740mwul.un\n\u2018ask\u2019\n\ubb3c\uace0\nmwul.ko\n\ubb3c\uc5b4\nmwul.e\n\ubb38\nmwun\n\u2018bite\u2019\n\uac77\uace0\nket.ko\n\uac77\uc5b4\nket.e\n\uac77\uc740ket.un\n\u2018roll up\u2019\n\uac77\uace0\nket.ko\n\uac78\uc5b4\nkel.e\n\uac78\uc740kel.un\n\u2018walk\u2019\n\uac78\uace0\nkel.ko\n\uac78\uc5b4\nkel.e\n\uac74\nken\n\u2018hang\u2019\n\uad7d\uace0\nkwup.ko\n\uad7d\uc5b4\nkwup.e\n\uad7d\uc740kwup.un\n\u2018be bent\u2019\n\uad7d\uace0\nkwup.ko\n\uad6c\uc6cc\nkwu.we\n\uad6c\uc6b4kwu.wun\n\u2018bake\u2019\n\uc774\ub974\uace0i.lu.ko\n\uc774\ub974\ub7eci.lu.le\n\uc774\ub978i.lun\n\u2018reach\u2019\n\uc774\ub974\uace0i.lu.ko\n\uc77c\ub7ec\nil.le\n\uc774\ub978i.lun\n\u2018say\u2019\nExample 1\u20138: \ub09c\u2018orchid\u2019\n\u2190\n\ub09cnan \u2018orchid\u2019\n\ub09c\u2018I\u2019\n\u2190\n\ub098", "\u2019\n\uad7d\uace0\nkwup.ko\n\uad7d\uc5b4\nkwup.e\n\uad7d\uc740kwup.un\n\u2018be bent\u2019\n\uad7d\uace0\nkwup.ko\n\uad6c\uc6cc\nkwu.we\n\uad6c\uc6b4kwu.wun\n\u2018bake\u2019\n\uc774\ub974\uace0i.lu.ko\n\uc774\ub974\ub7eci.lu.le\n\uc774\ub978i.lun\n\u2018reach\u2019\n\uc774\ub974\uace0i.lu.ko\n\uc77c\ub7ec\nil.le\n\uc774\ub978i.lun\n\u2018say\u2019\nExample 1\u20138: \ub09c\u2018orchid\u2019\n\u2190\n\ub09cnan \u2018orchid\u2019\n\ub09c\u2018I\u2019\n\u2190\n\ub098na \u2018I\u2019 + -n (topic)\n\ub09c\u2018which \ufb02ew\u2019\n\u2190\n\ub0a0- nal- \u2018\ufb02y\u2019 + -n (relative, past)\n\ub09c\u2018which got out\u2019\n\u2190\n\ub098- na- \u2018get out\u2019 + -n (relative, past)\nWe could also consider ambiguity in the senses of the noun nan, according to the Standard\nKorean Language Dictionary: nan1 \u2018egg\u2019, nan2 \u2018revolt\u2019, nan5 \u2018section (in newspaper)\u2019, nan6\n\u2018orchid\u2019, plus several infrequent readings.\nArabic is a language of rich morphology, both derivational and in\ufb02ectional. Because\nArabic script usually does not encode short vowels and omits yet some other diacritical\nmarks that would record the phonological form exactly, the degree of its morphological\nambiguity is considerably increased. In addition, Arabic orthography collapses certain word\nforms together. The problem of morphological disambiguation of Arabic encompasses not\nonly the r", "rphological\nambiguity is considerably increased. In addition, Arabic orthography collapses certain word\nforms together. The problem of morphological disambiguation of Arabic encompasses not\nonly the resolution of the structural components of words and their actual morphosyntactic\nproperties (i.e., morphological tagging [22, 23, 24]) but also tokenization and normalization\n[25], lemmatization, stemming, and diacritization [26, 27, 28].\nWhen in\ufb02ected syntactic words are combined in an utterance, additional phonological\nand orthographic changes can take place, as shown in Figure 1\u20131. In Sanskrit, one such\neuphony rule is known as external sandhi [29, 30]. Inverting sandhi during tokenization is\nusually nondeterministic in the sense that it can provide multiple solutions. In any language,\ntokenization decisions may impose constraints on the morphosyntactic properties of the\ntokens being reconstructed, which then have to be respected in further processing. The\ntight coupling between morphol", "nization decisions may impose constraints on the morphosyntactic properties of the\ntokens being reconstructed, which then have to be respected in further processing. The\ntight coupling between morphology and syntax has inspired proposals for disambiguating\nthem jointly rather than sequentially [4].\nCzech is a highly in\ufb02ected fusional language. Unlike agglutinative languages, in\ufb02ec-\ntional morphemes often represent several functions simultaneously, and there is no partic-\nular one-to-one correspondence between their forms and functions. In\ufb02ectional paradigms\n\n12\nChapter 1\nFinding the Structure of Words\ndir\u00afasat\u00af\u0131\n\u001c\u001b\u000e\u001d\u0011\u0006\u0018\u001e\ndrAsty\n\u2192\ndir\u00afasatu \u00af\u0131\n\u001f\u001b  \u000e\u001a\u0011\u0006\u0018\u001e\ndrAsp y\n\u2192\ndir\u00afasati \u00af\u0131\n\u001f\u001b  \u000e\u001a\u0011\u0006\u0018\u001e\ndrAsp y\n\u2192\ndir\u00afasata \u00af\u0131\n\u001f\u001b  \u000e\u001a\u0011\u0006\u0018\u001e\ndrAsp y\nmu\u0003allim\u00af\u0131ya\n!\u001b\"#$%\nmElmy\n\u2192\nmu\u0003allim\u00afu \u00af\u0131\n\u001f\u001b  &\"#$%\nmElmw y\n\u2192\nmu\u0003allim\u00af\u0131 \u00af\u0131\n\u001f\u001b  !\u001b\"#$%\nmElmy y\nkatabtum\u00afuh\u00afa \u0013\r&\"\u000e\u0010'\t\u000e\u0010(\nktbtmwhA\n\u2192\nkatabtum h\u00afa\n\u0013\r \u0019\u000e\u001d\u0010\t\u000e\u0010(\nktbtm hA\n\u0002i\u02c7gr\u00afa\u0002uhu\n\u000b\u0004\u0016\u0006\u0007)\t \u0006\u0004\nIjrAWh\n\u2192\n\u0002i\u02c7gr\u00afa\u0002u hu\n\u000b *\u0006\u0007)\t \u0006\u0004\nIjrA\u2019 h\n\u0002i\u02c7gr\u00afa\u0002ihi\n\u001a\u0004\u0005\u0006\u0007)\t \u0006\u0004\nIjrA}h\n\u2192\n", "mu\u0003allim\u00af\u0131 \u00af\u0131\n\u001f\u001b  !\u001b\"#$%\nmElmy y\nkatabtum\u00afuh\u00afa \u0013\r&\"\u000e\u0010'\t\u000e\u0010(\nktbtmwhA\n\u2192\nkatabtum h\u00afa\n\u0013\r \u0019\u000e\u001d\u0010\t\u000e\u0010(\nktbtm hA\n\u0002i\u02c7gr\u00afa\u0002uhu\n\u000b\u0004\u0016\u0006\u0007)\t \u0006\u0004\nIjrAWh\n\u2192\n\u0002i\u02c7gr\u00afa\u0002u hu\n\u000b *\u0006\u0007)\t \u0006\u0004\nIjrA\u2019 h\n\u0002i\u02c7gr\u00afa\u0002ihi\n\u001a\u0004\u0005\u0006\u0007)\t \u0006\u0004\nIjrA}h\n\u2192\n\u0002i\u02c7gr\u00afa\u0002i hu\n\u000b *\u0006\u0007)\t \u0006\u0004\nIjrA\u2019 h\n\u0002i\u02c7gr\u00afa\u0002ahu\n\u000b*\u0006\u0007)\t \u0006\u0004\nIjrA\u2019h\n\u2192\n\u0002i\u02c7gr\u00afa\u0002a hu\n\u000b *\u0006\u0007)\t \u0006\u0004\nIjrA\u2019 h\nli-\u2019l-\u0002asafi\n\f+\u0011\u0004,\nllOsf\n\u2192\nli \u2019l-\u0002asafi li\n\f+\u0011\u0004-\u0006 .\nl AlOsf\nFigure 1\u20131: Complex tokenization and normalization of euphony in Arabic. Three nominal cases are\nexpressed by the same word form with dir\u00afasat\u00af\u0131 \u2018my study\u2019 and mu\u0003allim\u00af\u0131ya \u2018my teachers\u2019, but the\noriginal case endings are distinct. In katabtum\u00afuh\u00afa \u2018you-masc-pl wrote them\u2019, the liaison vowel \u00afu is\ndropped when tokenized. Special attention is needed to normalize some orthographic conventions, such\nas the interaction of \u0002i\u02c7gr\u00afa\u0002 \u2018carrying out\u2019 and the cliticized hu \u2018his\u2019 respecting the case ending or the\nmerge of the de\ufb01nite article of \u0002asaf \u2018regret\u2019 with the preposition li \u2018for\u2019\n(i.e., schemes for \ufb01nding the form of a lexeme associated wi", " and the cliticized hu \u2018his\u2019 respecting the case ending or the\nmerge of the de\ufb01nite article of \u0002asaf \u2018regret\u2019 with the preposition li \u2018for\u2019\n(i.e., schemes for \ufb01nding the form of a lexeme associated with the required properties) in\nCzech are of numerous kinds, yet they tend to include nonunique forms in them.\nTable 1\u20134 lists the paradigms of several common Czech words. In\ufb02ectional paradigms\nfor nouns depend on the grammatical gender and the phonological structure of a lexeme.\nThe individual forms in a paradigm vary with grammatical number and case, which are the\nfree parameters imposed only by the context in which a word is used.\nLooking at the morphological variation of the word staven\u00b4\u0131 \u2018building\u2019, we might wonder\nwhy we should distinguish all the cases for it when this lexeme can take only four di\ufb00erent\nforms. Is the detail of the case system appropriate? The answer is yes, because we can \ufb01nd\nlinguistic evidence that leads to this case category abstraction. Just consider other words ", "ly four di\ufb00erent\nforms. Is the detail of the case system appropriate? The answer is yes, because we can \ufb01nd\nlinguistic evidence that leads to this case category abstraction. Just consider other words of\nthe same meaning in place of staven\u00b4\u0131 in various contexts. We conclude that there is indeed\na case distinction made by the underlying system, but it need not necessarily be expressed\nclearly and uniquely in the form of words.\nThe morphological phenomenon that some words or word classes show instances of\nsystematic homonymy is called syncretism. In particular, homonymy can occur due to\nneutralization and unin\ufb02ectedness with respect to some morphosyntactic parameters.\nThese cases of morphological syncretism are distinguished by the ability of the context to\ndemand the morphosyntactic properties in question, as stated by Baerman, Brown, and\nCorbett [10, p. 32]:\nWhereas neutralization is about syntactic irrelevance as re\ufb02ected in morphology,\nunin\ufb02ectedness is about morphology being unrespon", "es in question, as stated by Baerman, Brown, and\nCorbett [10, p. 32]:\nWhereas neutralization is about syntactic irrelevance as re\ufb02ected in morphology,\nunin\ufb02ectedness is about morphology being unresponsive to a feature that is\nsyntactically relevant.\nFor example, it seems \ufb01ne for syntax in Czech or Arabic to request the personal pronoun\nof the \ufb01rst-person feminine singular, equivalent to \u2018I\u2019, despite it being homonymous with\n\n1.2\nIssues and Challenges\n13\nTable 1\u20134: Morphological paradigms of the Czech words d\u02daum \u2018house\u2019,\nbudova \u2018building\u2019, stavba \u2018building\u2019, staven\u00b4\u0131 \u2018building\u2019. Despite systematic\nambiguities in them, the space of in\ufb02ectional parameters could not be\nreduced without losing the ability to capture all distinct forms elsewhere: S\nsingular, P plural number; 1 nominative, 2 genitive, 3 dative, 4 accusative, 5\nvocative, 6 locative, 7 instrumental case\nMasculine inanimate\nFeminine\nFeminine\nNeuter\nS1\nd\u02daum\nbudova\nstavba\nstaven\u00b4\u0131\nS2\ndomu\nbudovy\nstavby\nstaven\u00b4\u0131\nS3\ndomu\nbudov\u02c7e\nstav", "ive, 3 dative, 4 accusative, 5\nvocative, 6 locative, 7 instrumental case\nMasculine inanimate\nFeminine\nFeminine\nNeuter\nS1\nd\u02daum\nbudova\nstavba\nstaven\u00b4\u0131\nS2\ndomu\nbudovy\nstavby\nstaven\u00b4\u0131\nS3\ndomu\nbudov\u02c7e\nstavb\u02c7e\nstaven\u00b4\u0131\nS4\nd\u02daum\nbudovu\nstavbu\nstaven\u00b4\u0131\nS5\ndome\nbudovo\nstavbo\nstaven\u00b4\u0131\nS6\ndomu / dom\u02c7e\nbudov\u02c7e\nstavb\u02c7e\nstaven\u00b4\u0131\nS7\ndomem\nbudovou\nstavbou\nstaven\u00b4\u0131m\nP1\ndomy\nbudovy\nstavby\nstaven\u00b4\u0131\nP2\ndom\u02dau\nbudov\nstaveb\nstaven\u00b4\u0131\nP3\ndom\u02daum\nbudov\u00b4am\nstavb\u00b4am\nstaven\u00b4\u0131m\nP4\ndomy\nbudovy\nstavby\nstaven\u00b4\u0131\nP5\ndomy\nbudovy\nstavby\nstaven\u00b4\u0131\nP6\ndomech\nbudov\u00b4ach\nstavb\u00b4ach\nstaven\u00b4\u0131ch\nP7\ndomy\nbudovami\nstavbami\nstaven\u00b4\u0131mi\nthe \ufb01rst-person masculine singular. The reason is that for some other values of the person\ncategory, the forms of masculine and feminine gender are di\ufb00erent, and there exist syntactic\ndependencies that do take gender into account. It is not the case that the \ufb01rst-person singular\npronoun would have no gender nor that it would have both. We just observe unin\ufb02ectedness\nhere. On the other hand, we might claim ", " gender into account. It is not the case that the \ufb01rst-person singular\npronoun would have no gender nor that it would have both. We just observe unin\ufb02ectedness\nhere. On the other hand, we might claim that in English or Korean, the gender category is\nsyntactically neutralized if it ever was present, and the nuances between he and she, him\nand her, his and hers are only semantic.\nWith the notion of paradigms and syncretism in mind, we should ask what is the minimal\nset of combinations of morphosyntactic in\ufb02ectional parameters that covers the in\ufb02ectional\nvariability in a language. Morphological models that would like to de\ufb01ne a joint system of\nunderlying morphosyntactic properties for multiple languages would have to generalize the\nparameter space accordingly and neutralize any systematically void con\ufb01gurations.\n1.2.3\nProductivity\nIs the inventory of words in a language \ufb01nite, or is it unlimited? This question leads\ndirectly to discerning two fundamental approaches to language, summarized", " void con\ufb01gurations.\n1.2.3\nProductivity\nIs the inventory of words in a language \ufb01nite, or is it unlimited? This question leads\ndirectly to discerning two fundamental approaches to language, summarized in the dis-\ntinction between langue and parole by Ferdinand de Saussure, or in the competence versus\nperformance duality by Noam Chomsky.\nIn one view, language can be seen as simply a collection of utterances (parole) actually\npronounced or written (performance). This ideal data set can in practice be approximated\nby linguistic corpora, which are \ufb01nite collections of linguistic data that are studied with\nempirical methods and can be used for comparison when linguistic models are developed.\n\n14\nChapter 1\nFinding the Structure of Words\nYet, if we consider language as a system (langue), we discover in it structural devices\nlike recursion, iteration, or compounding that allow to produce (competence) an in\ufb01nite set\nof concrete linguistic utterances. This general potential holds for morphologic", "ver in it structural devices\nlike recursion, iteration, or compounding that allow to produce (competence) an in\ufb01nite set\nof concrete linguistic utterances. This general potential holds for morphological processes as\nwell and is called morphological productivity [31, 32].\nWe denote the set of word forms found in a corpus of a language as its vocabulary. The\nmembers of this set are word types, whereas every original instance of a word form is a word\ntoken.\nThe distribution of words [33] or other elements of language follows the \u201c80/20 rule,\u201d\nalso known as the law of the vital few. It says that most of the word tokens in a given corpus\ncan be identi\ufb01ed with just a couple of word types in its vocabulary, and words from the rest\nof the vocabulary occur much less commonly if not rarely in the corpus. Furthermore, new,\nunexpected words will always appear as the collection of linguistic data is enlarged.\nIn Czech, negation is a productive morphological operation. Verbs, nouns, adjectives, and\n", "rpus. Furthermore, new,\nunexpected words will always appear as the collection of linguistic data is enlarged.\nIn Czech, negation is a productive morphological operation. Verbs, nouns, adjectives, and\nadverbs can be pre\ufb01xed with ne- to de\ufb01ne the complementary lexical concept. In Example\n1\u20139, bude\u02c7s \u2018you will be\u2019 is the second-person singular of b\u00b4yt \u2018to be\u2019, and nebudu \u2018I will not\nbe\u2019 is the \ufb01rst-person singular of neb\u00b4yt, the negated b\u00b4yt. We could easily have \u02c7c\u00b4\u0131st \u2018to read\u2019\nand ne\u02c7c\u00b4\u0131st \u2018not to read\u2019, or we could create an adverbial phrase like noviny nenoviny that\nwould express \u2018indi\ufb00erence to newspapers\u2019 in general:\nExample 1\u20139: Bude\u02c7s \u02c7c\u00b4\u0131st ty noviny? Bude\u02c7s je \u02c7c\u00b4\u0131st? Nebudu je \u02c7c\u00b4\u0131st.\nyou-will read the newspaper? you-will it read? not-I-will it read.\nExample 1\u20139 has the meaning of Example 1\u20131 and Example 1\u20136. The word noviny\n\u2018newspaper\u2019 exists only in plural whether it signi\ufb01es one piece of newspaper or many of\nthem. We can literally translate noviny as the plural of novina \u2018n", " Example 1\u20131 and Example 1\u20136. The word noviny\n\u2018newspaper\u2019 exists only in plural whether it signi\ufb01es one piece of newspaper or many of\nthem. We can literally translate noviny as the plural of novina \u2018news\u2019 to see the origins of\nthe word as well as the fortunate analogy with English.\nIt is conceivable to include all negated lexemes into the lexicon and thereby again achieve\na \ufb01nite number of word forms in the vocabulary. Generally, though, the richness of a mor-\nphological system of a language can make this approach highly impractical.\nMost languages contain words that allow some of their structural components to repeat\nfreely. Consider the pre\ufb01x pra- related to a notion of \u2018generation\u2019 in Czech and how it can\nor cannot be iterated, as shown in Example 1\u201310:\nExample 1\u201310: vnuk \u2018grandson\u2019\npravnuk \u2018great-grandson\u2019\nprapra...vnuk \u2018great-great-...grandson\u2019\nles \u2018forest\u2019\nprales \u2018jungle\u2019, \u2018virgin forest\u2019\nzdroj \u2018source\u2019\nprazdroj \u2018urquell\u2019, \u2018original source\u2019\nstar\u00b4y \u2018old\u2019\nprastar\u00b4y \u2018time-honored\u2019, ", "vnuk \u2018great-grandson\u2019\nprapra...vnuk \u2018great-great-...grandson\u2019\nles \u2018forest\u2019\nprales \u2018jungle\u2019, \u2018virgin forest\u2019\nzdroj \u2018source\u2019\nprazdroj \u2018urquell\u2019, \u2018original source\u2019\nstar\u00b4y \u2018old\u2019\nprastar\u00b4y \u2018time-honored\u2019, \u2018dateless\u2019\nIn creative language, such as in blogs, chats, and emotive informal communication,\niteration is often used to accent intensity of expression. Creativity may, of course, go beyond\nthe rules of productivity itself [32].\nLet us give an example where creativity, productivity, and the issue of unknown words\nmeet nicely. According to Wikipedia, the word googol is a made-up word denoting the\nnumber \u201cone followed by one hundred zeros,\u201d and the name of the company Google is an\n\n1.3\nMorphological Models\n15\ninadvertent misspelling thereof. Nonetheless, both of these words successfully entered the\nlexicon of English where morphological productivity started working, and we now know the\nverb to google and nouns like googling or even googlish or googleology [34].\nThe original names have been a", "the\nlexicon of English where morphological productivity started working, and we now know the\nverb to google and nouns like googling or even googlish or googleology [34].\nThe original names have been adopted by other languages, too, and their own morpho-\nlogical processes have been triggered. In Czech, one says googlovat, googlit \u2018to google\u2019 or\nvygooglovat, vygooglit \u2018to google out\u2019, googlov\u00b4an\u00b4\u0131 \u2018googling\u2019, and so on. In Arabic, the names\nare transcribed as \u02c7g\u00afu\u02c7g\u00aful \u2018googol\u2019 and \u02c7g\u00afu\u02c7gil \u2018Google\u2019. The latter one got transformed to the\nverb \u02c7gaw\u02c7gal \u2018to google\u2019 through internal in\ufb02ection, as if there were a genuine root \u02c7g w \u02c7g l,\nand the corresponding noun \u02c7gaw\u02c7galah \u2018googling\u2019 exists as well.\n1.3\nMorphological Models\nThere are many possible approaches to designing and implementing morphological models.\nOver time, computational linguistics has witnessed the development of a number of for-\nmalisms and frameworks, in particular grammars of di\ufb00erent kinds and expressive power,\nwith which", "logical models.\nOver time, computational linguistics has witnessed the development of a number of for-\nmalisms and frameworks, in particular grammars of di\ufb00erent kinds and expressive power,\nwith which to address whole classes of problems in processing natural as well as formal\nlanguages.\nVarious domain-speci\ufb01c programming languages have been created that allow us to\nimplement the theoretical problem using hopefully intuitive and minimal programming\ne\ufb00ort. These special-purpose languages usually introduce idiosyncratic notations of programs\nand are interpreted using some restricted model of computation. The motivation for such\napproaches may partly lie in the fact that, historically, computational resources were too\nlimited compared to the requirements and complexity of the tasks being solved. Other\nmotivations are theoretical given that \ufb01nding a simple but accurate and yet generalizing\nmodel is the point of scienti\ufb01c abstraction.\nThere are also many approaches that do not resort to dom", "ed. Other\nmotivations are theoretical given that \ufb01nding a simple but accurate and yet generalizing\nmodel is the point of scienti\ufb01c abstraction.\nThere are also many approaches that do not resort to domain-speci\ufb01c programming.\nThey, however, have to take care of the runtime performance and e\ufb03ciency of the computa-\ntional model themselves. It is up to the choice of the programming methods and the design\nstyle whether such models turn out to be pure, intuitive, adequate, complete, reusable,\nelegant, or not.\nLet us now look at the most prominent types of computational approaches to morphology.\nNeedless to say, this typology is not strictly exclusive in the sense that comprehensive\nmorphological models and their applications can combine various distinct implementational\naspects, discussed next.\n1.3.1\nDictionary Lookup\nMorphological parsing is a process by which word forms of a language are associated with\ncorresponding linguistic descriptions. Morphological systems that specify these associa", "1.3.1\nDictionary Lookup\nMorphological parsing is a process by which word forms of a language are associated with\ncorresponding linguistic descriptions. Morphological systems that specify these associations\nby merely enumerating them case by case do not o\ufb00er any generalization means. Likewise\nfor systems in which analyzing a word form is reduced to looking it up verbatim in word\n\n16\nChapter 1\nFinding the Structure of Words\nlists, dictionaries, or databases, unless they are constructed by and kept in sync with more\nsophisticated models of the language.\nIn this context, a dictionary is understood as a data structure that directly enables\nobtaining some precomputed results, in our case word analyses. The data structure can\nbe optimized for e\ufb03cient lookup, and the results can be shared. Lookup operations are\nrelatively simple and usually quick. Dictionaries can be implemented, for instance, as lists,\nbinary search trees, tries, hash tables, and so on.\nBecause the set of associations between", "operations are\nrelatively simple and usually quick. Dictionaries can be implemented, for instance, as lists,\nbinary search trees, tries, hash tables, and so on.\nBecause the set of associations between word forms and their desired descriptions is\ndeclared by plain enumeration, the coverage of the model is \ufb01nite and the generative\npotential of the language is not exploited. Developing as well as verifying the association list\nis tedious, liable to errors, and likely ine\ufb03cient and inaccurate unless the data are retrieved\nautomatically from large and reliable linguistic resources.\nDespite all that, an enumerative model is often su\ufb03cient for the given purpose, deals eas-\nily with exceptions, and can implement even complex morphology. For instance, dictionary-\nbased approaches to Korean [35] depend on a large dictionary of all possible combinations\nof allomorphs and morphological alternations. These approaches do not allow development\nof reusable morphological rules, though [36].\nThe word li", "end on a large dictionary of all possible combinations\nof allomorphs and morphological alternations. These approaches do not allow development\nof reusable morphological rules, though [36].\nThe word list or dictionary-based approach has been used frequently in various\nad hoc implementations for many languages. We could assume that with the availability of\nimmense online data, extracting a high-coverage vocabulary of word forms is feasible these\ndays [37]. The question remains how the associated annotations are constructed and how\ninformative and accurate they are. References to the literature on the unsupervised learn-\ning and induction of morphology, which are methods resulting in structured and therefore\nnonenumerative models, are provided later in this chapter.\n1.3.2\nFinite-State Morphology\nBy \ufb01nite-state morphological models, we mean those in which the speci\ufb01cations written\nby human programmers are directly compiled into \ufb01nite-state transducers. The two most\npopular tools supporting", "ogy\nBy \ufb01nite-state morphological models, we mean those in which the speci\ufb01cations written\nby human programmers are directly compiled into \ufb01nite-state transducers. The two most\npopular tools supporting this approach, which have been cited in literature and for which\nexample implementations for multiple languages are available online, include XFST (Xerox\nFinite-State Tool) [9] and LexTools [11].5\nFinite-state transducers are computational devices extending the power of \ufb01nite-state\nautomata. They consist of a \ufb01nite set of nodes connected by directed edges labeled with\npairs of input and output symbols. In such a network or graph, nodes are also called states,\nwhile edges are called arcs. Traversing the network from the set of initial states to the set\nof \ufb01nal states along the arcs is equivalent to reading the sequences of encountered input\nsymbols and writing the sequences of corresponding output symbols.\nThe set of possible sequences accepted by the transducer de\ufb01nes the input language;\n", "nt to reading the sequences of encountered input\nsymbols and writing the sequences of corresponding output symbols.\nThe set of possible sequences accepted by the transducer de\ufb01nes the input language;\nthe set of possible sequences emitted by the transducer de\ufb01nes the output language. For\nexample, a \ufb01nite-state transducer could translate the in\ufb01nite regular language consisting\nof the words vnuk, pravnuk, prapravnuk, . . . to the matching words in the in\ufb01nite regular\nlanguage de\ufb01ned by grandson, great-grandson, great-great-grandson, . . .\n5. See http://www.fsmbook.com/ and http://compling.ai.uiuc.edu/catms/ respectively.\n\n1.3\nMorphological Models\n17\nThe role of \ufb01nite-state transducers is to capture and compute regular relations on sets\n[38, 9, 11].6 That is, transducers specify relations between the input and output languages.\nIn fact, it is possible to invert the domain and the range of a relation, that is, exchange the\ninput and the output. In \ufb01nite-state computational morphology, it is", "een the input and output languages.\nIn fact, it is possible to invert the domain and the range of a relation, that is, exchange the\ninput and the output. In \ufb01nite-state computational morphology, it is common to refer to the\ninput word forms as surface strings and to the output descriptions as lexical strings, if\nthe transducer is used for morphological analysis, or vice versa, if it is used for morphological\ngeneration.\nThe linguistic descriptions we would like to give to the word forms and their components\ncan be rather arbitrary and are obviously dependent on the language processed as well as\non the morphological theory followed. In English, a \ufb01nite-state transducer could analyze the\nsurface string children into the lexical string child [+plural], for instance, or generate women\nfrom woman [+plural]. For other examples of possible input and output strings, consider\nExample 1\u20138 or Figure 1\u20131.\nRelations on languages can also be viewed as functions. Let us have a relation R, and\nlet us ", "an [+plural]. For other examples of possible input and output strings, consider\nExample 1\u20138 or Figure 1\u20131.\nRelations on languages can also be viewed as functions. Let us have a relation R, and\nlet us denote by [\u03a3] the set of all sequences over some set of symbols \u03a3, so that the domain\nand the range of R are subsets of [\u03a3]. We can then consider R as a function mapping an\ninput string into a set of output strings, formally denoted by this type signature, where [\u03a3]\nequals String:\nR :: [\u03a3] \u2192{[\u03a3]}\nR :: String \u2192{String}\n(1.1)\nFinite-state transducers have been studied extensively for their formal algebraic proper-\nties and have proven to be suitable models for miscellaneous problems [9]. Their applications\nencoding the surface rather than lexical string associations as rewrite rules of phonology\nand morphology have been around since the two-level morphology model [39], further pre-\nsented in Computational Approaches to Morphology and Syntax [11] and Morphology and\nComputation [40].\nMorpholog", "\nand morphology have been around since the two-level morphology model [39], further pre-\nsented in Computational Approaches to Morphology and Syntax [11] and Morphology and\nComputation [40].\nMorphological operations and processes in human languages can, in the overwhelming\nnumber of cases and to a su\ufb03cient degree, be expressed in \ufb01nite-state terms. Beesley and\nKarttunen [9] stress concatenation of transducers as the method for factoring surface and\nlexical languages into simpler models and propose a somewhat unsystematic compile-\nreplace transducer operation for handling nonconcatenative phenomena in morphology.\nRoark and Sproat [11], however, argue that building morphological models in general using\ntransducer composition, which is pure, is a more universal approach.\nA theoretical limitation of \ufb01nite-state models of morphology is the problem of capturing\nreduplication of words or their elements (e.g., to express plurality) found in several human\nlanguages. A formal language that conta", "ion of \ufb01nite-state models of morphology is the problem of capturing\nreduplication of words or their elements (e.g., to express plurality) found in several human\nlanguages. A formal language that contains only words of the form \u03bb1+k, where \u03bb is some\narbitrary sequence of symbols from an alphabet and k \u2208{1, 2, . . . } is an arbitrary natural\nnumber indicating how many times \u03bb is repeated after itself, is not a regular language, not\neven a context-free language. General reduplication of strings of unbounded length is thus\nnot a regular-language operation. Coping with this problem in the framework of \ufb01nite-state\ntransducers is discussed by Roark and Sproat [11].\n6. Regular relations and regular languages are restricted in their structure by the limited memory of the\ndevice (i.e., the \ufb01nite set of con\ufb01gurations in which it can occur). Unlike with regular languages, intersection\nof regular relations can in general yield nonregular results [38].\n\n18\nChapter 1\nFinding the Structure of Words\nFi", " of con\ufb01gurations in which it can occur). Unlike with regular languages, intersection\nof regular relations can in general yield nonregular results [38].\n\n18\nChapter 1\nFinding the Structure of Words\nFinite-state technology can be applied to the morphological modeling of isolating and\nagglutinative languages in a quite straightforward manner. Korean \ufb01nite-state models are\ndiscussed by Kim et al. [41], Lee and Rim [42], and Han [43], to mention a few. For treat-\nments of nonconcatenative morphology using \ufb01nite-state frameworks, see especially Kay [44],\nBeesley [45], Kiraz [46], and Habash, Rambow, and Kiraz [47]. For comparison with \ufb01nite-\nstate models of the rich morphology of Czech, compare Skoumalov\u00b4a [48] and Sedl\u00b4a\u02c6cek and\nSmr\u02c7z [49].\nImplementing a re\ufb01ned \ufb01nite-state morphological model requires careful \ufb01ne-tuning of\nits lexicons, rewrite rules, and other components, while extending the code can lead to\nunexpected interactions in it, as noted by Oazer [50]. Convenient speci\ufb01cation l", " requires careful \ufb01ne-tuning of\nits lexicons, rewrite rules, and other components, while extending the code can lead to\nunexpected interactions in it, as noted by Oazer [50]. Convenient speci\ufb01cation languages\nlike those mentioned previously are needed because encoding the \ufb01nite-state transducers\ndirectly would be extremely arduous, error prone, and unintelligible.\nFinite-state tools are available in most general-purpose programming languages in the\nform of support for regular expression matching and substitution. While these may not\nbe the ultimate choice for building full-\ufb02edged morphological analyzers or generators of a\nnatural language, they are very suitable for developing tokenizers and morphological guessers\ncapable of suggesting at least some structure for words that are formed correctly but cannot\nbe identi\ufb01ed with concrete lexemes during full morphological parsing [9].\n1.3.3\nUni\ufb01cation-Based Morphology\nUni\ufb01cation-based approaches to morphology have been inspired by advances in", "ectly but cannot\nbe identi\ufb01ed with concrete lexemes during full morphological parsing [9].\n1.3.3\nUni\ufb01cation-Based Morphology\nUni\ufb01cation-based approaches to morphology have been inspired by advances in various for-\nmal linguistic frameworks aiming at enabling complete grammatical descriptions of human\nlanguages, especially head-driven phrase structure grammar (HPSG) [51], and by develop-\nment of languages for lexical knowledge representation, especially DATR [52]. The concepts\nand methods of these formalisms are often closely connected to those of logic programming.\nIn the excellent thesis by Erjavec [53], the scienti\ufb01c context is discussed extensively and\nprofoundly; refer also to the monographs by Carpenter [54] and Shieber [55].\nIn \ufb01nite-state morphological models, both surface and lexical forms are by themselves\nunstructured strings of atomic symbols. In higher-level approaches, linguistic information is\nexpressed by more appropriate data structures that can include complex values o", "al forms are by themselves\nunstructured strings of atomic symbols. In higher-level approaches, linguistic information is\nexpressed by more appropriate data structures that can include complex values or can be\nrecursively nested if needed. Morphological parsing P thus associates linear forms \u03c6 with\nalternatives of structured content \u03c8, cf. (1.1):\nP :: \u03c6 \u2192{\u03c8}\nP :: form \u2192{content}\n(1.2)\nErjavec [53] argues that for morphological modeling, word forms are best captured by\nregular expressions, while the linguistic content is best described through typed feature\nstructures. Feature structures can be viewed as directed acyclic graphs. A node in a feature\nstructure comprises a set of attributes whose values can be feature structures again. Nodes\nare associated with types, and atomic values are attributeless nodes distinguished by their\ntype. Instead of unique instances of values everywhere, references can be used to establish\nvalue instance identity. Feature structures are usually displayed as ", "ibuteless nodes distinguished by their\ntype. Instead of unique instances of values everywhere, references can be used to establish\nvalue instance identity. Feature structures are usually displayed as attribute-value matrices\nor as nested symbolic expressions.\nUni\ufb01cation is the key operation by which feature structures can be merged into a more\ninformative feature structure. Uni\ufb01cation of feature structures can also fail, which means\n\n1.3\nMorphological Models\n19\nthat the information in them is mutually incompatible. Depending on the \ufb02avor of the\nprocessing logic, uni\ufb01cation can be monotonic (i.e., information-preserving), or it can allow\ninheritance of default values and their overriding. In either case, information in a model can\nbe e\ufb03ciently shared and reused by means of inheritance hierarchies de\ufb01ned on the feature\nstructure types.\nMorphological models of this kind are typically formulated as logic programs, and uni\ufb01-\ncation is used to solve the system of constraints imposed by the m", "rchies de\ufb01ned on the feature\nstructure types.\nMorphological models of this kind are typically formulated as logic programs, and uni\ufb01-\ncation is used to solve the system of constraints imposed by the model. Advantages of this\napproach include better abstraction possibilities for developing a morphological grammar as\nwell as elimination of redundant information from it.\nHowever, morphological models implemented in DATR can, under certain assumptions,\nbe converted to \ufb01nite-state machines and are thus formally equivalent to them in the range\nof morphological phenomena they can describe [11]. Interestingly, one-level phonology [56]\nformulating phonological constraints as logic expressions can be compiled into \ufb01nite-state\nautomata, which can then be intersected with morphological transducers to exclude any\ndisturbing phonologically invalid surface strings [cf. 57, 53]\nUni\ufb01cation-based models have been implemented for Russian [58], Czech [59], Slovene\n[53], Persian [60], Hebrew [61], Arabic [", "exclude any\ndisturbing phonologically invalid surface strings [cf. 57, 53]\nUni\ufb01cation-based models have been implemented for Russian [58], Czech [59], Slovene\n[53], Persian [60], Hebrew [61], Arabic [62, 63], and other languages. Some rely on DATR;\nsome adopt, adapt, or develop other uni\ufb01cation engines.\n1.3.4\nFunctional Morphology\nThis group of morphological models includes not only the ones following the methodology\nof functional morphology [64], but even those related to it, such as morphological resource\ngrammars of Grammatical Framework [65]. Functional morphology de\ufb01nes its models using\nprinciples of functional programming and type theory. It treats morphological operations\nand processes as pure mathematical functions and organizes the linguistic as well as abstract\nelements of a model into distinct types of values and type classes.\nThough functional morphology is not limited to modeling particular types of mor-\nphologies in human languages, it is especially useful for fusional mo", "l into distinct types of values and type classes.\nThough functional morphology is not limited to modeling particular types of mor-\nphologies in human languages, it is especially useful for fusional morphologies. Linguistic\nnotions like paradigms, rules and exceptions, grammatical categories and parameters, lex-\nemes, morphemes, and morphs can be represented intuitively and succinctly in this ap-\nproach. Designing a morphological system in an accurate and elegant way is encouraged by\nthe computational setting, which supports logical decoupling of subproblems and reinforces\nthe semantic structure of a program by strong type checking.\nFunctional morphology implementations are intended to be reused as programming\nlibraries capable of handling the complete morphology of a language and to be incorporated\ninto various kinds of applications. Morphological parsing is just one usage of the system,\nthe others being morphological generation, lexicon browsing, and so on. Next to parsing\n(1.2), we c", "orporated\ninto various kinds of applications. Morphological parsing is just one usage of the system,\nthe others being morphological generation, lexicon browsing, and so on. Next to parsing\n(1.2), we can describe in\ufb02ection I, derivation D, and lookup L as functions of these generic\ntypes:\nI :: lexeme \u2192{parameter} \u2192{form}\n(1.3)\nD :: lexeme \u2192{parameter} \u2192{lexeme}\n(1.4)\nL :: content \u2192{lexeme}\n(1.5)\n\n20\nChapter 1\nFinding the Structure of Words\nA functional morphology model can be compiled into \ufb01nite-state transducers if needed,\nbut can also be used interactively in an interpreted mode, for instance. Computation within\na model may exploit lazy evaluation and employ alternative methods of e\ufb03cient parsing,\nlookup, and so on [see 66, 12].\nMany functional morphology implementations are embedded in a general-purpose pro-\ngramming language, which gives programmers more freedom with advanced programming\ntechniques and allows them to develop full-featured, real-world applications for their mod-\nels.", " general-purpose pro-\ngramming language, which gives programmers more freedom with advanced programming\ntechniques and allows them to develop full-featured, real-world applications for their mod-\nels. The Zen toolkit for Sanskrit morphology [67, 68] is written in OCaml. It in\ufb02uenced\nthe functional morphology framework [64] in Haskell, with which morphologies of Latin,\nSwedish, Spanish, Urdu [69], and other languages have been implemented.\nIn Haskell, in particular, developers can take advantage of its syntactic \ufb02exibility and\ndesign their own notation for the functional constructs that model the given problem. The\nnotation then constitutes a so-called domain-speci\ufb01c embedded language, which makes pro-\ngramming even more fun. Figure 1\u20132 illustrates how the ElixirFM implementation of Ara-\nbic morphology [12, 17] captures the structure of words and de\ufb01nes the lexicon. Despite\nthe entries being most informative, their format is simply similar to that found in printed\ndictionaries. Operator", "c morphology [12, 17] captures the structure of words and de\ufb01nes the lexicon. Despite\nthe entries being most informative, their format is simply similar to that found in printed\ndictionaries. Operators like >|, |<, |<< and labels like verb are just in\ufb01x functions; patterns\nand a\ufb03xes like FaCY, FCI, At are data constructors.\n|> \u201dd r\ny\u201d <|\n[\nFaCY\n\u2018 verb \u2018\n[\n\u201dknow \u201d , \u201d n o t i c e \u201d\n]\n\u2018 imperf \u2018\nFCI ,\nFACY\n\u2018 verb \u2018\n[\n\u201d f l a t t e r \u201d , \u201d d e c e i v e \u201d\n] ,\nHaFCY\n\u2018 verb \u2018\n[\n\u201d inform \u201d , \u201d l e t\nknow\u201d\n] ,\nlA >| \u201d \u2019 a\u201d >>| FCI |<< \u201d I y \u201d\n\u2018 adj \u2018\n[\n\u201d a g n o s t i c \u201d\n] ,\nFiCAL |< aT\n\u2018 noun \u2018\n[\n\u201d knowledge \u201d , \u201dknowing\u201d\n] ,\nMuFACY |< aT\n\u2018 noun \u2018\n[\n\u201d f l a t t e r y \u201d\n]\n\u2018 p l u r a l \u2018\nMuFACY |< At ,\nFACI\n\u2018 adj \u2018\n[\n\u201daware \u201d , \u201dknowing\u201d\n]\n]\nd r y \u001f\u001b \u0018\u001e\nfa\u0003\u00afa\nf\u0003\u00af\u0131\nf\u00afa\u0003\u00afa\n\u0002af\u0003\u00afa\nl\u00afa-\u0002a-f\u0003\u00af\u0131-\u00af\u0131y\nfi\u0003\u00afal-ah\nmuf\u00afa\u0003\u00afa-ah\nmuf\u00afa\u0003\u00afa-\u00afat\nf\u00afa\u0003\u00af\u0131\nknow, notice\nI (i)\ndar\u00afa \u001f\u0018\u001e\n\ufb02atter, deceive\nIII\nd\u00afar\u00afa \u001f\u0018\u0006\u001e\ninform, let know\nIV\n\u0002adr\u00afa \u001f\u0018\u001e\u0004\u0006\nagnostic\nl\u00afa-\u0002adr\u00af\u0131y \u001f\u001b \u0018\u001e\u0004\u0006-\nknowledge, knowing\ndir\u00afayah \u000e\u001a\u0005\u001b\u0006\u0018\u001e", "i\u0003\u00afal-ah\nmuf\u00afa\u0003\u00afa-ah\nmuf\u00afa\u0003\u00afa-\u00afat\nf\u00afa\u0003\u00af\u0131\nknow, notice\nI (i)\ndar\u00afa \u001f\u0018\u001e\n\ufb02atter, deceive\nIII\nd\u00afar\u00afa \u001f\u0018\u0006\u001e\ninform, let know\nIV\n\u0002adr\u00afa \u001f\u0018\u001e\u0004\u0006\nagnostic\nl\u00afa-\u0002adr\u00af\u0131y \u001f\u001b \u0018\u001e\u0004\u0006-\nknowledge, knowing\ndir\u00afayah \u000e\u001a\u0005\u001b\u0006\u0018\u001e\n\ufb02attery\nmud\u00afar\u00afah \u000e\u000b\u0006\u0018\u0006\u0003%\n(mud\u00afaray\u00afat \u000e/\u0013\u0005\u001b\u0018\u0006\u0003%)\naware, knowing\nd\u00afarin \u0018\u0006\u001e\nFigure 1\u20132: Excerpt from the ElixirFM lexicon and a layout generated from it. The source code of\nentries nested under the d r y root is shown in monospace font. Note the custom notation and the\neconomy yet informativeness of the declaration\n\n1.3\nMorphological Models\n21\nEven without the options provided by general-purpose programming languages, func-\ntional morphology models achieve high levels of abstraction. Morphological grammars in\nGrammatical Framework [65] can be extended with descriptions of the syntax and seman-\ntics of a language. Grammatical Framework itself supports multilinguality, and models of\nmore than a dozen languages are available in it as open-source software [70, 71].\nGrammars in the OpenCCG pro", "-\ntics of a language. Grammatical Framework itself supports multilinguality, and models of\nmore than a dozen languages are available in it as open-source software [70, 71].\nGrammars in the OpenCCG project [72] can be viewed as functional models, too.\nTheir formalism discerns declarations of features, categories, and families that provide type-\nsystem-like means for representing structured values and inheritance hierarchies on them.\nThe grammars leverage heavily the functionality to de\ufb01ne parametrized macros to mini-\nmize redundancy in the model and make required generalizations. Expansion of macros in\nthe source code has e\ufb00ects similar to inlining of functions. The original text of the gram-\nmar is reduced to associations between word forms and their morphosyntactic and lexical\nproperties.\n1.3.5\nMorphology Induction\nWe have focused on \ufb01nding the structure of words in diverse languages supposing we know\nwhat we are looking for. We have not considered the problem of discovering and induc", "\n1.3.5\nMorphology Induction\nWe have focused on \ufb01nding the structure of words in diverse languages supposing we know\nwhat we are looking for. We have not considered the problem of discovering and induc-\ning word structure without the human insight (i.e., in an unsupervised or semi-supervised\nmanner). The motivation for such approaches lies in the fact that for many languages,\nlinguistic expertise might be unavailable or limited, and implementations adequate to a\npurpose may not exist at all. Automated acquisition of morphological and lexical infor-\nmation, even if not perfect, can be reused for bootstrapping and improving the classical\nmorphological models, too.\nLet us skim over the directions of research in this domain. In the studies by\nHammarstr\u00a8om [73] and Goldsmith [74], the literature on unsupervised learning of mor-\nphology is reviewed in detail. Hammarstr\u00a8om divides the numerous approaches into three\nmain groups. Some works compare and cluster words based on their similarity acc", " on unsupervised learning of mor-\nphology is reviewed in detail. Hammarstr\u00a8om divides the numerous approaches into three\nmain groups. Some works compare and cluster words based on their similarity according to\nmiscellaneous metrics [75, 76, 77, 78]; others try to identify the prominent features of word\nforms distinguishing them from the unrelated ones. Most of the published approaches cast\nmorphology induction as the problem of word boundary and morpheme boundary detection,\nsometimes acquiring also lexicons and paradigms [79, 80, 81, 82, 83].7\nThere are several challenging issues about deducing word structure just from the forms\nand their context. They are caused by ambiguity [76] and irregularity [75] in morphology,\nas well as by orthographic and phonological alternations [85] and nonlinear morphological\nprocesses [86, 87].\nIn order to improve the chances of statistical inference, parallel learning of morphologies\nfor multiple languages is proposed by Snyder and Barzilay [88], resulti", "ar morphological\nprocesses [86, 87].\nIn order to improve the chances of statistical inference, parallel learning of morphologies\nfor multiple languages is proposed by Snyder and Barzilay [88], resulting in discovery of\nabstract morphemes. The discriminative log-linear model of Poon, Cherry, and Toutanova\n[89] enhances its generalization options by employing overlapping contextual features when\nmaking segmentation decisions [cf. 90].\n7. Compare these with a semisupervised approach to word hyphenation [84].\n\n22\nChapter 1\nFinding the Structure of Words\n1.4\nSummary\nIn this chapter, we learned that morphology can be looked at from opposing viewpoints:\none that tries to \ufb01nd the structural components from which words are built versus a more\nsyntax-driven perspective wherein the functions of words are the focus of the study. Another\ndistinction can be made between analytic and generative aspects of morphology or can\nconsider man-made morphological frameworks versus systems for unsupervised ind", " are the focus of the study. Another\ndistinction can be made between analytic and generative aspects of morphology or can\nconsider man-made morphological frameworks versus systems for unsupervised induction\nof morphology. Yet other kinds of issues are raised about how well and how easily the\nmorphological models can be implemented.\nWe described morphological parsing as the formal process recovering structured infor-\nmation from a linear sequence of symbols, where ambiguity is present and where multiple\ninterpretations should be expected.\nWe explored interesting morphological phenomena in di\ufb00erent types of languages and\nmentioned several hints in respect to multilingual processing and model development.\nWith Korean as a language where agglutination moderated by phonological rules is the\ndominant morphological process, we saw that a viable model of word decomposition can\nwork at the morphemes level, regardless of whether they are lexical or grammatical.\nIn Czech and Arabic as fusional la", "inant morphological process, we saw that a viable model of word decomposition can\nwork at the morphemes level, regardless of whether they are lexical or grammatical.\nIn Czech and Arabic as fusional languages with intricate systems of in\ufb02ectional and\nderivational parameters and lexically dependent word stem variation, such factorization is\nnot useful. Morphology is better described via paradigms associating the possible forms of\nlexemes with their corresponding properties.\nWe discussed various options for implementing either of these models using modern\nprogramming techniques.\nAcknowledgment\nWe would like to thank Petr Nov\u00b4ak for his valuable comments on an earlier draft of this\nchapter.\nBibliography\n[1] M. Liberman, \u201cMorphology.\u201d Linguistics 001, Lecture 7, University of Pennsylvania,\n2009. http://www.ling.upenn.edu/courses/Fall 2009/ling001/morphology.html.\n[2] M. Haspelmath, \u201cThe indeterminacy of word segmentation and the nature of mor-\nphology and syntax,\u201d Folia Linguistica, vol. 45", ". http://www.ling.upenn.edu/courses/Fall 2009/ling001/morphology.html.\n[2] M. Haspelmath, \u201cThe indeterminacy of word segmentation and the nature of mor-\nphology and syntax,\u201d Folia Linguistica, vol. 45, 2011.\n[3] H. Ku\u02c7cera and W. N. Francis, Computational Analysis of Present-Day American\nEnglish. Providence, RI: Brown University Press, 1967.\n[4] S. B. Cohen and N. A. Smith, \u201cJoint morphological and syntactic disambiguation,\u201d\nin Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Lan-\nguage Processing and Computational Natural Language Learning (EMNLP-CoNLL),\npp. 208\u2013217, 2007.\n\nBibliography\n23\n[5] T. Nakagawa, \u201cChinese and Japanese word segmentation using word-level and\ncharacter-level information,\u201d in Proceedings of 20th International Conference on Com-\nputational Linguistics, pp. 466\u2013472, 2004.\n[6] H. Shin and H. You, \u201cHybrid n-gram probability estimation in morphologically rich\nlanguages,\u201d in Proceedings of the 23rd Paci\ufb01c Asia Conference on Language, Infor-\nmat", "guistics, pp. 466\u2013472, 2004.\n[6] H. Shin and H. You, \u201cHybrid n-gram probability estimation in morphologically rich\nlanguages,\u201d in Proceedings of the 23rd Paci\ufb01c Asia Conference on Language, Infor-\nmation and Computation, 2009.\n[7] D. Z. Hakkani-T\u00a8ur, K. O\ufb02azer, and G. T\u00a8ur, \u201cStatistical morphological disambiguation\nfor agglutinative languages,\u201d in Proceedings of the 18th Conference on Computational\nLinguistics, pp. 285\u2013291, 2000.\n[8] G. T. Stump, In\ufb02ectional Morphology: A Theory of Paradigm Structure. Cambridge\nStudies in Linguistics, New York: Cambridge University Press, 2001.\n[9] K. R. Beesley and L. Karttunen, Finite State Morphology. CSLI Studies in Compu-\ntational Linguistics, Stanford, CA: CSLI Publications, 2003.\n[10] M. Baerman, D. Brown, and G. G. Corbett, The Syntax-Morphology Interface. A Study\nof Syncretism. Cambridge Studies in Linguistics, New York: Cambridge University\nPress, 2006.\n[11] B. Roark and R. Sproat, Computational Approaches to Morphology and Syntax. Oxford\nSur", "terface. A Study\nof Syncretism. Cambridge Studies in Linguistics, New York: Cambridge University\nPress, 2006.\n[11] B. Roark and R. Sproat, Computational Approaches to Morphology and Syntax. Oxford\nSurveys in Syntax and Morphology, New York: Oxford University Press, 2007.\n[12] O. Smr\u02c7z, \u201cFunctional Arabic morphology. Formal system and implementation,\u201d PhD\nthesis, Charles University in Prague, 2007.\n[13] H. Eifring and R. Theil, Linguistics for Students of Asian and African Languages.\nUniversitetet i Oslo, 2005.\n[14] B. Bickel and J. Nichols, \u201cFusion of selected in\ufb02ectional formatives & exponence of\nselected in\ufb02ectional formatives,\u201d in The World Atlas of Language Structures Online\n(M. Haspelmath, M. S. Dryer, D. Gil, and B. Comrie, eds.), ch. 20 & 21, Munich: Max\nPlanck Digital Library, 2008.\n[15] W. Fischer, A Grammar of Classical Arabic. Trans. Jonathan Rodgers. Yale Language\nSeries, New Haven, CT: Yale University Press, 2002.\n[16] K. C. Ryding, A Reference Grammar of Modern Standard A", ".\n[15] W. Fischer, A Grammar of Classical Arabic. Trans. Jonathan Rodgers. Yale Language\nSeries, New Haven, CT: Yale University Press, 2002.\n[16] K. C. Ryding, A Reference Grammar of Modern Standard Arabic. New York: Cam-\nbridge University Press, 2005.\n[17] O. Smr\u02c7z and V. Bielick\u00b4y, \u201cElixirFM.\u201d Functional Arabic Morphology, SourceForge.net,\n2010. http://sourceforge.net/projects/elixer-fm/.\n[18] T. Kamei, R. K\u00afono, and E. Chino, eds., The Sanseido Encyclopedia of Linguistics,\nVolume 6 Terms (in Japanese). Sanseido, 1996.\n[19] F. Karlsson, Finnish Grammar. Helsinki: Werner S\u00a8oderstr\u00a8om Osakenyhti\u00a8o, 1987.\n[20] J. Haji\u02c7c and B. Hladk\u00b4a, \u201cTagging in\ufb02ective languages: Prediction of morphological cat-\negories for a rich, structured tagset,\u201d in Proceedings of COLING-ACL 1998, pp. 483\u2013\n490, 1998.\n\n24\nChapter 1\nFinding the Structure of Words\n[21] J. Haji\u02c7c, \u201cMorphological tagging: Data vs. dictionaries,\u201d in Proceedings of NAACL-\nANLP 2000, pp. 94\u2013101, 2000.\n[22] N. Habash and O. Rambow, \u201cArabi", "\n\n24\nChapter 1\nFinding the Structure of Words\n[21] J. Haji\u02c7c, \u201cMorphological tagging: Data vs. dictionaries,\u201d in Proceedings of NAACL-\nANLP 2000, pp. 94\u2013101, 2000.\n[22] N. Habash and O. Rambow, \u201cArabic tokenization, part-of-speech tagging and mor-\nphological disambiguation in one fell swoop,\u201d in Proceedings of the 43rd Annual\nMeeting of the Association for Computational Linguistics (ACL\u201905), pp. 573\u2013580,\n2005.\n[23] N. A. Smith, D. A. Smith, and R. W. Tromble, \u201cContext-based morphological dis-\nambiguation with random \ufb01elds,\u201d in Proceedings of HLT/EMNLP 2005, pp. 475\u2013482,\n2005.\n[24] J. Haji\u02c7c, O. Smr\u02c7z, T. Buckwalter, and H. Jin, \u201cFeature-based tagger of approximations\nof functional Arabic morphology,\u201d in Proceedings of the 4th Workshop on Treebanks\nand Linguistic Theories (TLT 2005), pp. 53\u201364, 2005.\n[25] T. Buckwalter, \u201cIssues in Arabic orthography and morphology analysis,\u201d in COLING\n2004 Computational Approaches to Arabic Script-based Languages, pp. 31\u201334, 2004.\n[26] R. Nelken and S. ", "\u201364, 2005.\n[25] T. Buckwalter, \u201cIssues in Arabic orthography and morphology analysis,\u201d in COLING\n2004 Computational Approaches to Arabic Script-based Languages, pp. 31\u201334, 2004.\n[26] R. Nelken and S. M. Shieber, \u201cArabic diacritization using \ufb01nite-state transducers,\u201d\nin Proceedings of the ACL Workshop on Computational Approaches to Semitic Lan-\nguages, pp. 79\u201386, 2005.\n[27] I. Zitouni, J. S. Sorensen, and R. Sarikaya, \u201cMaximum entropy based restoration of\nArabic diacritics,\u201d in Proceedings of the 21st International Conference on Compu-\ntational Linguistics and 44th Annual Meeting of the Association for Computational\nLinguistics, pp. 577\u2013584, 2006.\n[28] N. Habash and O. Rambow, \u201cArabic diacritization through full morphological tag-\nging,\u201d in Human Language Technologies 2007: The Conference of the North American\nChapter of the Association for Computational Linguistics; Companion Volume, Short\nPapers, pp. 53\u201356, 2007.\n[29] G. Huet, \u201cLexicon-directed segmentation and tagging of Sanskrit,\u201d i", "f the North American\nChapter of the Association for Computational Linguistics; Companion Volume, Short\nPapers, pp. 53\u201356, 2007.\n[29] G. Huet, \u201cLexicon-directed segmentation and tagging of Sanskrit,\u201d in Proceedings of\nthe XIIth World Sanskrit Conference, pp. 307\u2013325, 2003.\n[30] G. Huet, \u201cFormal structure of Sanskrit text: Requirements analysis for a mechanical\nSanskrit processor,\u201d in Sanskrit Computational Linguistics: First and Second Inter-\nnational Symposia (G. Huet, A. Kulkarni, and P. Scharf, eds.), vol. 5402 of LNAI,\npp. 162\u2013199, Berlin: Springer, 2009.\n[31] F. Katamba and J. Stonham, Morphology. Basingstoke: Palgrave Macmillan, 2006.\n[32] L. Bauer, Morphological Productivity, Cambridge Studies in Linguistics. New York:\nCambridge University Press, 2001.\n[33] R. H. Baayen, Word Frequency Distributions, Text, Speech and Language Technology.\nBoston: Kluwer Academic Publishers, 2001.\n[34] A. Kilgarri\ufb00, \u201cGoogleology is bad science,\u201d Computational Linguistics, vol. 33, no. 1,\npp. 147\u201315", "cy Distributions, Text, Speech and Language Technology.\nBoston: Kluwer Academic Publishers, 2001.\n[34] A. Kilgarri\ufb00, \u201cGoogleology is bad science,\u201d Computational Linguistics, vol. 33, no. 1,\npp. 147\u2013151, 2007.\n\nBibliography\n25\n[35] H.-C. Kwon and Y.-S. Chae, \u201cA dictionary-based morphological analysis,\u201d in Proceed-\nings of Natural Language Processing Paci\ufb01c Rim Symposium, pp. 178\u2013185, 1991.\n[36] D.-B. Kim, K.-S. Choi, and K.-H. Lee, \u201cA computational model of Korean morphologi-\ncal analysis: A prediction-based approach,\u201d Journal of East Asian Linguistics, vol. 5,\nno. 2, pp. 183\u2013215, 1996.\n[37] A. Halevy, P. Norvig, and F. Pereira, \u201cThe unreasonable e\ufb00ectiveness of data,\u201d IEEE\nIntelligent Systems, vol. 24, no. 2, pp. 8\u201312, 2009.\n[38] R. M. Kaplan and M. Kay, \u201cRegular models of phonological rule systems,\u201d Computa-\ntional Linguistics, vol. 20, no. 3, pp. 331\u2013378, 1994.\n[39] K. Koskenniemi, \u201cTwo-level morphology: A general computational model for word\nform recognition and production,\u201d PhD the", "e systems,\u201d Computa-\ntional Linguistics, vol. 20, no. 3, pp. 331\u2013378, 1994.\n[39] K. Koskenniemi, \u201cTwo-level morphology: A general computational model for word\nform recognition and production,\u201d PhD thesis, University of Helsinki, 1983.\n[40] R. Sproat, Morphology and Computation. ACL\u2013MIT Press Series in Natural Language\nProcessing. Cambridge, MA: MIT Press, 1992.\n[41] D.-B. Kim, S.-J. Lee, K.-S. Choi, and G.-C. Kim, \u201cA two-level morphological analysis\nof Korean,\u201d in Proceedings of the 15th International Conference on Computational\nLinguistics, pp. 535\u2013539, 1994.\n[42] S.-Z. Lee and H.-C. Rim, \u201cKorean morphology with elementary two-level rules and\nrule features,\u201d in Proceedings of the Paci\ufb01c Association for Computational Linguistics,\npp. 182\u2013187, 1997.\n[43] N.-R. Han, \u201cKlex: A \ufb01nite-state trancducer lexicon of Korean,\u201d in Finite-state Meth-\nods and Natural Language Processing: 5th International Workshop, FSMNLP 2005,\npp. 67\u201377, Springer, 2006.\n[44] M. Kay, \u201cNonconcatenative \ufb01nite-state mor", "cducer lexicon of Korean,\u201d in Finite-state Meth-\nods and Natural Language Processing: 5th International Workshop, FSMNLP 2005,\npp. 67\u201377, Springer, 2006.\n[44] M. Kay, \u201cNonconcatenative \ufb01nite-state morphology,\u201d in Proceedings of the Third Con-\nference of the European Chapter of the ACL (EACL-87), pp. 2\u201310, ACL, 1987.\n[45] K. R. Beesley, \u201cArabic morphology using only \ufb01nite-state operations,\u201d in COLING-\nACL\u201998 Proceedings of the Workshop on Computational Approaches to Semitic lan-\nguages, pp. 50\u201357, 1998.\n[46] G. A. Kiraz, Computational Nonlinear Morphology with Emphasis on Semitic Lan-\nguages. Studies in Natural Language Processing, Cambridge: Cambridge University\nPress, 2001.\n[47] N. Habash, O. Rambow, and G. Kiraz, \u201cMorphological analysis and generation for\nArabic dialects,\u201d in Proceedings of the ACL Workshop on Computational Approaches\nto Semitic Languages, pp. 17\u201324, 2005.\n[48] H. Skoumalov\u00b4a, \u201cA Czech morphological lexicon,\u201d in Proceedings of the Third Meeting\nof the ACL Special Int", " of the ACL Workshop on Computational Approaches\nto Semitic Languages, pp. 17\u201324, 2005.\n[48] H. Skoumalov\u00b4a, \u201cA Czech morphological lexicon,\u201d in Proceedings of the Third Meeting\nof the ACL Special Interest Group in Computational Phonology, pp. 41\u201347, 1997.\n[49] R. Sedl\u00b4a\u02c7cek and P. Smr\u02c7z, \u201cA new Czech morphological analyser ajka,\u201d in Text, Speech\nand Dialogue, vol. 2166, pp. 100\u2013107, Berlin: Springer, 2001.\n\n26\nChapter 1\nFinding the Structure of Words\n[50] K. O\ufb02azer, \u201cComputational morphology.\u201d ESSLLI 2006 European Summer School in\nLogic, Language, and Information, 2006.\n[51] C. Pollard and I. A. Sag, Head-Driven Phrase Structure Grammar. Chicago: University\nof Chicago Press, 1994.\n[52] R. Evans and G. Gazdar, \u201cDATR: A language for lexical knowledge representation,\u201d\nComputational Linguistics, vol. 22, no. 2, pp. 167\u2013216, 1996.\n[53] T. Erjavec, \u201cUni\ufb01cation, inheritance, and paradigms in the morphology of natural\nlanguages,\u201d PhD thesis, University of Ljubljana, 1996.\n[54] B. Carpenter, T", "cs, vol. 22, no. 2, pp. 167\u2013216, 1996.\n[53] T. Erjavec, \u201cUni\ufb01cation, inheritance, and paradigms in the morphology of natural\nlanguages,\u201d PhD thesis, University of Ljubljana, 1996.\n[54] B. Carpenter, The Logic of Typed Feature Structures. Cambridge Tracts in Theoretical\nComputer Science 32, New York: Cambridge University Press, 1992.\n[55] S. M. Shieber, Constraint-Based Grammar Formalisms: Parsing and Type Inference\nfor Natural and Computer Languages. Cambridge, MA: MIT Press, 1992.\n[56] S. Bird and T. M. Ellison, \u201cOne-level phonology: Autosegmental representations and\nrules as \ufb01nite automata,\u201d Computational Linguistics, vol. 20, no. 1, pp. 55\u201390, 1994.\n[57] S. Bird and P. Blackburn, \u201cA logical approach to Arabic phonology,\u201d in Proceedings\nof the 5th Conference of the European Chapter of the Association for Computational\nLinguistics, pp. 89\u201394, 1991.\n[58] G. G. Corbett and N. M. Fraser, \u201cNetwork morphology: A DATR account of Russian\nnominal in\ufb02ection,\u201d Journal of Linguistics, vol. 29, p", "e Association for Computational\nLinguistics, pp. 89\u201394, 1991.\n[58] G. G. Corbett and N. M. Fraser, \u201cNetwork morphology: A DATR account of Russian\nnominal in\ufb02ection,\u201d Journal of Linguistics, vol. 29, pp. 113\u2013142, 1993.\n[59] J. Haji\u02c7c, \u201cUni\ufb01cation morphology grammar. Software system for multilanguage mor-\nphological analysis,\u201d PhD thesis, Charles University in Prague, 1994.\n[60] K. Megerdoomian, \u201cUni\ufb01cation-based Persian morphology,\u201d in Proceedings of CICLing\n2000, 2000.\n[61] R. Finkel and G. Stump, \u201cGenerating Hebrew verb morphology by default inheritance\nhierarchies,\u201d in Proceedings of the Workshop on Computational Approaches to Semitic\nLanguages, pp. 9\u201318, 2002.\n[62] S. R. Al-Najem, \u201cInheritance-based approach to Arabic verbal root-and-pattern mor-\nphology,\u201d in Arabic Computational Morphology. Knowledge-based and Empirical Meth-\nods (A. Soudi, A. van den Bosch, and G. Neumann, eds.), vol. 38, pp. 67\u201388, Berlin:\nSpringer, 2007.\n[63] S. K\u00a8opr\u00a8u and J. Miller, \u201cA uni\ufb01cation based approac", "logy. Knowledge-based and Empirical Meth-\nods (A. Soudi, A. van den Bosch, and G. Neumann, eds.), vol. 38, pp. 67\u201388, Berlin:\nSpringer, 2007.\n[63] S. K\u00a8opr\u00a8u and J. Miller, \u201cA uni\ufb01cation based approach to the morphological analy-\nsis and generation of Arabic,\u201d in CAASL-3: Third Workshop on Computational Ap-\nproaches to Arabic Script-based Languages, 2009.\n[64] M. Forsberg and A. Ranta, \u201cFunctional morphology,\u201d in Proceedings of the 9th\nACM SIGPLAN International Conference on Functional Programming, ICFP 2004,\npp. 213\u2013223, 2004.\n[65] A. Ranta, \u201cGrammatical Framework: A type-theoretical grammar formalism,\u201d Journal\nof Functional Programming, vol. 14, no. 2, pp. 145\u2013189, 2004.\n\nBibliography\n27\n[66] P. Ljungl\u00a8of, \u201cPure functional parsing. An advanced tutorial,\u201d Licenciate thesis,\nG\u00a8oteborg University & Chalmers University of Technology, 2002.\n[67] G. Huet, \u201cThe Zen computational linguistics toolkit,\u201d ESSLLI 2002 European Summer\nSchool in Logic, Language, and Information, 2002.\n[68] G. Huet,", "ersity & Chalmers University of Technology, 2002.\n[67] G. Huet, \u201cThe Zen computational linguistics toolkit,\u201d ESSLLI 2002 European Summer\nSchool in Logic, Language, and Information, 2002.\n[68] G. Huet, \u201cA functional toolkit for morphological and phonological processing,\napplication to a Sanskrit tagger,\u201d Journal of Functional Programming, vol. 15, no. 4,\npp. 573\u2013614, 2005.\n[69] M. Humayoun, H. Hammarstr\u00a8om, and A. Ranta, \u201cUrdu morphology, orthography and\nlexicon extraction,\u201d in CAASL-2: Second Workshop on Computational Approaches to\nArabic Script-based Languages, pp. 59\u201366, 2007.\n[70] A. Dada and A. Ranta, \u201cImplementing an open source Arabic resource grammar in\nGF,\u201d in Perspectives on Arabic Linguistics (M. A. Mughazy, ed.), vol. XX, pp. 209\u2013\n231, John Benjamins, 2007.\n[71] A. Ranta, \u201cGrammatical Framework.\u201d Programming Language for Multilingual Gram-\nmar Applications, http://www.grammaticalframework.org/, 2010.\n[72] J. Baldridge, S. Chatterjee, A. Palmer, and B. Wing, \u201cDotCCG and VisCC", "atical Framework.\u201d Programming Language for Multilingual Gram-\nmar Applications, http://www.grammaticalframework.org/, 2010.\n[72] J. Baldridge, S. Chatterjee, A. Palmer, and B. Wing, \u201cDotCCG and VisCCG: Wiki\nand programming paradigms for improved grammar engineering with OpenCCG,\u201d in\nProceedings of the Workshop on Grammar Engineering Across Frameworks, 2007.\n[73] H. Hammarstr\u00a8om, \u201cUnsupervised learning of morphology and the languages of the\nworld,\u201d PhD thesis, Chalmers University of Technology and University of Gothenburg,\n2009.\n[74] J. A. Goldsmith, \u201cSegmentation and morphology,\u201d in Computational Linguistics and\nNatural Language Processing Handbook (A. Clark, C. Fox, and S. Lappin, eds.),\npp. 364\u2013393, Chichester: Wiley-Blackwell, 2010.\n[75] D. Yarowsky and R. Wicentowski, \u201cMinimally supervised morphological analysis by\nmultimodal alignment,\u201d in Proceedings of the 38th Meeting of the Association for\nComputational Linguistics, pp. 207\u2013216, 2000.\n[76] P. Schone and D. Jurafsky, \u201cKnowledg", "vised morphological analysis by\nmultimodal alignment,\u201d in Proceedings of the 38th Meeting of the Association for\nComputational Linguistics, pp. 207\u2013216, 2000.\n[76] P. Schone and D. Jurafsky, \u201cKnowledge-free induction of in\ufb02ectional morphologies,\u201d\nin Proceedings of the North American Chapter of the Association for Computational\nLinguistics, pp. 183\u2013191, 2001.\n[77] S. Neuvel and S. A. Fulop, \u201cUnsupervised learning of morphology without mor-\nphemes,\u201d in Proceedings of the ACL-02 Workshop on Morphological and Phonological\nLearning, pp. 31\u201340, 2002.\n[78] N. Hathout, \u201cAcquistion of the morphological structure of the lexicon based on lexical\nsimilarity and formal analogy,\u201d in Coling 2008: Proceedings of the 3rd Textgraphs\nWorkshop on Graph-based Algorithms for Natural Language Processing, pp. 1\u20138,\n2008.\n[79] J. Goldsmith, \u201cUnsupervised learning of the morphology of a natural language,\u201d Com-\nputational Linguistics, vol. 27, no. 2, pp. 153\u2013198, 2001.\n\n28\nChapter 1\nFinding the Structure of Words", ",\n2008.\n[79] J. Goldsmith, \u201cUnsupervised learning of the morphology of a natural language,\u201d Com-\nputational Linguistics, vol. 27, no. 2, pp. 153\u2013198, 2001.\n\n28\nChapter 1\nFinding the Structure of Words\n[80] H. Johnson and J. Martin, \u201cUnsupervised learning of morphology for English and\nInuktikut,\u201d in Companion Volume of the Proceedings of the Human Language Tech-\nnologies: The Annual Conference of the North American Chapter of the Association\nfor Computational Linguistics 2003: Short Papers, pp. 43\u201345, 2003.\n[81] M. Creutz and K. Lagus, \u201cInduction of a simple morphology for highly-in\ufb02ecting\nlanguages,\u201d in Proceedings of the 7th Meeting of the ACL Special Interest Group in\nComputational Phonology, pp. 43\u201351, 2004.\n[82] M. Creutz and K. Lagus, \u201cUnsupervised models for morpheme segmentation and\nmorphology learning,\u201d ACM Transactions on Speech and Language Processing, vol. 4,\nno. 1, pp. 1\u201334, 2007.\n[83] C. Monson, J. Carbonell, A. Lavie, and L. Levin, \u201cParaMor: Minimally supervised\ninduction", "\nmorphology learning,\u201d ACM Transactions on Speech and Language Processing, vol. 4,\nno. 1, pp. 1\u201334, 2007.\n[83] C. Monson, J. Carbonell, A. Lavie, and L. Levin, \u201cParaMor: Minimally supervised\ninduction of paradigm structure and morphological analysis,\u201d in Proceedings of Ninth\nMeeting of the ACL Special Interest Group in Computational Morphology and Phonol-\nogy, pp. 117\u2013125, 2007.\n[84] F. M. Liang, \u201cWord Hy-phen-a-tion by Com-put-er,\u201d PhD thesis, Stanford University,\n1983.\n[85] V. Demberg, \u201cA language-independent unsupervised model for morphological segmen-\ntation,\u201d in Proceedings of the 45th Annual Meeting of the Association of Computational\nLinguistics, pp. 920\u2013927, 2007.\n[86] A. Clark, \u201cSupervised and unsupervised learning of Arabic morphology,\u201d in Ara-\nbic Computational Morphology. Knowledge-based and Empirical Methods (A. Soudi,\nA. van den Bosch, and G. Neumann, eds.), vol. 38, pp. 181\u2013200, Berlin: Springer, 2007.\n[87] A. Xanthos, Apprentissage automatique de la morphologie: le cas ", "edge-based and Empirical Methods (A. Soudi,\nA. van den Bosch, and G. Neumann, eds.), vol. 38, pp. 181\u2013200, Berlin: Springer, 2007.\n[87] A. Xanthos, Apprentissage automatique de la morphologie: le cas des structures racine-\nsch`eme. Sciences pour la communication, Bern: Peter Lang, 2008.\n[88] B. Snyder and R. Barzilay, \u201cUnsupervised multilingual learning for morphological\nsegmentation,\u201d in Proceedings of ACL-08: HLT, pp. 737\u2013745, 2008.\n[89] H. Poon, C. Cherry, and K. Toutanova, \u201cUnsupervised morphological segmentation\nwith log-linear models,\u201d in Proceedings of Human Language Technologies: Annual Con-\nference of the North American Chapter of the Association for Computational Linguis-\ntics, pp. 209\u2013217, 2009.\n[90] S. Della Pietra, V. Della Pietra, and J. La\ufb00erty, \u201cInducing features of random \ufb01elds,\u201d\nIEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 19, no. 4,\npp. 380\u2013393, 1997.\n\nIndex\n. (period), sentence segmentation markers, 30\n\u201c\u201d (Quotation marks), sentence segmentat", "\ufb01elds,\u201d\nIEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 19, no. 4,\npp. 380\u2013393, 1997.\n\nIndex\n. (period), sentence segmentation markers, 30\n\u201c\u201d (Quotation marks), sentence segmentation\nmarkers, 30\n! (Exclamation point), as sentence\nsegmentation marker, 30\n? (Question mark), sentence segmentation\nmarkers, 30\n80/20 rule (vital few), 14\na priori models, in document retrieval, 377\nAbbreviations, punctuation marks in, 30\nAbsity parser, rule-based semantic parsing,\n122\nAbstracts\nin automatic summarization, 397\nde\ufb01ned, 400\nAccumulative vector space model, for\ndocument retrieval, 374\u2013375\nAccuracy, in QA, 462\nACE. See Automatic content extraction\n(ACE)\nAcquis corpus\nfor evaluating IR systems, 390\nfor machine translation, 358\nAdequacy, of translation, 334\nAdjunctive arguments, PropBank verb\npredicates, 119\u2013120\nAER (Alignment-error rate), 343\nAEs (Analysis engines), UIMA, 527\nAgglutinative languages\n\ufb01nite-state technology applied to, 18\nlinear decomposition of words, 192\nmorpho", "pBank verb\npredicates, 119\u2013120\nAER (Alignment-error rate), 343\nAEs (Analysis engines), UIMA, 527\nAgglutinative languages\n\ufb01nite-state technology applied to, 18\nlinear decomposition of words, 192\nmorphological typology and, 7\nparsing issues related to morphology, 90\u201391\nAggregate processor, combining NLP engines,\n523\nAggregation architectures, for NLP. See also\nNatural language processing (NLP),\ncombining engines for\nGATE, 529\u2013530\nInfoSphere Streams, 530\u2013531\noverview of, 527\nUIMA, 527\u2013529\nAggregation models, for MLIR, 385\nAgreement feature, of coreference models, 301\nAir Travel Information System (ATIS)\nas resource for meaning representation, 148\nrule-based systems for semantic parsing,\n150\nsupervised systems for semantic parsing,\n150\u2013151\nAlgorithms. See by individual types\nAlignment-error rate (AER), 343\nAlignment, in RTE\nimplementing, 233\u2013236\nlatent alignment inference, 247\u2013248\nlearning alignment independently of\nentailment, 244\u2013245\nleveraging multiple alignments, 245\nmodeling, 226\nAllo", "te (AER), 343\nAlignment, in RTE\nimplementing, 233\u2013236\nlatent alignment inference, 247\u2013248\nlearning alignment independently of\nentailment, 244\u2013245\nleveraging multiple alignments, 245\nmodeling, 226\nAllomorphs, 6\n\u201calmost-parsing\u201d language model, 181\nAmbiguity\ndisambiguation problem in morphology, 91\nin interpretation of expressions, 10\u201313\nissues with morphology induction, 21\nPCFGs and, 80\u201383\nresolution in parsing, 80\nsentence segmentation markers and, 30\nstructural, 99\nin syntactic analysis, 61\ntypes of, 8\nword sense and. See Disambiguation\nsystems, word sense\nAnalysis engines (AEs), UIMA, 527\nAnalysis, in RTE framework\nannotators, 219\nimproving, 248\u2013249\nmultiview representation of, 220\u2013222\noverview of, 220\nAnalysis stage, of summarization system\nbuilding a summarization system and, 421\noverview of, 400\n551\n\n552\nIndex\nAnaphora resolution. See also Coreference\nresolution\nautomatic summarization and, 398\ncohesion of, 401\nmultilingual automatic summarization and,\n410\nQA architectures and, 43", "iew of, 400\n551\n\n552\nIndex\nAnaphora resolution. See also Coreference\nresolution\nautomatic summarization and, 398\ncohesion of, 401\nmultilingual automatic summarization and,\n410\nQA architectures and, 438\u2013439\nzero anaphora resolution, 249, 444\nAnchored speech recognition, 490\nAnchors, in SSTK, 246\nAnnotation/annotation guidelines\nentity detection and, 293\nin GALE, 478\nPenn Treebank and, 87\u201388\nphrase structure trees and, 68\u201369\nQA architectures and, 439\u2013440\nin RTE, 219, 222\u2013224\nsnippet processing and, 485\nfor treebanks, 62\nof utterances based on rule-based\ngrammars, 502\u2013503\nof utterances in spoken dialog systems, 513\nAnswers, in QA\ncandidate answer extraction. See Candidate\nanswer extraction, in QA\ncandidate answer generation. See\nCandidate answer generation, in QA\nevaluating correctness of, 461\u2013462\nscores for, 450\u2013453, 458\u2013459\nscoring component for, 435\ntype classi\ufb01cation of, 440\u2013442\nArabic\nambiguity in, 11\u201312\ncorpora for relation extraction, 317\ndistillation, 479, 490\u2013491\nEDT and, 286\nEli", "1\u2013462\nscores for, 450\u2013453, 458\u2013459\nscoring component for, 435\ntype classi\ufb01cation of, 440\u2013442\nArabic\nambiguity in, 11\u201312\ncorpora for relation extraction, 317\ndistillation, 479, 490\u2013491\nEDT and, 286\nElixirFM lexicon, 20\nencoding and script, 368\nEnglish-to-Arabic machine translation, 114\nas fusional language, 8\nGALE IOD and, 532, 534\u2013536\nIR and, 371\nirregularity in, 8\u20139\nlanguage modeling, 189\u2013191, 193\nmention detection experiments, 294\u2013296\nmorphemes in, 6\nmorphological analysis of, 191\nmultilingual issues in predicate-argument\nstructures, 146\u2013147\npolarity analysis of words and phrases, 269\nproductivity/creativity in, 15\nregional dialects not in written form, 195\nRTE in, 218\nstem-matching features for capturing\nmorphological similarities, 301\nTALES case study, 538\ntokens in, 4\ntranslingual summarization, 398\u2013399,\n424\u2013426\nuni\ufb01cation-based models, 19\nArchitectures\naggregation architectures for NLP, 527\u2013529\nfor question answering (QA), 435\u2013437\nof spoken dialog systems, 505\nsystem architecture", "marization, 398\u2013399,\n424\u2013426\nuni\ufb01cation-based models, 19\nArchitectures\naggregation architectures for NLP, 527\u2013529\nfor question answering (QA), 435\u2013437\nof spoken dialog systems, 505\nsystem architectures for distillation, 488\nsystem architectures for semantic parsing,\n101\u2013102\ntypes of EDT architectures, 286\u2013287\nArguments\nconsistency of argument identi\ufb01cation, 323\nevent extraction and, 321\u2013322\nin GALE distillation initiative, 475\nin RTE systems, 220\nArguments, predicate-argument recognition\nargument sequence information, 137\u2013138\nclassi\ufb01cation and identi\ufb01cation, 139\u2013140\ncore and adjunctive, 119\ndisallowing overlaps, 137\ndiscontiguous, 121\nidenti\ufb01cation and classi\ufb01cation, 123\nnoun arguments, 144\u2013146\nART (artifact) relation class, 312\nASCII\nas encoding scheme, 368\nparsing issues related, 89\nAsian Federation of Natural Language\nProcessing, 218\nAsian languages. See also by individual Asian\nlanguages\nmultilingual IR and, 366, 390\nQA and, 434, 437, 455, 460\u2013461, 466\nAsk.com, 435\nASR (automatic s", "n Federation of Natural Language\nProcessing, 218\nAsian languages. See also by individual Asian\nlanguages\nmultilingual IR and, 366, 390\nQA and, 434, 437, 455, 460\u2013461, 466\nAsk.com, 435\nASR (automatic speech recognition)\nsentence boundary annotation, 29\nsentence segmentation markers, 31\nASSERT (Automatic Statistical SEmantic\nRole Tagger), 147, 447\n\nIndex\n553\nATIS. See Air Travel Information System\n(ATIS)\nAtomic events, summarization and, 418\nAttribute features, in coreference models, 301\nAutomatic content extraction (ACE)\ncoreference resolution experiments, 302\u2013303\nevent extraction and, 320\u2013321\nmention detection and, 287, 294\nrelation extraction and, 311\u2013312\nin Rosetta Consortium distillation system,\n480\u2013481\nAutomatic speech recognition (ASR)\nsentence boundary annotation, 29\nsentence segmentation markers, 31\nAutomatic Statistical SEmantic Role Tagger\n(ASSERT), 147, 447\nAutomatic summarization\nbibliography, 427\u2013432\ncoherence and cohesion in, 401\u2013404\nextraction and modi\ufb01cation processes in", "ation markers, 31\nAutomatic Statistical SEmantic Role Tagger\n(ASSERT), 147, 447\nAutomatic summarization\nbibliography, 427\u2013432\ncoherence and cohesion in, 401\u2013404\nextraction and modi\ufb01cation processes in,\n399\u2013400\ngraph-based approaches, 401\nhistory of, 398\u2013399\nintroduction to, 397\u2013398\nlearning how to summarize, 406\u2013409\nLexPageRank, 406\nmultilingual. See Multilingual automatic\nsummarization\nstages of, 400\nsummary, 426\u2013427\nsurface-based features used in, 400\u2013401\nTextRank, 404\u2013406\nAutomatic Summary Evaluation based on\nn-gram graphs (AutoSummENG),\n419\u2013420\nBabel Fish\ncrosslingual question answering and, 455\nSystran, 331\nBackend services, of spoken dialog system,\n500\nBacko\ufb00smoothing techniques\ngeneralized backo\ufb00strategy, 183\u2013184\nin language model estimation, 172\nnonnormalized form, 175\nparallel backo\ufb00, 184\nBackus-Naur form, of context-free grammar,\n59\nBananaSplit, IR preprocessing and, 392\nBase phrase chunks, 132\u2013133\nBASEBALL system, in history of QA\nsystems, 434\nBasic Elements (BE)\nautomatic e", "ko\ufb00, 184\nBackus-Naur form, of context-free grammar,\n59\nBananaSplit, IR preprocessing and, 392\nBase phrase chunks, 132\u2013133\nBASEBALL system, in history of QA\nsystems, 434\nBasic Elements (BE)\nautomatic evaluation of summarization,\n417\u2013419\nmetrics in, 420\nBayes rule, for sentence or topic\nsegmentation, 39\u201340\nBayes theorem, maximum-likelihood\nestimation and, 376\nBayesian parameter estimation, 173\u2013174\nBayesian topic-based language models,\n186\u2013187\nBBN, event extraction and, 322\nBE (Basic Elements)\nautomatic evaluation of summarization,\n417\u2013419\nmetrics in, 420\nBE with Transformations for Evaluation\n(BEwTE), 419\u2013420\nBeam search\nmachine translation and, 346\nreducing search space using, 290\u2013291\nBell tree, for coreference resolution, 297\u2013298\nBengali. See Indian languages\nBerkeley word aligner, in machine translation,\n357\nBibliographic summaries, in automatic\nsummarization, 397\nBilingual latent semantic analysis (bLSA),\n197\u2013198\nBinary classi\ufb01er, in event matching, 323\u2013324\nBinary conditional model, ", "ine translation,\n357\nBibliographic summaries, in automatic\nsummarization, 397\nBilingual latent semantic analysis (bLSA),\n197\u2013198\nBinary classi\ufb01er, in event matching, 323\u2013324\nBinary conditional model, for probability of\nmention links, 297\u2013300\nBLEU\nmachine translation metrics, 334, 336\nmention detection experiments and, 295\nROUGE compared with, 415\u2013416\nBlock comparison method, for topic\nsegmentation, 38\nbLSA (bilingual latent semantic analysis),\n197\u2013198\nBLUE (Boeing Language Understanding\nEngine), 242\u2013244\nBM25 model, in document retrieval, 375\nBNC (British National Corpus), 118\nBoeing Language Understanding Engine\n(BLUE), 242\u2013244\n\n554\nIndex\nBoolean models\nfor document representation in monolingual\nIR, 372\nfor document retrieval, 374\nBoolean named entity \ufb02ags, in PSG, 126\nBootstrapping\nbuilding subjectivity lexicons, 266\u2013267\ncorpus-based approach to subjectivity and\nsentiment analysis, 269\ndictionary-based approach to subjectivity\nand sentiment analysis, 273\nranking approaches to subjecti", "ng subjectivity lexicons, 266\u2013267\ncorpus-based approach to subjectivity and\nsentiment analysis, 269\ndictionary-based approach to subjectivity\nand sentiment analysis, 273\nranking approaches to subjectivity and\nsentiment analysis, 275\u2013276\nsemisupervised approach to relation\nextraction, 318\nBoundary classi\ufb01cation problems\noverview of, 33\nsentence boundaries. See Sentence\nboundary detection\ntopic boundaries. See Topic segmentation\nBritish National Corpus (BNC), 118\nBrown Corpus, as resource for semantic\nparsing, 104\nBuckwalter Morphological Analyzer, 191\nC-ASSERT, software programs for semantic\nrole labeling, 147\nCall-\ufb02ow\nlocalization of, 514\nstrategy of dialog manager, 504\nvoice user interface (VUI) and, 505\u2013506\nCall routing, natural language and, 510\nCanadian Hansards\ncorpora for IR, 391\ncorpora for machine translation, 358\nCandidate answer extraction, in QA\nanswer scores, 450\u2013453\ncombining evidence, 453\u2013454\nstructural matching, 446\u2013448\nfrom structured sources, 449\u2013450\nsurface patterns, ", "pora for machine translation, 358\nCandidate answer extraction, in QA\nanswer scores, 450\u2013453\ncombining evidence, 453\u2013454\nstructural matching, 446\u2013448\nfrom structured sources, 449\u2013450\nsurface patterns, 448\u2013449\ntype-based, 446\nfrom unstructured sources, 445\nCandidate answer generation, in QA\ncomponents in QA architectures, 435\noverview of, 443\nCandidate boundaries, processing stages of\nsegmentation tasks, 48\nCanonization, deferred in RTE multiview\nrepresentation, 222\nCapitalization (Uppercase), sentence\nsegmentation markers, 30\nCAS (Common analysis structure), UIMA,\n527, 536\nCascading systems, types of EDT\narchitectures, 286\u2013287\nCase\nparsing issues related to, 88\nsentence segmentation markers, 30\nCatalan, 109\nCategorical ambiguity, word sense and, 104\nCause-and-e\ufb00ect relations, causal reasoning\nand, 250\nCCG (Combinatory Categorical Grammar),\n129\u2013130\nCFGs. See Context-free grammar (CFGs)\nCharacter n-gram models, 370\nChart decoding, tree-based models for\nmachine translation, 351\u2013352\nChart p", "ng\nand, 250\nCCG (Combinatory Categorical Grammar),\n129\u2013130\nCFGs. See Context-free grammar (CFGs)\nCharacter n-gram models, 370\nChart decoding, tree-based models for\nmachine translation, 351\u2013352\nChart parsing, worst-case parsing algorithm\nfor CFGs, 74\u201379\nCharts, IXIR distillation system, 488\u2013489\nCHILL (Constructive Heuristics Induction for\nLanguage Learning), 151\nChinese\nanaphora frequency in, 444\nchallenges of sentence and topic\nsegmentation, 30\ncorpora for relation extraction, 317\ncorpus-based approach to subjectivity and\nsentiment analysis, 274\u2013275\ncrosslingual language modeling, 197\u2013198\ndata sets related to summarization, 424\u2013426\ndictionary-based approach to subjectivity\nand sentiment analysis, 272\u2013273\ndistillation, 479, 490\u2013491\nEDT and, 286\nHowNet lexicon for, 105\nhuman assessment of word meaning, 333\nIR and, 366, 390\nisolating (analytic) languages, 7\nas isolating or analytic language, 7\nlanguage modeling in without word\nsegmentation, 193\u2013194\nlingPipe for word segmentation, 423\nmach", "rd meaning, 333\nIR and, 366, 390\nisolating (analytic) languages, 7\nas isolating or analytic language, 7\nlanguage modeling in without word\nsegmentation, 193\u2013194\nlingPipe for word segmentation, 423\nmachine translation and, 322, 354, 358\nmention detection experiments, 294\u2013296\n\nIndex\n555\nmultilingual issues in predicate-argument\nstructures, 146\u2013147\nphrase structure treebank, 70\npolarity analysis of words and phrases, 269\npreprocessing best practices in IR, 372\nQA and, 461, 464\nQA architectures and, 437\u2013438\nresources for semantic parsing, 122\nRTE in, 218\nscripts not using whitespace, 369\nsubjectivity and sentiment analysis,\n259\u2013260\nTALES case study, 538\ntranslingual summarization, 399, 410\nword segmentation and parsing, 89\u201390\nword segmentation in, 4\u20135\nword sense annotation in, 104\nChomsky, Noam, 13, 98\u201399\nChunk-based systems, 132\u2013133\nChunks\nde\ufb01ned, 292\nmeaning chunks in semantic parsing, 97\nCIDR algorithm, for multilingual\nsummarization, 411\nCitations\nevaluation in distillation, 493\nin GALE", ", 13, 98\u201399\nChunk-based systems, 132\u2013133\nChunks\nde\ufb01ned, 292\nmeaning chunks in semantic parsing, 97\nCIDR algorithm, for multilingual\nsummarization, 411\nCitations\nevaluation in distillation, 493\nin GALE distillation initiative, 477\nCKY algorithm, worst-case parsing for CFGs,\n76\u201378\nClass-based language models, 178\u2013179\nClasses\nlanguage modeling using morphological\ncategories, 193\nof relations, 311\nClassi\ufb01cation\nof arguments, 123, 139\u2013140\ndata-driven, 287\u2013289\ndynamic class context in PSG, 128\nevent extraction and, 321\u2013322\novercoming independence assumption,\n137\u2013138\nparadigms, 133\u2013137\nproblems related to sentence boundaries.\nSee Sentence boundary detection\nproblems related to topic boundaries. See\nTopic segmentation\nrelation extraction and, 312\u2013316\nClassi\ufb01cation tag lattice (trellis), searching\nfor mentions, 289\nClassi\ufb01ers\nin event matching, 323\u2013324\nlocalization of grammars and, 516\nmaximum entropy classi\ufb01ers, 37, 39\u201340\nin mention detection, 292\u2013293\npipeline of, 321\nin relation extraction, 3", "for mentions, 289\nClassi\ufb01ers\nin event matching, 323\u2013324\nlocalization of grammars and, 516\nmaximum entropy classi\ufb01ers, 37, 39\u201340\nin mention detection, 292\u2013293\npipeline of, 321\nin relation extraction, 313, 316\u2013317\nin subjectivity and sentiment analysis,\n270\u2013272, 274\nType classi\ufb01er in QA systems, 440\u2013442\nin word disambiguation, 110\nCLASSIFY function, 313\nClearTK tool, for building summarization\nsystem, 423\nCLIR. See Crosslingual information retrieval\n(CLIR)\nClitics\nCzech example, 5\nde\ufb01ned, 4\nCo-occurence, of words between languages,\n337\u2013338\nCoarse to \ufb01ne parsing, 77\u201378\nCode switchers\nimpact on sentence segmentation, 31\nmultilingual language modeling and,\n195\u2013196\nCOGEX, for answer scores in QA, 451\nCoherence, sentence-sentence connections\nand, 402\nCohesion, anaphora resolution and, 401\u2013402\nCollection language, in CLIR, 365\nCombination hypothesis, combining classi\ufb01ers\nto boost performance, 293\nCombinatory Categorical Grammar (CCG),\n129\u2013130\nCommon analysis structure (CAS), UIMA,\n527, 536\nCom", "lection language, in CLIR, 365\nCombination hypothesis, combining classi\ufb01ers\nto boost performance, 293\nCombinatory Categorical Grammar (CCG),\n129\u2013130\nCommon analysis structure (CAS), UIMA,\n527, 536\nCommunicator program, for meaning\nrepresentation, 148\u2013150\nComparators, RTE, 219, 222\u2013223\nCompetence vs. performance, Chomsky on, 13\nCompile/replace transducer (Beesley and\nKarttunen), 17\nComponentization of design, for NLP\naggregation, 524\u2013525\nComponents of words\nlexemes, 5\nmorphemes, 5\u20137\nmorphological typology and, 7\u20138\n\n556\nIndex\nCompound slitting\nBananaSplit tool, 392\nnormalization for fusional languages, 371\nComputational e\ufb03ciency\ndesired attributes of NLP aggregation,\n525\u2013526\nin GALE IOD, 537\nin GATE, 530\nin InfoSphere Streams, 530\u2013531\nin UIMA, 528\nComputational Natural Language Learning\n(CoNLL), 132\nConcatenative languages, 8\nConcept space, interlingual document\nrepresentations, 381\nConceptual density, as measure of semantic\nsimilarity, 112\nConditional probability, MaxEnt formula for,\n31", "LL), 132\nConcatenative languages, 8\nConcept space, interlingual document\nrepresentations, 381\nConceptual density, as measure of semantic\nsimilarity, 112\nConditional probability, MaxEnt formula for,\n316\nConditional random \ufb01elds (CRFs)\nin discriminative parsing model, 84\nmachine learning and, 342\nmeasuring token frequency, 369\nmention detection and, 287\nrelation extraction and, 316\nsentence or topic segmentation and, 39\u201340\nCon\ufb01dence weighted score (CWS), in QA, 463\nCoNLL (Computational Natural Language\nLearning), 132\nConstituents\natomic events and, 418\nin PSG, 127\nConstituents, in RTE\ncomparing annotation constituents, 222\u2013224\nmultiview representation of analysis and,\n220\nnumerical quantities (NUM), 221, 233\nConstraint-based language models, 177\nConstructive Heuristics Induction for\nLanguage Learning (CHILL), 151\nContent Analysis Toolkit (Tika), for\npreprocessing IR documents, 392\nContent word, in PSG, 125\u2013126\nContext, as measure of semantic similarity,\n112\nContext-dependent process, in ", " Learning (CHILL), 151\nContent Analysis Toolkit (Tika), for\npreprocessing IR documents, 392\nContent word, in PSG, 125\u2013126\nContext, as measure of semantic similarity,\n112\nContext-dependent process, in GALE IOD,\n536\u2013537\nContext features, of Rosetta Consortium\ndistillation system, 486\nContext-free grammar (CFGs)\nfor analysis of natural language syntax,\n60\u201361\ndependency graphs in syntax analysis,\n65\u201367\nrules of syntax, 59\nshift-reduce parsing, 72\u201373\nworst-case parsing algorithm, 74\u201378\nContextual subjectivity analysis, 261\nContradiction, in textual entailment, 211\nConversational speech, sentence segmentation\nin, 31\nCore arguments, PropBank verb predicates,\n119\nCoreference resolution. See also Anaphora\nresolution\nautomatic summarization and, 398\nBell tree for, 297\u2013298\nexperiments in, 302\u2013303\ninformation extraction and, 100, 285\u2013286\nMaxEnt model applied to, 300\u2013301\nmodels for, 298\u2013300\noverview of, 295\u2013296\nas relation extraction system, 311\nin RTE, 212, 227\nCorpora\nfor distillation, 480\u2013483\nfo", "ormation extraction and, 100, 285\u2013286\nMaxEnt model applied to, 300\u2013301\nmodels for, 298\u2013300\noverview of, 295\u2013296\nas relation extraction system, 311\nin RTE, 212, 227\nCorpora\nfor distillation, 480\u2013483\nfor document-level annotations, 274\nEuroparl (European Parliament), 295, 345\nfor IR systems, 390\u2013391\nfor machine translation (MT), 358\nfor relation extraction, 317\nfor semantic parsing, 104\u2013105\nfor sentence-level annotations, 271\u2013272\nfor subjectivity and sentiment analysis,\n262\u2013263, 274\u2013275\nfor summarization, 406, 425\nfor word/phrase-level annotations, 267\u2013269\nCoverage rate criteria, in language model\nevaluation, 170\nCran\ufb01eld paradigm, 387\nCreativity/productivity, and the unknown\nword problem, 13\u201315\nCRFs. See Conditional random \ufb01elds (CRFs)\nCross-Language Evaluation Forum (CLEF)\napplying to RTE to non-English languages,\n218\nIR and, 377, 390\nQA and, 434, 454, 460\u2013464\n\nIndex\n557\nCross-language mention propagation, 293, 295\nCross-lingual projections, 275\nCrossdocument coreference (XDC), in\nRose", "to non-English languages,\n218\nIR and, 377, 390\nQA and, 434, 454, 460\u2013464\n\nIndex\n557\nCross-language mention propagation, 293, 295\nCross-lingual projections, 275\nCrossdocument coreference (XDC), in\nRosetta Consortium distillation\nsystem, 482\u2013483\nCrossdocument Structure Theory Bank\n(CSTBank), 425\nCrossdocument structure theory (CST), 425\nCrosslingual distillation, 490\u2013491\nCrosslingual information retrieval (CLIR)\nbest practices, 382\ninterlingual document representations,\n381\u2013382\nmachine translation, 380\u2013381\noverview of, 365, 378\ntranslation-based approaches, 378\u2013380\nCrosslingual language modeling, 196\u2013198\nCrosslingual question answering, 454\u2013455\nCrosslingual summarization, 398\nCST (Crossdocument structure theory), 425\nCSTBank (Crossdocument Structure Theory\nBank), 425\nCube pruning, decoding phrase-based models,\n347\u2013348\nCWS (Con\ufb01dence weighted score), in QA, 463\nCyrillic alphabet, 371\nCzech\nambiguity in, 11\u201313\ndependency graphs in syntax analysis,\n62\u201365\ndependency parsing in, 79\n\ufb01nite-stat", "hrase-based models,\n347\u2013348\nCWS (Con\ufb01dence weighted score), in QA, 463\nCyrillic alphabet, 371\nCzech\nambiguity in, 11\u201313\ndependency graphs in syntax analysis,\n62\u201365\ndependency parsing in, 79\n\ufb01nite-state models, 18\nas fusional language, 8\nlanguage modeling, 193\nmorphological richness of, 355\nnegation indicated by in\ufb02ection, 5\nparsing issues related to morphology, 91\nproductivity/creativity in, 14\u201315\nsyntactic features used in sentence and\ntopic segmentation, 43\nuni\ufb01cation-based models, 19\nDAMSL (Dialog Act Markup in Several\nLayers), 31\nData-driven\nmachine translation, 331\nmention detection, 287\u2013289\nData formats, challenges in NLP aggregation,\n524\nData-manipulation capabilities\ndesired attributes of NLP aggregation, 526\nin GATE, 530\nin InfoSphere Streams, 531\nin UIMA, 528\u2013529\nData reorganization, speech-to-text (STT)\nand, 535\u2013536\nData sets\nfor evaluating IR systems, 389\u2013391\nfor multilingual automatic summarization,\n425\u2013426\nData types\nGALE Type System (GTS), 534\u2013535\nusage conventions for N", "ion, speech-to-text (STT)\nand, 535\u2013536\nData sets\nfor evaluating IR systems, 389\u2013391\nfor multilingual automatic summarization,\n425\u2013426\nData types\nGALE Type System (GTS), 534\u2013535\nusage conventions for NLP aggregation,\n540\u2013541\nDatabases\nof entity relations and events, 309\u2013310\nrelational, 449\nDATR, uni\ufb01cation-based morphology and,\n18\u201319\nDBpedia, 449\nde Saussure, Ferdinand, 13\nDecision trees, for sentence or topic\nsegmentation, 39\u201340\nDecoding phrase-based models\ncube pruning approach, 347\u2013348\noverview of, 345\u2013347\nDeep representation, in semantic\ninterpretation, 101\nDeep semantic parsing\ncoverage in, 102\noverview of, 98\nDefense Advanced Research Projects Agency\n(DARPA)\nGALE distillation initiative, 475\u2013476\nGALE IOD case study. See Interoperability\nDemo (IOD), GALE case study\nTopic Detection and Tracking (TDT)\nprogram, 32\u201333\nDe\ufb01nitional questions, QA and, 433\nDeletions metrics, machine translation, 335\nDependencies\nglobal similarity in RTE and, 247\nhigh-level features in event matching,\n324\u20133", "Tracking (TDT)\nprogram, 32\u201333\nDe\ufb01nitional questions, QA and, 433\nDeletions metrics, machine translation, 335\nDependencies\nglobal similarity in RTE and, 247\nhigh-level features in event matching,\n324\u2013326\nDependency graphs\nphrase structure trees compared with, 69\u201370\nin syntactic analysis, 63\u201367\nin treebank construction, 62\n\n558\nIndex\nDependency parsing\nimplementing RTE and, 227\nMinipar and Stanford Parser, 456\nMST algorithm for, 79\u201380\nshift-reduce parsing algorithm for, 73\nstructural matching and, 447\ntree edit distance based on, 240\u2013241\nworst-case parsing algorithm for CFGs, 78\nDependency trees\nnon projective, 65\u201367\noverview of, 130\u2013132\npatterns used in relation extraction, 318\nprojective, 64\u201365\nDerivation, parsing and, 71\u201372\nDevanagari, preprocessing best practices in\nIR, 371\nDialog Act Markup in Several Layers\n(DAMSL), 31\nDialog manager\ndirecting speech generation, 499\u2013500\noverview of, 504\u2013505\nDialog module (DM)\ncall-\ufb02ow localization and, 514\nvoice user interface and, 507\u2013508\nDialogs\n", "rkup in Several Layers\n(DAMSL), 31\nDialog manager\ndirecting speech generation, 499\u2013500\noverview of, 504\u2013505\nDialog module (DM)\ncall-\ufb02ow localization and, 514\nvoice user interface and, 507\u2013508\nDialogs\nforms of, 509\u2013510\nrules of, 499\u2013500\nDictionary-based approach, in subjectivity\nand sentiment analysis\ndocument-level annotations, 272\u2013273\nsentence-level annotations, 270\u2013271\nword/phrase-level annotations, 264\u2013267\nDictionary-based morphology, 15\u201316\nDictionary-based translations\napplying to CLIR, 380\ncrosslingual modeling and, 197\nDirected dialogs, 509\nDirected graphs, 79\u201380\nDirichlet distribution\nHierarchical Dirichlet process (HDP), 187\nlanguage models and, 174\nLatent Dirichlet allocation (LDA) model,\n186\nDIRT (Discovery of inference rules from text),\n242\nDisambiguation systems, word sense\noverview of, 105\nrule-based, 105\u2013109\nsemantic parsing and, 152\u2013153\nsemi-supervised, 114\u2013116\nsoftware programs for, 116\u2013117\nsupervised, 109\u2013112\nunsupervised, 112\u2013114\nDiscontiguous arguments, PropBank verb", "rview of, 105\nrule-based, 105\u2013109\nsemantic parsing and, 152\u2013153\nsemi-supervised, 114\u2013116\nsoftware programs for, 116\u2013117\nsupervised, 109\u2013112\nunsupervised, 112\u2013114\nDiscontiguous arguments, PropBank verb\npredicates, 121\nDiscourse commitments (beliefs), RTE system\nbased on, 239\u2013240\nDiscourse connectives, relating sentences by,\n29\nDiscourse features\nrelating sentences by discourse connectives,\n29\nin sentence and topic segmentation, 44\nDiscourse segmentation. See Topic\nsegmentation\nDiscourse structure\nautomatic summarization and, 398, 410\nRTE applications and, 249\nDiscovery of inference rules from text (DIRT),\n242\nDiscriminative language models\nmodeling using morphological categories,\n192\u2013193\nmodeling without word segmentation, 194\noverview of, 179\u2013180\nDiscriminative local classi\ufb01cation methods,\nfor sentence/topic boundary detection,\n36\u201338\nDiscriminative parsing models\nmorphological information in, 91\u201392\noverview of, 84\u201387\nDiscriminative sequence classi\ufb01cation\nmethods\ncomplexity of, 40\u201341\nov", "\nfor sentence/topic boundary detection,\n36\u201338\nDiscriminative parsing models\nmorphological information in, 91\u201392\noverview of, 84\u201387\nDiscriminative sequence classi\ufb01cation\nmethods\ncomplexity of, 40\u201341\noverview of, 34\nperformance of, 41\nfor sentence/topic boundary detection,\n38\u201339\nDistance-based reordering model, in machine\ntranslation, 344\nDistance, features of coreference models, 301\nDistillation\nbibliography, 495\u2013497\ncrosslingual, 490\u2013491\ndocument and corpus preparation, 480\u2013483\nevaluation and metrics, 491\u2013494\nexample, 476\u2013477\nindexing and, 483\n\nIndex\n559\nintroduction to, 475\u2013476\nmultimodal, 490\nquery answers and, 483\u2013487\nredundancy reduction, 489\u2013490\nrelevance and redundancy and, 477\u2013479\nrelevance detection, 488\u2013489\nRosetta Consortium system, 479\u2013480\nsummary, 495\nsystem architectures for, 488\nDM (Dialog module)\ncall-\ufb02ow localization and, 514\nvoice user interface and, 507\u2013508\nDocument-level annotations, for subjectivity\nand sentiment analysis\ncorpus-based, 274\ndictionary-based, 272\u2013273\n", "488\nDM (Dialog module)\ncall-\ufb02ow localization and, 514\nvoice user interface and, 507\u2013508\nDocument-level annotations, for subjectivity\nand sentiment analysis\ncorpus-based, 274\ndictionary-based, 272\u2013273\noverview of, 272\nDocument retrieval system, INDRI, 323\nDocument structure\nbibliography, 49\u201356\ncomparing segmentation methods, 40\u201341\ndiscourse features of segmentation methods,\n44\ndiscriminative local classi\ufb01cation method\nfor segmentation, 36\u201338\ndiscriminative sequence classi\ufb01cation\nmethod for segmentation, 38\u201339\ndiscussion, 48\u201349\nextensions for global modeling sentence\nsegmentation, 40\nfeatures of segmentation methods, 41\u201342\ngenerative sequence classi\ufb01cation method\nfor segmentation, 34\u201336\nhybrid methods for segmentation, 39\u201340\nintroduction to, 29\u201330\nlexical features of segmentation methods,\n42\u201343\nmethods for detecting probable sentence or\ntopic boundaries, 33\u201334\nperformance of segmentation methods, 41\nprocessing stages of segmentation tasks, 48\nprosodic features for segmentation, 45\u201348\nsen", "\u201343\nmethods for detecting probable sentence or\ntopic boundaries, 33\u201334\nperformance of segmentation methods, 41\nprocessing stages of segmentation tasks, 48\nprosodic features for segmentation, 45\u201348\nsentence boundary detection\n(segmentation), 30\u201332\nspeech-related features for segmentation, 45\nsummary, 49\nsyntactic features of segmentation methods,\n43\u201344\ntopic boundary detection (segmentation),\n32\u201333\ntypographical and structural features for\nsegmentation, 44\u201345\nDocument Understanding Conference (DUC),\n404, 424\nDocuments, in distillation systems\nindexing, 483\npreparing, 480\u2013483\nretrieving, 483\u2013484\nDocuments, in IR\ninterlingual representation, 381\u2013382\nmonolingual representation, 372\u2013373\npreprocessing, 366\u2013367\na priori models, 377\nreducing MLIR to CLIR, 383\u2013384\nsyntax and encoding, 367\u2013368\ntranslating entire collection, 379\nDocuments, QA searches, 444\nDomain dependent scope, for semantic\nparsing, 102\nDomain independent scope, for semantic\nparsing, 102\nDominance relations, 325\nDSO Corpus, of ", "lating entire collection, 379\nDocuments, QA searches, 444\nDomain dependent scope, for semantic\nparsing, 102\nDomain independent scope, for semantic\nparsing, 102\nDominance relations, 325\nDSO Corpus, of Sense-Tagged English, 104\nDUC (Document Understanding Conference),\n404, 424\nDutch\nIR and, 390\u2013391\nnormalization and, 371\nQA and, 439, 444, 461\nRTE in, 218\nEdit distance, features of coreference models,\n301\nEdit Distance Textual Entailment Suite\n(EDITS), 240\u2013241\nEDT. See Entity detection and tracking\n(EDT)\nElaborative summaries, in automatic\nsummarization, 397\nElixirFM lexicon, 20\nEllipsis, linguistic supports for cohesion, 401\nEM algorithm. See Expectation-maximization\n(EM) algorithm\nEncoding\nof documents in information retrieval, 368\nparsing issues related to, 89\n\n560\nIndex\nEnglish\ncall-\ufb02ow localization and, 514\nco-occurrence of words between languages,\n337\u2013339\ncorpora for relation extraction, 317\ncorpus-based approach to subjectivity and\nsentiment analysis, 271\u2013272\ncrosslingual language ", "ocalization and, 514\nco-occurrence of words between languages,\n337\u2013339\ncorpora for relation extraction, 317\ncorpus-based approach to subjectivity and\nsentiment analysis, 271\u2013272\ncrosslingual language modeling, 197\u2013198\ndependency graphs in syntax analysis, 65\ndiscourse parsers for, 403\ndistillation, 479, 490\u2013491\n\ufb01nite-state transducer applied to English\nexample, 17\nGALE IOD and, 532, 534\u2013536\nIR and, 390\nas isolating or analytic language, 7\nmachine translation and, 322, 354, 358\nmanually annotated corpora for, 274\nmention detection, 287\nmention detection experiments, 294\u2013296\nmultilingual issues in predicate-argument\nstructures, 146\u2013147\nnormalization and, 370\nphrase structure trees in syntax analysis, 62\npolarity analysis of words and phrases, 269\nproductivity/creativity in, 14\u201315\nQA and, 444, 461\nQA architectures and, 437\nRTE in, 218\nsentence segmentation markers, 30\nsubjectivity and sentiment analysis,\n259\u2013260, 262\nas SVO language, 356\nTALES case study, 538\ntokenization and, 410\ntransli", ", 461\nQA architectures and, 437\nRTE in, 218\nsentence segmentation markers, 30\nsubjectivity and sentiment analysis,\n259\u2013260, 262\nas SVO language, 356\nTALES case study, 538\ntokenization and, 410\ntranslingual summarization, 398\u2013399,\n424\u2013426\nword order and, 356\nWordNet and, 109\nEnrichment, in RTE\nimplementing, 228\u2013231\nmodeling, 225\nEnsemble clustering methods, in relation\nextraction, 317\u2013318\nEntities\nclassi\ufb01ers, 292\u2013293\nentity-based relation extraction, 314\u2013315\nevents. See Events\nrelations. See Relations\nresolution in semantic interpretation, 100\nEntity detection and tracking (EDT)\nBell tree for, 297\u2013298\nbibliography, 303\u2013307\ncombining entity and relation detection, 320\ncoreference models, 298\u2013300\ncoreference resolution, 295\u2013296\ndata-driven classi\ufb01cation, 287\u2013289\nexperiments in coreference resolution,\n302\u2013303\nexperiments in mention detection, 294\u2013295\nfeatures for mention detection, 291\u2013294\nintroduction to, 285\u2013287\nMaxEnt model applied to, 300\u2013301\nmention detection task, 287\nsearching for m", "olution,\n302\u2013303\nexperiments in mention detection, 294\u2013295\nfeatures for mention detection, 291\u2013294\nintroduction to, 285\u2013287\nMaxEnt model applied to, 300\u2013301\nmention detection task, 287\nsearching for mentions, 289\u2013291\nsummary, 303\nEquivalent terms, in GALE distillation\ninitiative, 475\nErrors\nmachine translation, 335\u2013337, 343, 349\nparsing, 141\u2013144\nsentence and topic segmentation, 41\nESA (Explicit semantic analysis), for\ninterlingual document representation,\n382\nEuroparl (European Parliament) corpus\nevaluating co-occurrence of word between\nlanguages, 337\nfor IR systems, 391\nfor machine translation, 358\nphrase translation tables, 345\nEuropean Language Resources Association,\n218\nEuropean languages. See also by individual\nlanguages\ncrosslingual question answering and, 455\nQA architectures and, 437\nwhitespace use in, 369\nEuropean Parliament Plenary Speech corpus,\n295\nEVALITA, applying to RTE to non-English\nlanguages, 218\nEvaluation, in automatic summarization\nautomated evaluation methodologie", "37\nwhitespace use in, 369\nEuropean Parliament Plenary Speech corpus,\n295\nEVALITA, applying to RTE to non-English\nlanguages, 218\nEvaluation, in automatic summarization\nautomated evaluation methodologies,\n415\u2013418\nmanual evaluation methodologies, 413\u2013415\n\nIndex\n561\noverview of, 412\u2013413\nrecent developments in, 418\u2013419\nEvaluation, in distillation\ncitation checking, 493\nGALE and, 492\nmetrics, 493\u2013494\noverview of, 491\u2013492\nrelevance and redundancy and, 492\u2013493\nEvaluation, in IR\nbest practices, 391\ndata sets for, 389\u2013390\nexperimental setup for, 387\nmeasures in, 388\u2013389\noverview of, 386\u2013387\nrelevance assessment, 387\u2013388\ntrec-eval tool for, 393\nEvaluation, in MT\nautomatic evaluation, 334\u2013335\nhuman assessment, 332\u2013334\nmeaning and, 332\nmetrics for, 335\u2013337\nEvaluation, in QA\nanswer correctness, 461\u2013462\nperformance metrics, 462\u2013464\ntasks, 460\u2013461\nEvaluation, in RTE\ngeneral model and, 224\nimproving, 251\u2013252\nperformance evaluation, 213\u2013214\nEvaluation, of aggregated NLP, 541\nEvaluative summaries, in aut", "performance metrics, 462\u2013464\ntasks, 460\u2013461\nEvaluation, in RTE\ngeneral model and, 224\nimproving, 251\u2013252\nperformance evaluation, 213\u2013214\nEvaluation, of aggregated NLP, 541\nEvaluative summaries, in automatic\nsummarization, 397\nEvents. See also Entities\nextraction, 320\u2013322\nfuture directions in extraction, 326\nmatching, 323\u2013326\nmoving beyond sentence processing, 323\noverview of, 320\nresolution in semantic interpretation, 100\nExceptions\nchallenges in NLP aggregation, 524\nfunctional morphology models and, 19\nExclamation point (!), as sentence\nsegmentation marker, 30\nExistence classi\ufb01er, in relation extraction, 313\nExpansion documents, query expansion and,\n377\nExpansion rules, features of predicate-\nargument structures, 145\nExpectation-maximization (EM) algorithm\nsplit-merge over trees using, 83\nsymmetrization and, 340\u2013341\nword alignment between languages and,\n339\u2013340\nExperiments\nin coreference resolution, 302\u2013303\nin mention detection, 294\u2013295\nsetting up for IR evaluation, 387\nExplicit seman", "\nsymmetrization and, 340\u2013341\nword alignment between languages and,\n339\u2013340\nExperiments\nin coreference resolution, 302\u2013303\nin mention detection, 294\u2013295\nsetting up for IR evaluation, 387\nExplicit semantic analysis (ESA), for\ninterlingual document representation,\n382\neXtended WordNet (XWN), 451\nExtraction\nin automatic summarization, 399\u2013400\nas classi\ufb01cation problem, 312\u2013313\nof events, 320\u2013322, 326\nof relations, 310\u2013311\nExtraction, in QA\ncandidate extraction from structured\nsources, 449\u2013450\ncandidate extraction from unstructured\nsources, 445\u2013449\ncandidate extraction techniques in QA, 443\nExtracts\nin automatic summarization, 397\nde\ufb01ned, 400\nExtrinsic evaluation, of summarization, 412\nF-measure, in mention detection, 294\nFactoid QA systems\nanswer correctness, 461\nanswer scores and, 450\u2013453\nbaseline, 443\ncandidate extraction or generation and, 435\nchallenges in, 464\u2013465\ncrosslingual question answering and, 454\nevaluation tasks, 460\u2013461\nextracting using high-level searches, 445\nextracting usi", "line, 443\ncandidate extraction or generation and, 435\nchallenges in, 464\u2013465\ncrosslingual question answering and, 454\nevaluation tasks, 460\u2013461\nextracting using high-level searches, 445\nextracting using structural matching, 446\nMURAX and, 434\nperformance metrics, 462\u2013463\nquestions, 433\ntype classi\ufb01cation of, 440\nFactoids, in manual evaluation of\nsummarization, 413\nFactored (cascaded) model, 313\nFactored language models (FLM)\nmachine translation and, 355\n\n562\nIndex\nFactored language models (continued)\nmorphological categories in, 193\noverview of, 183\u2013184\nFeature extractors\nbuilding summarization systems, 423\ndistillation and, 485\u2013486\nsummarization and, 406\nFeatures\nin mention detection system, 291\u2013294\ntyped feature structures and uni\ufb01cation,\n18\u201319\nin word disambiguation system, 110\u2013112\nFeatures, in sentence or topic segmentation\nde\ufb01ned, 33\ndiscourse features, 44\nlexical features, 42\u201343\noverview of, 41\u201342\npredictions based on, 29\nprosodic features, 45\u201348\nspeech-related features, 45\nsynta", "ures, in sentence or topic segmentation\nde\ufb01ned, 33\ndiscourse features, 44\nlexical features, 42\u201343\noverview of, 41\u201342\npredictions based on, 29\nprosodic features, 45\u201348\nspeech-related features, 45\nsyntactic features, 43\u201344\ntypographical and structural features,\n44\u201345\nFertility, word alignment and, 340\nFiles types, document syntax and, 367\u2013368\nFinite-state morphology, 16\u201318\nFinite-state transducers, 16\u201317, 20\nFinnish\nas agglutinative language, 7\nIR and, 390\u2013391\nirregular verbs, 10\nlanguage modeling, 189\u2013191\nparsing issues related to morphology, 91\nsummarization and, 399\nFIRE (Forum for Information Retrieval\nEvaluation), 390\nFlexible, distributed componentization\ndesired attributes of NLP aggregation,\n524\u2013525\nin GATE, 530\nin InfoSphere Streams, 530\nin UIMA, 528\nFLM. See Factored language models (FLM)\nFluency, of translation, 334\nForum for Information Retrieval Evaluation\n(FIRE), 390\nFraCaS corpus, applying natural logic to\nRTE, 246\nFrame elements\nin PSG, 126\nsemantic frames in FrameNet, 11", "s (FLM)\nFluency, of translation, 334\nForum for Information Retrieval Evaluation\n(FIRE), 390\nFraCaS corpus, applying natural logic to\nRTE, 246\nFrame elements\nin PSG, 126\nsemantic frames in FrameNet, 118\nFrameNet\nlimitation of, 122\u2013123\nresources, 122\nresources for predicate-argument\nrecognition, 118\u2013122\nFreebase, 449\nFrench\nautomatic speech recognition (ASR), 179\ndictionary-based approach to subjectivity\nand sentiment analysis, 267\nhuman assessment of translation English to,\n332\u2013333\nIR and, 378, 390\u2013391\nlanguage modeling, 188\nlocalization of spoken dialog systems, 513\nmachine translation and, 350, 353\u2013354, 358\nphrase structure trees in syntax analysis,\n62\npolarity analysis of words and phrases, 269\nQA and, 454, 461\nRTE in, 217\u2013218\ntranslingual summarization, 398\nword segmentation and, 90\nWordNet and, 109\nFunctional morphology, 19\u201321\nFunctions, viewing language relations as, 17\nFusional languages\nfunctional morphology models and, 19\nmorphological typology and, 8\nnormalization and, 371\npre", "dNet and, 109\nFunctional morphology, 19\u201321\nFunctions, viewing language relations as, 17\nFusional languages\nfunctional morphology models and, 19\nmorphological typology and, 8\nnormalization and, 371\npreprocessing best practices in IR, 371\nGALE. See Global Autonomous Language\nExploitation (GALE)\nGALE Type System (GTS), 534\u2013535\nGATE. See General Architecture for Text\nEngineering (GATE)\nGazetteer, features of mention detection\nsystem, 293\nGEN-AFF (general-a\ufb03liation), relation class,\n312\nGender\nambiguity resolution, 13\nmultilingual approaches to grammatical\ngender, 398\n\nIndex\n563\nGeneral Architecture for Text Engineering\n(GATE)\nattributes of, 530\nhistory of summarization systems, 399\noverview of, 529\u2013530\nsummarization frameworks, 422\nGeneral Inquirer, subjectivity and sentiment\nanalysis lexicon, 262\nGeneralized backo\ufb00strategy, in FLM, 183\u2013184\nGenerative parsing models, 83\u201384\nGenerative sequence classi\ufb01cation methods\ncomplexity of, 40\noverview of, 34\nperformance of, 41\nfor sentence/topic boun", " 262\nGeneralized backo\ufb00strategy, in FLM, 183\u2013184\nGenerative parsing models, 83\u201384\nGenerative sequence classi\ufb01cation methods\ncomplexity of, 40\noverview of, 34\nperformance of, 41\nfor sentence/topic boundary detection,\n34\u201336\nGeometric vector space model, for document\nretrieval, 375\nGeoQuery\nresources for meaning representation, 149\nsupervised systems for semantic parsing,\n151\nGerman\nco-occurrence of words between languages,\n337\u2013339\ndictionary-based approach to subjectivity\nand sentiment analysis, 265\u2013266, 273\ndiscourse parsers for, 403\nas fusional language, 8\nIR and, 390\u2013392\nlanguage modeling, 189\nmention detection, 287\nmorphological richness of, 354\u2013355\nnormalization, 370\u2013371\nOOV rate in, 191\nphrase-based model for decoding, 345\npolarity analysis of words and phrases, 269\nQA and, 461\nRTE in, 218\nsubjectivity and sentiment analysis, 259,\n276\nsummarization and, 398, 403\u2013404, 420\nWordNet and, 109\nGermanic languages, language modeling for,\n189\nGetService process, of voice user interface\n(VUI", " 218\nsubjectivity and sentiment analysis, 259,\n276\nsummarization and, 398, 403\u2013404, 420\nWordNet and, 109\nGermanic languages, language modeling for,\n189\nGetService process, of voice user interface\n(VUI), 506\u2013507\nGiza, machine translation program, 423\nGIZA toolkit, for machine translation, 357\nGlobal Autonomous Language Exploitation\n(GALE)\ndistillation initiative of DARPA, 475\u2013476\nevaluation in distillation, 492\nInteroperability Demo case study. See\nInteroperability Demo (IOD), GALE\ncase study\nmetrics for evaluating distillation, 494\nrelevance and redundancy in, 477\u2013479\nGlobal linear model, discriminative approach\nto learning, 84\nGood-Turing\nmachine translation and, 345\nsmoothing techniques in language model\nestimation, 172\nGoogle, 435\nGoogle Translate, 331, 455\nGrammars\nCombinatory Categorical Grammar (CCG),\n129\u2013130\ncontext-free. See Context-free grammar\n(CFGs)\nhead-driven phrase structure grammar\n(HPSG), 18\nlocalization of, 514, 516\u2013517\nmorphological resource grammars, 19, 21\nphrase st", "cal Grammar (CCG),\n129\u2013130\ncontext-free. See Context-free grammar\n(CFGs)\nhead-driven phrase structure grammar\n(HPSG), 18\nlocalization of, 514, 516\u2013517\nmorphological resource grammars, 19, 21\nphrase structure. See Phrase Structure\nGrammar (PSG)\nprobabilistic context-free. See Probabilistic\ncontext-free grammars (PCFGs)\nrule-based grammars in speech recognition,\n501\u2013503\nTree-Adjoining Grammar (TAG), 130\nvoice user interface (VUI), 508\u2013509\nGrammatical Framework, 19, 21\nGraph-based approaches, to automatic\nsummarization\napplying RST to summarization, 402\u2013404\ncoherence and cohesion and, 401\u2013402\nLexPageRank, 406\noverview of, 401\nTextRank, 404\u2013406\nGraph generation, in RTE\nimplementing, 231\u2013232\nmodeling, 226\nGraphemes, 4\nGreedy best-\ufb01t decoding, in mention\ndetection, 322\nGroups, aligning views in RTE, 233\n\n564\nIndex\nGrow-diag-\ufb01nal method, for word alignment,\n341\nGTS (GALE Type System), 534\u2013535\nGujarati. See Indian languages\nHDP (Hierarchical Dirichlet process), 187\nHead-driven phrase structure", "RTE, 233\n\n564\nIndex\nGrow-diag-\ufb01nal method, for word alignment,\n341\nGTS (GALE Type System), 534\u2013535\nGujarati. See Indian languages\nHDP (Hierarchical Dirichlet process), 187\nHead-driven phrase structure grammar\n(HPSG), 18\nHead word\ndependency trees and, 131\nin Phrase Structure Grammar (PSG), 124\nHeadlines, typographical and structural\nfeatures for sentence and topic\nsegmentation, 44\u201345\nHebrew\nencoding and script, 368\npreprocessing best practices in IR, 371\ntokens in, 4\nuni\ufb01cation-based models, 19\nHELM (hidden event language model)\napplied to sentence segmentation, 36\nmethods for sentence or topic segmentation,\n40\nHidden event language model (HELM)\napplied to sentence segmentation, 36\nmethods for sentence or topic segmentation,\n40\nHidden Markov model (HMM)\napplied to topic and sentence segmentation,\n34\u201336\nmeasuring token frequency, 369\nmention detection and, 287\nmethods for sentence or topic segmentation,\n39\nword alignment between languages and,\n340\nHierarchical Dirichlet process (HDP), 1", "tation,\n34\u201336\nmeasuring token frequency, 369\nmention detection and, 287\nmethods for sentence or topic segmentation,\n39\nword alignment between languages and,\n340\nHierarchical Dirichlet process (HDP), 187\nHierarchical phrase-based models, in machine\ntranslation, 350\u2013351\nHierarchical phrase pairs, in machine\ntranslation, 351\nHigh-level features, in event matching, 324\nHindi. See also Indian languages\nIR and, 390\nresources for semantic parsing, 122\ntranslingual summarization, 399\nHistory, conditional context of probability,\n83\nHMM. See Hidden Markov model (HMM)\nHomonymy\nin Korean, 10\nword sense ambiguities and, 104\nHowNet\ndictionary-based approach to subjectivity\nand sentiment analysis, 272\u2013273\nsemantic parsing resource, 105\nHTML Parser, preprocessing IR documents,\n392\nHunalign tool, for machine translation, 357\nHungarian\ndependency graphs in syntax analysis, 65\nIR and, 390\nmorphological richness of, 355\nHybrid methods, for segmentation, 39\u201340\nHypergraphs, worst-case parsing algorithm\nfor ", "ne translation, 357\nHungarian\ndependency graphs in syntax analysis, 65\nIR and, 390\nmorphological richness of, 355\nHybrid methods, for segmentation, 39\u201340\nHypergraphs, worst-case parsing algorithm\nfor CFGs, 74\u201379\nHypernyms, 442\nHyponymy, 310\nHypotheses, machine translation and, 346\nIBM Models, for machine translation, 338\u2013341\nIdenti\ufb01cation, of arguments, 123, 139\u2013140\nIDF. See Inverse document frequency (IDF)\nIE. See Information extraction (IE)\nILP (Integer linear programming), 247\nImplementation process, in RTE\nalignment, 233\u2013236\nenrichment, 228\u2013231\ngraph generation, 231\u2013232\ninference, 236\u2013238\noverview of, 227\npreprocessing, 227\u2013228\ntraining, 238\nIMS (It Makes Sense), program for word\nsense disambiguation, 117\nIndependence assumption\ndocument retrieval and, 372\novercoming in predicate-argument\nstructure, 137\u2013138\nIndexes\nof documents in distillation system, 483\nfor IR generally, 366\nlatent semantic indexing (LSI), 381\nfor monolingual IR, 373\u2013374\nfor multilingual IR, 383\u2013384\nphrase indice", "ent\nstructure, 137\u2013138\nIndexes\nof documents in distillation system, 483\nfor IR generally, 366\nlatent semantic indexing (LSI), 381\nfor monolingual IR, 373\u2013374\nfor multilingual IR, 383\u2013384\nphrase indices, 366, 369\u2013370\npositional indices, 366\ntranslating MLIR queries, 384\n\nIndex\n565\nIndian languages, IR and. See also Hindi, 390\nINDRI document retrieval system, 323\nInexact retrieval models, for monolingual\ninformation retrieval, 374\nInfAP metrics, for IR performance, 389\nInference, textual. See Textual inference\nIn\ufb02ectional paradigms\nin Czech, 11\u201312\nin morphologically rich languages, 189\nInformation context, as measure of semantic\nsimilarity, 112\nInformation extraction (IE). See also Entity\ndetection and tracking (EDT)\nde\ufb01ned, 285\nentity and event resolution and, 100\nInformation retrieval (IR)\nbibliography, 394\u2013396\ncrosslingual. See Crosslingual information\nretrieval (CLIR)\ndata sets used in evaluation of, 389\u2013391\ndistillation compared with, 475\ndocument preprocessing for, 366\u2013367\ndocument", "\nbibliography, 394\u2013396\ncrosslingual. See Crosslingual information\nretrieval (CLIR)\ndata sets used in evaluation of, 389\u2013391\ndistillation compared with, 475\ndocument preprocessing for, 366\u2013367\ndocument syntax and encoding, 367\u2013368\nevaluation in, 386\u2013387, 391\nintroduction to, 366\nkey word searches in, 433\nmeasures in, 388\u2013389\nmonolingual. See Monolingual information\nretrieval\nmultilingual. See Multilingual information\nretrieval (MLIR)\nnormalization and, 370\u2013371\npreprocessing best practices, 371\u2013372\nredundancy problem and, 488\nrelevance assessment, 387\u2013388\nsummary, 393\ntokenization and, 369\u2013370\ntools, software, and resources, 391\u2013393\ntranslingual, 491\nInformative summaries, in automatic\nsummarization, 401\u2013404\nInfoSphere Streams, 530\u2013531\nInsertion metric, in machine translation, 335\nInteger linear programming (ILP), 247\nInteractive voice response (IVR), 505, 511\nInteroperability Demo (IOD), GALE case\nstudy\ncomputational e\ufb03ciency, 537\n\ufb02exible application building with, 537\nfunctional descri", "near programming (ILP), 247\nInteractive voice response (IVR), 505, 511\nInteroperability Demo (IOD), GALE case\nstudy\ncomputational e\ufb03ciency, 537\n\ufb02exible application building with, 537\nfunctional description, 532\u2013534\nimplementing, 534\u2013537\noverview of, 531\u2013532\nInteroperability, in aggregated NLP, 540\nInterpolation, language model adaptation\nand, 176\nIntrinsic evaluation, of summarization, 412\nInverse document frequency (IDF)\nanswer scores in QA and, 450\u2013451\ndocument representation in monoligual IR,\n373\nrelationship questions and, 488\nsearching over unstructured sources, 445\nInverted indexes, for monolingual information\nretrieval, 373\u2013374\nIOD case study. See Interoperability Demo\n(IOD), GALE case study\nIR. See Information retrieval (IR)\nIrregularity\nde\ufb01ned, 8\nissues with morphology induction, 21\nin linguistic models, 8\u201310\nIRSTLM toolkit, for machine translation, 357\nIsolating (analytic) languages\n\ufb01nite-state technology applied to, 18\nmorphological typology and, 7\nIt Makes Sense (IMS), prog", "21\nin linguistic models, 8\u201310\nIRSTLM toolkit, for machine translation, 357\nIsolating (analytic) languages\n\ufb01nite-state technology applied to, 18\nmorphological typology and, 7\nIt Makes Sense (IMS), program for word\nsense disambiguation, 117\nItalian\ndependency graphs in syntax analysis, 65\nIR and, 390\u2013391\nnormalization and, 371\npolarity analysis of words and phrases, 269\nQA and, 461\nRTE in, 218\nsummarization and, 399\nWordNet and, 109\nIVR (interactive voice response), 505, 511\nIXIR distillation system, 488\u2013489\nJapanese\nas agglutinative language, 7\nanaphora frequency in, 444\ncall-\ufb02ow localization and, 514\ncrosslingual QA, 455\ndiscourse parsers for, 403\nEDT and, 286\nGeoQuery corpus translated into, 149\nIR and, 390\n\n566\nIndex\nJapanese (continued)\nirregular verbs, 10\nlanguage modeling, 193\u2013194\npolarity analysis of words and phrases, 269\npreprocessing best practices in IR, 371\u2013372\nQA architectures and, 437\u2013438, 461, 464\nsemantic parsing, 122, 151\nsubjectivity and sentiment analysis, 259,\n267\u201327", "arity analysis of words and phrases, 269\npreprocessing best practices in IR, 371\u2013372\nQA architectures and, 437\u2013438, 461, 464\nsemantic parsing, 122, 151\nsubjectivity and sentiment analysis, 259,\n267\u2013271\nword order and, 356\nword segmentation in, 4\u20135\nJAVELIN system, for QA, 437\nJoint inference, NLP and, 320\nJoint systems\noptimization vs. interoperability in\naggregated NLP, 540\ntypes of EDT architectures, 286\nJoshua machine translation program, 357, 423\nJRC-Acquis corpus\nfor evaluating IR systems, 390\nfor machine translation, 358\nKBP (Knowledge Base Population), of Text\nAnalysis Conferences (TAC), 481\u2013482\nKernel functions, SVM mapping and, 317\nKernel methods, for relation extraction, 319\nKeyword searches\nin IR, 433\nsearching over unstructured sources,\n443\u2013445\nKL-ONE system, for predicate-argument\nrecognition, 122\nKneser-Ney smoothing technique, in language\nmodel estimation, 172\nKnowledge Base Population (KBP), of Text\nAnalysis Conferences (TAC), 481\u2013482\nKorean\nas agglutinative language, 7\n", "ent\nrecognition, 122\nKneser-Ney smoothing technique, in language\nmodel estimation, 172\nKnowledge Base Population (KBP), of Text\nAnalysis Conferences (TAC), 481\u2013482\nKorean\nas agglutinative language, 7\nambiguity in, 10\u201311\ndictionary-based approach in, 16\nEDT and, 286\nencoding and script, 368\n\ufb01nite-state models, 18\ngender, 13\ngenerative parsing model, 92\nIR and, 390\nirregular verbs, 10\nlanguage modeling, 190\nlanguage modeling using subword units, 192\nmorphemes in, 6\u20137\npolarity analysis of words and phrases, 269\npreprocessing best practices in IR, 371\u2013372\nresources for semantic parsing, 122\nword segmentation in, 4\u20135\nKRISPER program, for rule-based semantic\nparsing, 151\nLanguage identi\ufb01cation, in MLIR, 383\nLanguage models\nadaptation, 176\u2013178\nBayesian parameter estimation, 173\u2013174\nBayesian topic-based, 186\u2013187\nbibliography, 199\u2013208\nclass-based, 178\u2013179\ncrosslingual, 196\u2013198\ndiscriminative, 179\u2013180\nfor document retrieval, 375\u2013376\nevaluation of, 170\u2013171\nfactored, 183\u2013184\nintroduction to, 169\nl", "-based, 186\u2013187\nbibliography, 199\u2013208\nclass-based, 178\u2013179\ncrosslingual, 196\u2013198\ndiscriminative, 179\u2013180\nfor document retrieval, 375\u2013376\nevaluation of, 170\u2013171\nfactored, 183\u2013184\nintroduction to, 169\nlanguage-speci\ufb01c problems, 188\u2013189\nlarge-scale models, 174\u2013176\nMaxEnt, 181\u2013183\nmaximum-likelihood estimation and\nsmoothing, 171\u2013173\nmorphological categories in, 192\u2013193\nfor morphologically rich languages,\n189\u2013191\nmultilingual, 195\u2013196\nn-gram approximation, 170\nneural network, 187\u2013188\nspoken vs. written languages and, 194\u2013195\nsubword unit selection, 191\u2013192\nsummary, 198\nsyntax-based, 180\u2013181\ntree-based, 185\u2013186\ntypes of, 178\nvariable-length, 179\nword segmentation and, 193\u2013194\nThe Language Understanding Annotated\nCorpus, 425\nLangue and parole (de Saussure), 13\nLatent Dirichlet allocation (LDA) model, 186\nLatent semantic analysis (LSA)\nbilingual (bLSA), 197\u2013198\nlanguage model adaptation and, 176\u2013177\nprobabilistic (PLSA), 176\u2013177\n\nIndex\n567\nLatent semantic indexing (LSI), 381\nLatin\nas fusional ", "del, 186\nLatent semantic analysis (LSA)\nbilingual (bLSA), 197\u2013198\nlanguage model adaptation and, 176\u2013177\nprobabilistic (PLSA), 176\u2013177\n\nIndex\n567\nLatent semantic indexing (LSI), 381\nLatin\nas fusional language, 8\nmorphologies of, 20\npreprocessing best practices in IR, 371\ntransliteration of scripts to, 368\nLatvian\nIR and, 390\nsummarization and, 399\nLDA (Latent Dirichlet allocation) model, 186\nLDC. See Linguistic Data Consortium (LDC)\nLDOCE (Longman Dictionary of\nContemporary English), 104\nLEA. See Lexical entailment algorithm (LEA)\nLearning, discriminative approach to, 84\nLemmas\nde\ufb01ned, 5\nmachine translation metrics and, 336\nmapping terms to, 370\nLemmatizers\nmapping terms to lemmas, 370\npreprocessing best practices in IR, 371\nLemur IR framework, 392\nLesk algorithm, 105\u2013106\nLexemes\nfunctional morphology models and, 19\noverview of, 5\nLexical chains, in topic segmentation, 38, 43\nLexical choice, in machine translation, 354\u2013355\nLexical collocation, 401\nLexical entailment algorithm (LEA)\nali", "l morphology models and, 19\noverview of, 5\nLexical chains, in topic segmentation, 38, 43\nLexical choice, in machine translation, 354\u2013355\nLexical collocation, 401\nLexical entailment algorithm (LEA)\nalignment stage of RTE model, 236\nenrichment stage of RTE model, 228\u2013231\ninference stage of RTE model, 237\npreprocessing stage in RTE model, 227\u2013228\ntraining stage of RTE model, 238\nLexical features\ncontext as, 110\nin coreference models, 301\nin event matching, 324\nin mention detection, 292\nof relation extraction systems, 314\nin sentence and topic segmentation, 42\u201343\nLexical matching, 212\u2013213\nLexical ontologies, relation extraction and,\n310\nLexical strings, 17, 18\nLexicon, of languages\nbuilding, 265\u2013266\ndictionary-based approach to subjectivity\nand sentiment analysis, 270, 273\nElixirFM lexicon of Arabic, 20\nsets of lexemes constituting, 5\nsubjectivity and sentiment analysis with,\n262, 275\u2013276\nLexPageRank, approach to automatic\nsummarization, 406, 411\nLexTools, for \ufb01nite-state morphology, 16\nLi", "Arabic, 20\nsets of lexemes constituting, 5\nsubjectivity and sentiment analysis with,\n262, 275\u2013276\nLexPageRank, approach to automatic\nsummarization, 406, 411\nLexTools, for \ufb01nite-state morphology, 16\nLinear model interpolation, for smoothing\nlanguage model estimates, 173\nLinearRank algorithm, learning\nsummarization, 408\nlingPipe tool, for summarization, 423\nLinguistic challenges, in MT\nlexical choice, 354\u2013355\nmorphology and, 355\nword order and, 356\nLinguistic Data Consortium (LDC)\ncorpora for machine translation, 358\nevaluating co-occurrence of word between\nlanguages, 337\nhistory of summarization systems, 399\nOntoNotes corpus, 104\non sentence segmentation markers in\nconversational speech, 31\nsummarization frameworks, 422\nList questions\nextension to, 453\nQA and, 433\nLocal collocations, features of supervised\nsystems, 110\u2013111\nLocalization, of spoken dialog systems\ncall-\ufb02ow localization, 514\nlocalization of grammars, 516\u2013517\noverview of, 513\u2013514\nprompt localization, 514\u2013516\ntesting, 519\u2013520", "es of supervised\nsystems, 110\u2013111\nLocalization, of spoken dialog systems\ncall-\ufb02ow localization, 514\nlocalization of grammars, 516\u2013517\noverview of, 513\u2013514\nprompt localization, 514\u2013516\ntesting, 519\u2013520\ntraining, 517\u2013519\nLog-linear models, phrase-based models for\nMT, 348\u2013349\nLogic-based representation, applying to RTE,\n242\u2013244\nLogographic scripts, preprocessing best\npractices in IR, 371\nLong-distance dependencies, syntax-based\nlanguage models for, 180\u2013181\nLongman Dictionary of Contemporary\nEnglish (LDOCE), 104\n\n568\nIndex\nLookup operations, dictionaries and, 16\nLoudness, prosodic cues, 45\u201347\nLow-level features, in event matching, 324\nLucene\ndocument indexing with, 483\ndocument retrieval with, 483\u2013484\nIR frameworks, 392\nLUNAR QA system, 434\nMachine learning. See also Conditional\nrandom \ufb01elds (CRFs)\nevent extraction and, 322\nmeasuring token frequency, 369\nsummarization and, 406\u2013409\nword alignment as learning problem,\n341\u2013343\nMachine translation (MT)\nalignment models, 340\nautomatic evaluatio", "CRFs)\nevent extraction and, 322\nmeasuring token frequency, 369\nsummarization and, 406\u2013409\nword alignment as learning problem,\n341\u2013343\nMachine translation (MT)\nalignment models, 340\nautomatic evaluation, 334\u2013335\nbibliography, 360\u2013363\nchart decoding, 351\u2013352\nCLIR applied to, 380\u2013381\nco-occurrence of words and, 337\u2013338\ncoping with model size, 349\u2013350\ncorpora for, 358\ncrosslingual QA and, 454\ncube pruning approach to decoding,\n347\u2013348\ndata reorganization and, 536\ndata resources for, 356\u2013357\ndecoding phrase-based models, 345\u2013347\nexpectation maximization (EM) algorithm,\n339\u2013340\nfuture directions, 358\u2013359\nin GALE IOD, 532\u2013533\nhierarchical phrase-based models, 350\u2013351\nhistory and current state of, 331\u2013332\nhuman assessment and, 332\u2013334\nIBM Model 1, 338\u2013339\nlexical choice, 354\u2013355\nlinguistic choices, 354\nlog-linear models and parameter tuning,\n348\u2013349\nmeaning evaluation, 332\nmetrics, 335\u2013337\nmorphology and, 355\nmultilingual automatic summarization and,\n410\noverview of, 331\nparaphrasing and, 59\np", ", 354\nlog-linear models and parameter tuning,\n348\u2013349\nmeaning evaluation, 332\nmetrics, 335\u2013337\nmorphology and, 355\nmultilingual automatic summarization and,\n410\noverview of, 331\nparaphrasing and, 59\nphrase-based models, 343\u2013344\nprograms for, 423\nRTE applied to, 217\u2013218\nin RTTS, 538\nsentences as processing unit in, 29\nstatistical. See Statistical machine\ntranslation (SMT)\nsummary, 359\nsymmetrization, 340\u2013341\nsyntactic models, 352\u2013354\nsystems for, 357\u2013358\nin TALES, 538\ntools for, 356\u2013357, 392\ntraining issues, 197\ntraining phrase-based models, 344\u2013345\ntranslation-based approach to CLIR,\n378\u2013380\ntree-based models, 350\nword alignment and, 337, 341\u2013343\nword order and, 356\nMAP (maximum a posteriori)\nBayesian parameter estimation and,\n173\u2013174\nlanguage model adaptation and, 177\u2013178\nMAP (Mean average precision), metrics for\nIR systems, 389\nMarathi, 390\nMargin infused relaxed algorithm (MIRA)\nmethods for sentence or topic segmentation,\n39\nunsupervised approaches to machine\nlearning, 342\nMarkov mo", "e precision), metrics for\nIR systems, 389\nMarathi, 390\nMargin infused relaxed algorithm (MIRA)\nmethods for sentence or topic segmentation,\n39\nunsupervised approaches to machine\nlearning, 342\nMarkov model. See also Hidden Markov\nmodel (HMM), 34\u201336\nMatches, machine translation metrics, 335\nMatching events, 323\u2013326\nMate retrieval setup, relevance assessment\nand, 388\nMaxEnt model\napplied to distillation, 480\nclassi\ufb01ers for relation extraction, 316\u2013317\nclassi\ufb01ers for sentence or topic\nsegmentation, 37, 39\u201340\ncoreference resolution with, 300\u2013301\nlanguage model adaptation and, 177\nmemory-based learning compared with, 322\nmention detection, 287\u2013289\n\nIndex\n569\nmodeling using morphological categories,\n193\nmodeling without word segmentation, 194\noverview of, 181\u2013183\nsubjectivity and sentiment analysis with,\n274\nunsupervised approaches to machine\nlearning, 342\nMaximal marginal relevance (MMR), in\nautomatic summarization, 399\nMaximum a posteriori (MAP)\nBayesian parameter estimation and,\n173\u2013174\nlan", "s with,\n274\nunsupervised approaches to machine\nlearning, 342\nMaximal marginal relevance (MMR), in\nautomatic summarization, 399\nMaximum a posteriori (MAP)\nBayesian parameter estimation and,\n173\u2013174\nlanguage model adaptation and, 177\u2013178\nMaximum-likelihood estimation\nBayesian parameter estimation and,\n173\u2013174\nas parameter estimation language model,\n171\u2013173\nused with document models in information\nretrieval, 375\u2013376\nMEAD system, for automatic summarization,\n410\u2013411, 423\nMean average precision (MAP), metrics for\nIR systems, 389\nMean reciprocal rank (MRR), metrics for QA\nsystems, 462\u2013463\nMeaning chunks, semantic parsing and, 97\nMeaning of words. See Word meaning\nMeaning representation\nAir Travel Information System (ATIS), 148\nCommunicator program, 148\u2013149\nGeoQuery, 149\noverview of, 147\u2013148\nRoboCup, 149\nrule-based systems for, 150\nsemantic interpretation and, 101\nsoftware programs for, 151\nsummary, 153\u2013154\nsupervised systems for, 150\u2013151\nMeasures. See Metrics\nMedia Resource Control Protocol ", "Cup, 149\nrule-based systems for, 150\nsemantic interpretation and, 101\nsoftware programs for, 151\nsummary, 153\u2013154\nsupervised systems for, 150\u2013151\nMeasures. See Metrics\nMedia Resource Control Protocol (MRCP),\n504\nMeeting Recorder Dialog Act (MRDA), 31\nMemory-based learning, 322\nMEMT (multi-engine machine translation), in\nGALE IOD, 532\u2013533\nMention detection\nBell tree and, 297\ncomputing probability of mention links,\n297\u2013300\ndata-driven classi\ufb01cation, 287\u2013289\nexperiments in, 294\u2013295\nfeatures for, 291\u2013294\ngreedy best-\ufb01t decoding, 322\nMaxEnt model applied to entity-mention\nrelationships, 301\nmention-matching features in event\nmatching, 324\noverview of, 287\nproblems in information extraction,\n285\u2013286\nin Rosetta Consortium distillation system,\n480\u2013481\nsearching for mentions, 289\u2013291\nMention-synchronous process, 297\nMentions\nentity relations and, 310\u2013311\nnamed, nominal, prenominal, 287\nMeronymy, 310\nMERT (minimum error rate training), 349\nMETEOR, metrics for machine translation,\n336\nMETONYMY cl", "onous process, 297\nMentions\nentity relations and, 310\u2013311\nnamed, nominal, prenominal, 287\nMeronymy, 310\nMERT (minimum error rate training), 349\nMETEOR, metrics for machine translation,\n336\nMETONYMY class, ACE, 312\nMetrics\ndistillation, 491\u2013494\ngraph generation and, 231\nIR, 388\nmachine translation, 335\u2013337\nmagnitude of RTE metrics, 233\nfor multilingual automatic summarization,\n419\u2013420\nQA, 462\u2013464\nRTE annotation constituents, 222\u2013224\nMicrosoft, history of QA systems and, 435\nMinimum error rate training (MERT), 349\nMinimum spanning trees (MSTs), 79\u201380\nMinipar\ndependency parsing with, 456\nrule-based dependency parser, 131\u2013132\nMIRA (margin infused relaxed algorithm)\nmethods for sentence or topic segmentation,\n39\nunsupervised approaches to machine\nlearning, 342\nMixed initiative dialogs, in spoken dialog\nsystems, 509\n\n570\nIndex\nMLIR. See Multilingual information retrieval\n(MLIR)\nMLIS-MUSI summarization system, 399\nMMR (maximal marginal relevance), in\nautomatic summarization, 399\nModels, infor", "n dialog\nsystems, 509\n\n570\nIndex\nMLIR. See Multilingual information retrieval\n(MLIR)\nMLIS-MUSI summarization system, 399\nMMR (maximal marginal relevance), in\nautomatic summarization, 399\nModels, information retrieval\nmonolingual, 374\u2013376\nselection best practices, 377\u2013378\nModels, word alignment\nEM algorithm, 339\u2013340\nIBM Model 1, 338\u2013339\nimprovements on IBM Model 1, 340\nModern Standard Arabic (MSA), 189\u2013191\nModi\ufb01cation processes, in automatic\nsummarization, 399\u2013400\nModi\ufb01er word, dependency trees and, 131\nMonolingual information retrieval. See also\nInformation retrieval (IR)\ndocument a priori models, 377\ndocument representation, 372\u2013373\nindex structures, 373\u2013374\nmodel selection best practices, 377\u2013378\nmodels for, 374\u2013376\noverview of, 372\nquery expansion technique, 376\u2013377\nMonotonicity\napplying natural logic to RTE, 246\nde\ufb01ned, 224\nMorfessor package, for identifying\nmorphemes, 191\u2013192\nMorphemes\nabstract in morphology induction, 21\nautomatic algorithms for identifying,\n191\u2013192\nde\ufb01ned, 4\nexa", "g natural logic to RTE, 246\nde\ufb01ned, 224\nMorfessor package, for identifying\nmorphemes, 191\u2013192\nMorphemes\nabstract in morphology induction, 21\nautomatic algorithms for identifying,\n191\u2013192\nde\ufb01ned, 4\nexamples of, 6\u20137\nfunctional morphology models and, 19\nJapanese text segmented into, 438\nlanguage modeling for morphologically rich\nlanguages, 189\noverview of, 5\u20136\nparsing issues related to, 90\u201391\ntypology and, 7\u20138\nMorphological models\nautomating (morphology induction), 21\ndictionary-based, 15\u201316\n\ufb01nite-state, 16\u201318\nfunctional, 19\u201321\noverview of, 15\nuni\ufb01cation-based, 18\u201319\nMorphological parsing\nambiguity and, 10\u201313\ndictionary lookup and, 15\ndiscovery of word structure by, 3\nirregularity and, 8\u201310\nissues and challenges, 8\nMorphology\ncategories in language models, 192\u2013193\ncompared with syntax and phonology and\northography, 3\ninduction, 21\nlanguage models for morphologically rich\nlanguages, 189\u2013191\nlinguistic challenges in machine translation,\n355\nparsing issues related to, 90\u201392\ntypology, 7\u20138\nMor", "nology and\northography, 3\ninduction, 21\nlanguage models for morphologically rich\nlanguages, 189\u2013191\nlinguistic challenges in machine translation,\n355\nparsing issues related to, 90\u201392\ntypology, 7\u20138\nMorphs (segments)\ndata-sparseness problem and, 286\nde\ufb01ned, 5\nfunctional morphology models and, 19\nnot all morphs can be assumed to be\nmorphemes, 7\ntypology and, 8\nMoses system\ngrow-diag-\ufb01nal method, 341\nmachine translation, 357, 423\nMPQA corpus\nmanually annotated corpora for English, 274\nsubjectivity and sentiment analysis, 263,\n272\nMRCP (Media Resource Control Protocol),\n504\nMRDA (Meeting Recorder Dialog Act), 31\nMRR (Mean reciprocal rank), metrics for QA\nsystems, 462\u2013463\nMSA (Modern Standard Arabic), 189\u2013191\nMSE (Multilingual Summarization\nEvaluation), 399, 425\nMSTs (minimum spanning trees), 79\u201380\nMultext Dataset, corpora for evaluating IR\nsystems, 390\nMulti-engine machine translation (MEMT),\nin GALE IOD, 532\u2013533\nMultilingual automatic summarization\nautomated evaluation methodologies,\n415\u20134", "\u201380\nMultext Dataset, corpora for evaluating IR\nsystems, 390\nMulti-engine machine translation (MEMT),\nin GALE IOD, 532\u2013533\nMultilingual automatic summarization\nautomated evaluation methodologies,\n415\u2013418\n\nIndex\n571\nbuilding a summarization system, 420\u2013421,\n423\u2013424\nchallenges in, 409\u2013410\ncompetitions related to, 424\u2013425\ndata sets for, 425\u2013426\ndevices/tools for, 423\nevaluating quality of summaries, 412\u2013413\nframeworks summarization system can be\nimplemented in, 422\u2013423\nmanual evaluation methodologies, 413\u2013415\nmetrics for, 419\u2013420\nrecent developments, 418\u2013419\nsystems for, 410\u2013412\nMultilingual information retrieval (MLIR)\naggregation models, 385\nbest practices, 385\u2013386\nde\ufb01ned, 382\nindex construction, 383\u2013384\nlanguage identi\ufb01cation, 383\noverview of, 365\nquery translation, 384\nMultilingual language modeling, 195\u2013196\nMultilingual Summarization Evaluation\n(MSE), 399, 425\nMultimodal distillation, 490\nMultiple reference translations, 336\nMultiple views, overcoming parsing errors,\n142\u2013144\nMURAX, 43", "e modeling, 195\u2013196\nMultilingual Summarization Evaluation\n(MSE), 399, 425\nMultimodal distillation, 490\nMultiple reference translations, 336\nMultiple views, overcoming parsing errors,\n142\u2013144\nMURAX, 434\nn-gram\nlocalization of grammars and, 516\ntrigrams, 502\u2013503\nn-gram approximation\nlanguage model evaluation and, 170\u2013171\nlanguage-speci\ufb01c modeling problems,\n188\u2013189\nmaximum-likelihood estimation, 171\u2013172\nsmoothing techniques in language model\nestimation, 172\nstatistical language models using, 170\nsubword units used with, 192\nn-gram models. See also Phrase indices\nAutoSummENG graph, 419\ncharacter models, 370\nde\ufb01ned, 369\u2013370\ndocument representation in monolingual\nIR, 372\u2013373\nNa\u00a8\u0131ve Bayes\nclassi\ufb01ers for relation extraction, 316\nsubjectivity and sentiment analysis, 274\nNamed entity recognition (NER)\naligning views in RTE, 233\nautomatic summarization and, 398\ncandidate answer generation and, 449\nchallenges in RTE, 212\nenrichment stage of RTE model, 229\u2013230\nfeatures of supervised systems, 112\ngr", "R)\naligning views in RTE, 233\nautomatic summarization and, 398\ncandidate answer generation and, 449\nchallenges in RTE, 212\nenrichment stage of RTE model, 229\u2013230\nfeatures of supervised systems, 112\ngraph generation stage of RTE model, 231\nimpact on searches, 444\nimplementing RTE and, 227\ninformation extraction and, 100\nmention detection related to, 287\nin PSG, 125\u2013126\nQA architectures and, 439\nin Rosetta Consortium distillation system,\n480\nin RTE, 221\nNational Institute of Standards and\nTechnology (NIST)\nBLEU score, 295\nrelation extraction and, 311\nsummarization frameworks, 422\ntextual entailment and, 211, 213\nNatural language\ncall routing, 510\nparsing, 57\u201359\nNatural language generation (NLG), 503\u2013504\nNatural language processing (NLP)\napplications of syntactic parsers, 59\napplying to non-English languages, 218\ndistillation and. See Distillation\nextraction of document structure as aid in,\n29\njoint inference, 320\nmachine translation and, 331\nminimum spanning trees (MST) and, 79\nmultiview", "nglish languages, 218\ndistillation and. See Distillation\nextraction of document structure as aid in,\n29\njoint inference, 320\nmachine translation and, 331\nminimum spanning trees (MST) and, 79\nmultiview representation of analysis, 220\u2013222\npackages for, 253\nproblems in information extraction, 286\nrelation extraction and, 310\nRTE applied to NLP problems, 214\nRTE as sub\ufb01eld of. See Recognizing textual\nentailment (RTE)\nsyntactic analysis of natural language, 57\ntextual inference, 209\n\n572\nIndex\nNatural language processing (NLP),\ncombining engines for aggregation\narchitectures, 527\nbibliography, 548\u2013549\ncomputational e\ufb03ciency, 525\u2013526\ndata-manipulation capacity, 526\n\ufb02exible, distributed componentization,\n524\u2013525\nGALE Interoperability Demo case study,\n531\u2013537\nGeneral Architecture for Text Engineering\n(GATE), 529\u2013530\nInfoSphere Streams, 530\u2013531\nintroduction to, 523\u2013524\nlessons learned, 540\u2013542\nrobust processing, 526\u2013527\nRTTS case study, 538\u2013540\nsummary, 542\nTALES case study, 538\nUnstructured In", "ing\n(GATE), 529\u2013530\nInfoSphere Streams, 530\u2013531\nintroduction to, 523\u2013524\nlessons learned, 540\u2013542\nrobust processing, 526\u2013527\nRTTS case study, 538\u2013540\nsummary, 542\nTALES case study, 538\nUnstructured Information Management\nArchitecture (UIMA), 527\u2013529,\n542\u2013547\nNatural Language Toolkit (NLTK), 422\nNatural language understanding (NLU), 209\nNatural logic-based representation, applying\nto RTE, 245\u2013246\nNDCG (Normalized discounting cumulative\ngain), 389\nNER. See Named entity recognition (NER)\nNeural network language models (NNLMs)\nlanguage modeling using morphological\ncategories, 193\noverview of, 187\u2013188\nNeural networks, approach to machine\nlearning, 342\nNeutralization, homonyms and, 12\nThe New York Times Annotated Corpus, 425\nNewsBlaster, for automatic summarization,\n411\u2013412\nNII Test Collection for IR Systems (NTCIR)\nanswer scores in QA and, 453\ndata sets for evaluating IR systems, 390\nevaluation of QA, 460\u2013464\nhistory of QA systems and, 434\nNIST. See National Institute of Standards\nand Techn", "or IR Systems (NTCIR)\nanswer scores in QA and, 453\ndata sets for evaluating IR systems, 390\nevaluation of QA, 460\u2013464\nhistory of QA systems and, 434\nNIST. See National Institute of Standards\nand Technology (NIST)\nNLG (natural language generation), 503\u2013504\nNLP. See Natural language processing (NLP)\nNLTK (Natural Language Toolkit), 422\nNNLMs (neural network language models)\nlanguage modeling using morphological\ncategories, 193\noverview of, 187\u2013188\nNOMinalization LEXicon (NOMLEX), 121\nNon projective dependency trees, 65\u201366\nNonlinear languages, morphological typology\nand, 8\nNormalization\nArabic, 12\noverview of, 370\u2013371\ntokens and, 4\nZ-score normalization, 385\nNormalized discounting cumulative gain\n(NDCG), 389\nNorwegian, 461\nNoun arguments, 144\u2013146\nNoun head, of prepositional phrases in PSB,\n127\nNTCIR. See NII Test Collection for IR\nSystems (NTCIR)\nNumerical quantities (NUM) constituents, in\nRTE, 221, 233\nObjective word senses, 261\nOCR (Optical character recognition), 31\nOne vs. All (OVA) a", "7\nNTCIR. See NII Test Collection for IR\nSystems (NTCIR)\nNumerical quantities (NUM) constituents, in\nRTE, 221, 233\nObjective word senses, 261\nOCR (Optical character recognition), 31\nOne vs. All (OVA) approach, 136\u2013137\nOntoNotes corpus, 104\nOOV (out of vocabulary)\ncoverage rates in language models, 170\nmorphologically rich languages and, 189\u2013190\nOOV rate\nin Germanic languages, 191\ninventorying morphemes and, 192\nlanguage modeling without word\nsegmentation, 194\nOpen-domain QA systems, 434\nOpen Standard by the Organization for the\nAdvancement of Structured\nInformation Standards (OASIS), 527\nOpenCCG project, 21\nopenNLP, 423\nOpinion questions, QA and, 433\nOpinionFinder\nas rule-based system, 263\nsubjectivity and sentiment analysis,\n271\u2013272, 275\u2013276\nsubjectivity and sentiment analysis lexicon,\n262\n\nIndex\n573\nOptical character recognition (OCR), 31\nOPUS project, corpora for machine\ntranslation, 358\nOrdinal constituent position, in PSG, 127\nORG-AFF (organization-a\ufb03liation) class,\n311\u2013312\nOrthogr", "\n\nIndex\n573\nOptical character recognition (OCR), 31\nOPUS project, corpora for machine\ntranslation, 358\nOrdinal constituent position, in PSG, 127\nORG-AFF (organization-a\ufb03liation) class,\n311\u2013312\nOrthography\nArabic, 11\nissues with morphology induction, 21\nOut of vocabulary (OOV)\ncoverage rates in language models, 170\nmorphologically rich languages and,\n189\u2013190\nPageRank\nautomatic summarization, 401\nLexPageRank compared with, 406\nTextRank compared with, 404\nParadigms\nclassi\ufb01cation, 133\u2013137\nfunctional morphology models and, 19\nin\ufb02ectional paradigms in Czech, 11\u201312\nin\ufb02ectional paradigms in morphologically\nrich languages, 189\nParaEval\nautomatic evaluation of summarization,\n418\nmetrics in, 420\nParagraphs, sentences forming, 29\nParallel backo\ufb00, 184\nParameter estimation language models\nBayesian parameter estimation, 173\u2013174\nlarge-scale models, 174\u2013176\nmaximum-likelihood estimation and\nsmoothing, 171\u2013173\nParameter tuning, 348\u2013349\nParameters, functional morphology models\nand, 19\nParaphrasing, parsi", "eter estimation, 173\u2013174\nlarge-scale models, 174\u2013176\nmaximum-likelihood estimation and\nsmoothing, 171\u2013173\nParameter tuning, 348\u2013349\nParameters, functional morphology models\nand, 19\nParaphrasing, parsing natural language and,\n58\u201359\nParasitic gap recovery, in RTE, 249\nparole and langue (de Saussure), 13\nParsing\nalgorithms for, 70\u201372\nambiguity resolution in, 80\nde\ufb01ned, 97\ndependency parsing, 79\u201380\ndiscriminative models, 84\u201387\ngenerative models, 83\u201384\nhypergraphs and chart parsing, 74\u201379\nnatural language, 57\u201359\nsemantic parsing. See semantic parsing\nsentences as processing unit in, 29\nshift-reduce parsing, 72\u201373\nPart of speech (POS)\nclass-based language models and, 178\nfeatures of supervised systems, 110\nimplementing RTE and, 227\nnatural language grammars and, 60\nin PSG, 125\u2013127\nQA architectures and, 439\nin Rosetta Consortium distillation system,\n480\nfor sentence segmentation, 43\nsyntactic analysis of natural language,\n57\u201358\nPART-WHOLE relation class, 311\nPartial order method, for ranking ", "es and, 439\nin Rosetta Consortium distillation system,\n480\nfor sentence segmentation, 43\nsyntactic analysis of natural language,\n57\u201358\nPART-WHOLE relation class, 311\nPartial order method, for ranking sentences,\n407\nParticle language model, subword units in,\n192\nPartition function, in MaxEnt formula, 316\nPASCAL. See Pattern Analysis, Statistical\nModelling and Computational\nLearning (PASCAL)\nPath\nin CCG, 130\nin PSG, 124, 128\u2013129\nin TAG, 130\nfor verb sense disambiguation, 112\nPattern Analysis, Statistical Modelling and\nComputational Learning (PASCAL)\nevaluating textual entailment, 213\nRTE challenge, 451\u2013452\ntextual entailment and, 211\nPauses, prosodic cues, 45\u201347\nPeer surveys, in evaluation of summarization,\n412\nPenn Treebank\ndependency trees and, 130\u2013132\nparsing issues and, 87\u201389\nperformance degradation and, 147\nphrase structure trees in, 68, 70\nPropBank and, 123\nPER (Position-independent error rate), 335\nPER-SOC (personal-social) relation class, 311\nPerformance\nof aggregated NLP, 541\n\n5", " degradation and, 147\nphrase structure trees in, 68, 70\nPropBank and, 123\nPER (Position-independent error rate), 335\nPER-SOC (personal-social) relation class, 311\nPerformance\nof aggregated NLP, 541\n\n574\nIndex\nPerformance (continued)\ncombining classi\ufb01ers to boost (Combination\nhypothesis), 293\ncompetence vs. performance (Chomsky), 13\nof document segmentation methods, 41\nevaluating IR, 389\nevaluating QA, 462\u2013464\nevaluating RTE, 213\u2013214\nfeature performance in predicate-argument\nstructure, 138\u2013140\nPenn Treebank, 147\nPeriod (.), sentence segmentation markers, 30\nPerplexity\ncriteria in language model evaluation,\n170\u2013171\ninventorying morphemes and, 192\nlanguage modeling using morphological\ncategories, 193\nlanguage modeling without word\nsegmentation, 194\nPersian\nIR and, 390\nuni\ufb01cation-based models, 19\nPhoenix, 150\nPhonemes, 4\nPhonology\ncompared with morphology and syntax and\northography, 3\nissues with morphology induction, 21\nPhrasal verb collocations, in PSG, 126\nPhrase-based models, for MT\nco", " 19\nPhoenix, 150\nPhonemes, 4\nPhonology\ncompared with morphology and syntax and\northography, 3\nissues with morphology induction, 21\nPhrasal verb collocations, in PSG, 126\nPhrase-based models, for MT\ncoping with model size, 349\u2013350\ncube pruning approach to decoding,\n347\u2013348\ndecoding, 345\u2013347\nhierarchical phrase-based models, 350\u2013351\nlog-linear models and parameter tuning,\n348\u2013349\noverview of, 343\u2013344\ntraining, 344\u2013345\nPhrase feature, in PSG, 124\nPhrase indices, tokenization and, 366, 369\u2013370\nPhrase-level annotations, for subjectivity and\nsentiment analysis\ncorpus-based, 267\u2013269\ndictionary-based, 264\u2013267\noverview of, 264\nPhrase Structure Grammar (PSG), 124\u2013129\nPhrase structure trees\nexamples of, 68\u201370\nmorphological information in, 91\nin syntactic analysis, 67\ntreebank construction and, 62\nPhrases\nearly approaches to summarization and, 400\ntypes in CCG, 129\u2013130\nPHYS (physical) relation class, 311\nPipeline approach, to event extraction,\n320\u2013321\nPitch, prosodic cues, 45\u201347\nPivot language, tr", "ases\nearly approaches to summarization and, 400\ntypes in CCG, 129\u2013130\nPHYS (physical) relation class, 311\nPipeline approach, to event extraction,\n320\u2013321\nPitch, prosodic cues, 45\u201347\nPivot language, translation-based approach to\nCLIR, 379\u2013380\nPolarity\ncorpus-based approach to subjectivity and\nsentiment analysis, 269\nrelationship to monotonicity, 246\nword sense classi\ufb01ed by, 261\nPolysemy, 104\nPortuguese\nIR and, 390\u2013391\nQA and, 461\nRTE in, 218\nPOS. See Part of speech (POS)\nPosition-independent error rate (PER), 335\nPositional features, approaches to\nsummarization and, 401\nPositional indices, tokens and, 366\nPosting lists, term relationships in document\nretrieval, 373\u2013374\nPre-reordering, word order in machine\ntranslation, 356\nPreboundary lengthening, in sentence\nsegmentation, 47\nPrecision, IR evaluation measure, 388\nPredicate-argument structure\nbase phrase chunks, 132\u2013133\nclassi\ufb01cation paradigms, 133\u2013137\nCombinatory Categorical Grammar (CCG),\n129\u2013130\ndependency trees, 130\u2013132\nfeature perfo", "evaluation measure, 388\nPredicate-argument structure\nbase phrase chunks, 132\u2013133\nclassi\ufb01cation paradigms, 133\u2013137\nCombinatory Categorical Grammar (CCG),\n129\u2013130\ndependency trees, 130\u2013132\nfeature performance, salience, and selection,\n138\u2013140\nFrameNet resources, 118\u2013119\nmultilingual issues, 146\u2013147\nnoun arguments, 144\u2013146\nother resources, 121\u2013122\novercoming parsing errors, 141\u2013144\n\nIndex\n575\novercoming the independence assumption,\n137\u2013138\nPhrase Structure Grammar (PSG), 124\u2013129\nPropBank resources, 119\u2013121\nrobustness across genres, 147\nsemantic interpretation and, 100\nsemantic parsing. See Predicate-argument\nstructure\nsemantic role labeling, 118\nsizing training data, 140\u2013141\nsoftware programs for, 147\nstructural matching and, 447\u2013448\nsummary, 153\nsyntactic representation, 123\u2013124\nsystems, 122\u2013123\nTree-Adjoining Grammar, 130\nPredicate context, in PSG, 129\nPredicate feature, in Phrase Structure\nGrammar (PSG), 124\nPrepositional phrase adjunct, features of\nsupervised systems, 111\nPreprocessin", "\u2013123\nTree-Adjoining Grammar, 130\nPredicate context, in PSG, 129\nPredicate feature, in Phrase Structure\nGrammar (PSG), 124\nPrepositional phrase adjunct, features of\nsupervised systems, 111\nPreprocessing, in IR\nbest practices, 371\u2013372\ndocuments for information retrieval,\n366\u2013367\ntools for, 392\nPreprocessing, in RTE\nimplementing, 227\u2013228\nmodeling, 224\u2013225\nPreprocessing queries, 483\nPreterminals. See Part of speech (POS)\nPrevious role, in PSG, 126\nPRF (Pseudo relevance feedback)\nas alternative to query expansion, 445\noverview of, 377\nPrivate states. See also Subjectivity and\nsentiment analysis, 260\nProbabilistic context-free grammars (PCFGs)\nfor ambiguity resolution, 80\u201383\ndependency graphs in syntax analysis,\n66\u201367\ngenerative parsing models, 83\u201384\nparsing techniques, 78\nProbabilistic latent semantic analysis\n(PLSA), 176\u2013177\nProbabilistic models\ndocument a priori models, 377\nfor document retrieval, 375\nProbability\nhistory of, 83\nMaxEnt formula for conditional probability,\n316\nProductivity/", "emantic analysis\n(PLSA), 176\u2013177\nProbabilistic models\ndocument a priori models, 377\nfor document retrieval, 375\nProbability\nhistory of, 83\nMaxEnt formula for conditional probability,\n316\nProductivity/creativity, and the unknown\nword problem, 13\u201315\nProjective dependency trees\noverview of, 64\u201365\nworst-case parsing algorithm for CFGs, 78\nProjectivity\nin dependency analysis, 64\nnon projective dependency trees, 65\u201367\nprojective dependency trees, 64\u201365\nPrompt localization, spoken dialog systems,\n514\u2013516\nPropBank\nannotation of, 447\ndependency trees and, 130\u2013132\nlimitation of, 122\nPenn Treebank and, 123\nas resource for predicate-argument\nrecognition, 119\u2013122\ntagging text with arguments, 124\nProsody\nde\ufb01ned, 45\nsentence and topic segmentation, 45\u201348\nPseudo relevance feedback (PRF)\nas alternative to query expansion, 445\noverview of, 377\nPSG (Phrase Structure Grammar), 124\u2013129\nPublications, resources for RTE, 252\nPunctuation\nin PSG, 129\ntypographical and structural features for\nsentence and topic ", "query expansion, 445\noverview of, 377\nPSG (Phrase Structure Grammar), 124\u2013129\nPublications, resources for RTE, 252\nPunctuation\nin PSG, 129\ntypographical and structural features for\nsentence and topic segmentation, 44\u201345\nPUNDIT, 122\nPushdown automaton, in CFGs, 72\nPyramid, for manual evaluation of\nsummarization, 413\u2013415\nQA. See Question answering (QA)\nQUALM QA system, 434\nQueries\nevaluation in distillation, 492\npreprocessing, 483\nQA architectures and, 439\nsearching unstructured sources, 443\u2013445\ntranslating CLIR queries, 379\ntranslating MLIR queries, 384\n\n576\nIndex\nQuery answering distillation system\ndocument retrieval, 483\u2013484\noverview of, 483\nplanning stage, 487\npreprocessing queries, 483\nsnippet \ufb01ltering, 484\nsnippet processing, 485\u2013487\nQuery expansion\napplying to CLIR queries, 380\nfor improving information retrieval, 376\u2013377\nsearching over unstructured sources, 445\nQuery generation, in QA architectures, 435\nQuery language, in CLIR, 365\nQuestion analysis, in QA, 435, 440\u2013443\nQuestion ", "proving information retrieval, 376\u2013377\nsearching over unstructured sources, 445\nQuery generation, in QA architectures, 435\nQuery language, in CLIR, 365\nQuestion analysis, in QA, 435, 440\u2013443\nQuestion answering (QA)\nanswer scores, 450\u2013453\narchitectures, 435\u2013437\nbibliography, 467\u2013473\ncandidate extraction from structured\nsources, 449\u2013450\ncandidate extraction from unstructured\nsources, 445\u2013449\ncase study, 455\u2013460\nchallenges in, 464\u2013465\ncrosslingual, 454\u2013455\nevaluating answer correctness, 461\u2013462\nevaluation tasks, 460\u2013461\nintroduction to and history of, 433\u2013435\nIR compared with, 366\nperformance metrics, 462\u2013464\nquestion analysis, 440\u2013443\nRTE applied to, 215\nsearching over unstructured sources,\n443\u2013445\nsource acquisition and preprocessing,\n437\u2013440\nsummary, 465\u2013467\nQuestion mark (?), sentence segmentation\nmarkers, 30\nQuestions, in GALE distillation initiative, 475\nQuotation marks (\u201c\u201d), sentence segmentation\nmarkers, 30\nR summarization frameworks, 422\nRandLM toolkit, for machine translation, 3", "mentation\nmarkers, 30\nQuestions, in GALE distillation initiative, 475\nQuotation marks (\u201c\u201d), sentence segmentation\nmarkers, 30\nR summarization frameworks, 422\nRandLM toolkit, for machine translation, 357\nRandom forest language models (RFLMs)\nmodeling using morphological categories,\n193\ntree-based modeling, 185\u2013186\nRanks methods, for sentences, 407\nRDF (Resource Description Framework), 450\nReal-Time Translation Services (RTTS),\n538\u2013540\nRealization stage, of summarization systems\nbuilding a summarization system and, 421\noverview of, 400\nRecall, IR evaluation measures, 388\nRecall-Oriented Understudy for Gisting\nEvaluation (ROUGE)\nautomatic evaluation of summarization,\n415\u2013418\nmetrics in, 420\nRecognizing textual entailment (RTE)\nalignment, 233\u2013236\nanalysis, 220\nanswer scoring and, 464\napplications of, 214\nbibliography, 254\u2013258\ncase studies, 238\u2013239\nchallenge of, 212\u2013213\ncomparing constituents in, 222\u2013224\ndeveloping knowledge resources for,\n249\u2013251\ndiscourse commitments extraction case\nstudy", "ons of, 214\nbibliography, 254\u2013258\ncase studies, 238\u2013239\nchallenge of, 212\u2013213\ncomparing constituents in, 222\u2013224\ndeveloping knowledge resources for,\n249\u2013251\ndiscourse commitments extraction case\nstudy, 239\u2013240\nenrichment, 228\u2013231\nevaluating performance of, 213\u2013214\nframework for, 219\ngeneral model for, 224\u2013227\ngraph generation, 231\u2013232\nimplementation of, 227\nimproving analytics, 248\u2013249\nimproving evaluation, 251\u2013252\ninference, 236\u2013238\nintroduction to, 209\u2013210\ninvesting/applying to new problems, 249\nlatent alignment inference, 247\u2013248\nlearning alignment independently of\nentailment, 244\u2013245\nleveraging multiple alignments, 245\nlimited dependency context for global\nsimilarity, 247\nlogical representation and inference,\n242\u2013244\nmachine translation, 217\u2013218\nmultiview representation, 220\u2013222\nnatural logic and, 245\u2013246\nin non-English languages, 218\u2013219\n\nIndex\n577\nPASCAL challenge, 451\npreprocessing, 227\u2013228\nproblem de\ufb01nition, 210\u2013212\nQA and, 215, 433\u2013434\nrequirements for RTE framework, 219\u2013220\nr", "l logic and, 245\u2013246\nin non-English languages, 218\u2013219\n\nIndex\n577\nPASCAL challenge, 451\npreprocessing, 227\u2013228\nproblem de\ufb01nition, 210\u2013212\nQA and, 215, 433\u2013434\nrequirements for RTE framework, 219\u2013220\nresources for, 252\u2013253\nsearching for relations, 215\u2013217\nsummary, 253\u2013254\nSyntactic Semantic Tree Kernels (SSTKs),\n246\u2013247\ntraining, 238\ntransformation-based approaches to,\n241\u2013242\ntree edit distance case study, 240\u2013241\nRecombination, machine translation and, 346\nRecursive transition networks (RTNs), 150\nRedundancy, in distillation\ndetecting, 492\u2013493\noverview of, 477\u2013479\nreducing, 489\u2013490\nRedundancy, in IR, 488\nReduplication of words, limits of \ufb01nite-state\nmodels, 17\nReference summaries, 412, 419\nRegular expressions\nsurface patterns for extracting candidate\nanswers, 449\nin type-based candidate extraction, 446\nRegular relations, \ufb01nite-state transducers\ncapturing and computing, 17\nRelated terms, in GALE distillation initiative,\n475\nRelation extraction systems\nclassi\ufb01cation approach, 312\u2013313\nco", "xtraction, 446\nRegular relations, \ufb01nite-state transducers\ncapturing and computing, 17\nRelated terms, in GALE distillation initiative,\n475\nRelation extraction systems\nclassi\ufb01cation approach, 312\u2013313\ncoreference resolution as, 311\nfeatures of classi\ufb01cation-based systems,\n313\u2013316\nkernel methods for, 319\noverview of, 310\nsupervised and unsupervised, 317\u2013319\nRelational databases, 449\nRelations\nbibliography, 327\u2013330\nclassi\ufb01ers for, 316\ncombining entity and relation detection, 320\nbetween constituents in RTE, 220\ndetection in Rosetta Consortium\ndistillation system, 480\u2013482\nextracting, 310\u2013313\nfeatures of classi\ufb01cation-based extractors,\n313\u2013316\nintroduction to, 309\u2013310\nkernel methods for extracting, 319\nrecognition impacting searches, 444\nsummary, 326\u2013327\nsupervised and unsupervised approaches to\nextracting, 317\u2013319\ntransitive closure of, 324\u2013326\ntypes of, 311\u2013312\nRelationship questions, QA and, 433, 488\nRelevance, feedback and query expansion,\n376\u2013377\nRelevance, in distillation\nanalysis of, 4", "extracting, 317\u2013319\ntransitive closure of, 324\u2013326\ntypes of, 311\u2013312\nRelationship questions, QA and, 433, 488\nRelevance, feedback and query expansion,\n376\u2013377\nRelevance, in distillation\nanalysis of, 492\u2013493\ndetecting, 488\u2013489\nexamples of irrelevant answers, 477\noverview of, 477\u2013479\nredundancy reduction and, 488\u2013490\nRelevance, in IR\nassessment, 387\u2013388\nevaluation, 386\nRemote operation, challenges in NLP\naggregation, 524\nResource Description Framework (RDF), 450\nResources, for RTE\ndeveloping knowledge resources, 249\u2013251\noverview of, 252\u2013253\nRestricted domains, history of QA systems,\n434\nResult pooling, relevance assessment and, 387\nRewrite rules (in phonology and morphology),\n17\nRFLMs (Random forest language models)\nmodeling using morphological categories,\n193\ntree-based modeling, 185\u2013186\nRhetorical structure theory (RST), applying\nto summarization, 401\u2013404\nRoboCup, for meaning representation, 149\nRobust processing\ndesired attributes of NLP aggregation,\n526\u2013527\nin GATE, 529\nin InfoSphere", "etorical structure theory (RST), applying\nto summarization, 401\u2013404\nRoboCup, for meaning representation, 149\nRobust processing\ndesired attributes of NLP aggregation,\n526\u2013527\nin GATE, 529\nin InfoSphere Streams, 531\nin UIMA, 529\nRobust risk minimization (RRM), mention\ndetection and, 287\n\n578\nIndex\nRoget\u2019s Thesaurus\nsemantic parsing, 104\nword sense disambiguation, 106\u2013107\nRole extractors, classi\ufb01ers for relation\nextraction, 316\nRomanian\napproaches to subjectivity and sentiment\nanalysis, 276\u2013277\ncorpus-based approach to subjectivity and\nsentiment analysis, 271\u2013272\ncross-lingual projections, 275\ndictionary-based approach to subjectivity\nand sentiment analysis, 264\u2013266, 270\nIR and, 390\nQA and, 461\nsubjectivity and sentiment analysis, 259\nsummarization and, 399\nRomanization, transliteration of scripts to\nLatin (Roman) alphabet, 368\nRosetta Consortium system\ndocument and corpus preparation, 480\u2013483\nindexing and, 483\noverview of, 479\u2013480\nquery answers and, 483\u2013487\nROUGE (Recall-Oriented Underst", "ripts to\nLatin (Roman) alphabet, 368\nRosetta Consortium system\ndocument and corpus preparation, 480\u2013483\nindexing and, 483\noverview of, 479\u2013480\nquery answers and, 483\u2013487\nROUGE (Recall-Oriented Understudy for\nGisting Evaluation)\nautomatic evaluation of summarization,\n415\u2013418\nmetrics in, 420\nRRM (robust risk minimization), mention\ndetection and, 287\nRST (rhetorical structure theory), applying\nto summarization, 401\u2013404\nRTNs (recursive transition networks), 150\nRTTS (Real-Time Translation Services),\n538\u2013540\nRule-based grammars, in speech recognition,\n501\u2013502\nRule-based sentence segmentation, 31\u201332\nRule-based systems\ndictionary-based approach to subjectivity\nand sentiment analysis, 270\nfor meaning representation, 150\nstatistical models compared with, 292\nsubjectivity and sentiment analysis, 267\nword and phrase-level annotations in\nsubjectivity and sentiment analysis,\n263\nfor word sense disambiguation, 105\u2013109\nRules, functional morphology models and, 19\nRussian\nlanguage modeling using subwor", "\nword and phrase-level annotations in\nsubjectivity and sentiment analysis,\n263\nfor word sense disambiguation, 105\u2013109\nRules, functional morphology models and, 19\nRussian\nlanguage modeling using subword units, 192\nparsing issues related to morphology, 91\nuni\ufb01cation-based models, 19\nSALAAM algorithms, 114\u2013115\nSALSA project, for predicate-argument\nrecognition, 122\nSanskrit\nambiguity in, 11\nas fusional language, 8\nZen toolkit for morphology of, 20\nSAPT (semantically augmented parse tree),\n151\nScalable entailment relation recognition\n(SERR), 215\u2013217\nSCGIS (Sequential conditional generalized\niterative scaling), 289\nScores\nranking answers in QA, 435, 450\u2013453,\n458\u2013459\nranking sentences, 407\nsentence relevance in distillation systems,\n485\u2013486\nScripts\npreprocessing best practices in IR, 371\u2013372\ntransliteration and direction of, 368\nSCUs (summarization content units), in\nPyramid method, 414\u2013415\nSearch component, in QA architectures, 435\nSearches\nbroadening to overcome parsing errors, 144\nin menti", "sliteration and direction of, 368\nSCUs (summarization content units), in\nPyramid method, 414\u2013415\nSearch component, in QA architectures, 435\nSearches\nbroadening to overcome parsing errors, 144\nin mention detection, 289\u2013291\nover unstructured sources in QA, 443\u2013445\nQA architectures and, 439\nQA vs. IR, 433\nreducing search space using beam search,\n290\u2013291\nfor relations, 215\u2013217\nSEE (Summary Evaluation Environment), 413\nSeeds, unsupervised systems and, 112\nSegmentation\nin aggregated NLP, 540\nsentence boundaries. See Sentence\nboundary detection\ntopic boundaries. See Topic segmentation\nSemantic concordance (SEMCOR) corpus,\nWordNet, 104\n\nIndex\n579\nSemantic interpretation\nentity and event resolution, 100\nmeaning representation, 101\noverview of, 98\u201399\npredicate-argument structure and, 100\nstructural ambiguity and, 99\nword sense and, 99\u2013100\nSemantic parsing\nAir Travel Information System (ATIS), 148\nbibliography, 154\u2013167\nCommunicator program, 148\u2013149\ncorpora for, 104\u2013105\nentity and event resolution", "ambiguity and, 99\nword sense and, 99\u2013100\nSemantic parsing\nAir Travel Information System (ATIS), 148\nbibliography, 154\u2013167\nCommunicator program, 148\u2013149\ncorpora for, 104\u2013105\nentity and event resolution, 100\nGeoQuery, 149\nintroduction to, 97\u201398\nmeaning representation, 101, 147\u2013148\nas part of semantic interpretation, 98\u201399\npredicate-argument structure. See\nPredicate-argument structure\nresource availability for disambiguation of\nword sense, 104\u2013105\nRoboCup, 149\nrule-based systems, 105\u2013109, 150\nsemi-supervised systems, 114\u2013116\nsoftware programs for, 116\u2013117, 151\nstructural ambiguity and, 99\nsummary, 151\nsupervised systems, 109\u2013112, 150\u2013151\nsystem paradigms, 101\u2013102\nunsupervised systems, 112\u2013114\nword sense and, 99\u2013100, 102\u2013105\nSemantic role labeling (SRL). See also\nPredicate-argument structure\nchallenges in RTE and, 212\ncombining dependency parsing with, 132\nimplementing RTE and, 227\novercoming independence assumption,\n137\u2013138\npredicate-argument structure training, 447\nin Rosetta Consortium ", "hallenges in RTE and, 212\ncombining dependency parsing with, 132\nimplementing RTE and, 227\novercoming independence assumption,\n137\u2013138\npredicate-argument structure training, 447\nin Rosetta Consortium distillation system,\n480\nin RTE, 221\nsentences as processing unit in, 29\nfor shallow semantic parsing, 118\nSemantically augmented parse tree (SAPT),\n151\nSemantics\nde\ufb01ned, 97\nexplicit semantic analysis (ESA), 382\nfeatures of classi\ufb01cation-based relation\nextraction systems, 315\u2013316\n\ufb01nding entity relations, 310\nlatent semantic indexing (LSI), 381\nQA and, 439\u2013440\nstructural matching and, 446\u2013447\ntopic detection and, 33\nSEMCOR (semantic concordance) corpus,\nWordNet, 104\nSEMEVAL, 263\nSemi-supervised systems, for word sense\ndisambiguation, 114\u2013116\nSemistructured data, candidate extraction\nfrom, 449\u2013450\nSemKer system, applying syntactic tree\nkernels to RTE, 246\nSense induction, unsupervised systems and,\n112\nSENSEVAL, for word sense disambiguation,\n105\u2013107\nSentence boundary detection\ncomparing segm", "50\nSemKer system, applying syntactic tree\nkernels to RTE, 246\nSense induction, unsupervised systems and,\n112\nSENSEVAL, for word sense disambiguation,\n105\u2013107\nSentence boundary detection\ncomparing segmentation methods, 40\u201341\ndetecting probable sentence or topic\nboundaries, 33\u201334\ndiscourse features, 44\ndiscriminative local classi\ufb01cation method\nfor, 36\u201338\ndiscriminative sequence classi\ufb01cation\nmethod for, 38\u201339\nextensions for global modeling, 40\nfeatures of segmentation methods, 41\u201342\ngenerative sequence classi\ufb01cation method,\n34\u201336\nhybrid methods, 39\u201340\nimplementing RTE and, 227\nintroduction to, 29\nlexical features, 42\u201343\noverview of, 30\u201332\nperformance of, 41\nprocessing stages of, 48\nprosodic features, 45\u201348\nspeech-related features, 45\nsyntactic features, 43\u201344\ntypographical and structural features, 44\u201345\nSentence-level annotations, for subjectivity\nand sentiment analysis\ncorpus-based approach, 271\u2013272\ndictionary-based approach, 270\u2013271\noverview of, 269\n\n580\nIndex\nSentence splitters, tools", "tures, 44\u201345\nSentence-level annotations, for subjectivity\nand sentiment analysis\ncorpus-based approach, 271\u2013272\ndictionary-based approach, 270\u2013271\noverview of, 269\n\n580\nIndex\nSentence splitters, tools for building\nsummarization systems, 423\nSentences\ncoherence of sentence-sentence connections,\n402\nextracting within-sentence relations, 310\nmethods for learning rank of, 407\nparasitic gap recovery, 249\nprocessing for event extraction, 323\nrelevance in distillation systems, 485\u2013486\nunits in sentence segmentation, 33\nunsupervised approaches to selection, 489\nSentential complement, features of supervised\nsystems, 111\nSentential forms, parsing and, 71\u201372\nSentiment analysis. See Subjectivity and\nsentiment analysis\nSentiWordNet, 262\nSequential conditional generalized iterative\nscaling (SCGIS), 289\nSERR (scalable entailment relation\nrecognition), 215\u2013217\nShallow semantic parsing\ncoverage in semantic parsing, 102\noverview of, 98\nsemantic role labeling for, 118\nstructural matching and, 447\nShalman", "SERR (scalable entailment relation\nrecognition), 215\u2013217\nShallow semantic parsing\ncoverage in semantic parsing, 102\noverview of, 98\nsemantic role labeling for, 118\nstructural matching and, 447\nShalmaneser program, for semantic role\nlabeling, 147\nShift-reduce parsing, 72\u201373\nSHRDLU QA system, 434\nSIGHAN, Chinese word segmentation, 194\nSIGLEX (Special Group on LEXicon), 103\nSimilarity enablement, relation extraction\nand, 310\nSlovene uni\ufb01cation-based model, 19\nSLU (statistical language understanding)\ncontinuous improvement cycle in dialog\nsystems, 512\u2013513\ngenerations of dialog systems, 511\u2013512\nSmoothing techniques\nLaplace smoothing, 174\nmachine translation and, 345\nn-gram approximation, 172\u2013173\nSMT. See Statistical machine translation\n(SMT)\nSnippets, in distillation\ncrosslingual distillation and, 491\nevaluation, 492\u2013493\n\ufb01ltering, 484\nmain and supporting, 477\u2013478\nmultimodal distillation and, 490\nplanning and, 487\nprocessing, 485\u2013487\nSnowball Stemmer, 392\nSoftware programs\nfor meaning repres", "nd, 491\nevaluation, 492\u2013493\n\ufb01ltering, 484\nmain and supporting, 477\u2013478\nmultimodal distillation and, 490\nplanning and, 487\nprocessing, 485\u2013487\nSnowball Stemmer, 392\nSoftware programs\nfor meaning representation, 151\nfor predicate-argument structure, 147\nfor semantic parsing, 116\u2013117\nSort expansion, machine translation phrase\ndecoding, 347\u2013348\nSources, in QA\nacquiring, 437\u2013440\ncandidate extraction from structured,\n449\u2013450\ncandidate extraction from unstructured,\n445\u2013449\nsearching over unstructured, 443\u2013445\nSpanish\ncode switching example, 31, 195\u2013196\ncorpus-based approach to subjectivity and\nsentiment analysis, 272\ndiscriminative approach to parsing, 91\u201392\nGeoQuery corpus translated into, 149\nIR and, 390\u2013391\nlocalization of spoken dialog systems,\n513\u2013514, 517\u2013520\nmention detection experiments, 294\u2013296\nmorphologies of, 20\npolarity analysis of words and phrases,\n269\nQA and, 461\nresources for semantic parsing, 122\nRTE in, 218\nsemantic parser for, 151\nsummarization and, 398\nTAC and, 424\nTALES c", "294\u2013296\nmorphologies of, 20\npolarity analysis of words and phrases,\n269\nQA and, 461\nresources for semantic parsing, 122\nRTE in, 218\nsemantic parser for, 151\nsummarization and, 398\nTAC and, 424\nTALES case study, 538\nWordNet and, 109\nSpecial Group on LEXicon (SIGLEX), 103\nSpeech\ndiscourse features in topic or sentence\nsegmentation, 44\nlexical features in sentence segmentation,\n42\nprosodic features for sentence or topic\nsegmentation, 45\u201348\nsentence segmentation accuracy, 41\n\nIndex\n581\nSpeech generation\ndialog manager directing, 499\u2013500\nspoken dialog systems and, 503\u2013504\nSpeech recognition\nanchored speech recognition, 490\nautomatic speech recognition (ASR), 29, 31\nlanguage modeling using subword units, 192\nMaxEnt model applied to, 181\u2013183\nMorfessor package applied to, 191\u2013192\nneural network language models applied to,\n188\nrule-based grammars in, 501\u2013502\nspoken dialog systems and, 500\u2013503\nSpeech Recognition Grammar Speci\ufb01cation\n(SRGS), 501\u2013502\nSpeech-to-text (STT)\ndata reorganization and, 5", "nguage models applied to,\n188\nrule-based grammars in, 501\u2013502\nspoken dialog systems and, 500\u2013503\nSpeech Recognition Grammar Speci\ufb01cation\n(SRGS), 501\u2013502\nSpeech-to-text (STT)\ndata reorganization and, 535\u2013536\nin GALE IOD, 532\u2013533\nNLP and, 523\u2013524\nin RTTS, 538\nSplit-head concept, in parsing, 78\nSpoken dialog systems\narchitecture of, 505\nbibliography, 521\u2013522\ncall-\ufb02ow localization, 514\ncontinuous improvement cycle in, 512\u2013513\ndialog manager, 504\u2013505\nforms of dialogs, 509\u2013510\nfunctional diagram of, 499\u2013500\ngenerations of, 510\u2013512\nintroduction to, 499\nlocalization of, 513\u2013514\nlocalization of grammars, 516\u2013517\nnatural language call routing, 510\nprompt localization, 514\u2013516\nspeech generation, 503\u2013504\nspeech recognition and understanding,\n500\u2013503\nsummary, 520\u2013521\ntesting, 519\u2013520\ntraining, 517\u2013519\ntranscription and annotation of utterances,\n513\nvoice user interface (VUI), 505\u2013509\nSpoken languages, vs. written languages and\nlanguage models, 194\u2013195\nSRGS (Speech Recognition Grammar\nSpeci\ufb01cation),", "transcription and annotation of utterances,\n513\nvoice user interface (VUI), 505\u2013509\nSpoken languages, vs. written languages and\nlanguage models, 194\u2013195\nSRGS (Speech Recognition Grammar\nSpeci\ufb01cation), 501\u2013502\nSRILM (Stanford Research Institute\nLanguage Modeling)\noverview of, 184\nSRILM toolkit for machine translation, 357\nSRL. See Semantic role labeling (SRL)\nSSI (Structural semantic interconnections)\nalgorithm, 107\u2013109\nSSTKs (Syntactic Semantic Tree Kernels),\n246\u2013247\nStacks, of hypotheses in machine translation,\n346\nStanford Parser, dependency parsing with,\n456\nStanford Research Institute Language\nModeling (SRILM)\noverview of, 184\nSRILM toolkit for machine translation, 357\nSTART QA system, 435\u2013436\nStatic knowledge, in textual entailment, 210\nStatistical language models\nn-gram approximation, 170\u2013171\noverview of, 169\nrule-based systems compared with, 292\nspoken vs. written languages and, 194\u2013195\ntranslation with, 331\nStatistical language understanding (SLU)\ncontinuous improvement cycle i", ", 170\u2013171\noverview of, 169\nrule-based systems compared with, 292\nspoken vs. written languages and, 194\u2013195\ntranslation with, 331\nStatistical language understanding (SLU)\ncontinuous improvement cycle in dialog\nsystems, 512\u2013513\ngenerations of dialog systems, 511\u2013512\nStatistical machine translation (SMT)\napplying to CLIR, 381\ncross-language mention propagation,\n293\u2013294\nevaluating co-occurrence of words, 337\u2013338\nmention detection experiments, 293\u2013294\nStemmers\nmapping terms to stems, 370\npreprocessing best practices in IR, 371\nSnowball Stemmer, 392\nStems, mapping terms to, 370\nStop-words, removing in normalization, 371\nStructural ambiguity, 99\nStructural features\nof classi\ufb01cation-based relation extraction\nsystems, 314\nsentence and topic segmentation, 44\u201345\nStructural matching, for candidate extraction\nin QA, 446\u2013448\n\n582\nIndex\nStructural semantic interconnections (SSI)\nalgorithm, 107\u2013109\nStructure\nof documents. See Document structure\nof words. See Word structure\nStructured data\ncandidate ex", "raction\nin QA, 446\u2013448\n\n582\nIndex\nStructural semantic interconnections (SSI)\nalgorithm, 107\u2013109\nStructure\nof documents. See Document structure\nof words. See Word structure\nStructured data\ncandidate extraction from structured\nsources, 449\u2013450\ncandidate extraction from unstructured\nsources, 445\u2013449\nStructured knowledge, 434\nStructured language model, 181\nStructured queries, 444\nSTT (Speech-to-text). See Speech-to-text\n(STT)\nSubcategorization\nin PSG, 125\nin TAG, 130\nfor verb sense disambiguation, 112\nSubclasses, of relations, 311\nSubject/object presence, features of\nsupervised systems, 111\nSubject, object, verb (SOV) word order, 356\nSubjectivity, 260\nSubjectivity analysis, 260\nSubjectivity and sentiment analysis\napplied to English, 262\nbibliography, 278\u2013281\ncomparing approaches to, 276\u2013277\ncorpora for, 262\u2013263\nde\ufb01nitions, 260\u2013261\ndocument-level annotations, 272\u2013274\nintroduction to, 259\u2013260\nlexicons and, 262\nranking approaches to, 274\u2013276\nsentence-level annotations, 269, 270\u2013272\nsummary, 2", "rpora for, 262\u2013263\nde\ufb01nitions, 260\u2013261\ndocument-level annotations, 272\u2013274\nintroduction to, 259\u2013260\nlexicons and, 262\nranking approaches to, 274\u2013276\nsentence-level annotations, 269, 270\u2013272\nsummary, 277\ntools for, 263\u2013264\nword and phrase level annotations, 264\u2013269\nSubstitution, linguistic supports for cohesion,\n401\nSubword units, selecting for language models,\n191\u2013192\nSUMMA\nhistory of summarization systems, 399\nfor multilingual automatic summarization,\n411\nsummarization frameworks, 423\nSUMMARIST, 398\nSummarization, automatic. See Automatic\nsummarization\nSummarization content units (SCUs), in\nPyramid method, 414\u2013415\nSummary Evaluation Environment (SEE),\n413\nSummBank\nhistory of summarization systems, 399\nsummarization data set, 425\nSupertags, in TAG, 130\nSupervised systems\nfor meaning representation, 150\u2013151\nfor relation extraction, 317\u2013319\nfor sentence segmentation, 37\nfor word sense disambiguation, 109\u2013112\nSupport vector machines (SVMs)\nclassi\ufb01ers for relation extraction, 316\u2013317\ncorpu", "entation, 150\u2013151\nfor relation extraction, 317\u2013319\nfor sentence segmentation, 37\nfor word sense disambiguation, 109\u2013112\nSupport vector machines (SVMs)\nclassi\ufb01ers for relation extraction, 316\u2013317\ncorpus-based approach to subjectivity and\nsentiment analysis, 272, 274\nmention detection and, 287\nmethods for sentence or topic segmentation,\n37\u201339\ntraining and test software, 135\u2013137\nunsupervised approaches to machine\nlearning, 342\nSurface-based features, in automatic\nsummarization, 400\u2013401\nSurface patterns, for candidate extraction in\nQA, 448\u2013449\nSurface strings\ninput words in input/output language\nrelations, 17\nuni\ufb01cation-based morphology and, 18\nSVMs. See Support vector machines (SVMs)\nSVO (subject, verb, object) word order, 356\nSwedish\nIR and, 390\u2013391\nmorphologies of, 20\nsemantic parsing and, 122\nsummarization and, 399\nSwiRL program, for semantic role labeling,\n147\nSyllabic scripts, 371\nSymmetrization, word alignment and,\n340\u2013341\nSyncretism, 8\nSynonyms\nanswers in QA systems and, 442\nmachin", "122\nsummarization and, 399\nSwiRL program, for semantic role labeling,\n147\nSyllabic scripts, 371\nSymmetrization, word alignment and,\n340\u2013341\nSyncretism, 8\nSynonyms\nanswers in QA systems and, 442\nmachine translation metrics and, 336\n\nIndex\n583\nSyntactic features\nof classi\ufb01cation-based relation extraction\nsystems, 315\nof coreference models, 301\nof mention detection system, 292\nin sentence and topic segmentation, 43\u201344\nSyntactic models, for machine translation,\n352\u2013354\nSyntactic pattern, in PSG, 126\nSyntactic relations, features of supervised\nsystems, 111\nSyntactic representation, in\npredicate-argument structure, 123\u2013124\nSyntactic roles, in TAG, 130\nSyntactic Semantic Tree Kernels (SSTKs),\n246\u2013247\nSyntactic Structures (Chomsky), 98\u201399\nSyntax\nambiguity resolution, 80\nbibliography, 92\u201395\ncompared with morphology and phonology\nand orthography, 3\ncontext-free grammar (CFGs) and, 59\u201361\ndependency graphs for analysis of, 63\u201367\ndiscriminative parsing models, 84\u201387\nof documents in IR, 367\u2013368\ngene", "d with morphology and phonology\nand orthography, 3\ncontext-free grammar (CFGs) and, 59\u201361\ndependency graphs for analysis of, 63\u201367\ndiscriminative parsing models, 84\u201387\nof documents in IR, 367\u2013368\ngenerative parsing models, 83\u201384\nintroduction to, 57\nminimum spanning trees and dependency\nparsing, 79\u201380\nmorphology and, 90\u201392\nparsing algorithms for, 70\u201372\nparsing natural language, 57\u201359\nphrase structure trees for analysis of, 67\u201370\nprobabilistic context-free grammars, 80\u201383\nQA and, 439\u2013440\nshift-reduce parsing, 72\u201373\nstructural matching and, 446\u2013447\nsummary, 92\ntokenization, case, and encoding and,\n87\u201389\ntreebanks data-driven approach to, 61\u201363\nword segmentation and, 89\u201390\nworst-case parsing algorithm for CFGs,\n74\u201379\nSyntax-based language models, 180\u2013181\nSynthetic languages, morphological typology\nand, 7\nSystem architectures\nfor distillation, 488\nfor semantic parsing, 101\u2013102\nSystem paradigms, for semantic parsing,\n101\u2013102\nSystran\u2019s Babel\ufb01sh program, 331\nTAC. See Text Analysis Conferences ", "logy\nand, 7\nSystem architectures\nfor distillation, 488\nfor semantic parsing, 101\u2013102\nSystem paradigms, for semantic parsing,\n101\u2013102\nSystran\u2019s Babel\ufb01sh program, 331\nTAC. See Text Analysis Conferences (TAC)\nTAG (Tree-Adjoining Grammar), 130\nTALES (Translingual Automated Language\nExploitation System), 538\nTamil\nas agglutinative language, 7\nIR and, 390\nTask-based evaluation, of translation, 334\nTBL (transformation-based learning), for\nsentence segmentation, 37\nTDT (Topic Detection and Tracking)\nprogram, 32\u201333, 42, 425\u2013426\nTelugu, 390\nTemplates, in GALE distillation initiative,\n475\nTemporal cue words, in PSG, 127\u2013128\nTER (Translation-error rate), 337\nTerm-document matrix, document\nrepresentation in monolingual IR, 373\nTerm frequency-inverse document frequency\n(TF-IDF)\nmultilingual automatic summarization and,\n411\nQA scoring and, 450\u2013451\nunsupervised approaches to sentence\nselection, 489\nTerm frequency (TF)\nTF document model, 373\nunsupervised approaches to sentence\nselection, 489\nTerms\nappl", "mmarization and,\n411\nQA scoring and, 450\u2013451\nunsupervised approaches to sentence\nselection, 489\nTerm frequency (TF)\nTF document model, 373\nunsupervised approaches to sentence\nselection, 489\nTerms\napplying RTE to unknown, 217\nearly approaches to summarization and, 400\nin GALE distillation initiative, 475\nmapping term vectors to topic vectors, 381\nmapping to lemmas, 370\nposting lists, 373\u2013374\nTerrier IR framework, 392\nText Analysis Conferences (TAC)\ncompetitions related to summarization,\n424\ndata sets related to summarization, 425\n\n584\nIndex\nText Analysis Conferences (TAC) (continued)\nevaluation of QA systems, 460\u2013464\nhistory of QA systems, 434\nKnowledge Base Population (KBP),\n481\u2013482\nlearning summarization, 408\nText REtrieval Conference (TREC)\ndata sets for evaluating IR systems,\n389\u2013390\nevaluation of QA systems, 460\u2013464\nhistory of QA systems, 434\nredundancy reduction, 489\nText Tiling method (Hearst)\nsentence segmentation, 42\ntopic segmentation, 37\u201338\nText-to-speech (TTS)\narchitecture o", "aluation of QA systems, 460\u2013464\nhistory of QA systems, 434\nredundancy reduction, 489\nText Tiling method (Hearst)\nsentence segmentation, 42\ntopic segmentation, 37\u201338\nText-to-speech (TTS)\narchitecture of spoken dialog systems, 505\nhistory of dialog managers, 504\nlocalization of grammars and, 514\nin RTTS, 538\nspeech generation, 503\u2013504\nTextRank, graphical approaches to automatic\nsummarization, 404\u2013406\nTextual entailment. See also Recognizing\ntextual entailment (RTE)\ncontradiction in, 211\nde\ufb01ned, 210\nentailment pairs, 210\nTextual inference\nimplementing, 236\u2013238\nlatent alignment inference, 247\u2013248\nmodeling, 226\u2013227\nNLP and, 209\nRTE and, 242\u2013244\nTF-IDF (term frequency-inverse document\nfrequency)\nmultilingual automatic summarization and,\n411\nQA scoring and, 450\u2013451\nunsupervised approaches to sentence\nselection, 489\nTF (term frequency)\nTF document model, 373\nunsupervised approaches to sentence\nselection, 489\nThai\nas isolating or analytic language, 7\nword segmentation in, 4\u20135\nThot program, for ", "ence\nselection, 489\nTF (term frequency)\nTF document model, 373\nunsupervised approaches to sentence\nselection, 489\nThai\nas isolating or analytic language, 7\nword segmentation in, 4\u20135\nThot program, for machine translation, 423\nTika (Content Analysis Toolkit), for\npreprocessing IR documents, 392\nTinySVM software, for SVM training and\ntesting, 135\u2013136\nToken streams, 372\u2013373\nTokenization\nArabic, 12\ncharacter n-gram models and, 370\nmultilingual automatic summarization and,\n410\nnormalization and, 370\u2013371\nparsing issues related to, 87\u201388\nphrase indices and, 369\u2013370\nin Rosetta Consortium distillation system,\n480\nword segmentation and, 369\nTokenizers, tools for building summarization\nsystems, 423\nTokens\nlexical features in sentence segmentation,\n42\u201343\nmapping between scripts (normalization),\n370\u2013371\nMLIR indexes and, 384\noutput from information retrieval, 366\nprocessing stages of segmentation tasks, 48\nin sentence segmentation, 30\ntranslating MLIR queries, 384\nin word structure, 4\u20135\nTop-k models", "\nMLIR indexes and, 384\noutput from information retrieval, 366\nprocessing stages of segmentation tasks, 48\nin sentence segmentation, 30\ntranslating MLIR queries, 384\nin word structure, 4\u20135\nTop-k models, for monolingual information\nretrieval, 374\nTopic-dependent language model adaptation,\n176\nTopic Detection and Tracking (TDT)\nprogram, 32\u201333, 42, 425\u2013426\nTopic or domain, features of supervised\nsystems, 111\nTopic segmentation\ncomparing segmentation methods, 40\u201341\ndiscourse features, 44\ndiscriminative local classi\ufb01cation method,\n36\u201338\ndiscriminative sequence classi\ufb01cation\nmethod, 38\u201339\nextensions for global modeling, 40\nfeatures of, 41\u201342\ngenerative sequence classi\ufb01cation method,\n34\u201336\nhybrid methods, 39\u201340\n\nIndex\n585\nintroduction to, 29\nlexical features, 42\u201343\nmethods for detecting probable topic\nboundaries, 33\u201334\noverview of, 32\u201333\nperformance of, 41\nprocessing stages of segmentation tasks, 48\nprosodic features, 45\u201348\nspeech-related features, 45\nsyntactic features, 43\u201344\ntypographical an", "opic\nboundaries, 33\u201334\noverview of, 32\u201333\nperformance of, 41\nprocessing stages of segmentation tasks, 48\nprosodic features, 45\u201348\nspeech-related features, 45\nsyntactic features, 43\u201344\ntypographical and structural features,\n44\u201345\nTopics, mapping term vectors to topic\nvectors, 381\nTraces nodes, Treebanks, 120\u2013121\nTraining\nissues related to machine translation (MT),\n197\nminimum error rate training (MERT), 349\nphrase-based models, 344\u2013345\npredicate-argument structure, 140\u2013141, 447\nrecognizing textual entailment (RTE), 238\nin RTE, 238\nspoken dialog systems, 517\u2013519\nstage of RTE model, 238\nsupport vector machines (SVMs), 135\u2013137\nTranscription\nof utterances based on rule-based\ngrammars, 502\u2013503\nof utterances in spoken dialog systems, 513\nTransducers, \ufb01nite-state, 16\u201317\nTransformation-based approaches, applying\nto RTE, 241\u2013242\nTransformation-based learning (TBL), for\nsentence segmentation, 37\nTransformation stage, of summarization\nsystems, 400, 421\nTransitive closure, of relations, 324\u2013326\nTra", "ches, applying\nto RTE, 241\u2013242\nTransformation-based learning (TBL), for\nsentence segmentation, 37\nTransformation stage, of summarization\nsystems, 400, 421\nTransitive closure, of relations, 324\u2013326\nTranslation\nhuman assessment of word meaning,\n333\u2013334\nby machines. See Machine translation (MT)\ntranslation-based approach to CLIR,\n378\u2013380\nTranslation-error rate (TER), 337\nTranslingual Automated Language\nExploitation System (TALES), 538\nTranslingual information retrieval, 491\nTranslingual summarization. See also\nAutomatic summarization, 398\nTransliteration, mapping text between\nscripts, 368\nTREC. See Text REtrieval Conference\n(TREC)\ntrec-eval, evaluation of IR systems, 393\nTree-Adjoining Grammar (TAG), 130\nTree-based language models, 185\u2013186\nTree-based models, for MT\nchart decoding, 351\u2013352\nhierarchical phrase-based models, 350\u2013351\nlinguistic choices and, 354\noverview of, 350\nsyntactic models, 352\u2013354\nTree edit distance, applying to RTE, 240\u2013241\nTreebanks\ndata-driven approach to syntactic a", "rarchical phrase-based models, 350\u2013351\nlinguistic choices and, 354\noverview of, 350\nsyntactic models, 352\u2013354\nTree edit distance, applying to RTE, 240\u2013241\nTreebanks\ndata-driven approach to syntactic analysis,\n61\u201363\ndependency graphs in syntax analysis,\n63\u201367\nphrase structure trees in syntax analysis,\n67\u201370\ntraces nodes marked as arguments in\nPropBank, 120\u2013121\nworst-case parsing algorithm for CFGs, 77\nTrigger models, dynamic self-adapting\nlanguage models, 176\u2013177\nTriggers\nconsistency of, 323\n\ufb01nding event triggers, 321\u2013322\nTrigrams, 502\u2013503\nTroponymy, 310\nTuning sets, 348\nTurkish\ndependency graphs in syntax analysis, 62,\n65\nGeoQuery corpus translated into, 149\nlanguage modeling for morphologically rich\nlanguages, 189\u2013191\nlanguage modeling using morphological\ncategories, 192\u2013193\nmachine translation and, 354\nmorphological richness of, 355\nparsing issues related to morphology, 90\u201391\nsemantic parser for, 151\nsyntactic features used in sentence and\ntopic segmentation, 43\n\n586\nIndex\nType-based", "lation and, 354\nmorphological richness of, 355\nparsing issues related to morphology, 90\u201391\nsemantic parser for, 151\nsyntactic features used in sentence and\ntopic segmentation, 43\n\n586\nIndex\nType-based candidate extraction, in QA, 446,\n451\nType classi\ufb01er\nanswers in QA systems, 440\u2013442\nin relation extraction, 313\nType system, GALE Type System (GTS),\n534\u2013535\nTyped feature structures, uni\ufb01cation-based\nmorphology and, 18\u201319\nTypographical features, sentence and topic\nsegmentation, 44\u201345\nTypology, morphological, 7\u20138\nUCC (UIMA Component Container), 537\nUIMA. See Unstructured Information\nManagement Architecture (UIMA)\nUnderstanding, spoken dialog systems and,\n500\u2013503\nUnicode (UTF-8/UTF-16)\nencoding and script, 368\nparsing issues related to encoding systems,\n89\nUni\ufb01cation-based morphology, 18\u201319\nUnigram models (Yamron), 35\u201336\nUnin\ufb02ectedness, homonyms and, 12\nUnits of thought, interlingual document\nrepresentations, 381\nUnknown terms, applying RTE to, 217\nUnknown word problem, 8, 13\u201315\nUnstructure", "gram models (Yamron), 35\u201336\nUnin\ufb02ectedness, homonyms and, 12\nUnits of thought, interlingual document\nrepresentations, 381\nUnknown terms, applying RTE to, 217\nUnknown word problem, 8, 13\u201315\nUnstructured data, candidate extraction\nfrom, 445\u2013449\nUnstructured Information Management\nArchitecture (UIMA)\nattributes of, 528\u2013529\nGALE IOD and, 535, 537\noverview of, 527\u2013528\nRTTS and, 538\u2013540\nsample code, 542\u2013547\nsummarization frameworks, 422\nUIMA Component Container (UCC), 537\nUnstructured text, history of QA systems\nand, 434\nUnsupervised adaptation, language model\nadaptation and, 177\nUnsupervised systems\nmachine learning, 342\nrelation extraction, 317\u2013319\nsentence selection, 489\nsubjectivity and sentiment analysis, 264\nword sense disambiguation, 112\u2013114\nUpdate summarization, in automatic\nsummarization, 397\nUppercase (capitalization), sentence\nsegmentation markers, 30\nUTF-8/UTF-16 (Unicode)\nencoding and script, 368\nparsing issues related to encoding systems,\n89\nUtterances, in spoken dialog systems", "n, 397\nUppercase (capitalization), sentence\nsegmentation markers, 30\nUTF-8/UTF-16 (Unicode)\nencoding and script, 368\nparsing issues related to encoding systems,\n89\nUtterances, in spoken dialog systems\nrule-based approach to transcription and\nannotation, 502\u2013503\ntranscription and annotation of, 513\nVariable-length language models, 179\nVector space model\ndocument representation in monolingual\nIR, 372\u2013373\nfor document retrieval, 374\u2013375\nVerb clustering, in PSG, 125\nVerb sense, in PSG, 126\u2013127\nVerb, subject, object (VSO) word order, 356\nVerbNet, resources for predicate-argument\nrecognition, 121\nVerbs\nfeatures of predicate-argument structures,\n145\nrelation extraction and, 310\nVietnamese\nas isolating or analytic language, 7\nNER task in, 287\nViews\nin GALE IOD, 534\nRTE systems, 220\nVital few (80/20 rule), 14\nViterbi algorithm\napplied to Rosetta Consortium distillation\nsystem, 480\nmethods for sentence or topic segmentation,\n39\u201340\nsearching for mentions, 291\nVocabulary\nindexing IR output, 366\nla", "0 rule), 14\nViterbi algorithm\napplied to Rosetta Consortium distillation\nsystem, 480\nmethods for sentence or topic segmentation,\n39\u201340\nsearching for mentions, 291\nVocabulary\nindexing IR output, 366\nlanguage models and, 169\nin morphologically rich languages, 190\nproductivity/creativity and, 14\ntopic segmentation methods, 38\n\nIndex\n587\nVoice Extensible Markup Language. See\nVoiceXML (Voice Extensible Markup\nLanguage)\nVoice feature, in PSG, 124\nVoice of sentence, features of supervised\nsystems, 111\nVoice quality, prosodic modeling and, 47\nVoice user interface (VUI)\ncall-\ufb02ow, 505\u2013506\ndialog module (DM) of, 507\u2013508\nGetService process of, 506\u2013507\ngrammars of, 508\u2013509\nVUI completeness principle, 509\u2013510\nVoiceXML (Voice Extensible Markup\nLanguage)\narchitecture of spoken dialog systems, 505\ngenerations of dialog systems, 511\u2013512\nhistory of dialog managers, 504\nVUI. See Voice user interface (VUI)\nW3C (World Wide Web Consortium), 504\nWASP program, for rule-based semantic\nparsing systems, 151\nWeb 2", "ons of dialog systems, 511\u2013512\nhistory of dialog managers, 504\nVUI. See Voice user interface (VUI)\nW3C (World Wide Web Consortium), 504\nWASP program, for rule-based semantic\nparsing systems, 151\nWeb 2.0, accelerating need for crosslingual\nretrieval, 365\nWER (word-error rate), machine translation\nmetrics and, 336\u2013337\nWhitespace\npreprocessing best practices in IR, 371\nin word separation, 369\nWikipedia\nanswer scores in QA and, 452\nfor automatic word sense disambiguation,\n115\u2013116\ncrosslingual question answering and, 455\nas example of explicit semantic analysis,\n382\npredominance of English in, 438\nWikiRelate! program, for word sense\ndisambiguation, 117\nWiktionary\ncrosslingual question answering and, 455\nas example of explicit semantic analysis,\n382\nWitten-Bell smoothing technique, in language\nmodel estimation, 172\nWolfram Alpha QA system, 435\nWord alignment, cross-language mention\npropagation, 293\nWord alignment, in MT\nalignment models, 340\nBerkeley word aligner, 357\nco-occurrence of words ", "odel estimation, 172\nWolfram Alpha QA system, 435\nWord alignment, cross-language mention\npropagation, 293\nWord alignment, in MT\nalignment models, 340\nBerkeley word aligner, 357\nco-occurrence of words between languages,\n337\u2013338\nEM algorithm, 339\u2013340\nIBM Model 1, 338\u2013339\nas machine learning problem, 341\u2013343\noverview of, 337\nsymmetrization, 340\u2013341\nWord boundary detection, 227\nWord-error rate (WER), machine translation\nmetrics and, 336\u2013337\nWord lists. See Dictionary-based morphology\nWord meaning\nautomatic evaluation, 334\u2013335\nevaluation of, 332\nhuman assessment of, 332\u2013334\nWord order, 356\nWord/phrase-level annotations, for\nsubjectivity and sentiment analysis\ncorpus-based approach, 267\u2013269\ndictionary-based approach, 264\u2013267\noverview of, 264\nWord segmentation\nin Chinese, Japanese, Thai, and Korean\nwriting systems, 4\u20135\nlanguages lacking, 193\u2013194\nphrase indices based on, 369\u2013370\npreprocessing best practices in IR, 371\nsyntax and, 89\u201390\ntokenization and, 369\nWord sense\nclassifying according to ", "an\nwriting systems, 4\u20135\nlanguages lacking, 193\u2013194\nphrase indices based on, 369\u2013370\npreprocessing best practices in IR, 371\nsyntax and, 89\u201390\ntokenization and, 369\nWord sense\nclassifying according to subjectivity and\npolarity, 261\ndisambiguation, 105, 152\u2013153\noverview of, 102\u2013104\nresources, 104\u2013105\nrule-based systems, 105\u2013109\nsemantic interpretation and, 99\u2013100\nsemi-supervised systems, 114\u2013116\nsoftware programs for, 116\u2013117\nsupervised systems, 109\u2013112\nunsupervised systems, 112\u2013114\nWord sequence, 169\nWord structure\nambiguity in interpretation of expressions,\n10\u201313\n\n588\nIndex\nWord structure (continued)\nautomated morphology (morphology\ninduction), 21\nbibliography, 22\u201328\ndictionary-based morphology, 15\u201316\n\ufb01nite-state morphology, 16\u201318\nfunctional morphology, 19\u201321\nintroduction to, 3\u20134\nirregularity in linguistic models, 8\u201310\nissues and challenges, 8\nlexemes, 5\nmorphemes, 5\u20137\nmorphological models, 15\nmorphological typology, 7\u20138\nproductivity/creativity and the unknown\nword problem, 13\u201315\nsumma", "ity in linguistic models, 8\u201310\nissues and challenges, 8\nlexemes, 5\nmorphemes, 5\u20137\nmorphological models, 15\nmorphological typology, 7\u20138\nproductivity/creativity and the unknown\nword problem, 13\u201315\nsummary, 22\ntokens and, 4\u20135\nuni\ufb01cation-based morphology, 18\u201319\nunits in sentence segmentation, 33\nWordNet\nclassifying word sense according to\nsubjectivity and polarity, 261\neXtended WordNet (XWN), 451\nfeatures of supervised systems, 112\nhierarchical concept information in, 109\nQA answer scores and, 452\nas resource for domain-speci\ufb01c information,\n122\nRTE applied to machine translation, 218\nSEMCOR (semantic concordance) corpus,\n104\u2013105\nsubjectivity and sentiment analysis\nlexicons, 262\nsynonyms, 336\nword sense disambiguation and, 117\nWorld Wide Web Consortium (W3C), 504\nWritten languages, vs. spoken languages in\nlanguage modeling, 194\u2013195\nWSJ, 147\nXDC (Crossdocument coreference), in\nRosetta Consortium distillation\nsystem, 482\u2013483\nXerox Finite-State Tool (XFST), 16\nXWN (eXtended WordNet), 451\nYamCh", "anguages in\nlanguage modeling, 194\u2013195\nWSJ, 147\nXDC (Crossdocument coreference), in\nRosetta Consortium distillation\nsystem, 482\u2013483\nXerox Finite-State Tool (XFST), 16\nXWN (eXtended WordNet), 451\nYamCha software, for SVM training and\ntesting, 135\u2013136\nYarowsky algorithm, for word sense\ndisambiguation, 114\u2013116\nZ-score normalization, for MLIR aggregation,\n385\nZen toolkit for morphology, applying to\nSanskrit, 20\nZero anaphora resolution, 249, 444\n", "\nPraise for Predictive Analytics\n\u201cLittered with lively examples . . .\u201d\n\u2014The Financial Times\n\u201cReaders will \ufb01nd this a mesmerizing and fascinating study. I know I\ndid! . . . I was entranced by the book.\u201d\n\u2014The Seattle Post-Intelligencer\n\u201cSiegel is a capable and passionate spokesman with a compelling vision.\u201d\n\u2014Analytics Magazine\n\u201cA must-read for the normal layperson.\u201d\n\u2014Journal of Marketing Analytics\n\u201cThis book is an operating manual for twenty-\ufb01rst-century life. Drawing\npredictions from big data is at the heart of nearly everything, whether it\u2019s in\nscience, business, \ufb01nance, sports, or politics. And Eric Siegel is the ideal\nguide.\u201d\n\u2014Stephen Baker, author, The Numerati and Final Jeopardy: The Story\nof Watson, the Computer That Will Transform Our World\n\u201cSimultaneously entertaining, informative, and nuanced. Siegel goes behind\nthe hype and makes the science exciting.\u201d\n\u2014Rayid Ghani, Chief Data Scientist,\nObama for America 2012 Campaign\n\u201cThe most readable (for we laymen) \u2018big data\u2019 book I\u2019ve co", "and nuanced. Siegel goes behind\nthe hype and makes the science exciting.\u201d\n\u2014Rayid Ghani, Chief Data Scientist,\nObama for America 2012 Campaign\n\u201cThe most readable (for we laymen) \u2018big data\u2019 book I\u2019ve come across. By far.\nGreat vignettes/stories.\u201d\n\u2014Tom Peters, coauthor, In Search of Excellence\n\u201cThe future is right now\u2014you\u2019re living in it. Read this book to gain\nunderstanding of where we are and where we\u2019re headed.\u201d\n\u2014Roger Craig, record-breaking analytical Jeopardy!\nchampion; Data Scientist, Digital Reasoning\n\n\u201cA clear and compelling explanation of the power of predictive analytics and\nhow it can transform companies and even industries.\u201d\n\u2014Anthony Goldbloom, founder and CEO, Kaggle.com\n\u201cThe de\ufb01nitive book of this industry has arrived. Dr. Siegel has achieved\nwhat few have even attempted: an accessible, captivating tome on predictive\nanalytics that is a must-read for all interested in its potential\u2014and peril.\u201d\n\u2014Mark Berry, VP, People Insights, ConAgra Foods\n\u201cI\u2019ve always been a passionate dat", "essible, captivating tome on predictive\nanalytics that is a must-read for all interested in its potential\u2014and peril.\u201d\n\u2014Mark Berry, VP, People Insights, ConAgra Foods\n\u201cI\u2019ve always been a passionate data geek, but I never thought it might be\npossible to convey the excitement of data mining to a lay audience. That is\nwhat Eric Siegel does in this book. The stories range from inspiring to\ndownright scary\u2014read them and \ufb01nd out what we\u2019ve been up to while you\nweren\u2019t paying attention.\u201d\n\u2014Michael J. A. Berry, author of Data Mining Techniques, Third Edition\n\u201cEric Siegel is the Kevin Bacon of the predictive analytics world, organizing\nconferences where insiders trade knowledge and share recipes. Now, he has\nthrown the doors open for you. Step in and explore how data scientists are\nrewriting the rules of business.\u201d\n\u2014Kaiser Fung, VP, Vimeo; author of Numbers Rule Your World\n\u201cWritten in a lively language, full of great quotes, real-world examples, and\ncase studies, it is a pleasure to read. The mor", "s of business.\u201d\n\u2014Kaiser Fung, VP, Vimeo; author of Numbers Rule Your World\n\u201cWritten in a lively language, full of great quotes, real-world examples, and\ncase studies, it is a pleasure to read. The more technical audience will enjoy\nchapters on The Ensemble Effect and uplift modeling\u2014both very hot trends.\nI highly recommend this book!\u201d\n\u2014Gregory Piatetsky-Shapiro, Editor, KDnuggets;\nfounder, KDD Conferences\n\u201cExciting and engaging\u2014reads like a thriller! Predictive analytics has its roots\nin people\u2019s daily activities and, if successful, affects people\u2019s actions. By way of\nexamples, Siegel describes both the opportunities and the threats predictive\nanalytics brings to the real world.\u201d\n\u2014Marianna Dizik, Statistician, Google\n\n\u201cA fascinating page-turner about the most important new form of informa-\ntion technology.\u201d\n\u2014Emiliano Pasqualetti, CEO, DomainsBot Inc.\n\u201cSucceeds where others have failed\u2014by demystifying big data and providing\nreal-world examples of how organizations are leveraging the pow", "\ntion technology.\u201d\n\u2014Emiliano Pasqualetti, CEO, DomainsBot Inc.\n\u201cSucceeds where others have failed\u2014by demystifying big data and providing\nreal-world examples of how organizations are leveraging the power of\npredictive analytics to drive measurable change.\u201d\n\u2014Jon Francis, Senior Data Scientist, Nike\n\u201cIn a fascinating series of examples, Siegel shows how companies have made\nmoney predicting what customers will do. Once you start reading, you will\nnot be able to put it down.\u201d\n\u2014Arthur Middleton Hughes, VP, Database Marketing Institute;\nauthor of Strategic Database Marketing, Fourth Edition\n\u201cExcellent. Each chapter makes the complex comprehensible, making heavy\nuse of graphics to give depth and clarity. It gets you thinking about what else\nmight be done with predictive analytics.\u201d\n\u2014Edward Nazarko, Client Technical Advisor, IBM\n\u201cWhat is predictive analytics? This book gives a practical and up-to-date\nanswer, adding new dimension to the topic and serving as an excellent\nreference.\u201d\n\u2014Ramendra K.", ", Client Technical Advisor, IBM\n\u201cWhat is predictive analytics? This book gives a practical and up-to-date\nanswer, adding new dimension to the topic and serving as an excellent\nreference.\u201d\n\u2014Ramendra K. Sahoo, Senior VP,\nRisk Management and Analytics, Citibank\n\u201cCompeting on information is no longer a luxury\u2014it\u2019s a matter of survival.\nDespite its successes, predictive analytics has penetrated only so far, relative to\nits potential. As a result, lessons and case studies such as those provided in\nSiegel\u2019s book are in great demand.\u201d\n\u2014Boris Evelson, VP and Principal Analyst, Forrester Research\n\u201cFascinating and beautifully conveyed. Siegel is a leading thought leader in\nthe space\u2014a must-have for your bookshelf!\u201d\n\u2014Sameer Chopra, Chief Analytics Of\ufb01cer, Orbitz Worldwide\n\n\u201cA brilliant overview\u2014strongly recommended to everyone curious about\nthe analytics \ufb01eld and its impact on our modern lives.\u201d\n\u2014Kerem Tomak, VP of Marketing Analytics, Macys.com\n\u201cEric explains the science behind predictive analyti", "ly recommended to everyone curious about\nthe analytics \ufb01eld and its impact on our modern lives.\u201d\n\u2014Kerem Tomak, VP of Marketing Analytics, Macys.com\n\u201cEric explains the science behind predictive analytics, covering both the\nadvantages and the limitations of prediction. A must-read for everyone!\u201d\n\u2014Azhar Iqbal, VP and Econometrician,\nWells Fargo Securities, LLC\n\u201cPredictive Analytics delivers a ton of great examples across business sectors of\nhow companies extract actionable, impactful insights from data. Both the\nnovice and the expert will \ufb01nd interest and learn something new.\u201d\n\u2014Chris Pouliot, Director, Algorithms and Analytics, Net\ufb02ix\n\u201cIn this new world of big data, machine learning, and data scientists, Eric\nSiegel brings deep understanding to deep analytics.\u201d\n\u2014Marc Parrish, VP, Membership, Barnes & Noble\n\u201cA detailed outline for how we might tame the world\u2019s unpredictability. Eric\nadvocates quite clearly how some choices are predictably more pro\ufb01table\nthan others\u2014and I agree!\u201d\n\u2014Dennis R.", "Barnes & Noble\n\u201cA detailed outline for how we might tame the world\u2019s unpredictability. Eric\nadvocates quite clearly how some choices are predictably more pro\ufb01table\nthan others\u2014and I agree!\u201d\n\u2014Dennis R. Mortensen, CEO of Visual Revenue,\nformer Director of Data Insights at Yahoo!\n\u201cThis book is an invaluable contribution to predictive analytics. Eric\u2019s\nexplanation of how to anticipate future events is thought provoking and\na great read for everyone.\u201d\n\u2014Jean Paul Isson, Global VP Business Intelligence and Predictive\nAnalytics, Monster Worldwide; coauthor, Win with Advanced Business\nAnalytics: Creating Business Value from Your Data\n\u201cPredictive analytics is the key to unlocking new value at a previously\nunimaginable economic scale. In this book, Siegel explains how, doing an\nexcellent job to bridge theory and practice.\u201d\n\u2014Sergo Grigalashvili, VP of Information Technology,\nCrawford & Company\n\n\u201cPredictive analytics has been steeped in fear of the unknown. Eric Siegel\ndistinctively clari\ufb01es, remov", "e theory and practice.\u201d\n\u2014Sergo Grigalashvili, VP of Information Technology,\nCrawford & Company\n\n\u201cPredictive analytics has been steeped in fear of the unknown. Eric Siegel\ndistinctively clari\ufb01es, removing the mystery and exposing its many bene\ufb01ts.\u201d\n\u2014Jane Kuberski, Engineering and Analytics,\nNationwide Insurance\n\u201cAs predictive analytics moves from fashionable to mainstream, Siegel\nremoves the complexity and shows its power.\u201d\n\u2014Rajeeve Kaul, Senior VP, Of\ufb01ceMax\n\u201cDr. Siegel humanizes predictive analytics. He blends analytical rigor with\nreal-life examples with an ease that is remarkable in his \ufb01eld. The book is\ninformative, fun, and easy to understand. I \ufb01nished reading it in one sitting. A\nmust-read . . . not just for data scientists!\u201d\n\u2014Madhu Iyer, Marketing Statistician, Intuit\n\u201cAn engaging encyclopedia \ufb01lled with real-world applications that should\nmotivate anyone still sitting on the sidelines to jump into predictive analytics\nwith both feet.\u201d\n\u2014Jared Waxman, Web Marketer at LegalZoom,\np", "ncyclopedia \ufb01lled with real-world applications that should\nmotivate anyone still sitting on the sidelines to jump into predictive analytics\nwith both feet.\u201d\n\u2014Jared Waxman, Web Marketer at LegalZoom,\npreviously at Adobe, Amazon, and Intuit\n\u201cSiegel covers predictive analytics from start to \ufb01nish, bringing it to life and\nleaving you wanting more.\u201d\n\u2014Brian Seeley, Manager, Risk Analytics, Paychex, Inc.\n\u201cA wonderful look into the world of predictive analytics from the perspec-\ntive of a true practitioner.\u201d\n\u2014Shawn Hushman, VP, Analytic Insights,\nKelley Blue Book\n\u201cA must\u2014Predictive Analytics provides an amazing view of the analytical\nmodels that predict and in\ufb02uence our lives on a daily basis. Siegel makes it a\nbreeze to understand, for all readers.\u201d\n\u2014Zhou Yu, Online-to-Store Analyst, Google\n\n\u201cAs our ability to collect and analyze information improves, experts like Eric\nSiegel are our guides to the mysteries unlocked and the moral questions that\narise.\u201d\n\u2014Jules Polonetsky, Co-Chair and Director", "our ability to collect and analyze information improves, experts like Eric\nSiegel are our guides to the mysteries unlocked and the moral questions that\narise.\u201d\n\u2014Jules Polonetsky, Co-Chair and Director, Future of Privacy\nForum; former Chief Privacy Of\ufb01cer, AOL and DoubleClick\n\u201cHighly recommended. As Siegel shows in his very readable new book, the\nresults achieved by those adopting predictive analytics to improve decision\nmaking are game changing.\u201d\n\u2014James Taylor, CEO, Decision Management Solutions\n\u201cAn engaging, humorous introduction to the world of the data scientist.\nDr. Siegel demonstrates with many real-life examples how predictive analytics\nmakes big data valuable.\u201d\n\u2014David McMichael, VP, Advanced Business Analytics\n\u201cAn excellent exposition on the next generation of business intelligence\u2014\nit\u2019s really mankind\u2019s latest quest for arti\ufb01cial intelligence.\u201d\n\u2014Christopher Hornick, President and CEO,\nHBSC Strategic Services\n\n\nCover image: Winona Nelson\nCover design: Wiley\nInterior image design", "\nit\u2019s really mankind\u2019s latest quest for arti\ufb01cial intelligence.\u201d\n\u2014Christopher Hornick, President and CEO,\nHBSC Strategic Services\n\n\nCover image: Winona Nelson\nCover design: Wiley\nInterior image design: Matt Kornhaas\nCopyright \ue0022016 by Eric Siegel. All rights reserved.\nPublished by John Wiley & Sons, Inc., Hoboken, New Jersey.\nPublished simultaneously in Canada.\nJeopardy!\ue004is a registered trademark of Jeopardy Productions, Inc.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by\nany means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as permitted\nunder Section 107 or 108 of the 1976 United States Copyright Act, without either the prior written permission\nof the Publisher, or authorization through payment of the appropriate per-copy fee to the Copyright Clearance\nCenter, 222 Rosewood Drive, Danvers, MA 01923, (978) 750\u20138400, fax (978) 646\u20138600, or on the Web\nat www.copyright.com. Requests to the P", "ment of the appropriate per-copy fee to the Copyright Clearance\nCenter, 222 Rosewood Drive, Danvers, MA 01923, (978) 750\u20138400, fax (978) 646\u20138600, or on the Web\nat www.copyright.com. Requests to the Publisher for permission should be addressed to the Permissions\nDepartment, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030, (201) 748\u20136011,\nfax (201) 748\u20136008, or online at www.wiley.com/go/permissions.\nLimit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in\npreparing this book, they make no representations or warranties with the respect to the accuracy or\ncompleteness of the contents of this book and speci\ufb01cally disclaim any implied warranties of merchantability or\n\ufb01tness for a particular purpose. No warranty may be created or extended by sales representatives or written sales\nmaterials. The advice and strategies contained herein may not be suitable for your situation. You should consult\nwith a professional where appropriate. ", "ended by sales representatives or written sales\nmaterials. The advice and strategies contained herein may not be suitable for your situation. You should consult\nwith a professional where appropriate. Neither the publisher nor the author shall be liable for damages arising\nherefrom.\nFor general information about our other products and services, please contact our Customer Care\nDepartment within the United States at (800) 762\u20132974, outside the United States at (317) 572\u20133993 or\nfax (317) 572\u20134002.\nWiley publishes in a variety of print and electronic formats and by print-on-demand. Some material included\nwith standard print versions of this book may not be included in e-books or in print-on-demand. If this book\nrefers to media such as a CD or DVD that is not included in the version you purchased, you may download this\nmaterial at http://booksupport.wiley.com. For more information about Wiley products, visit www.wiley.com.\nLibrary of Congress Cataloging-in-Publication Data:\nNames: Siegel, ", "sed, you may download this\nmaterial at http://booksupport.wiley.com. For more information about Wiley products, visit www.wiley.com.\nLibrary of Congress Cataloging-in-Publication Data:\nNames: Siegel, Eric, 1968-\nTitle: Predictive analytics : the power to predict who will click, buy, lie,\nor die / Eric Siegel.\nDescription: Revised and Updated Edition. | Hoboken : Wiley, 2016. | Revised\nedition of the author\u2019s Predictive analytics, 2013. | Includes index.\nIdenti\ufb01ers: LCCN 2015031895 (print) | LCCN 2015039877 (ebook) |\nISBN 9781119145677 (paperback) | ISBN 9781119145684 (pdf) |\nISBN 9781119153658 (epub)\nSubjects: LCSH: Social sciences\u2014Forecasting. | Economic forecasting |\nPrediction (Psychology) | Social prediction. | Human behavior. | BISAC:\nBUSINESS & ECONOMICS / Consumer Behavior. | BUSINESS & ECONOMICS /\nEconometrics. | BUSINESS & ECONOMICS / Marketing / General.\nClassi\ufb01cation: LCC H61.4 .S54 2016 (print) | LCC H61.4 (ebook) | DDC\n303.49\u2014dc23\nLC record available at http://lccn.loc.gov", "SS & ECONOMICS /\nEconometrics. | BUSINESS & ECONOMICS / Marketing / General.\nClassi\ufb01cation: LCC H61.4 .S54 2016 (print) | LCC H61.4 (ebook) | DDC\n303.49\u2014dc23\nLC record available at http://lccn.loc.gov/2015031895\nPrinted in the United States of America\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n\nThis book is dedicated with all my heart to my mother,\nLisa Schamberg, and my father, Andrew Siegel.\n\nContents\nForeword\nThomas H. Davenport\nxvii\nPreface to the Revised and Updated Edition\nxxi\nWhat\u2019s new and who\u2019s this book for\u2014the Predictive\nAnalytics FAQ\nPreface to the Original Edition\nxxix\nWhat is the occupational hazard of predictive analytics?\nIntroduction\nThe Prediction Effect\n1\nHow does predicting human behavior combat risk, fortify healthcare,\ntoughen crime \ufb01ghting, boost sales, and cut costs? Why must a\ncomputer learn in order to predict? How can lousy predictions be\nextremelyvaluable?Whatmakesdataexceptionallyexciting?Howis\ndatasciencelikeporn?Whyshouldn\u2019tcomputersbecalledcomputers?\nWhy do organizations pred", "learn in order to predict? How can lousy predictions be\nextremelyvaluable?Whatmakesdataexceptionallyexciting?Howis\ndatasciencelikeporn?Whyshouldn\u2019tcomputersbecalledcomputers?\nWhy do organizations predict when you will die?\nChapter 1\nLiftoff! Prediction Takes Action (deployment)\n23\nHow much guts does it take to deploy a predictive model into \ufb01eld\noperation,andwhatdoyoustandtogain?Whathappenswhenaman\ninvests his entire life savings into his own predictive stock market\ntrading system?\nxiii\n\nChapter 2\nWith Power Comes Responsibility: Hewlett-Packard,\nTarget, the Cops, and the NSA Deduce Your Secrets (ethics)\n47\nHow do we safely harness a predictive machine that can foresee job\nresignation, pregnancy, and crime? Are civil liberties at risk? Why\ndoesoneleadinghealthinsurancecompanypredictpolicyholderdeath?\nTwo extended sidebars reveal: 1) Does the government undertake\nfraud detection more for its citizens or for self-preservation, and 2) for\nwhat compelling purpose does the NSA need your dat", "olderdeath?\nTwo extended sidebars reveal: 1) Does the government undertake\nfraud detection more for its citizens or for self-preservation, and 2) for\nwhat compelling purpose does the NSA need your data even if you\nhave no connection to crime whatsoever, and can the agency use\nmachine learning supercomputers to \ufb01ght terrorism without endan-\ngering human rights?\nChapter 3\nThe Data Effect: A Glut at the End of the Rainbow (data)\n103\nWeareuptoourearsindata,buthowmuchcanthisrawmaterialreally\ntell us? What actually makes it predictive? What are the most bizarre\ndiscoveriesfromdata?Whenwe\ufb01ndaninterestinginsight,whyarewe\noften better off not asking why? In what way is bigger data more\ndangerous?Howdoweavoidbeingfooledbyrandomnoiseandensure\nscienti\ufb01c discoveries are trustworthy?\nChapter 4\nThe Machine That Learns: A Look inside Chase\u2019s\nPrediction of Mortgage Risk (modeling)\n147\nWhat form of risk has the perfect disguise? How does prediction\ntransform risk to opportunity? What should all business", "ne That Learns: A Look inside Chase\u2019s\nPrediction of Mortgage Risk (modeling)\n147\nWhat form of risk has the perfect disguise? How does prediction\ntransform risk to opportunity? What should all businesses learn from\ninsurance companies? Why does machine learning require art in\naddition to science? What kind of predictive model can be understood\nby everyone? How can we con\ufb01dently trust a machine\u2019s predictions?\nWhy couldn\u2019t prediction prevent the global \ufb01nancial crisis?\nxiv\nContents\n\nChapter 5\nThe Ensemble Effect: Netflix, Crowdsourcing, and\nSupercharging Prediction (ensembles)\n185\nTo crowdsource predictive analytics\u2014outsource it to the public at\nlarge\u2014acompanylaunchesitsstrategy,data,andresearchdiscoveries\ninto the public spotlight. How can this possibly help the company\ncompete?Whatkeyinnovationinpredictiveanalyticshascrowdsourc-\ning helped develop? Must supercharging predictive precision involve\noverwhelming complexity, or is there an elegant solution? Is there\nwisdom in nonhuman crowds", "tioninpredictiveanalyticshascrowdsourc-\ning helped develop? Must supercharging predictive precision involve\noverwhelming complexity, or is there an elegant solution? Is there\nwisdom in nonhuman crowds?\nChapter 6\nWatson and the Jeopardy! Challenge (question answering)\n207\nHow does Watson\u2014IBM\u2019s Jeopardy!-playing computer\u2014work?\nWhydoesitneedpredictivemodelinginordertoanswerquestions,and\nwhat secret sauce empowers its high performance? How does the\niPhone\u2019s Siri compare? Why is human language such a challenge for\ncomputers? Is arti\ufb01cial intelligence possible?\nChapter 7\nPersuasion by the Numbers: How Telenor, U.S. Bank,\nand the Obama Campaign Engineered Influence (uplift)\n251\nWhat is the scienti\ufb01c key to persuasion? Why does some marketing\n\ufb01ercely back\ufb01re? Why is human behavior the wrong thing to predict?\nWhat should all businesses learn about persuasion from presidential\ncampaigns?WhatvoterpredictionshelpedObamawinin2012more\nthanthedetectionofswingvoters?Howcoulddoctorskillfewerpatients\nin", "to predict?\nWhat should all businesses learn about persuasion from presidential\ncampaigns?WhatvoterpredictionshelpedObamawinin2012more\nthanthedetectionofswingvoters?Howcoulddoctorskillfewerpatients\ninadvertently? How is a person like a quantum particle? Riddle:\nWhatoftenhappenstoyouthatcannotbeperceivedandthatyoucan\u2019t\neven be sure has happened afterward\u2014but that can be predicted in\nadvance?\nContents\nxv\n\nAfterword\n291\nEleven Predictions for the First Hour of 2022\nAppendices\nA. The Five Effects of Prediction\n295\nB. Twenty Applications of Predictive Analytics\n296\nC. Prediction People\u2014Cast of \u201cCharacters\u201d\n300\nHands-On Guide\n303\nResources for Further Learning\nAcknowledgments\n307\nAbout the Author\n311\nIndex\n313\nAlso see the Central Tables (color insert) for a cross-industry compendium\nof 182 examples of predictive analytics.\nThis book\u2019s Notes\u2014120 pages of citations and comments pertaining to the\nchapters above\u2014are available online at www.PredictiveNotes.com.\nxvi\nContents\n\nForeword\nThis book d", "ples of predictive analytics.\nThis book\u2019s Notes\u2014120 pages of citations and comments pertaining to the\nchapters above\u2014are available online at www.PredictiveNotes.com.\nxvi\nContents\n\nForeword\nThis book deals with quantitative efforts to predict human behavior. One of\nthe earliest efforts to do that was in World War II. Norbert Wiener, the\nfather of \u201ccybernetics,\u201d began trying to predict the behavior of German\nairplane pilots in 1940\u2014with the goal of shooting them from the sky. His\nmethod was to take as input the trajectory of the plane from its observed\nmotion, consider the pilot\u2019s most likely evasive maneuvers, and predict\nwhere the plane would be in the near future so that a \ufb01red shell could hit it.\nUnfortunately, Wiener could predict only one second ahead of a plane\u2019s\nmotion, but 20 seconds of future trajectory were necessary to shoot down a\nplane.\nIn Eric Siegel\u2019s book, however, you will learn about a large number of\nprediction efforts that are much more successful. Computers have got", "s of future trajectory were necessary to shoot down a\nplane.\nIn Eric Siegel\u2019s book, however, you will learn about a large number of\nprediction efforts that are much more successful. Computers have gotten a\nlot faster since Wiener\u2019s day, and we have a lot more data. As a result, banks,\nretailers, political campaigns, doctors and hospitals, and many more organi-\nzations have been quite successful of late at predicting the behavior of\nparticular humans. Their efforts have been helpful at winning customers,\nelections, and battles with disease.\nMy view\u2014and Siegel\u2019s, I would guess\u2014is that this predictive activity has\ngenerally been good for humankind. In the context of healthcare, crime, and\nterrorism, it can save lives. In the context of advertising, using predictions is\nmore ef\ufb01cient and could conceivably save both trees (for direct mail and\ncatalogs) and the time and attention of the recipient. In politics, it seems to\nreward those candidates who respect the scienti\ufb01c method (some might\nd", " conceivably save both trees (for direct mail and\ncatalogs) and the time and attention of the recipient. In politics, it seems to\nreward those candidates who respect the scienti\ufb01c method (some might\ndisagree, but I see that as a positive).\nxvii\n\nHowever, as Siegel points out\u2014early in the book, which is admirable\u2014\nthese approaches can also be used in somewhat harmful ways. \u201cWith great\npower comes great responsibility,\u201d he notes in quoting Spider-Man. The\nimplication is that we must be careful as a society about how we use\npredictive models, or we may be restricted from using and bene\ufb01ting from\nthem. Like other powerful technologies or disruptive human innovations,\npredictive analytics is essentially amoral and can be used for good or evil. To\navoid the evil applications, however, it is certainly important to understand\nwhat is possible with predictive analytics, and you will certainly learn that if\nyou keep reading.\nThis book is focused on predictive analytics, which is not the only typ", "ainly important to understand\nwhat is possible with predictive analytics, and you will certainly learn that if\nyou keep reading.\nThis book is focused on predictive analytics, which is not the only type of\nanalytics, but the most interesting and important type. I don\u2019t think we need\nmore books anyway on purely descriptive analytics, which only describe the\npast and don\u2019t provide any insight as to why it happened. I also often refer in\nmy own writing to a third type of analytics\u2014\u201cprescriptive\u201d\u2014that tells its\nusers what to do through controlled experiments or optimization. Those\nquantitative methods are much less popular, however, than predictive\nanalytics.\nThis book and the ideas behind it are a good counterpoint to the work of\nNassim Nicholas Taleb. His books, including The Black Swan, suggest that\nmany efforts at prediction are doomed to fail because of randomness and the\ninherent unpredictability of complex events. Taleb is no doubt correct that\nsome events are black swans that are be", "est that\nmany efforts at prediction are doomed to fail because of randomness and the\ninherent unpredictability of complex events. Taleb is no doubt correct that\nsome events are black swans that are beyond prediction, but the fact is that\nmost human behavior is quite regular and predictable. The many examples\nthat Siegel provides of successful prediction remind us that most swans are\nwhite.\nSiegel also resists the blandishments of the \u201cbig data\u201d movement.\nCertainly some of the examples he mentions fall into this category\u2014data\nthat is too large or unstructured to be easily managed by conventional\nrelational databases. But the point of predictive analytics is not the relative\nsize or unruliness of your data, but what you do with it. I have found that\n\u201cbig data often equals small math,\u201d and many big data practitioners are\ncontent just to use their data to create some appealing visual analytics. That\u2019s\nnot nearly as valuable as creating a predictive model.\nxviii\nForeword\n\nSiegel has fashion", "ny big data practitioners are\ncontent just to use their data to create some appealing visual analytics. That\u2019s\nnot nearly as valuable as creating a predictive model.\nxviii\nForeword\n\nSiegel has fashioned a book that is both sophisticated and fully accessible to\nthe non-quantitative reader. It\u2019s got great stories, great illustrations, and an\nentertaining tone. Such non-quants should de\ufb01nitely read this book, because\nthere is little doubt that their behavior will be analyzed and predicted\nthroughout their lives. It\u2019s also quite likely that most non-quants will\nincreasingly have to consider, evaluate, and act on predictive models at work.\nIn short, we live in a predictive society. The best way to prosper in it is to\nunderstand the objectives, techniques, and limits of predictive models. And\nthe best way to do that is simply to keep reading this book.\n\u2014Thomas H. Davenport\nThomas H. Davenport is the President\u2019s\nDistinguished Professor at Babson College,\na fellow of the MIT Center for Digital", "e best way to do that is simply to keep reading this book.\n\u2014Thomas H. Davenport\nThomas H. Davenport is the President\u2019s\nDistinguished Professor at Babson College,\na fellow of the MIT Center for Digital Business,\nSenior Advisor to Deloitte Analytics,\nand cofounder of the International Institute for Analytics.\nHe is the coauthor of Competing on Analytics,\nBig Data @ Work, and several other books on analytics.\nForeword\nxix\n\nPreface to the Revised and\nUpdated Edition\nWhat\u2019s New and Who\u2019s This Book for\u2014\nThe Predictive Analytics FAQ\nData Scientist: The Sexiest Job of the Twenty-\ufb01rst Century\n\u2014Title of a Harvard Business Review article by\nThomas Davenport and DJ Patil, who in 2015\nbecame the \ufb01rst U.S. Chief Data Scientist\nPrediction is booming. It reinvents industries and runs the world.\nMore and more, predictive analytics (PA) drives commerce, manufactur-\ning, healthcare, government, and law enforcement. In these spheres, organi-\nzations operate more effectively by way of predicting behavior\u2014i", "ore, predictive analytics (PA) drives commerce, manufactur-\ning, healthcare, government, and law enforcement. In these spheres, organi-\nzations operate more effectively by way of predicting behavior\u2014i.e., the\noutcome for each individual customer, employee, patient, voter, and suspect.\nEveryone\u2019s doing it. Accenture and Forrester both report that PA\u2019s\nadoption has more than doubled in recent years. Transparency Market\nResearch projects the PA market will reach $6.5 billion within a few years.\nA Gartner survey ranked business intelligence and analytics as the current\nnumber one investment priority of chief information of\ufb01cers. And in a\nSalesforce.com study, PA showed the highest growth rate of all sales tech\ntrends, more than doubling its adoption in the next 18 months. High-\nperformance sales teams are four times more likely to already be using PA\nthan underperformers.\nxxi\n\nI am a witness to PA\u2019s expanding deployment across industries. Predictive\nAnalytics World (PAW), the conference se", "es teams are four times more likely to already be using PA\nthan underperformers.\nxxi\n\nI am a witness to PA\u2019s expanding deployment across industries. Predictive\nAnalytics World (PAW), the conference series I founded, has hosted over\n10,000 attendees since its launch in 2009 and is expanding well beyond its\noriginal PAW Business events. With the expert assistance of industry partners,\nwe\u2019ve launched the industry-focused events PAW Government, PAW\nHealthcare, PAW Financial, PAW Workforce, and PAW Manufacturing,\nevents for senior executives, and the news site The Predictive Analytics Times.\nSince the publication of this book\u2019s \ufb01rst edition in 2013, I have been\ncommissioned to deliver keynote addresses in each of these industries: market-\ning, market research, e-commerce, \ufb01nancial services, insurance, news media,\nhealthcare, pharmaceuticals, government, human resources, travel, real estate,\nconstruction, and law, plus executive summits and university conferences.\nWant a future career in fut", "urance, news media,\nhealthcare, pharmaceuticals, government, human resources, travel, real estate,\nconstruction, and law, plus executive summits and university conferences.\nWant a future career in futurology? The demand is blowing up.\nMcKinsey forecasts a near-term U.S. shortage of 140,000 analytics experts\nand 1.5 million managers \u201cwith the skills to understand and make decisions\nbased on analysis of big data.\u201d LinkedIn\u2019s number one \u201cHottest Skills That\nGot People Hired\u201d is \u201cstatistical analysis and data mining.\u201d\nPA is like Moneyball for . . . money.\nFrequently Asked Questions about\nPredictive Analytics\nWho is this book for?\nEveryone. It\u2019s easily understood by all readers. Rather than a how-to for\nhands-on techies, the book serves lay readers, technology enthusiasts,\nexecutives, and analytics experts alike by covering new case studies and\nthe latest state-of-the-art techniques.\nIs the idea of predictive analytics hard to understand?\nNot at all. The heady, sophisticated notion of learn", "tics experts alike by covering new case studies and\nthe latest state-of-the-art techniques.\nIs the idea of predictive analytics hard to understand?\nNot at all. The heady, sophisticated notion of learning from data to predict may\nsound beyond reach, but breeze through the short Introduction chapter and\nyou\u2019ll see: The basic idea is clear, accessible, and undeniably far-reaching.\nxxii\nPreface to the Revised and Updated Edition\n\nIs this book a how-to?\nNo, it is a conceptually complete, substantive introduction and industry\noverview.\nNot a how-to? Then why should techies read it?\nAlthough this mathless introduction is understandable by any reader\u2014\nincluding those with no technical background\u2014here\u2019s why it also affords\nvalue for would-be and established hands-on practitioners:\n\u2022 A great place to start\u2014provides prerequisite conceptual knowledge\nfor those who will go on to learn the hands-on practice or will serve in\nan executive or management role in the deployment of PA.\n\u2022 Detailed case stu", "o start\u2014provides prerequisite conceptual knowledge\nfor those who will go on to learn the hands-on practice or will serve in\nan executive or management role in the deployment of PA.\n\u2022 Detailed case studies\u2014explores the real-world deployment of PA by\nChase, IBM, HP, Net\ufb02ix, the NSA, Target, U.S. Bank, and more.\n\u2022 A compendium of 182 mini-case studies\u2014the Central Tables,\ndivided into nine industry groups, include examples from BBC,\nCitibank, ConEd, Facebook, Ford, Google, the IRS, Match.com,\nMTV, PayPal, P\ufb01zer, Spotify, Uber, UPS, Wikipedia, and more.\n\u2022 Advanced, cutting-edge topics\u2014the last three chapters introduce\nsub\ufb01eldsneweventomany seniorexperts:Ensemblemodels,IBM Watson\u2019s\nquestion answering, and uplift modeling. No matter how experienced you\nare, starting with a conceptually rich albeit non-technical overview may\nbene\ufb01t you more than you\u2019d expect\u2014especially for uplift modeling. The\nNotes for these three chapters then provide comprehensive references to\ntechnically deep sources (ava", "t non-technical overview may\nbene\ufb01t you more than you\u2019d expect\u2014especially for uplift modeling. The\nNotes for these three chapters then provide comprehensive references to\ntechnically deep sources (available at www.PredictiveNotes.com).\n\u2022 Privacy and civil liberties\u2014the second chapter tackles the particular\nethical concerns that arise when harnessing PA\u2019s power.\n\u2022 Holistic industry overview\u2014the book extends more broadly than a\nstandard technology introduction\u2014all of the above adds up to a survey of\nthe \ufb01eld that sheds light on its societal, commercial, and ethical context.\nThat said, burgeoning practitioners who wish to jump directly to a more\ntraditional, technically in-depth or hands-on treatment of this topic should\nPreface to the Revised and Updated Edition\nxxiii\n\nconsider themselves warned: This is not the book you are seeking (but it\nmakes a good gift; any of your relatives would be able to understand it and\nlearn about your \ufb01eld of interest).\nAs with introductions to other \ufb01elds ", "rned: This is not the book you are seeking (but it\nmakes a good gift; any of your relatives would be able to understand it and\nlearn about your \ufb01eld of interest).\nAs with introductions to other \ufb01elds of science and engineering, if you are\npursuing a career in the \ufb01eld, this book will set the foundation, yet only whet\nyour appetite for more. At the end of this book, you are guided by the\nHands-On Guide on where to go next for the technical how-to and\nadvanced underlying theory and math.\nWhat is the purpose of this book?\nI wrote this book to demonstrate why PA is intuitive, powerful, and awe-\ninspiring. It\u2019s a book about the most in\ufb02uential and valuable achievements of\ncomputerized prediction and the two things that make it possible: the people\nbehind it and the fascinating science that powers it.\nWhile there are a number of books that approach the how-to side of PA,\nthis book serves a different purpose (which turned out to be a rewarding\nchallenge for its author): sharing with a wider a", "rs it.\nWhile there are a number of books that approach the how-to side of PA,\nthis book serves a different purpose (which turned out to be a rewarding\nchallenge for its author): sharing with a wider audience a complete picture of\nthe \ufb01eld, from the way in which it empowers organizations, down to the\ninner workings of predictive modeling.\nWith its impact on the world growing so quickly, it\u2019s high time the\npredictive power of data\u2014and how to scienti\ufb01cally tap it\u2014be demysti\ufb01ed.\nLearning from data to predict human behavior is no longer arcane.\nHow technical does this book get?\nWhile accessible and friendly to newcomers of any background, this book\nexplores \u201cunder the hood\u201d far enough to reveal the inner workings of decision\ntrees (Chapter 4), an exemplary form of predictive model that serves well as a\nplace to start learning about PA, and often as a strong \ufb01rst option when\nexecuting a PA project.\nI strove to go as deep as possible\u2014substantive across the gamut of\nfascinating topics related ", "ell as a\nplace to start learning about PA, and often as a strong \ufb01rst option when\nexecuting a PA project.\nI strove to go as deep as possible\u2014substantive across the gamut of\nfascinating topics related to PA\u2014while still sustaining interest and accessibility\nnot only for neophyte users, but even for those interested in the \ufb01eld\navocationally, curious about science and how it is changing the world.\nxxiv\nPreface to the Revised and Updated Edition\n\nIs this a university textbook?\nThis book has served as a textbook at more than 30 colleges and universities.\nA former computer science professor, I wrote this introduction to be\nconceptually complete. In the table of contents, the words in parentheses\nbeside each chapter\u2019s \u201ccatchy\u201d title reveal an outline that covers the\nfundamentals: (1) model deployment, (2) ethics, (3) data, (4) predictive modeling,\n(5) ensemble models, (6) question answering, and (7) uplift modeling. To guide\nreading assignments, see the diagram under the next question below.\n", "ment, (2) ethics, (3) data, (4) predictive modeling,\n(5) ensemble models, (6) question answering, and (7) uplift modeling. To guide\nreading assignments, see the diagram under the next question below.\nHowever, this is not written in the formal style of a textbook; rather, I\nsought to deliver an entertaining, engaging, relevant work that illustrates the\nconcepts largely via anecdotes.\nFor instructors considering this book for course material, additional\nresources and information may be found at www.teachPA.com.\nHow should I read this book?\nThe chapters of this book build upon one another. Some depend only on\n\ufb01rst reading the Introduction, but others build cumulatively. The \ufb01gure\nbelow depicts these dependencies\u2014read a chapter only after \ufb01rst reading the\none it points up to. For example, Chapter 3 assumes you\u2019ve already read\nChapter 1, which assumes you\u2019ve read the Introduction.\nDependencies between chapters. An arrow pointing up means,\n\u201cRead the chapter above \ufb01rst.\u201d\nPreface to the Revise", "ter 3 assumes you\u2019ve already read\nChapter 1, which assumes you\u2019ve read the Introduction.\nDependencies between chapters. An arrow pointing up means,\n\u201cRead the chapter above \ufb01rst.\u201d\nPreface to the Revised and Updated Edition\nxxv\n\nNote: If you are reading the e-book version, be sure not to miss the Central Tables (a\ncompendium of 182 mini-case studies), the link for which may be less visibly located\ntoward the end of the table of contents.\nWhat\u2019s new in the \u201cRevised and Updated\u201d edition of\nPredictive Analytics?\n\u2022 The Real Reason the NSA Wants Your Data: Automatic Sus-\npect Discovery. A special sidebar in Chapter 2 (on ethics in PA)\npresumes\u2014with much evidence\u2014that the National Security Agency\nconsiders PA a strategic priority. Can the organization use PA without\nendangering civil liberties?\n\u2022 Dozens of new examples from Facebook, Hopper, Shell, Uber,\nUPS, the U.S. government, and more. The Central Tables\u2019\ncompendium of mini-case studies has grown to 182 entries, including\nbreaking examples", "Dozens of new examples from Facebook, Hopper, Shell, Uber,\nUPS, the U.S. government, and more. The Central Tables\u2019\ncompendium of mini-case studies has grown to 182 entries, including\nbreaking examples.\n\u2022 A much-needed warning regarding bad science. Chapter 3, \u201cThe\nData Effect,\u201d includes an in-depth section about an all-too-common\npitfall and how we avoid it, i.e., how to successfully tap data\u2019s potential\nwithout being fooled by random noise, ensuring sound discoveries are\nmade.\n\u2022 Even more extensive Notes, updated and expanded to 120 pages,\nnow moved online. Now located at www.PredictiveNotes.com, the\nNotes include citations and comments that pertain to the above new\ncontent, as well as updated citations throughout chapters.\nWhere can I learn more after this book, such as a how-to for\nhands-on practice?\n\u2022 The Hands-On Guide at the end of this book\u2014reading and\ntraining options that guide getting started\n\u2022 This book\u2019s website\u2014videos, articles, and more resources: www\n.thepredictionbook.c", "s-on practice?\n\u2022 The Hands-On Guide at the end of this book\u2014reading and\ntraining options that guide getting started\n\u2022 This book\u2019s website\u2014videos, articles, and more resources: www\n.thepredictionbook.com\nxxvi\nPreface to the Revised and Updated Edition\n\n\u2022 Predictive Analytics World\u2014the leading cross-vendor conference\nseries in North America and Europe, which includes advanced training\nworkshop days and the industry-speci\ufb01c events PAW Business, PAW\nGovernment, PAW Healthcare, PAW Financial, PAW Workforce, and\nPAW Manufacturing: www.pawcon.com\n\u2022 The Predictive Analytics Guide\u2014articles, industry portals, and\nother resources: www.pawcon.com/guide\n\u2022 Predictive Analytics Applied\u2014the author\u2019s online training work-\nshop, which, unlike this book, is a how-to. Access immediately, on-\ndemand at any time: www.businessprediction.com\n\u2022 The Predictive Analytics Times\u2014the premier resource: industry news,\ntechnical articles, videos, events, and community: www.predictive\nanalyticstimes.com\nPreface to the ", "e: www.businessprediction.com\n\u2022 The Predictive Analytics Times\u2014the premier resource: industry news,\ntechnical articles, videos, events, and community: www.predictive\nanalyticstimes.com\nPreface to the Revised and Updated Edition\nxxvii\n\nPreface to the Original Edition\nYesterday is history, tomorrow is a mystery, but today is a gift. That\u2019s why we call it\nthe present.\n\u2014Attributed to A. A. Milne, Bil Keane, and Oogway,\nthe wise turtle in Kung Fu Panda\nPeople look at me funny when I tell them what I do. It\u2019s an occupational\nhazard.\nThe Information Age suffers from a glaring omission. This claim may\nsurprise many, considering we are actively recording Everything That\nHappens in the World. Moving beyond history books that document\nimportant events, we\u2019ve progressed to systems that log every click, payment,\ncall, crash, crime, and illness. With this in place, you would expect lovers of\ndata to be satis\ufb01ed, if not spoiled rotten.\nBut this apparent in\ufb01nity of information excludes the very events", ", payment,\ncall, crash, crime, and illness. With this in place, you would expect lovers of\ndata to be satis\ufb01ed, if not spoiled rotten.\nBut this apparent in\ufb01nity of information excludes the very events that\nwould be most valuable to know of: things that haven\u2019t happened yet.\nEveryone craves the power to see the future; we are collectively obsessed\nwith prediction. We bow to prognostic deities. We empty our pockets for\npalm readers. We hearken to horoscopes, adore astrology, and feast upon\nfortune cookies.\nBut many people who salivate for psychics also spurn science. Their innate\nresponse says \u201cyuck\u201d\u2014it\u2019s either too hard to understand or too boring. Or\nperhaps many believe prediction by its nature is just impossible without\nsupernatural support.\nxxix\n\nThere\u2019s a lighthearted TV show I like premised on this very theme, Psych,\nin which a sharp-eyed detective\u2014a modern-day, data-driven Sherlock\nHolmesian hipster\u2014has perfected the art of observation so masterfully,\nthe cops believe his spot-on", "emised on this very theme, Psych,\nin which a sharp-eyed detective\u2014a modern-day, data-driven Sherlock\nHolmesian hipster\u2014has perfected the art of observation so masterfully,\nthe cops believe his spot-on deductions must be an admission of guilt.\nThe hero gets out of this pickle by conforming to the norm: He simply\ninforms the police he is psychic, thereby managing to stay out of prison and\ncontinuing to \ufb01ght crime. Comedy ensues.\nI\u2019ve experienced the same impulse, for example, when receiving the\noccasional friendly inquiry as to my astrological sign. But, instead of posing as\na believer, I turn to humor: \u201cI\u2019m a Scorpio, and Scorpios don\u2019t believe in\nastrology.\u201d\nThe more common cocktail party interview asks what I do for a living. I\nbrace myself for eyes glazing over as I carefully enunciate: predictive analytics. Most\npeople have the luxury of describing their job in a single word: doctor, lawyer,\nwaiter, accountant, or actor. But, for me, describing this largely unknown \ufb01eld\nhijacks the ", "predictive analytics. Most\npeople have the luxury of describing their job in a single word: doctor, lawyer,\nwaiter, accountant, or actor. But, for me, describing this largely unknown \ufb01eld\nhijacks the conversation every time. Any attempt to be succinct falls \ufb02at:\nI\u2019m a business consultant in technology. They aren\u2019t satis\ufb01ed and ask, \u201cWhat\nkind of technology?\u201d\nI make computers predict what people will do. Bewilderment results, accom-\npanied by complete disbelief and a little fear.\nI make computers learn from data to predict individual human behavior. Bewil-\nderment, plus nobody wants to talk about data at a party.\nI analyze data to \ufb01nd patterns. Eyes glaze over even more; awkward pauses\nsink amid a sea of abstraction.\nI help marketers target which customers will buy or cancel. They sort of get it, but\nthis wildly undersells and pigeonholes the \ufb01eld.\nI predict customer behavior, like when Target famously predicted whether you are\npregnant. Moonwalking ensues.\nSo I wrote this book to demon", "et it, but\nthis wildly undersells and pigeonholes the \ufb01eld.\nI predict customer behavior, like when Target famously predicted whether you are\npregnant. Moonwalking ensues.\nSo I wrote this book to demonstrate for you why predictive analytics is\nintuitive, powerful, and awe-inspiring.\nI have good news: A little prediction goes a long way. I call this The Prediction\nEffect, a theme that runs throughout the book. The potency of prediction is\nxxx\nPreface to the Original Edition\n\npronounced\u2014as long as the predictions are better than guessing. This effect\nrenders predictive analytics believable. We don\u2019t have to do the impossible\nand attain true clairvoyance. The story is exciting yet credible: Putting odds\non the future to lift the fog just a bit off our hazy view of tomorrow means\npay dirt. In this way, predictive analytics combats risk, boosts sales, cuts costs,\nforti\ufb01es healthcare, streamlines manufacturing, conquers spam, toughens\ncrime \ufb01ghting, optimizes social networks, and wins electio", "his way, predictive analytics combats risk, boosts sales, cuts costs,\nforti\ufb01es healthcare, streamlines manufacturing, conquers spam, toughens\ncrime \ufb01ghting, optimizes social networks, and wins elections.\nDo you have the heart of a scientist or a businessperson? Do you feel more\nexcited by the very idea of prediction, or by the value it holds for the world?\nI was struck by the notion of knowing the unknowable. Prediction seems to\ndefy a law of nature: You cannot see the future because it isn\u2019t here yet. We\n\ufb01nd a workaround by building machines that learn from experience. It\u2019s the\nregimented discipline of using what we do know\u2014in the form of data\u2014to\nplace increasingly accurate odds on what\u2019s coming next. We blend the best of\nmath and technology, systematically tweaking until our scienti\ufb01c hearts are\ncontent to derive a system that peers right through the previously\nimpenetrable barrier between today and tomorrow.\nTalk about boldly going where no one has gone before!\nSome people are in sa", "arts are\ncontent to derive a system that peers right through the previously\nimpenetrable barrier between today and tomorrow.\nTalk about boldly going where no one has gone before!\nSome people are in sales; others are in politics. I\u2019m in prediction, and it\u2019s\nawesome.\nPreface to the Original Edition\nxxxi\n\n\nIntroduction\nThe Prediction Effect\nI\u2019m just like you. I succeed at times, and at others I fail. Some days good\nthings happen to me, some days bad. We always wonder how things could\nhave gone differently. I begin with seven brief tales of woe:\n1. In 2009 I just about destroyed my right knee downhill skiing in Utah.\nThe jump was no problem; it was landing that presented an issue. For\nknee surgery, I had to pick a graft source from which to reconstruct my\nbusted ACL (the knee\u2019s central ligament). The choice is a tough one\nand can make the difference between living with a good knee or a bad\nknee. I went with my hamstring. Could the hospital have selected a\nmedically better option for my cas", "t). The choice is a tough one\nand can make the difference between living with a good knee or a bad\nknee. I went with my hamstring. Could the hospital have selected a\nmedically better option for my case?\n2. Despite all my suffering, it was really my health insurance company\nthat paid dearly\u2014knee surgery is expensive. Could the company have\nbetter anticipated the risk of accepting a ski jumping fool as a customer and priced\nmy insurance premium accordingly?\n3. Back in 1995 another incident caused me suffering, although it hurt\nless. I fell victim to identity theft, costing me dozens of hours of\nbureaucratic baloney and tedious paperwork to clear up my dam-\naged credit rating. Could the creditors have prevented the \ufb01asco by detecting\n1\n\nthat the accounts were bogus when they were \ufb01led under my name in the \ufb01rst\nplace?\n4. With my name cleared, I recently took out a mortgage to buy an\napartment. Was it a good move, or should my \ufb01nancial adviser have warned\nme the property could soon be outva", "y name in the \ufb01rst\nplace?\n4. With my name cleared, I recently took out a mortgage to buy an\napartment. Was it a good move, or should my \ufb01nancial adviser have warned\nme the property could soon be outvalued by my mortgage?\n5. While embarking on vacation, I asked the neighboring airplane\npassenger what price she\u2019d paid for her ticket, and it was much less\nthan I\u2019d paid. Before I booked the \ufb02ight, could I have determined the airfare\nwas going to drop?\n6. My professional life is susceptible, too. My business is faring well, but a\ncompany always faces the risk of changing economic conditions and\ngrowing competition. Could we protect the bottom line by foreseeing which\nmarketing activities and other investments will pay off, and which will amount to\nburnt capital?\n7. Small ups and downs determine your fate and mine, every day. A\nprecise spam \ufb01lter has a meaningful impact on almost every working\nhour. We depend heavily on effective Internet search for work,\nhealth (e.g., exploring knee surgery", " your fate and mine, every day. A\nprecise spam \ufb01lter has a meaningful impact on almost every working\nhour. We depend heavily on effective Internet search for work,\nhealth (e.g., exploring knee surgery options), home improvement,\nand most everything else. We put our faith in personalized music\nand movie recommendations from Spotify and Net\ufb02ix. After all\nthese years, my mailbox wonders why companies don\u2019t know me\nwell enough to send less junk mail (and sacri\ufb01ce fewer trees\nneedlessly).\nThese predicaments matter. They can make or break your day, year, or life.\nBut what do they all have in common?\nThese challenges\u2014and many others like them\u2014are best addressed with\nprediction. Will the patient\u2019s outcome from surgery be positive? Will the\ncredit applicant turn out to be a fraudster? Will the homeowner face a bad\nmortgage? Will the airfare go down? Will the customer respond if mailed a\nbrochure? By predicting these things, it is possible to fortify healthcare,\ncombat risk, conquer spam, toughe", "owner face a bad\nmortgage? Will the airfare go down? Will the customer respond if mailed a\nbrochure? By predicting these things, it is possible to fortify healthcare,\ncombat risk, conquer spam, toughen crime \ufb01ghting, boost sales, and cut\ncosts.\n2\nIntroduction\n\nPrediction in Big Business\u2014The Destiny of\nAssets\nThere\u2019s another angle. Beyond bene\ufb01ting you and me as consumers, predic-\ntion serves the organization, empowering it with an entirely new form of\ncompetitive armament. Corporations positively pounce on prediction.\nIn the mid-1990s, an entrepreneurial scientist named Dan Steinberg\ndelivered predictive capabilities unto the nation\u2019s largest bank, Chase, to\nassist with their management of millions of mortgages. This mammoth\nenterprise put its faith in Dan\u2019s predictive technology, deploying it to drive\ntransactional decisions across a tremendous mortgage portfolio. What did this\nguy have on his r\u00e9sum\u00e9?\nPrediction is power. Big business secures a killer competitive stronghold\nby predict", "g it to drive\ntransactional decisions across a tremendous mortgage portfolio. What did this\nguy have on his r\u00e9sum\u00e9?\nPrediction is power. Big business secures a killer competitive stronghold\nby predicting the future destiny and value of individual assets. In this case, by\ndriving mortgage decisions with predictions about the future payment\nbehavior of homeowners, Chase curtailed risk, boosted pro\ufb01t, and witnessed\na windfall.\nIntroducing . . . the Clairvoyant Computer\nCompelled to grow and propelled to the mainstream, predictive technology\nis commonplace and affects everyone, every day. It impacts your experiences\nin undetectable ways as you drive, shop, study, vote, see the doctor,\ncommunicate, watch TV, earn, borrow, or even steal.\nThis book is about the most in\ufb02uential and valuable achievements of\ncomputerized prediction, and the two things that make it possible: the\npeople behind it, and the fascinating science that powers it.\nMaking such predictions poses a tough challenge. Each pre", "ements of\ncomputerized prediction, and the two things that make it possible: the\npeople behind it, and the fascinating science that powers it.\nMaking such predictions poses a tough challenge. Each prediction\ndepends on multiple factors: The various characteristics known about\neach patient, each homeowner, each consumer, and each e-mail that\nmay be spam. How shall we attack the intricate problem of putting all\nthese pieces together for each prediction?\nIntroduction\n3\n\nThe idea is simple, although that doesn\u2019t make it easy. The challenge is\ntackled by a systematic, scienti\ufb01c means to develop and continually improve\nprediction\u2014to literally learn to predict.\nThe solution is machine learning\u2014computers automatically developing\nnew knowledge and capabilities by furiously feeding on modern society\u2019s\ngreatest and most potent unnatural resource: data.\n\u201cFeed Me!\u201d\u2014Food for Thought for the Machine\nData is the new oil.\n\u2014European Consumer Commissioner Meglena Kuneva\nThe only source of knowledge is ex", "\u2019s\ngreatest and most potent unnatural resource: data.\n\u201cFeed Me!\u201d\u2014Food for Thought for the Machine\nData is the new oil.\n\u2014European Consumer Commissioner Meglena Kuneva\nThe only source of knowledge is experience.\n\u2014Albert Einstein\nIn God we trust. All others must bring data.\n\u2014William Edwards Deming (a business professor famous\nfor work in manufacturing)\nMost people couldn\u2019t be less interested in data. It can seem like such dry,\nboring stuff. It\u2019s a vast, endless regimen of recorded facts and \ufb01gures, each\nalone as mundane as the most banal tweet, \u201cI just bought some new\nsneakers!\u201d It\u2019s the unsalted, \ufb02avorless residue deposited en masse as businesses\nchurn away.\nDon\u2019t be fooled! The truth is that data embodies a priceless collection of\nexperience from which to learn. Every medical procedure, credit application,\nFacebook post, movie recommendation, fraudulent act, spammy e-mail, and\npurchase of any kind\u2014each positive or negative outcome, each successful or\nfailed sales call, each incident, ev", "dit application,\nFacebook post, movie recommendation, fraudulent act, spammy e-mail, and\npurchase of any kind\u2014each positive or negative outcome, each successful or\nfailed sales call, each incident, event, and transaction\u2014is encoded as data and\nwarehoused. This glut grows by an estimated 2.5 quintillion bytes per day\n(that\u2019s a 1 with 18 zeros after it). And so a veritable Big Bang has set off,\ndelivering an epic sea of raw materials, a plethora of examples so great in\nnumber, only a computer could manage to learn from them. Used correctly,\ncomputers avidly soak up this ocean like a sponge.\n4\nIntroduction\n\nAs data piles up, we have ourselves a genuine gold rush. But data isn\u2019t the\ngold. I repeat, data in its raw form is boring crud. The gold is what\u2019s\ndiscovered therein.\nThe process of machines learning from data unleashes the power of this\nexploding resource. It uncovers what drives people and the actions they\ntake\u2014what makes us tick and how the world works. With the new\nknowledge gaine", "nes learning from data unleashes the power of this\nexploding resource. It uncovers what drives people and the actions they\ntake\u2014what makes us tick and how the world works. With the new\nknowledge gained, prediction is possible.\nThis learning process discovers insightful gems such as:1\n\u2022 Early retirement decreases your life expectancy.\n\u2022 Online daters more consistently rated as attractive receive less interest.\n\u2022 Rihanna fans are mostly political Democrats.\n\u2022 Vegetarians miss fewer \ufb02ights.\n\u2022 Local crime increases after public sporting events.\nMachine learning builds upon insights such as these in order to develop\npredictive capabilities, following a number-crunching, trial-and-error pro-\ncess that has its roots in statistics and computer science.\n1 See Chapter 3 for more details on these examples.\nIntroduction\n5\n\nI Knew You Were Going to Do That\nWith this power at hand, what do we want to predict? Every important thing\na person does is valuable to predict, namely: consume, think, work, q", "mples.\nIntroduction\n5\n\nI Knew You Were Going to Do That\nWith this power at hand, what do we want to predict? Every important thing\na person does is valuable to predict, namely: consume, think, work, quit, vote,\nlove, procreate, divorce, mess up, lie, cheat, steal, kill, and die. Let\u2019s explore some\nexamples.2\n2 For more examples and further detail, see this book\u2019s Central Tables.\nPEOPLE CONSUME\n\u2022 Hollywood studios predict the success of a screenplay if produced.\n\u2022 Net\ufb02ix awarded $1 million to a team of scientists who best improved\ntheir recommendation system\u2019s ability to predict which movies you\nwill like.\n\u2022 The Hopper app helps you get the best deal on a \ufb02ight by\nrecommending whether you should buy or wait, based on its\nprediction as to whether the airfare will change.\n\u2022 Australian energy company Energex predicts electricity demand in\norder to decide where to build out its power grid, and Con Edison\npredicts system failure in the face of high levels of consumption.\n\u2022 Wall Street \ufb01rms t", "company Energex predicts electricity demand in\norder to decide where to build out its power grid, and Con Edison\npredicts system failure in the face of high levels of consumption.\n\u2022 Wall Street \ufb01rms trade algorithmically, buying and selling based on\nthe prediction of stock prices.\n\u2022 Companies predict which customer will buy their products in order\nto target their marketing, from U.S. Bank down to small companies\nlike Harbor Sweets (candy) and Vermont Country Store (\u201ctop\nquality and hard-to-\ufb01nd classic products\u201d). These predictions dictate\nthe allocations of precious marketing budgets. Some companies\nliterally predict how to best in\ufb02uence you to buy more (the topic of\nChapter 7).\n\u2022 Prediction drives the coupons you get at the grocery cash register.\nU.K. grocery giant Tesco, the world\u2019s third-largest retailer, predicts\nwhich discounts will be redeemed in order to target more than\n6\nIntroduction\n\n100 million personalized coupons annually at cash registers across 13\ncountries. Similarly, K", "d-largest retailer, predicts\nwhich discounts will be redeemed in order to target more than\n6\nIntroduction\n\n100 million personalized coupons annually at cash registers across 13\ncountries. Similarly, Kmart, Kroger, Ralph\u2019s, Safeway, Stop & Shop,\nTarget, and Winn-Dixie follow in kind.\n\u2022 Predicting mouse clicks pays off massively. Since websites are often\npaid per click for the advertisements they display, they predict which\nad you\u2019re mostly likely to click in order to instantly choose which\none to show you. This, in effect, selects more relevant ads and drives\nmillions in newly found revenue.\n\u2022 Facebook predicts which of the thousands of posts by your friends\nwill interest you most every time you view the news feed (unless you\nchange the default setting). The social network also predicts the\nsuggested \u201cpeople you may know,\u201d not to mention which ads\nyou\u2019re likely to click.\nPEOPLE LOVE, WORK, PROCREATE, AND DIVORCE\n\u2022 The leading career-focused social network, LinkedIn, predicts your\njob sk", "\nsuggested \u201cpeople you may know,\u201d not to mention which ads\nyou\u2019re likely to click.\nPEOPLE LOVE, WORK, PROCREATE, AND DIVORCE\n\u2022 The leading career-focused social network, LinkedIn, predicts your\njob skills.\n\u2022 Online dating leaders Match.com, OkCupid, and eHarmony pre-\ndict which hottie on your screen would be the best bet at your side.\n\u2022 Target predicts customer pregnancy in order to market relevant\nproducts accordingly. Nothing foretells consumer need like pre-\ndicting the birth of a new consumer.\n\u2022 Clinical researchers predict in\ufb01delity and divorce. There\u2019s even a\nself-help website tool to put odds on your marriage\u2019s long-term\nsuccess (www.divorceprobability.com).\nPEOPLE THINK AND DECIDE\n\u2022 Obama was reelected in 2012 with the help of voter prediction. The\nObama for America campaign predicted which voters would be\npositively persuaded by campaign contact (a call, door knock, \ufb02ier,\nor TV ad), and which would actually be inadvertently in\ufb02uenced to\n(continued )\nIntroduction\n7\n\n(continued ", "ted which voters would be\npositively persuaded by campaign contact (a call, door knock, \ufb02ier,\nor TV ad), and which would actually be inadvertently in\ufb02uenced to\n(continued )\nIntroduction\n7\n\n(continued )\nvote adversely by contact. Employed to drive campaign decisions for\nmillions of swing state voters, this method was shown to successfully\nconvince more voters to choose Obama than traditional campaign\ntargeting. Hillary for America 2016 is positioning to apply the same\ntechnique.\n\u2022 \u201cWhat did you mean by that?\u201d Systems have learned to ascertain the\nintent behind the written word. Citibank and PayPal detect the\ncustomer sentiment about their products, and one researcher\u2019s\nmachine can tell which Amazon.com book reviews are sarcastic.\n\u2022 Student essay grade prediction has been developed for possible use to\nautomatically grade. The system grades as accurately as human\ngraders.\n\u2022 There\u2019s a machine that can participate in the same capacity as\nhumans in the United States\u2019 most popular broadcast c", "ble use to\nautomatically grade. The system grades as accurately as human\ngraders.\n\u2022 There\u2019s a machine that can participate in the same capacity as\nhumans in the United States\u2019 most popular broadcast celebration\nof human knowledge and cultural literacy. On the TV quiz show\nJeopardy!, IBM\u2019s Watson computer triumphed. This machine\nlearned to work pro\ufb01ciently enough with English to predict the\nanswers to free-form inquiries across an open range of topics and\ndefeat the two all-time human champs.\n\u2022 Computers can literally read your mind. Researchers trained systems\nto decode a scan of your brain and determine which type of object\nyou\u2019re thinking about\u2014such as certain tools, buildings, and food\u2014\nwith over 80 percent accuracy for some human subjects.\nPEOPLE QUIT\n\u2022 Hewlett-Packard (HP) earmarks each and every one of its more than\n300,000 worldwide employees according to \u201cFlight Risk,\u201d the\nexpected chance he or she will quit their job, so that managers\nmay intervene in advance where possible an", "ch and every one of its more than\n300,000 worldwide employees according to \u201cFlight Risk,\u201d the\nexpected chance he or she will quit their job, so that managers\nmay intervene in advance where possible and plan accordingly\notherwise.\n\u2022 Ever experience frustration with your cell phone service? Your\nservice provider endeavors to know. All major wireless carriers\n8\nIntroduction\n\npredict how likely it is you will cancel and switch to a competitor\u2014\npossibly before you have even conceived a plan to do so\u2014based on\nfactors such as dropped calls, your phone usage, billing information,\nand whether your contacts have already defected.\n\u2022 FedEx stays ahead of the game by predicting\u2014with 65 to 90 percent\naccuracy\u2014which customers are at risk of defecting to a competitor.\n\u2022 The American Public University System predicted student dropouts\nand used these predictions to intervene successfully; the University\nof Alabama, Arizona State University, Iowa State University, Okla-\nhoma State University, and the Net", "em predicted student dropouts\nand used these predictions to intervene successfully; the University\nof Alabama, Arizona State University, Iowa State University, Okla-\nhoma State University, and the Netherlands\u2019 Eindhoven University\nof Technology predict dropouts as well.\n\u2022 Wikipedia predicts which of its editors, who work for free as a labor\nof love to keep this priceless online asset alive, are going to\ndiscontinue their valuable service.\n\u2022 Researchers at Harvard Medical School predict that if your friends\nstop smoking, you\u2019re more likely to do so yourself as well. Quitting\nsmoking is contagious.\nPEOPLE MESS UP\n\u2022 Insurance companies predict who is going to crash a car or hurt\nthemselvesanotherway(suchasaskiaccident).Allstatepredictsbodily\ninjury liability from car crashes based on the characteristics of the\ninsuredvehicle,demonstratingimprovementstopredictionthatcould\nbe worth an estimated $40 million annually. Another top insurance\nprovider reported savings of almost $50 million per y", "acteristics of the\ninsuredvehicle,demonstratingimprovementstopredictionthatcould\nbe worth an estimated $40 million annually. Another top insurance\nprovider reported savings of almost $50 million per year by expanding\nits actuarial practices with advanced predictive techniques.\n\u2022 Ford is learning from data so its cars can detect when the driver is not\nalert due to distraction, fatigue, or intoxication and take action such\nas sounding an alarm.\n\u2022 Researchers have identi\ufb01ed aviation incidents that are \ufb01ve times\nmore likely than average to be fatal, using data from the National\nTransportation Safety Board.\n(continued )\nIntroduction\n9\n\n(continued )\n\u2022 All large banks and credit card companies predict which debtors are\nmost likely to turn delinquent, failing to pay back their loans or\ncredit card balances. Collection agencies prioritize their efforts with\npredictions of which tactic has the best chance to recoup the most\nfrom each defaulting debtor.\nPEOPLE GET SICK AND DIE\nI\u2019m not afraid of d", " balances. Collection agencies prioritize their efforts with\npredictions of which tactic has the best chance to recoup the most\nfrom each defaulting debtor.\nPEOPLE GET SICK AND DIE\nI\u2019m not afraid of death; I just don\u2019t want to be there when it happens.\n\u2014Woody Allen\n\u2022 In 2013, the Heritage Provider Network handed over $500,000 to a\nteam of scientists who won an analytics competition to best predict\nindividual hospital admissions. By following these predictions,\nproactive preventive measures can take a healthier bite out of the\ntens of billions of dollars spent annually on unnecessary hospitaliza-\ntions. Similarly, the University of Pittsburgh Medical Center pre-\ndicts short-term hospital readmissions, so doctors can be prompted\nto think twice before a hasty discharge.\n\u2022 At Stanford University, a machine learned to diagnose breast cancer\nbetter than human doctors by discovering an innovative method that\nconsiders a greater number of factors in a tissue sample.\n\u2022 Researchers at Brigham Yo", "ty, a machine learned to diagnose breast cancer\nbetter than human doctors by discovering an innovative method that\nconsiders a greater number of factors in a tissue sample.\n\u2022 Researchers at Brigham Young University and the University of\nUtah correctly predict about 80 percent of premature births (and\nabout 80 percent of full-term births), based on peptide biomarkers,\nas found in a blood exam as early as week 24 of pregnancy.\n\u2022 University researchers derived a method to detect patient schizo-\nphrenia from transcripts of their spoken words alone.\n\u2022 A growing number of life insurance companies go beyond conven-\ntional actuarial tables and employ predictive technology to establish\nmortality risk. It\u2019s not called death insurance, but they calculate when\nyou are going to die.\n10\nIntroduction\n\n\u2022 Beyond life insurance, one top-\ufb01ve health insurance company\npredicts the probability that elderly insurance policyholders will\npass away within 18 months, based on clinical markers in the\ninsured\u2019s re", "Beyond life insurance, one top-\ufb01ve health insurance company\npredicts the probability that elderly insurance policyholders will\npass away within 18 months, based on clinical markers in the\ninsured\u2019s recent medical claims. Fear not\u2014it\u2019s actually done for\nbenevolent purposes.\n\u2022 Researchers predict your risk of death in surgery based on aspects of\nyou and your condition to help inform medical decisions.\n\u2022 By following one common practice, doctors regularly\u2014yet unin-\ntentionally\u2014sacri\ufb01ce some patients in order to save others, and this\nis done completely without controversy. But this would be lessened\nby predicting something besides diagnosis or outcome: healthcare\nimpact (impact prediction is the topic of Chapter 7).\nPEOPLE LIE, CHEAT, STEAL, AND KILL\n\u2022 Most medium-size and large banks employ predictive technology to\ncounter the ever-blooming assault of fraudulent checks, credit card\ncharges, and other transactions. Citizens Bank developed the capac-\nity to decrease losses resulting from ch", "redictive technology to\ncounter the ever-blooming assault of fraudulent checks, credit card\ncharges, and other transactions. Citizens Bank developed the capac-\nity to decrease losses resulting from check fraud by 20 percent.\nHewlett-Packard saved $66 million by detecting fraudulent war-\nranty claims.\n\u2022 Predictive computers help decide who belongs in prison. To assist\nwith parole and sentencing decisions, of\ufb01cials in states such as\nOregon and Pennsylvania consult prognostic machines that assess\nthe risk a convict will offend again.\n\u2022 Murder is widely considered impossible to predict with meaningful\naccuracy in general, but within at-risk populations predictive meth-\nods can be effective. Maryland analytically generates predictions as to\nwhich inmates will kill or be killed. University and law enforcement\nresearchers have developed predictive systems that foretell murder\namong those previously convicted for homicide.\n\u2022 One fraud expert at a large bank in the United Kingdom extended\nhis w", "aw enforcement\nresearchers have developed predictive systems that foretell murder\namong those previously convicted for homicide.\n\u2022 One fraud expert at a large bank in the United Kingdom extended\nhis work to discover a small pool of terror suspects based on their\n(continued )\nIntroduction\n11\n\nThe Limits and Potential of Prediction\nAn economist is an expert who will know tomorrow why the things he predicted\nyesterday didn\u2019t happen.\n\u2014Earl Wilson\nHow come you never see a headline like \u201cPsychic Wins Lottery\u201d?\n\u2014Jay Leno\nEach of the preceding accomplishments is powered by prediction, which is\nin turn a product of machine learning. A striking difference exists between\nthese varied capabilities and science \ufb01ction: They aren\u2019t \ufb01ction. At this point,\nI predict that you won\u2019t be surprised to hear that those examples represent\n(continued )\nbanking activities. While few details have been disclosed publicly,\nit\u2019s clear that the National Security Agency also considers this type of\nanalysis a strategic", " those examples represent\n(continued )\nbanking activities. While few details have been disclosed publicly,\nit\u2019s clear that the National Security Agency also considers this type of\nanalysis a strategic priority in order to automatically discover previ-\nously unknown potential suspects.\n\u2022 Police patrol the areas predicted to spring up as crime hot spots in\ncities such as Chicago, Memphis, and Richmond, Va.\n\u2022 Inspired by the TV crime drama Lie to Me about a microexpression\nreader, researchers at the University at Buffalo trained a system to\ndetect lies with 82 percent accuracy by observing eye movements\nalone.\n\u2022 As a professor at Columbia University in the late 1990s, I had a team\nof teaching assistants who employed cheating-detection software to\npatrol hundreds of computer programming homework submissions\nfor plagiarism.\n\u2022 The IRS predicts if you are cheating on your taxes.\n12\nIntroduction\n\nonly a small sample. You can safely predict that the power of prediction is\nhere to stay.\nBut are ", "rk submissions\nfor plagiarism.\n\u2022 The IRS predicts if you are cheating on your taxes.\n12\nIntroduction\n\nonly a small sample. You can safely predict that the power of prediction is\nhere to stay.\nBut are these claims too bold? As the Danish physicist Niels Bohr put it,\n\u201cPrediction is very dif\ufb01cult, especially if it\u2019s about the future.\u201d After all, isn\u2019t\nprediction basically impossible? The future is unknown, and uncertainty is\nthe only thing about which we\u2019re certain.\nLet me be perfectly clear. It\u2019s fuzzy. Accurate prediction is generally not\npossible. The weather is predicted with only about 50 percent accuracy, and\nit doesn\u2019t get easier predicting the behavior of humans, be they patients,\ncustomers, or criminals.\nGood news! Predictions need not be accurate to score big value. For\ninstance, one of the most straightforward commercial applications of\nIntroduction\n13\n\npredictive technology is deciding whom to target when a company sends\ndirect mail. If the learning process identi\ufb01es a careful", "e of the most straightforward commercial applications of\nIntroduction\n13\n\npredictive technology is deciding whom to target when a company sends\ndirect mail. If the learning process identi\ufb01es a carefully de\ufb01ned group of\ncustomers who are predicted to be, say, three times more likely than average\nto respond positively to the mail, the company pro\ufb01ts big-time by preemp-\ntively removing likely nonresponders from the mailing list. And those non-\nresponders in turn bene\ufb01t, contending with less junk mail.\nPrediction\u2014A person who sees a sales brochure today buys a\nproduct tomorrow.\nIn this way the business, already playing a sort of numbers game by\nconducting mass marketing in the \ufb01rst place, tips the balance delicately\nyet signi\ufb01cantly in its favor\u2014and does so without highly accurate predic-\ntions. In fact, its utility withstands quite poor accuracy. If the overall\nmarketing response is at 1 percent, the so-called hot pocket with three\ntimes as many would-be responders is at 3 percent. So, in", "s. In fact, its utility withstands quite poor accuracy. If the overall\nmarketing response is at 1 percent, the so-called hot pocket with three\ntimes as many would-be responders is at 3 percent. So, in this case, we can\u2019t\ncon\ufb01dently predict the response of any one particular customer. Rather, the\nvalue is derived from identifying a group of people who\u2014in aggregate\u2014will\ntend to behave in a certain way.\nThis demonstrates in a nutshell what I call The Prediction Effect.\nPredicting better than pure guesswork, even if not accurately, delivers\nreal value. A hazy view of what\u2019s to come outperforms complete darkness\nby a landslide.\nThe Prediction Effect: A little prediction goes a long way.\n14\nIntroduction\n\nThis is the \ufb01rst of \ufb01ve Effects introduced in this book. You may have\nheard of the butter\ufb02y, Doppler, and placebo effects. Stay tuned here for\nthe Data, Induction, Ensemble, and Persuasion Effects. Each of these Effects\nencompasses the fun part of science and technology: an intuitive hook th", " Doppler, and placebo effects. Stay tuned here for\nthe Data, Induction, Ensemble, and Persuasion Effects. Each of these Effects\nencompasses the fun part of science and technology: an intuitive hook that\nreveals how it works and why it succeeds.\nThe Field of Dreams\nPeople . . . operate with beliefs and biases. To the extent you can eliminate both and\nreplace them with data, you gain a clear advantage.\n\u2014Michael Lewis, Moneyball: The Art of Winning an Unfair Game\nWhat \ufb01eld of study or branch of science are we talking about here? Learning\nhow to predict from data is sometimes called machine learning\u2014but it turns\nout this is mostly an academic term you \ufb01nd used within research labs,\nconference papers, and university courses (full disclosure: I taught the\nMachine Learning graduate course at Columbia University a couple of\ntimes in the late 1990s). These arenas are a priceless wellspring, but they\naren\u2019t where the rubber hits the road. In commercial, industrial, and\ngovernment applications\u2014in", "mbia University a couple of\ntimes in the late 1990s). These arenas are a priceless wellspring, but they\naren\u2019t where the rubber hits the road. In commercial, industrial, and\ngovernment applications\u2014in the real-world usage of machine learning to\npredict\u2014it\u2019s called something else, something that in fact is the very topic of\nthis book:\nPredictive analytics (PA)\u2014Technology that learns from experience (data) to predict\nthe future behavior of individuals in order to drive better decisions.3\n3 In this de\ufb01nition, individuals is a broad term that can refer to people as well as other\norganizational elements. Most examples in this book involve predicting people, such\nas customers, debtors, applicants, employees, students, patients, donors, voters,\ntaxpayers, potential suspects, and convicts. However, PA also applies to individual\ncompanies (e.g., for business-to-business), products, locations, restaurants, vehicles,\nships, \ufb02ights, deliveries, buildings, manholes, transactions, Facebook posts, mo", ", PA also applies to individual\ncompanies (e.g., for business-to-business), products, locations, restaurants, vehicles,\nships, \ufb02ights, deliveries, buildings, manholes, transactions, Facebook posts, movies,\nsatellites, stocks, Jeopardy! questions, and much more. Whatever the domain, PA\nrenders predictions over scalable numbers of individuals.\nIntroduction\n15\n\nBuilt upon computer science and statistics and bolstered by devoted\nconferences and university degree programs, PA has emerged as its own\ndiscipline. But beyond a \ufb01eld of science, PA is a movement that exerts a\nforceful impact. Millions of decisions a day determine whom to call, mail,\napprove, test, diagnose, warn, investigate, incarcerate, set up on a date, and\nmedicate. PA is the means to drive per-person decisions empirically, as guided\nby data. By answering this mountain of smaller questions, PA may in fact\nanswer the biggest question of all: How can we improve the effectiveness of all these\nmassive functions across government,", "ided\nby data. By answering this mountain of smaller questions, PA may in fact\nanswer the biggest question of all: How can we improve the effectiveness of all these\nmassive functions across government, healthcare, business, nonpro\ufb01t, and law enforce-\nment work?\nPredictions drive how organizations treat and serve an individual,\nacross the frontline operations that de\ufb01ne a functional society.\nIn this way, PA is a completely different animal from forecasting. Forecasting\nmakes aggregate predictions on a macroscopic level. How will the economy\nfare? Which presidential candidate will win more votes in Ohio? Whereas\nforecasting estimates the total number of ice cream cones to be purchased\nnext month in Nebraska, PA tells you which individual Nebraskans are most\nlikely to be seen with cone in hand.\nPA leads within the growing trend to make decisions more \u201cdata\ndriven,\u201d relying less on one\u2019s \u201cgut\u201d and more on hard, empirical evidence.\nEnter this fact-based domain and you\u2019ll be attacked by buzzw", ".\nPA leads within the growing trend to make decisions more \u201cdata\ndriven,\u201d relying less on one\u2019s \u201cgut\u201d and more on hard, empirical evidence.\nEnter this fact-based domain and you\u2019ll be attacked by buzzwords, includ-\ning analytics, big data, data science, and business intelligence. While PA \ufb01ts\n16\nIntroduction\n\nunderneath each of these umbrellas, these evocative terms refer more to the\nculture and general skill sets of technologists who do an assortment of\ncreative, innovative things with data, rather than alluding to any speci\ufb01c\ntechnology or method. These areas are broad; in some cases, they refer\nsimply to standard Excel reports\u2014that is, to things that are important and\nrequire a great deal of craft, but may not rely on science or sophisticated\nmath. And so they are more subjectively de\ufb01ned. As Mike Loukides, a vice\npresident at the innovation publisher O\u2019Reilly, once put it, \u201cData science is\nlike porn\u2014you know it when you see it.\u201d Another term, data mining, is\noften used as a synonym ", "d. As Mike Loukides, a vice\npresident at the innovation publisher O\u2019Reilly, once put it, \u201cData science is\nlike porn\u2014you know it when you see it.\u201d Another term, data mining, is\noften used as a synonym for PA, but as an evocative metaphor depicting\n\u201cdigging around\u201d through data in one fashion or another, it is often used\nmore broadly as well.\nOrganizational Learning\nThe powerhouse organizations of the Internet era, which include Google and\nAmazon . . . have business models that hinge on predictive models based on machine\nlearning.\n\u2014Professor Vasant Dhar, Stern School of Business,\nNew York University\nA breakthrough in machine learning would be worth 10 Microsofts.\n\u2014Bill Gates\nAn organization is sort of a \u201cmegaperson,\u201d so shouldn\u2019t it \u201cmegalearn\u201d? A\ngroup comes together for the collective bene\ufb01t of its members and those it\nserves, be it a company, government, hospital, university, or charity. Once\nformed, it gains from division of labor, mutually complementary skills, and\nthe ef\ufb01ciency of ", "\ufb01t of its members and those it\nserves, be it a company, government, hospital, university, or charity. Once\nformed, it gains from division of labor, mutually complementary skills, and\nthe ef\ufb01ciency of mass production. The result is more powerful than the sum\nof its parts. Collective learning is the organization\u2019s next logical step to\nfurther leverage this power. Just as a salesperson learns over time from her\npositive and negative interactions with sales leads, her successes, and failures,\nPA is the process by which an organization learns from the experience it has\nIntroduction\n17\n\ncollectively gained across its team members and computer systems. In fact, an\norganization that doesn\u2019t leverage its data in this way is like a person with a\nphotographic memory who never bothers to think.\nWith only a few striking exceptions, we \ufb01nd that organizations, rather\nthan individuals, bene\ufb01t by employing PA. Organizations make the many,\nmany operational decisions for which there\u2019s ample room for impr", "nly a few striking exceptions, we \ufb01nd that organizations, rather\nthan individuals, bene\ufb01t by employing PA. Organizations make the many,\nmany operational decisions for which there\u2019s ample room for improve-\nment; organizations are intrinsically inef\ufb01cient and wasteful on a grand\nscale. Marketing casts a wide net\u2014junk mail is marketing money wasted\nand trees felled to print unread brochures. An estimated 80 percent of all\ne-mail is spam. Risky debtors are given too much credit. Applications for\ngovernment bene\ufb01ts are backlogged and delayed. And it\u2019s organizations\nthat have the data to power the predictions that drive improvements in\nthese operations.\nIn the commercial sector, pro\ufb01t is a driving force. You can well imagine\nthe booming incentives intrinsic to rendering everyday routines more\nef\ufb01cient, marketing more precisely, catching more fraud, avoiding bad\ndebtors, and luring more online customers. Upgrading how business is\ndone, PA rocks the enterprise\u2019s economies of scale, optimizing ", "\ufb01cient, marketing more precisely, catching more fraud, avoiding bad\ndebtors, and luring more online customers. Upgrading how business is\ndone, PA rocks the enterprise\u2019s economies of scale, optimizing operations\nright where it makes the biggest difference.\nThe New Super Geek: Data Scientists\nThe alternative [to thinking ahead] would be to think backwards . . . and that\u2019s just\nremembering.\n\u2014Sheldon, the theoretical physicist on The Big Bang Theory\nOpportunities abound, but the pro\ufb01t incentive is not the only driving\nforce. The source, the energy that makes it work, is Geek Power! I speak of\nthe enthusiasm of technical practitioners. Truth be told, my passion for PA\ndidn\u2019t originate from its value to organizations. I am in it for the fun. The\nidea of a machine that can actually learn seems so cool to me that I care\nmore about what happens inside the magic box than its outer usefulness.\n18\nIntroduction\n\nIndeed, perhaps that\u2019s the de\ufb01ning motivator that quali\ufb01es one as a geek.\nWe love the t", " cool to me that I care\nmore about what happens inside the magic box than its outer usefulness.\n18\nIntroduction\n\nIndeed, perhaps that\u2019s the de\ufb01ning motivator that quali\ufb01es one as a geek.\nWe love the technology; we\u2019re in awe of it. Case in point: The leading free,\nopen-source software tool for PA, called R (a one-letter, geeky name), has\na rapidly expanding base of users as well as enthusiastic volunteer devel-\nopers who add to and support its functionalities. Great numbers of\nprofessionals and amateurs alike \ufb02ock to public PA competitions with a\ntremendous spirit of \u201ccoopetition.\u201d We operate within organizations, or\nconsult across them. We\u2019re in demand, so we \ufb02y a lot. But we \ufb02y coach, at\nbest Economy Plus.\nThe Art of Learning\nWhatcha gonna do with your CPU to reach its potentiality?\nUse your noggin when you log in to crank it exponentially.\nThe endeavor that will render my obtuse computer clever:\nSelf-improve impeccably by way of trial and error.\nOnce upon a time, humanity created The", "our noggin when you log in to crank it exponentially.\nThe endeavor that will render my obtuse computer clever:\nSelf-improve impeccably by way of trial and error.\nOnce upon a time, humanity created The Ultimate General Purpose\nMachine and, in an inexplicable \ufb01t of understatement, decided to call it \u201ca\ncomputer\u201d (a word that until this time had simply meant a person who did\ncomputations by hand). This automaton could crank through any\ndemanding, detailed set of endless instructions without fail or error and\nwith nary a complaint; within just a few decades, its speed became so\nblazingly brisk that humanity could only exclaim, \u201cGosh, we really\ncranked that!\u201d An obviously much better name for this device would\nhave been the appropriately grand La Machine, but a few decades later this\nname was hyperbolically bestowed upon a food processor (I am not\njoking). Quel dommage. \u201cWhat should we do with the computer? What\u2019s\nits true potential, and how do we achieve it?\u201d humanity asked of itself in\nwo", "yperbolically bestowed upon a food processor (I am not\njoking). Quel dommage. \u201cWhat should we do with the computer? What\u2019s\nits true potential, and how do we achieve it?\u201d humanity asked of itself in\nwonderment.\nA computer and your brain have something in common that renders them\nboth mysterious, yet at the same time easy to take for granted. If while\nIntroduction\n19\n\npondering what this might be you heard a pin drop, you have your answer.\nThey are both silent. Their mechanics make no sound. Sure, a computer may\nhave a disk drive or cooling fan that stirs\u2014just as one\u2019s noggin may emit\nwheezes, sneezes, and snores\u2014but the mammoth grunt work that takes place\ntherein involves no \u201cmoving parts,\u201d so these noiseless efforts go along\ncompletely unwitnessed. The smooth delivery of content on your\nscreen\u2014and ideas in your mind\u2014can seem miraculous.4\nThey\u2019re both powerful as heck, your brain and your computer. So could\ncomputers be successfully programmed to think, feel, or become truly\nintelligent", "reen\u2014and ideas in your mind\u2014can seem miraculous.4\nThey\u2019re both powerful as heck, your brain and your computer. So could\ncomputers be successfully programmed to think, feel, or become truly\nintelligent? Who knows? At best these are stimulating philosophical ques-\ntions that are dif\ufb01cult to answer, and at worst they are subjective benchmarks\nfor which success could never be conclusively established. But thankfully we\ndo have some clarity: There is one truly impressive, profound human\nendeavor computers can undertake. They can learn.\nBut how? It turns out that learning\u2014generalizing from a list of examples,\nbe it a long list or a short one\u2014is more than just challenging. It\u2019s a\nphilosophically deep dilemma. Machine learning\u2019s task is to \ufb01nd patterns\nthat appear not only in the data at hand, but in general, so that what is learned\nwill hold true in new situations never yet encountered. At the core, this\nability to generalize is the magic bullet of PA. There is a true art in the design\nof the", " in general, so that what is learned\nwill hold true in new situations never yet encountered. At the core, this\nability to generalize is the magic bullet of PA. There is a true art in the design\nof these computer methods. We\u2019ll explore more later, but for now I\u2019ll give\nyou a hint. The machine actually learns more about your next likely action\nby studying others than by studying you.\nWhile I\u2019m dispensing teasers that leave you hanging, here\u2019s one more.\nThis book\u2019s \ufb01nal chapter answers the riddle: What often happens to you that\n4 Silence is characteristic to solid state electronics, but computers didn\u2019t have to be\nbuilt that way. The idea of a general-purpose, instruction-following machine is\nabstract, not af\ufb01xed to the notion of electricity. You could construct a computer of\ncogs and wheels and levers, powered by steam or gasoline. I mean, I wouldn\u2019t\nrecommend it, but you could. It would be slow, big, and loud, and nobody would\nbuy it.\n20\nIntroduction\n\ncannot be witnessed, and that you c", "and levers, powered by steam or gasoline. I mean, I wouldn\u2019t\nrecommend it, but you could. It would be slow, big, and loud, and nobody would\nbuy it.\n20\nIntroduction\n\ncannot be witnessed, and that you can\u2019t even be sure has happened afterward\u2014but that\ncan be predicted in advance?\nLearning from data to predict is only the \ufb01rst step. To take the next step\nand act on predictions is to fearlessly gamble. Let\u2019s kick off Chapter 1 with a\nsuspenseful story that shows why launching PA feels like blasting off in a\nrocket.\nIntroduction\n21\n\n\nCHAPTER 1\nLiftoff! Prediction Takes Action\nHow much guts does it take to deploy a predictive model into \ufb01eld operation, and what\ndo you stand to gain? What happens when a man invests his entire life savings into his\nown predictive stock market trading system? Launching predictive analytics means to act\non its predictions, applying what\u2019s been learned, what\u2019s been discovered within data.\nIt\u2019s a leap many take\u2014you can\u2019t win if you don\u2019t play.\nIn the mid-1990s, an", "hing predictive analytics means to act\non its predictions, applying what\u2019s been learned, what\u2019s been discovered within data.\nIt\u2019s a leap many take\u2014you can\u2019t win if you don\u2019t play.\nIn the mid-1990s, an ambitious postdoc researcher couldn\u2019t stand to wait any\nlonger.Afterconsultingwithhiswife,heloadedtheirentirelifesavingsintoastock\nmarket prediction system of his own design\u2014a contraption he had developed\nmoonlighting on the side. Like Dr. Henry Jekyll imbibing his own untested\npotion in the moonlight, the young Dr. John Elder un\ufb02inchingly pressed \u201cgo.\u201d\nThere is a scary moment every time new technology is launched. A\nspaceship lifting off may be the quintessential portrait of technological greatness\nand national prestige, but the image leaves out a small group of spouses terri\ufb01ed\nto the very point of psychological trauma. Astronauts are in essence stunt pilots,\nvoluntarily strapping themselves in to serve as guinea pigs for a giant experi-\nment, willing to sacri\ufb01ce themselves in order to ", "point of psychological trauma. Astronauts are in essence stunt pilots,\nvoluntarily strapping themselves in to serve as guinea pigs for a giant experi-\nment, willing to sacri\ufb01ce themselves in order to be part of history.\nFrom grand challenges are born great achievements. We\u2019ve taken strolls\non our moon, and in more recent years a $10 million Grand Challenge prize\nwas awarded to the \ufb01rst nongovernmental organization to develop a reusable\nmanned spacecraft. Driverless cars have been unleashed\u2014\u201cLook, Ma, no\nhands!\u201d Fueled as well by millions of dollars in prize money, they navigate\nautonomously around the campuses of Google and BMW.\nReplace the roar of rockets with the crunch of data, and the ambitions\nare no less far-reaching, \u201cboldly going\u201d not to space but to a new \ufb01nal\n23\n\nfrontier: predicting the future. This frontier is just as exciting to explore,\nyet less dangerous and uncomfortable (outer space is a vacuum, and\nvacuums totally suck). Millions in grand challenge prize money go towa", "ting the future. This frontier is just as exciting to explore,\nyet less dangerous and uncomfortable (outer space is a vacuum, and\nvacuums totally suck). Millions in grand challenge prize money go toward\naverting the unnecessary hospitalization of each patient and predicting the\nidiosyncratic preferences of each individual consumer. The TV quiz show\nJeopardy! awarded $1.5 million in prize money for a face-off between man\nand machine that demonstrated dramatic progress in predicting the\nanswers to questions (IBM invested a lot more than that to achieve\nthis win, as detailed in Chapter 6). Organizations are literally keeping kids\nin school, keeping the lights on, and keeping crime down with predictive\nanalytics (PA). And success is its own reward when analytics wins a\npolitical election, a baseball championship, or . . . did I mention manag-\ning a \ufb01nancial portfolio?\nBlack-box trading\u2014driving \ufb01nancial trading decisions automatically with a\nmachine\u2014is the holy grail of data-driven decision", "seball championship, or . . . did I mention manag-\ning a \ufb01nancial portfolio?\nBlack-box trading\u2014driving \ufb01nancial trading decisions automatically with a\nmachine\u2014is the holy grail of data-driven decision making. It\u2019s a black box\ninto which current \ufb01nancial environmental conditions are fed, with buy/\nhold/sell decisions spit out the other end. It\u2019s black (i.e., opaque) because you\ndon\u2019t care what\u2019s on the inside, as long as it makes good decisions. When\nworking, it trumps any other conceivable business proposal in the world:\nYour computer is now a box that turns electricity into money.\nAnd so with the launch of his stock trading system, John Elder took on his\nown personal grand challenge. Even if stock market prediction would\nrepresent a giant leap for mankind, this was no small step for John himself.\nIt\u2019s an occasion worthy of mixing metaphors. By putting all his eggs into one\nanalytical basket, John was taking a healthy dose of his own medicine.\nBefore continuing with the story of John\u2019s", "himself.\nIt\u2019s an occasion worthy of mixing metaphors. By putting all his eggs into one\nanalytical basket, John was taking a healthy dose of his own medicine.\nBefore continuing with the story of John\u2019s blast-off, let\u2019s establish how\nlaunching a predictive system works, not only for black-box trading but\nacross a multitude of applications.\nGoing Live\nLearning from data is virtually universally useful. Master it and you\u2019ll be welcomed\nnearly everywhere!\n\u2014John Elder\n24\nPredictive Analytics\n\nNew groundbreaking stories of PA in action are pouring in. A few key\ningredients have opened these \ufb02oodgates:\n\u2022 wildly increasing loads of data;\n\u2022 cultural shifts as organizations learn to appreciate, embrace, and inte-\ngrate predictive technology;\n\u2022 improved software solutions to deliver PA to organizations.\nBut this \ufb02ood built up its potential in the \ufb01rst place simply because\npredictive technology boasts an inherent generality\u2014there are just so\nmany conceivable ways to make use of it. Want to come up ", "s.\nBut this \ufb02ood built up its potential in the \ufb01rst place simply because\npredictive technology boasts an inherent generality\u2014there are just so\nmany conceivable ways to make use of it. Want to come up with your own\nnew innovative use for PA? You need only two ingredients.\nEACH APPLICATION OF PA IS DEFINED BY:\n1. What\u2019s predicted: the kind of behavior (i.e., action, event, or\nhappening) to predict for each individual, stock, or other kind of\nelement.\n2. What\u2019s done about it: the decisions driven by prediction; the action\ntaken by the organization in response to or informed by each\nprediction.\nGiven its open-ended nature, the list of application areas is so broad and\nthe list of example stories is so long that it presents a minor\ndata-management challenge in and of itself! So I placed this big list\n(182 examples total) into nine tables in the center of this book. Take a \ufb02ip\nthrough to get a feel for just how much is going on. That\u2019s the sexy\npart\u2014it\u2019s the \u201ccenterfold\u201d of this book. The Ce", "g list\n(182 examples total) into nine tables in the center of this book. Take a \ufb02ip\nthrough to get a feel for just how much is going on. That\u2019s the sexy\npart\u2014it\u2019s the \u201ccenterfold\u201d of this book. The Central Tables divulge cases\nof predicting: stock prices, risk, delinquencies, accidents, sales, donations,\nclicks, cancellations, health problems, hospital admissions, fraud, tax\nevasion, crime, malfunctions, oil \ufb02ow, electricity outages, approvals\nfor government bene\ufb01ts, thoughts, intention, answers, opinions, lies,\ngrades, dropouts, friendship, romance, pregnancy, divorce, jobs, quit-\nting, wins, votes, and more. The application areas are growing at a\nbreakneck pace.\nLiftoff! Prediction Takes Action\n25\n\nWithin this long list, the quintessential application for business is the one\ncovered in the Introduction for mass marketing:\nPA APPLICATION: TARGETING DIRECT MARKETING\n1. What\u2019s predicted: Which customers will respond to marketing\ncontact.\n2. What\u2019s done about it: Contact customers more l", "he Introduction for mass marketing:\nPA APPLICATION: TARGETING DIRECT MARKETING\n1. What\u2019s predicted: Which customers will respond to marketing\ncontact.\n2. What\u2019s done about it: Contact customers more likely to respond.\nAs we saw, this use of PA illustrates The Prediction Effect.\nLet\u2019s take a moment to see how straightforward it is to calculate the\nsheer value resulting from The Prediction Effect. Imagine you have a\ncompany with a mailing list of a million prospects. It costs $2 to mail to\neach one, and you have observed that one out of 100 of them will buy\nyour product (i.e., 10,000 responses). You take your chances and mail to\nthe entire list.\nIf you pro\ufb01t $220 for each rare positive response, then you pocket:\nOverall profit \u0088 Revenue \" Cost\n\u0088 \u0085$220 \u0002 10; 000 responses\u0086 \" \u0085$2 \u0002 1 million\u0086\nWhip out your calculator\u2014that\u2019s $200,000 pro\ufb01t. Are you happy yet? I\ndidn\u2019t think so.\nIf you are new to the arena of direct marketing (welcome!), you\u2019ll notice\nwe\u2019re playing a kind of wild numbers gam", "Whip out your calculator\u2014that\u2019s $200,000 pro\ufb01t. Are you happy yet? I\ndidn\u2019t think so.\nIf you are new to the arena of direct marketing (welcome!), you\u2019ll notice\nwe\u2019re playing a kind of wild numbers game, amassing great waste, like one\nmillion monkeys chucking darts across a chasm in the general direction of a\ndartboard. As turn-of-the-century marketing pioneer John Wanamaker\nfamously put it, \u201cHalf the money I spend on advertising is wasted; the\ntrouble is I don\u2019t know which half.\u201d The bad news is that it\u2019s actually more\nthan half; the good news is that PA can learn to do better.\nThe Prediction Effect: A little prediction goes a long way.\n26\nPredictive Analytics\n\nA Faulty Oracle Everyone Loves\nThe \ufb01rst step toward predicting the future is admitting you can\u2019t.\n\u2014Stephen Dubner, Freakonomics Radio, March 30, 2011\nThe \u201cprediction paradox\u201d: The more humility we have about our ability to make\npredictions, the more successful we can be in planning for the future.\n\u2014Nate Silver, The Signal and th", "dio, March 30, 2011\nThe \u201cprediction paradox\u201d: The more humility we have about our ability to make\npredictions, the more successful we can be in planning for the future.\n\u2014Nate Silver, The Signal and the Noise: Why So Many\nPredictions Fail\u2014but Some Don\u2019t\nYour resident \u201coracle,\u201d PA, tells you which customers are most likely to\nrespond. It earmarks a quarter of the entire list and says, \u201cThese folks are\nthree times more likely to respond than average!\u201d So now you have a\nshort list of 250,000 customers of whom 3 percent will respond\u20147,500\nresponses.\nOracle, shmoracle! These predictions are seriously inaccurate\u2014we still\ndon\u2019t have strong con\ufb01dence when contacting any one customer, given this\nmeasly 3 percent response rate. However, the overall IQ of your dart-\nthrowing monkeys has taken a real boost. If you send mail to only this short\nlist then you pro\ufb01t:\nOverall profit \u0088 Revenue \" Cost\n\u0088 \u0085$220 \u0002 7; 500 responses\u0086 \" \u0085$2 \u0002 250; 000\u0086\nThat\u2019s $1,150,000 pro\ufb01t. You just improved your pro\ufb01t 5.75 ", "oost. If you send mail to only this short\nlist then you pro\ufb01t:\nOverall profit \u0088 Revenue \" Cost\n\u0088 \u0085$220 \u0002 7; 500 responses\u0086 \" \u0085$2 \u0002 250; 000\u0086\nThat\u2019s $1,150,000 pro\ufb01t. You just improved your pro\ufb01t 5.75 times over by\nmailing to fewer people (and, in so doing, expending fewer trees). In\nparticular, you predicted who wasn\u2019t worth contacting and simply left\nthem alone. Thus you cut your costs by three-quarters in exchange for\nlosing only one-quarter of sales. That\u2019s a deal I\u2019d take any day.\nIt\u2019s not hard to put a value on prediction. As you can see, even if\npredictions themselves are generated from sophisticated mathematics, it\ntakes only simple arithmetic to roll up the plethora of predictions\u2014some\naccurate, and others not so much\u2014and reveal the aggregate bottom-line\neffect. This isn\u2019t just some abstract notion; The Prediction Effect means\nbusiness.\nLiftoff! Prediction Takes Action\n27\n\nPredictive Protection\nThus, value has emerged from just a little predictive insight, a small prognostic\nnu", "e abstract notion; The Prediction Effect means\nbusiness.\nLiftoff! Prediction Takes Action\n27\n\nPredictive Protection\nThus, value has emerged from just a little predictive insight, a small prognostic\nnudge in the right direction. It\u2019s easy to draw an analogy to science \ufb01ction,\nwhere just a bit of supernatural foresight can go a long way. Nicolas Cage kicks\nsome serious bad-guy butt in the movie Next, based on a story by Philip K.\nDick. His weapon? Pure prognostication. He can see the future, but only two\nminutes ahead. It\u2019s enough prescience to do some damage. An unarmed\ncivilian with a soft heart and the best of intentions, he winds up marching\nthrough something of a war zone, surrounded by a posse of heavily armed\nFBI agents who obey his every gesture. He sees the damage of every booby\ntrap, sniper, and mean-faced grunt before it happens and so can command just\nthe right moves for this Superhuman Risk-Aversion Team, avoiding one\ncalamity after another.\nIn a way, deploying PA makes a Su", "p, sniper, and mean-faced grunt before it happens and so can command just\nthe right moves for this Superhuman Risk-Aversion Team, avoiding one\ncalamity after another.\nIn a way, deploying PA makes a Superhuman Risk-Aversion Team of the\norganization just the same. Every decision an organization makes, each step it\ntakes, incurs risk. Imagine the protective bene\ufb01t of foreseeing each pitfall so\nthat it may be avoided\u2014each criminal act, stock value decline, hospitaliza-\ntion, bad debt, traf\ufb01c jam, high school dropout . . . and each ignored\nmarketing brochure that was a waste to mail. Organizational risk management,\ntraditionally the act of defending against singular, macrolevel incidents like\nthe crash of an aircraft or an economy, now broadens to \ufb01ght a myriad of\nmicrolevel risks.\nHey, it\u2019s not all bad news. We win by foreseeing good behavior as well,\nsince it often signals an opportunity to gain. The name of the game is\n\u201cPredict \u2019n\u2019 Pounce\u201d when it pops up on the radar that a customer is ", "t all bad news. We win by foreseeing good behavior as well,\nsince it often signals an opportunity to gain. The name of the game is\n\u201cPredict \u2019n\u2019 Pounce\u201d when it pops up on the radar that a customer is likely to\nbuy, a stock value is likely to increase, a voter is likely to swing, or the apple\nof one\u2019s online dating eye is likely to reciprocate.\nA little glimpse into the future gives you power because it gives you\noptions. In some cases the obvious decision is to act in order to avert what\nmay not be inevitable, be it crime, loss, or sickness. On the positive side, in\nthe case of foreseeing demand, you act to exploit it. Either way, prediction\nserves to drive decisions.\nLet\u2019s turn to a real case, a $1 million example.\n28\nPredictive Analytics\n\nA Silent Revolution Worth a Million\nWhen an organization goes live with PA, it unleashes a massive army, but it\u2019s\nan army of ants. These ants march out to the front lines of an organization\u2019s\noperations, the places where there\u2019s contact with the lik", "ization goes live with PA, it unleashes a massive army, but it\u2019s\nan army of ants. These ants march out to the front lines of an organization\u2019s\noperations, the places where there\u2019s contact with the likes of customers,\nstudents, or patients\u2014the people served by the organization. Within these\ninteractions, the ant army, guided by predictions, improves millions of small\ndecisions. The process goes largely unnoticed, under the radar . . . until\nsomeone bothers to look at how it\u2019s adding up. The improved decisions may\neach be ant-sized, relatively speaking, but there are so many that they come\nto a powerful net effect.\nIn 2005, I was digging in the trenches, neck deep in data for a client who\nwanted more clicks on their website. To be precise, they wanted more clicks\non their sponsors\u2019 ads. This was about the money\u2014more clicks, more\nmoney. The site had gained tens of millions of users over the years, and\nwithin just several months\u2019 worth of tracking data that they handed me, there\nwere 50 mi", " was about the money\u2014more clicks, more\nmoney. The site had gained tens of millions of users over the years, and\nwithin just several months\u2019 worth of tracking data that they handed me, there\nwere 50 million rows of learning data\u2014no small treasure trove from which\nto learn to predict . . . clicks.\nAdvertising is an inevitable part of media, be it print, television, or your\nonline experience. Benjamin Franklin forgot to include it when he pro-\nclaimed, \u201cIn this world nothing can be said to be certain, except death and\ntaxes.\u201d The \ufb02agship Internet behemoth Google credits ads as its greatest\nsource of revenue. It\u2019s the same with Facebook.\nBut on this website, ads told a slightly different story than usual, which\nfurther ampli\ufb01ed the potential win of predicting user clicks. The client was\na leading student grant and scholarship search service, with one in three\ncollege-bound high school seniors using it: an arcane niche, but just the\none over which certain universities and military recruiter", "ing student grant and scholarship search service, with one in three\ncollege-bound high school seniors using it: an arcane niche, but just the\none over which certain universities and military recruiters were drooling.\nOne ad for a university included a strong pitch, naming itself \u201cAmerica\u2019s\nleader in creative education\u201d and culminating with a button that begged to\nbe clicked: \u201cYes, please have someone from the Art Institute\u2019s Admissions\nOf\ufb01ce contact me!\u201d And you won\u2019t be surprised to hear that creditors\nwere also placing ads, at the ready to provide these students another source\nof funds: loans. The sponsors would pay up to $25 per lead\u2014for each\nLiftoff! Prediction Takes Action\n29\n\nwould-be recruit. That\u2019s good compensation for one little click of the\nmouse. What\u2019s more, since the ads were largely relevant to the users,\nclosely related to their purpose on the website, the response rates climbed\nup to an unusually high 5 percent. So this little business, owned by a well-\nknown online jo", " largely relevant to the users,\nclosely related to their purpose on the website, the response rates climbed\nup to an unusually high 5 percent. So this little business, owned by a well-\nknown online job-hunting \ufb01rm, was earning well. Any small improve-\nment meant real revenue.\nBut improving ad selection is a serious challenge. At certain intervals, users\nwere exposed to a full-page ad, selected from a pool of 291 options. The trick\nis selecting the best one for each user. The website currently selected which\nad to show based simply on the revenue it generated on average, with no\nregard to the particular user. The universally strongest ad was always shown\n\ufb01rst. Although this tactic forsakes the possibility of matching ads to individual\nusers, it\u2019s a formidable champion to unseat. Some sponsor ads, such as certain\nuniversities, paid such a high bounty per click, and were clicked so often, that\nshowing any user a less powerful ad seemed like a crazy thing to consider,\nsince doing so would ", "r ads, such as certain\nuniversities, paid such a high bounty per click, and were clicked so often, that\nshowing any user a less powerful ad seemed like a crazy thing to consider,\nsince doing so would risk losing currently established value.\nThe Perils of Personalization\nBy trusting predictions in order to customize for the individual, you take on\nrisk. A predictive system boldly proclaims, \u201cEven though ad A is so strong\noverall, for this particular user it is worth the risk of going with ad B.\u201d For this\nreason, most online ads are not personalized for the individual user\u2014even\nGoogle\u2019s AdWords, which allows you to place textual ads alongside search\nresults as well as on other Web pages, determines which ad to display by Web\npage context, the ad\u2019s click rate, and the advertiser\u2019s bid (what it is willing to\npay for a click). It is not determined by anything known or predicted about\nthe particular viewer who is going to actually see the ad.\nBut weathering this risk carries us to a new fron", "t is willing to\npay for a click). It is not determined by anything known or predicted about\nthe particular viewer who is going to actually see the ad.\nBut weathering this risk carries us to a new frontier of customization. For\nbusiness, it promises to \u201cpersonalize!,\u201d \u201cincrease relevance!,\u201d and \u201cengage\none-to-one marketing!\u201d The bene\ufb01ts reach beyond personalizing marketing\ntreatment to customizing the individual treatment of patients and suspected\ncriminals as well. During a speech about satisfying our widely varying\npreferences in choice of spaghetti sauce\u2014chunky? sweet? spicy?\u2014Malcolm\n30\nPredictive Analytics\n\nGladwell said, \u201cPeople . . . were looking for . . . universals, they were\nlooking for one way to treat all of us[;] . . . all of science through the\nnineteenth century and much of the twentieth was obsessed with universals.\nPsychologists, medical scientists, economists were all interested in \ufb01nding out\nthe rules that govern the way all of us behave. But that changed, right? What\n", "e twentieth was obsessed with universals.\nPsychologists, medical scientists, economists were all interested in \ufb01nding out\nthe rules that govern the way all of us behave. But that changed, right? What\nis the great revolution of science in the last 10, 15 years? It is the movement\nfrom the search for universals to the understanding of variability. Now in\nmedical science we don\u2019t want to know . . . just how cancer works; we\nwant to know how your cancer is different from my cancer.\u201d\nFrom medical issues to consumer preferences, individualization trumps\nuniversals. And so it goes with ads:\nPA APPLICATION: PREDICTIVE ADVERTISEMENT TARGETING\n1. What\u2019s predicted: Which ad each customer is most likely to click.\n2. What\u2019s done about it: Display the best ad (based on the likelihood of\na click as well as the bounty paid by its sponsor).\nI set up PA to perform ad targeting for my client, and the company launched\nit in a head-to-head, champion/challenger competition to the death against\ntheir existin", "as the bounty paid by its sponsor).\nI set up PA to perform ad targeting for my client, and the company launched\nit in a head-to-head, champion/challenger competition to the death against\ntheir existing system. The loser would surely be relegated to the bin of\nsecond-class ideas that just don\u2019t make as much cash. To prepare for this\nbattle, we armed PA with powerful weaponry. The predictions were\ngenerated from machine learning across 50 million learning cases, each\ndepicting a microlesson from history of the form, \u201cUser Mary was shown ad\nA and she did click it\u201d (a positive case) or \u201cUser John was shown ad B and he\ndid not click it\u201d (a negative case).\nThe learning technology employed to pick the best ad for each user was a\nNa\u00efve Bayes model. Rev. Thomas Bayes was an eighteenth-century mathe-\nmatician, and the \u201cNa\u00efve\u201d part means that we take a very smart man\u2019s ideas and\ncompromise them in a way that simpli\ufb01es yet makes their application feasible,\nresulting in a practical method that\u2019s of", "-\nmatician, and the \u201cNa\u00efve\u201d part means that we take a very smart man\u2019s ideas and\ncompromise them in a way that simpli\ufb01es yet makes their application feasible,\nresulting in a practical method that\u2019s often considered good enough at prediction\nand scales to the task at hand. I went with this method for its relative simplicity,\nsince in fact I needed to generate 291 such models, one for each ad. Together,\nthese models predict which ad a user is most likely to click on.\nLiftoff! Prediction Takes Action\n31\n\nDeployment\u2019s Detours and Delays\nAs with a rocket ship, launching PA looks great on paper. You design and\nconstruct the technology, place it on the launchpad, and wait for the green\nlight. But just when you\u2019re about to hit \u201cgo,\u201d the launch is scrubbed. Then\ndelayed. Then scrubbed again. The Wright brothers and others, galvanized\nby the awesome promise of a newly discovered wing design that generates\nlift, endured an uncharted, rocky road, faltering, \ufb02oundering, and risking life\nand limb un", "ight brothers and others, galvanized\nby the awesome promise of a newly discovered wing design that generates\nlift, endured an uncharted, rocky road, faltering, \ufb02oundering, and risking life\nand limb until all the kinks were out.\nFor ad targeting and other real-time PA deployments, predictions have got\nto zoom in at warp speed in order to provide value. Our online world tolerates\nno delay when it\u2019s time to choose which ad to display, determine whether to\nbuy a stock, decide whether to authorize a credit card charge, recommend a\nmovie, \ufb01lter an e-mail for viruses, or answer a question on Jeopardy! A real-time\nPA solution must be directly integrated into operational systems, such as\nwebsites or credit card processing facilities. If you are newly integrating PA\nwithin an organization, this can be a signi\ufb01cant project for the software\nengineers, who often have their hands full with maintenance tasks just to\nkeep the business operating normally. Thus, the deployment phase of a PA\nproject take", "be a signi\ufb01cant project for the software\nengineers, who often have their hands full with maintenance tasks just to\nkeep the business operating normally. Thus, the deployment phase of a PA\nproject takes much more than simply receiving a nod from senior management\nto go live: It demands major construction. By the time the programmers\ndeployed my predictive ad selection system, the data over which I had tuned it\nwas already about 11 months old. Were the facets of what had been learned still\nrelevant almost one year later, or would prediction\u2019s power peter out?\nIn Flight\nThis is Major Tom to Ground Control\nI\u2019m stepping through the door\nAnd I\u2019m \ufb02oating in a most peculiar way . . .\n\u2014\u201cSpace Oddity\u201d by David Bowie\nOnce launched, PA enters an eerie, silent waiting period, like you\u2019re \ufb02oating\nin orbit and nothing is moving. But the fact is, in a low orbit around Earth\nyou\u2019re actually screaming along at over 14,000 miles per hour. Unlike the\ndrama of launching a rocket or erecting a skyscraper, t", "it and nothing is moving. But the fact is, in a low orbit around Earth\nyou\u2019re actually screaming along at over 14,000 miles per hour. Unlike the\ndrama of launching a rocket or erecting a skyscraper, the launch of PA is a\n32\nPredictive Analytics\n\nrelatively stealthy maneuver. It goes live, but daily activities exhibit no\nimmediately apparent change. After the ad-targeting project\u2019s launch, if you\nchecked out the website, it would show you an ad as usual, and you could\nwonder whether the system made any difference in this one choice. This is\nwhat computers do best. They hold the power to silently enact massive\nprocedural changes that often go uncredited, since most aren\u2019t directly\nwitnessed by any one person.\nBut, under the surface, a sea change is in play, as if the entire ocean has\nbeen recon\ufb01gured. You actually notice the impact only when you examine\nan aggregated report.\nIn my client\u2019s deployment, predictive ad selection triumphed. The client\nconducted a head-to-head comparison, sele", "con\ufb01gured. You actually notice the impact only when you examine\nan aggregated report.\nIn my client\u2019s deployment, predictive ad selection triumphed. The client\nconducted a head-to-head comparison, selecting ads for half the users with the\nexisting champion system and the other half with the new predictive system,\nand reported that the new system generated at least 3.6 percent more revenue,\nwhichamounts to$1 million every 19months, given the rate atwhichrevenue\nwas already coming in. This was for the website\u2019s full-page ads only; many\nmore (smaller) ads are embedded within functional Web pages, which could\npotentially also be boosted with a similar PA project.\nNo new customers, no new sponsors, no changes to business contracts, no\nmaterials or computer hardware needed, no new full-time employees or\nongoing effort\u2014solely an improvement to decision making was needed to\ngenerate cold, hard cash. In a well-oiled, established system like the one my\nclient had, even a small improvement of 3.6 ", "yees or\nongoing effort\u2014solely an improvement to decision making was needed to\ngenerate cold, hard cash. In a well-oiled, established system like the one my\nclient had, even a small improvement of 3.6 percent amounts to something\nsubstantial. The gains of an incremental tweak can be even more dramatic: In\nthe insurance business, one company reports that PA saves almost $50 million\nannually by decreasing its loss ratio by half a percentage point.\nSo how did these models predict each click?\nElementary, My Dear: The Power of Observation\nJust like Sherlock Holmes drawing conclusions by sizing up a suspect,\nprediction comes of astute observation: What\u2019s known about each indi-\nvidual provides a set of clues about what he or she may do next. The\nchance a user will click on a certain ad depends on all sorts of elements,\nincluding the individual\u2019s current school year, gender, and e-mail domain\nLiftoff! Prediction Takes Action\n33\n\n(Hotmail, Yahoo, Gmail, etc.); the ratio of the individual\u2019s SAT w", "all sorts of elements,\nincluding the individual\u2019s current school year, gender, and e-mail domain\nLiftoff! Prediction Takes Action\n33\n\n(Hotmail, Yahoo, Gmail, etc.); the ratio of the individual\u2019s SAT written-\nto-math scores (is the user more a verbal person or more a math person?),\nand on and on.\nIn fact, this website collected a wealth of information about its users. To\n\ufb01nd out which grants and scholarships they\u2019re eligible for, users answer\ndozens of questions about their school performance, academic interests,\nextracurricular activities, prospective college majors, parents\u2019 degrees, and\nmore. So the table of learning data was long (at 50 million examples) and was\nalso wide, with each row holding all the information known about the user\nat the moment the person viewed an ad.\nIt can sound like a tall order: harnessing millions of examples in order to learn\nhow to incorporate the various factoids known about each individual so that prediction is\npossible. But we can break this down into", "like a tall order: harnessing millions of examples in order to learn\nhow to incorporate the various factoids known about each individual so that prediction is\npossible. But we can break this down into a couple of parts, and suddenly it\ngets much simpler. Let\u2019s start with the contraption that makes the predic-\ntions, the electronic Sherlock Holmes that knows how to consider all these\nfactors and roll them up into a single prediction for the individual.\nPredictive model\u2014a mechanism that predicts a behavior of an individual, such as click,\nbuy, lie, or die. It takes characteristics of the individual as input and provides a\npredictive score as output. The higher the score, the more likely it is that the individual\nwill exhibit the predicted behavior.\nA predictive model (depicted throughout this book as a \u201cgolden\u201d egg, albeit\nin black and white) scores an individual:\nCharacteristics\nof an Individual\nPredictive\nModel\nPredictive\nScore\nA predictive model is the means by which the attributes of", "s book as a \u201cgolden\u201d egg, albeit\nin black and white) scores an individual:\nCharacteristics\nof an Individual\nPredictive\nModel\nPredictive\nScore\nA predictive model is the means by which the attributes of an individual are\nfactored together for prediction. There are many ways to do this. One is to\nweigh each characteristic and then add them up\u2014perhaps females boost\ntheir score by 33.4, Hotmail users decrease their score by 15.7, and so on.\n34\nPredictive Analytics\n\nEach element counts toward or against the \ufb01nal score for that individual.\nThis is called a linear model, generally considered quite simple and limited,\nalthough usually much better than nothing.\nOther models are composed of rules, like this real example:\nThis rule is a valuable \ufb01nd, since the overall probability of responding to\nthe Art Institute\u2019s ad is only 2.7 percent, so we\u2019ve identi\ufb01ed a pocket of avid\nclickers, relatively speaking.\nIt is interesting that those who have indicated a military interest are more\nlikely to show i", "Art Institute\u2019s ad is only 2.7 percent, so we\u2019ve identi\ufb01ed a pocket of avid\nclickers, relatively speaking.\nIt is interesting that those who have indicated a military interest are more\nlikely to show interest in the Art Institute. We can speculate, but it\u2019s\nimportant not to assume there is a causal relationship. For example, it\nmay be that people who complete more of their pro\ufb01le are just more likely\nto click in general, across all kinds of ads.\nVarious types of models compete to make the most accurate predictions.\nModels that combine a bunch of rules like the one just shown are\u2014relatively\nspeaking\u2014on the simpler side. Alternatively, we can go more \u201csupermath\u201d\non the prediction problem, employing complex formulas that predict more\neffectively but are almost impossible to understand by human eyes.\nBut all predictive models share the same objective: They consider the\nvarious factors of an individual in order to derive a single predictive score for\nthat individual. This score is then used ", " eyes.\nBut all predictive models share the same objective: They consider the\nvarious factors of an individual in order to derive a single predictive score for\nthat individual. This score is then used to drive an organizational decision,\nguiding which action to take.\nIF the individual\nis still in high school\nAND\nexpects to graduate college within three years\nAND\nindicates certain military interest\nAND\nhas not been shown this ad yet\nTHEN the probability of clicking on the ad for the Art Institute is\n13.5 percent.\nLiftoff! Prediction Takes Action\n35\n\nBefore using a model, we\u2019ve got to build it. Machine learning builds the\npredictive model:\nData\nMachine\nLearning\nPredictive\nModel\nMachine learning crunches data to build the model, a brand-new prediction\nmachine. The model is the product of this learning technology\u2014it is itself\nthe very thing that has been learned. For this reason, machine learning is also\ncalled predictive modeling, which is a more common term in the commercial\nworld. If def", "learning technology\u2014it is itself\nthe very thing that has been learned. For this reason, machine learning is also\ncalled predictive modeling, which is a more common term in the commercial\nworld. If deferring to the older metaphorical term data mining, the predictive\nmodel is the unearthed gem.\nPredictive modeling generates the entire model from scratch. All the\nmodel\u2019s math, weights, or rules are created automatically by the computer.\nThe machine learning process is designed to accomplish this task, to\nmechanically develop new capabilities from data. This automation is the\nmeans by which PA builds its predictive power.\nThe hunter returns back to the tribe, proudly displaying his kill. So, too, a\ndata scientist posts her model on the bulletin board near the company ping-\npong table. The hunter hands over the kill to the cook, and the data scientist\ncooks up her model, translates it to a standard computer language, and\ne-mails it to an engineer for integration. A well-fed tribe shows the ", "er hands over the kill to the cook, and the data scientist\ncooks up her model, translates it to a standard computer language, and\ne-mails it to an engineer for integration. A well-fed tribe shows the love; a\npsyched executive issues a bonus.\nTo Act Is to Decide\nKnowing is not enough; we must act.\n\u2014Johann Wolfgang von Goethe\nOnce you develop a model, don\u2019t pat yourself on the back just yet. Predictions\ndon\u2019t help unless you do something about them. They\u2019re just thoughts, just\n36\nPredictive Analytics\n\nideas. They may beastute, brilliant gems that glimmer like the most polished of\ncrystal balls, but displaying them on a shelf gains you nothing\u2014they just sit\nthere and look smart.\nUnlike a report sitting dormant on the desk, PA leaps out of the lab and\ntakes action. In this way, it stands above other forms of analysis, data science,\nand data mining. It desires deployment and loves to be launched\u2014because,\nin what it foretells, it mandates movement.\nThe predictive score for each individual di", " other forms of analysis, data science,\nand data mining. It desires deployment and loves to be launched\u2014because,\nin what it foretells, it mandates movement.\nThe predictive score for each individual directly informs the decision of\nwhat action to take with that individual. Doctors take a second look at\npatients predicted to be readmitted, and service agents contact customers\npredicted to cancel. Predictive scores issue imperatives to mail, call, offer a\ndiscount, recommend a product, show an ad, expend sales resources, audit, investigate,\ninspect for \ufb02aws, approve a loan, or buy a stock. By acting on the predictions\nproduced by machine learning, the organization is now applying what\u2019s been\nlearned, modifying its everyday operations for the better.\nTo make this point, we have mangled the English language. Proponents like\nto say that PA is actionable. Its output directly informs actions, commanding the\norganization about what to do next. But with this use of vocabulary, industry\ninsiders ", "lish language. Proponents like\nto say that PA is actionable. Its output directly informs actions, commanding the\norganization about what to do next. But with this use of vocabulary, industry\ninsiders have stolen the word actionable, which originally meant worthy of legal action\n(i.e., \u201csue-able\u201d), and morphed it. They did so because they\u2019re tired of seeing\nsharp-looking reports that provide only a vague, unsure sense of direction.\nWith this word\u2019s new meaning established, \u201cyour \ufb02y is unzipped\u201d is\nactionable (it is clear what to do\u2014you can and should take action to remedy),\nbut \u201cyou\u2019re going bald\u201d is not (there\u2019s no cure; nothing to be done). Better\nyet, \u201cI predict you will buy these button-\ufb02y jeans and this snazzy hat\u201d is\nactionable to a salesperson.\nLaunching PA into action delivers a critical new edge in the competitive\nworld of business. One sees massive commoditization taking place today as\nthe faces of corporations appear to blend together. They all seem to sell pretty\nmuch the sa", "tical new edge in the competitive\nworld of business. One sees massive commoditization taking place today as\nthe faces of corporations appear to blend together. They all seem to sell pretty\nmuch the same thing and act in pretty much the same ways. To stand above\nthe crowd, where can a company turn?\nAs Thomas Davenport and Jeanne Harris put it in Competing on Analytics: The\nNew Science of Winning, \u201cAt a time when companies in many industries offer\nsimilar products and use comparable technology, high-performance business\nLiftoff! Prediction Takes Action\n37\n\nprocesses are among the last remaining points of differentiation.\u201d Enter PA.\nSurvey results have in fact shown that \u201ca tougher competitive environment\u201d is\nby far the strongest reason why organizations adopt this technology.\nBut while the launch of PA brings real change, it can also wreak havoc by\nintroducing new risk. With this in mind, we now return to John\u2019s story.\nA Perilous Launch\nDr. John Elder bet it all on a predictive model. He", "aunch of PA brings real change, it can also wreak havoc by\nintroducing new risk. With this in mind, we now return to John\u2019s story.\nA Perilous Launch\nDr. John Elder bet it all on a predictive model. He concocted it in the lab,\npacked it into a black box, and unleashed it on the stock market. Some\npeople make their own bed in which they must then passively lie. But John\nhad climbed way up high to take a leap of faith. Diving off a mountaintop\nwith newly constructed, experimental wings, he wondered how long it\nmight take before he could be sure he was \ufb02ying rather than crashing.\nThe risks stared John in the face. His and his wife\u2019s full retirement savings\nwere in the hands of an experimental device, launched into oblivion and\ndestined for one of the same two outcomes achieved by every rocket: glory\nor mission failure. Discovering pro\ufb01table market patterns that sustain is the\nmission of thousands of traders operating in what John points out is a brutally\ncompetitive environment; doing so a", " glory\nor mission failure. Discovering pro\ufb01table market patterns that sustain is the\nmission of thousands of traders operating in what John points out is a brutally\ncompetitive environment; doing so automatically with machine learning is\nthe most challenging of ambitions, considered impossible by many. It doesn\u2019t\nhelp that a stock market scientist is completely on his own, since work in this\narea is shrouded in secrecy, leaving virtually no potential to learn from the\nsuccesses and failures of others. Academics publish, marketers discuss, but\nquants hide away in their Batcaves. What can look great on paper might be\nstricken with a weakness that destroys or an error that bankrupts. John puts it\nplainly: \u201cWall Street is the hardest data mining problem.\u201d\nThe evidence of danger was palpable, as John had recently uncovered a\ncrippling \ufb02aw in an existing predictive trading system and personally escorted\nit to its grave. Opportunity had come knocking on the door of a small \ufb01rm\ncalled Delta Fi", "s John had recently uncovered a\ncrippling \ufb02aw in an existing predictive trading system and personally escorted\nit to its grave. Opportunity had come knocking on the door of a small \ufb01rm\ncalled Delta Financial in the form of a black-box trading system purported to\npredict movements of the Standard & Poor\u2019s (S&P) 500 with 70 percent\naccuracy. Built by a proud scientist, the system promised to make millions, so\nstakeholders were \ufb02ying around all dressed up in suits, actively lining up\n38\nPredictive Analytics\n\ninvestors prepared to place a huge bet. Among potential early investors, Delta\nwas leading the way for others, taking a central, in\ufb02uential role. The \ufb01rm was\nknown for investigating and championing cutting-edge approaches, weath-\nering the risk inherent to innovation. As a necessary precaution, Delta sought\nto empirically validate this system. The \ufb01rm turned to John, who was\nconsulting for them on the side while pursuing his doctorate at the University\nof Virginia in Charlottesville. ", "ution, Delta sought\nto empirically validate this system. The \ufb01rm turned to John, who was\nconsulting for them on the side while pursuing his doctorate at the University\nof Virginia in Charlottesville. John\u2019s work for Delta often involved inspecting,\nand sometimes debunking, black-box trading systems.\nHow do you prove a machine is broken if you\u2019re not allowed to look\ninside it? Healthy skepticism bolstered John\u2019s resolve, since the claimed\n70 percent accuracy raised red \ufb02ags as quite possibly too darn good to be true.\nBut he was not granted access to the predictive model. With secrecy reigning\nsupreme, the protocol for this type of audit dictated that John receive only\nthe numerical results, along with a few adjectives that described its design:\nnew, unique, powerful! With meager evidence, John sought to prove a crime\nhe couldn\u2019t even be sure had been committed.\nBefore each launch, organizations establish con\ufb01dence in PA by\n\u201cpredicting the past\u201d (aka backtesting). The predictive model mu", "ohn sought to prove a crime\nhe couldn\u2019t even be sure had been committed.\nBefore each launch, organizations establish con\ufb01dence in PA by\n\u201cpredicting the past\u201d (aka backtesting). The predictive model must prove\nitself on historical data before its deployment. Conducting a kind of simulated\nprediction, the model evaluates across data from last week, last month, or last\nyear. Feeding on input that could only have been known at a given time, the\nmodel spits out its prediction, which then matches against what we now\nalready know took place thereafter. Would the S&P 500 go down or up on\nMarch 21, 1991? If the model gets this retrospective question right, based only\nondataavailablebyMarch20,1991(thedayjustbefore),wehaveevidencethe\nmodel works. These retrospective predictions\u2014without the manner in which\nthey had been derived\u2014were all John had to work with.\nHouston, We Have a Problem\nEven the most elite of engineers commit the most mundane and costly of errors.\nIn late 1998, NASA launched the Ma", "which\nthey had been derived\u2014were all John had to work with.\nHouston, We Have a Problem\nEven the most elite of engineers commit the most mundane and costly of errors.\nIn late 1998, NASA launched the Mars Climate Orbiter on a daunting nine-\nmonth trip to Mars, a mission that fewer than half the world\u2019s launched probes\nheaded for that destination have completed successfully. This $327.6 million\nLiftoff! Prediction Takes Action\n39\n\ncalamity crashed and burned, due not to the \ufb02ip of fate\u2019s coin, but rather a simple\nsnafu. The spacecraft came too close to Mars and disintegrated in its atmosphere.\nThe source of the navigational bungle? One system expected to receive\ninformation in metric units (newton-seconds), but a computer programmer\nfor another system had it speak in English imperialunits(pound-seconds). Oops.\nJohn stared at a screen of numbers, wondering if anything was wrong and,\nif so, whether he could \ufb01nd it. From the long list of impressive\u2014yet\nretrospective\u2014predictions, he plainly s", "nd-seconds). Oops.\nJohn stared at a screen of numbers, wondering if anything was wrong and,\nif so, whether he could \ufb01nd it. From the long list of impressive\u2014yet\nretrospective\u2014predictions, he plainly saw the promise of huge pro\ufb01ts\nthat had everyone involved so excited. If he proved there was a \ufb02aw,\nvindication; if not, lingering uncertainty. The task at hand was to reverse\nengineer: Given the predictions the system generated, could he infer how it\nworked under the hood, essentially eking out the method in its madness?\nThis was ironic, since all predictive modeling is a kind of reverse engineering\nto begin with. Machine learning starts with the data, an encoding of things\nthat have happened, and attempts to uncover patterns that generated or\nexplained the data in the \ufb01rst place. John was attempting to deduce what the\nother team had deduced. His guide? Informal hunches and ill-informed\ninferences, each of which could be pursued only by way of trial and error,\ntesting each hypothetical mes", "empting to deduce what the\nother team had deduced. His guide? Informal hunches and ill-informed\ninferences, each of which could be pursued only by way of trial and error,\ntesting each hypothetical mess-up he could dream up by programming it by\nhand and comparing it to the retrospective predictions he had been given.\nHis perseverance \ufb01nally paid off: John uncovered a true \ufb02aw, thereby\n\ufb02inging back the curtain to expose a \ufb02ustered Wizard of Oz. It turned out that\nthe prediction engine committed the most sacrilegious of cheats by looking at\nthe one thing it must not be permitted to see. It had looked at the future. The\nbattery of impressive retrospective predictions weren\u2019t true predictions at all.\nRather, they were based in part on a three-day average calculated across\nyesterday, today . . . and tomorrow. The scientists had probably intended to\nincorporate a three-day average leading up to today, but had inadvertently\nshifted the window by a day. Oops. This crippling bug delivered the de", " and tomorrow. The scientists had probably intended to\nincorporate a three-day average leading up to today, but had inadvertently\nshifted the window by a day. Oops. This crippling bug delivered the dead-\ncertain prognosis that this predictive model would not perform well if\ndeployed into the \ufb01eld. Any prediction it would generate today could not\nincorporate the very thing it was designed to foresee\u2014tomorrow\u2019s stock\nprice\u2014since, well, it isn\u2019t known yet. So, if foolishly deployed, its accuracy\ncould never match the exaggerated performance falsely demonstrated across\n40\nPredictive Analytics\n\nthe historical data. John revealed this bug by reverse engineering it. On a\nhunch, he handcrafted a method with the same type of bug and showed that its\npredictions closely matched those of the trading system.\nA predictive model will sink faster than the Titanic if you don\u2019t seal all its\n\u201ctime leaks\u201d before launch. But this kind of \u201cleak from the future\u201d is\ncommon, if mundane. Although core to the ve", "ystem.\nA predictive model will sink faster than the Titanic if you don\u2019t seal all its\n\u201ctime leaks\u201d before launch. But this kind of \u201cleak from the future\u201d is\ncommon, if mundane. Although core to the very integrity of prediction, it\u2019s\nan easy mistake to make, given that each model is backtested over historical\ndata for which prediction is not, strictly speaking, possible. The relative\nfuture is always readily available in the testing data, easy to inadvertently\nincorporate into the very model trying to predict it. Such temporal leaks\nachieve status as a commonly known gotcha among PA practitioners. If this\nwere an episode of Star Trek, our beloved, hypomanic engineer Scotty\nwould be screaming, \u201cCaptain, we\u2019re losing our temporal integrity!\u201d\nIt was with no pleasure that John delivered the disappointing news to his\nclient, Delta Financial: He had debunked the system, essentially exposing it\nas inadvertent fraud. High hopes were dashed as another fairy tale bit the\ndust, but gratitude quick", "appointing news to his\nclient, Delta Financial: He had debunked the system, essentially exposing it\nas inadvertent fraud. High hopes were dashed as another fairy tale bit the\ndust, but gratitude quickly ensued as would-be investors realized they\u2019d just\ndodged a bullet. The wannabe inventor of the system suffered dismay but\nwas better off knowing now; it would have hit the fan much harder\npostlaunch, possibly including prosecution for fraud, even if inadvertently\ncommitted. The project was aborted.\nThe Little Model That Could\nEven the young practitioner that he was, John was a go-to data man for\nentrepreneurs in black-box trading. One such investor moved to Charlottes-\nville, but only after John Elder, PhD, new doctorate degree in hand, had just\nrelocated to Houston in order to continue his academic rite of passage with a\npostdoc research position at Rice University. He\u2019d left quite an impression\nback in Charlottesville, though; people in both the academic and commer-\ncial sectors alike", " academic rite of passage with a\npostdoc research position at Rice University. He\u2019d left quite an impression\nback in Charlottesville, though; people in both the academic and commer-\ncial sectors alike referred the investor to John. Despite John\u2019s distance, the\ninvestor hired him to prepare, launch, and monitor a new black-box mission\nremotely from Houston. It seemed as good a place as any for the project\u2019s\nMission Control.\nLiftoff! Prediction Takes Action\n41\n\nAnd so it was time for John to move beyond the low-risk role of\nevaluating other people\u2019s predictive systems and dare to build one of his\nown. Over several months, he and a small team of colleagues built upon core\ninsights from the investor and produced a new, promising black-box trading\nmodel. John was champing at the bit to launch it and put it to the test. All the\nstars were aligned for liftoff except one: The money people didn\u2019t trust it yet.\nThere was good reason to believe in John. Having recently completed his\ndoctorate deg", "t and put it to the test. All the\nstars were aligned for liftoff except one: The money people didn\u2019t trust it yet.\nThere was good reason to believe in John. Having recently completed his\ndoctorate degree, he was armed with a fresh, talented mind, yet had already\ngained an impressively wide range of data-crunching problem-solving\nexperience. On the academic side, his PhD thesis had broken records among\nresearchers as the most ef\ufb01cient way to optimize for a certain broad class of\nsystem engineering problems (machine learning is itself a kind of optimiza-\ntion problem). He had also taken on predicting the species of a bat from its\necholocation signals (the chirps bats make for their radar). And in the\ncommercial world, John\u2019s pregrad positions had dropped him right into\nthe thick of machine learning systems that steer for aerospace \ufb02ight and that\ndetect cooling pipe cracks in nuclear reactors, not to mention projects for\nDelta Financial looking over the shoulders of other black-box quants", "earning systems that steer for aerospace \ufb02ight and that\ndetect cooling pipe cracks in nuclear reactors, not to mention projects for\nDelta Financial looking over the shoulders of other black-box quants.\nAnd now John\u2019s latest creation absolutely itched to be deployed.\nBacktesting against historical data, all indications whispered con\ufb01dent\npromises for what this thing could do once set in motion. As John puts\nit, \u201cA slight pattern emerged from the overwhelming noise; we had stumbled\nacross a persistent pricing inef\ufb01ciency in a corner of the market, a small edge\nover the average investor, which appeared repeatable.\u201d Inef\ufb01ciencies are\nwhat traders live for. A perfectly ef\ufb01cient market can\u2019t be played, but if you\ncan identify the right imperfection, it\u2019s payday.\nPA APPLICATION: BLACK-BOX TRADING\n1. What\u2019s predicted: Whether a stock will go up or down.\n2. What\u2019s done about it: Buy stocks that will go up; sell those that will\ngo down.\nJohn could not get the green light. As he strove to convinc", "\n1. What\u2019s predicted: Whether a stock will go up or down.\n2. What\u2019s done about it: Buy stocks that will go up; sell those that will\ngo down.\nJohn could not get the green light. As he strove to convince the investor,\ncold feet prevailed. It appeared they were stuck in a stalemate. After all, this\n42\nPredictive Analytics\n\nguy might not get past his jitters until he could see the system succeed, yet it\ncouldn\u2019t succeed while stuck on the launchpad. The time was now, as each\nday marked lost opportunity.\nAfter a disconcerting meeting that seemed to go nowhere, John went\nhome and had a sit-down with his wife, Elizabeth. What supportive spouse\ncould possibly resist the seduction of her beloved\u2019s ardent excitement and\nstrong belief in his own abilities? She gave him the go-ahead to risk it all, a\nmove that could threaten their very home. But he still needed buy-in from\none more party.\nDelivering his appeal to the client investor raised questions, concerns,\nand eyebrows. John wanted to launch w", "move that could threaten their very home. But he still needed buy-in from\none more party.\nDelivering his appeal to the client investor raised questions, concerns,\nand eyebrows. John wanted to launch with his own personal funds, which\nmeant no risk whatsoever to the client and would resolve any doubts by\n\ufb01eld-testing John\u2019s model. But this unorthodox step would be akin to the\ndubious choice to act as one\u2019s own defense attorney. When an individual is\nwithout great personal means, this kind of thing is often frowned upon. It\nconveys overcon\ufb01dent, foolish brashness. Even if the client wanted to\ntruly believe, it would be another thing to expect the same from\ncoinvestors who hadn\u2019t gotten to know and trust John. But with every\nlaunch, proponents gamble something \ufb01erce. John had set the rules for the\ngame he\u2019d chosen to play.\nHe received his answer from the investor: \u201cGo for it!\u201d This meant there\nwas nothing to prevent moving forward. It could have also meant the\ninvestor was prepared to wri", "r the\ngame he\u2019d chosen to play.\nHe received his answer from the investor: \u201cGo for it!\u201d This meant there\nwas nothing to prevent moving forward. It could have also meant the\ninvestor was prepared to write off the project entirely, feeling there was\nnothing left to lose.\nHouston, We Have Liftoff\nPractitioners of PA often put their own professional lives a bit on the line to\npush forward, but this case was extreme. Like baseball\u2019s Billy Beane of the\nOakland A\u2019s, who literally risked his entire career to deploy and \ufb01eld-test an\nanalytical approach to team management, John risked everything he had. It\nwas early 1994, and John\u2019s individual retirement account (IRA) amounted to\nlittle more than $40,000. He put it all in.\nLiftoff! Prediction Takes Action\n43\n\n\u201cGoing live with black-box trading is really exciting and really scary,\u201d says\nJohn. \u201cIt\u2019s a roller coaster that never stops. The coaster takes on all these\nthrilling ups and downs, but with a very real chance it could go off the rails.\u201d\nAs w", "lly exciting and really scary,\u201d says\nJohn. \u201cIt\u2019s a roller coaster that never stops. The coaster takes on all these\nthrilling ups and downs, but with a very real chance it could go off the rails.\u201d\nAs with baseball, he points out, slumps aren\u2019t slumps at all\u2014they\u2019re\ninevitable statistical certainties. Each one leaves you wondering, \u201cIs this\nfalling feeling part of a safe ride, or is something broken?\u201d A key component\nto his system was a cleverly designed means to detect real quality, a measure\nof system integrity that revealed whether recent success had been truly\ndeserved or had come about just due to dumb luck.\nFrom the get-go, the predictive engine rocked. It increased John\u2019s assets at\na rate of 40 percent per year, which meant that after two years his money had\ndoubled.\nThe client investor was quickly impressed and soon put in a couple of\nmillion dollars himself. A year later, the predictive model was managing a\n$20 million fund across a group of investors, and eventually the investm", "or was quickly impressed and soon put in a couple of\nmillion dollars himself. A year later, the predictive model was managing a\n$20 million fund across a group of investors, and eventually the investment\npool increased to a few hundred million dollars. With this much on tap,\nevery win of the system was multiplicatively magni\ufb01ed.\nNo question about it: All involved relished this \ufb01esta, and the party raged\non and on, continuing almost nine years, consistently outperforming the\noverall market all along. The system chugged, autonomously trading among a\ndozen market sectors such as technology, transportation, and healthcare. John\nsays the system \u201cbeat the market each year and exhibited only two-thirds its\nstandard deviation\u2014a home run as measured by risk-adjusted return.\u201d\nBut all good things must come to an end, and just as John had talked his\nclient up, he later had to talk him down. After nearly a decade, the key\nmeasure of system integrity began to decline. John was adamant that they\nwere", "t come to an end, and just as John had talked his\nclient up, he later had to talk him down. After nearly a decade, the key\nmeasure of system integrity began to decline. John was adamant that they\nwere running on fumes, so with little ceremony the entire fund was wound\ndown. The system was halted in time, before catastrophe could strike. In the\nend, all the investors came out ahead.\nA Passionate Scientist\nThe early success of this streak had quickly altered John\u2019s life. Once the\nproject was cruising, he had begun supporting his rapidly growing family\n44\nPredictive Analytics\n\nwith ease. The project was taking only a couple of John\u2019s hours each day to\nmonitor, tweak, and refresh what was a fundamentally stable, unchanging\nmethod within the black box. What\u2019s a man to do? Do you put your feet up\nand sip wine inde\ufb01nitely, with the possible interruption of family trips to\nDisney World? After all, John had thus far always burned the candle at both\nends out of \ufb01nancial necessity, with summer jo", "p\nand sip wine inde\ufb01nitely, with the possible interruption of family trips to\nDisney World? After all, John had thus far always burned the candle at both\nends out of \ufb01nancial necessity, with summer jobs during college, part-time\nwork during graduate school, and this black-box project, which itself had\nbegun as a moonlighting gig during his postdoc. Or do you follow the logical\nbusiness imperative: Pounce on your successes, using all your free bandwidth\nto \ufb01nd ways to do more of the same?\nJohn\u2019s passion for the craft transcended these self-serving responses to his\ngood fortune. That is to say, he contains the spirit of the geek. He jokes about\nthe endless insatiability of his own appetite for the stimulation of fresh\nscienti\ufb01c challenges. He\u2019s addicted to tackling something new. There is but\none antidote: a growing list of diverse projects. So, two years into the stock\nmarket project, he wrapped up his postdoc, packed up his family, and moved\nback to Charlottesville to start his own dat", "t\none antidote: a growing list of diverse projects. So, two years into the stock\nmarket project, he wrapped up his postdoc, packed up his family, and moved\nback to Charlottesville to start his own data mining company.\nAnd so John launched Elder Research, now the largest predictive analytics\nservices \ufb01rm (pure play) in North America. A narrow focus is key to the\nsuccess of many businesses, but Elder Research\u2019s advantage is quite the\nopposite: its diversity. The company\u2019s portfolio reaches far beyond \ufb01nance to\ninclude all major commercial sectors and many branches of government.\nJohn has also earned a top-echelon position in the industry. He coauthors\nmassive textbooks, frequently chairs or keynotes at Predictive Analytics\nWorld conferences, takes cameos as a university professor, and served \ufb01ve\nyears as a presidential appointee on a national security technology panel.\nLaunching Prediction into Inner Space\nWith stories like John\u2019s coming to light, organizations are jumping on the PA\nband", " \ufb01ve\nyears as a presidential appointee on a national security technology panel.\nLaunching Prediction into Inner Space\nWith stories like John\u2019s coming to light, organizations are jumping on the PA\nbandwagon. One such \ufb01rm, a mammoth international organization, focuses\nthe power of prediction introspectively, casting PA\u2019s keen gaze on its own\nemployees. Read on to witness the windfall and the fallout when scientists\ndare to ask: Do people like being predicted?\nLiftoff! Prediction Takes Action\n45\n\nAbout the Author\nEric Siegel, PhD, founder of the Predictive Ana-\nlytics World conference series and executive edi-\ntor of The Predictive Analytics Times, makes the\nhow and why of predictive analytics understand-\nable and captivating. Eric is a former Columbia\nUniversity professor\u2014who used to sing educa-\ntional songs to his students\u2014and a renowned\nspeaker, educator, and leader in the \ufb01eld.\nEric has appeared on Al Jazeera America,\nBloomberg TV and Radio, Business News Net-\nwork (Canada), Fox News,", "uca-\ntional songs to his students\u2014and a renowned\nspeaker, educator, and leader in the \ufb01eld.\nEric has appeared on Al Jazeera America,\nBloomberg TV and Radio, Business News Net-\nwork (Canada), Fox News, Israel National Radio,\nNPR Marketplace, Radio National (Australia),\nand TheStreet. He and this book have been featured in Businessweek, CBS\nMoneyWatch, The Financial Times, Forbes, Forrester, Fortune, The Huf\ufb01ngton Post,\nThe New York Review of Books, Newsweek, The Seattle Post-Intelligencer, The Wall\nStreet Journal, The Washington Post, and WSJ MarketWatch.\nEric Siegelis available for select lectures. Toinquire: www.ThePredictionBook.com\nInterested in employing predictive analytics at your organization?\n\u2022 Access the author\u2019s online, on-demand training workshop, Predictive\nAnalytics Applied: www.businessprediction.com\n\u2022 GetstartedwiththePredictiveAnalyticsGuide:www.pawcon.com/guide\n\u2022 Follow Eric Siegel on Twitter: @predictanalytic\n311\n\n\n", "ww.businessprediction.com\n\u2022 GetstartedwiththePredictiveAnalyticsGuide:www.pawcon.com/guide\n\u2022 Follow Eric Siegel on Twitter: @predictanalytic\n311\n\n\n", "Business Intelligence\nand Analytics\nSystems for Decision Support \nTENTH EDITION\nRamesh Sharda \u2022 Dursun Delen \u2022 Efraim Turban\nGLOBAL\nEDITION\n\nBUSINESS INTELLIGENCE  \nAND ANALYTICS:\nSYSTEMS FOR DECISION SUPPORT\nBoston Columbus Indianapolis New York San Francisco Upper Saddle River  \nAmsterdam Cape Town Dubai London Madrid Milan Munich Paris Montr\u00e9al Toronto  \nDelhi Mexico City S\u00e3o Paulo Sydney Hong Kong Seoul Singapore Taipei Tokyo\nT E N T H  E D I T I O N\nGlobal Edition\nRamesh Sharda\nOklahoma State University\nDursun Delen\nOklahoma State University\nEfraim Turban\nUniversity of Hawaii\nWith contributions by\nJ. E. Aronson\nThe University of Georgia\nTing-Peng Liang\nNational Sun Yat-sen University\nDavid King\nJDA Software Group, Inc.\n\nBusiness Intelligence and Analytics: Systems\nfor Decision Support PDF eBook, Global\nEdition\nTable of Contents\nCover\nTitle Page\nContents\nPreface\nAbout the Authors\nPart I Decision Making and Analytics: An Overview\nChapter 1 An Overview of Business Intelligence, Analy", " PDF eBook, Global\nEdition\nTable of Contents\nCover\nTitle Page\nContents\nPreface\nAbout the Authors\nPart I Decision Making and Analytics: An Overview\nChapter 1 An Overview of Business Intelligence, Analytics, and Decision Support\n1.1 Opening Vignette: Magpie Sensing Employs Analytics to Manage a Vaccine Supply\nChain Effectively and Safely\n1.2 Changing Business Environments and Computerized Decision Support\nThe Business PressuresResponsesSupport Model\n1.3 Managerial Decision Making\nThe Nature of Managers Work\nThe Decision-Making Process\n1.4 Information Systems Support for Decision Making\n1.5 An Early Framework for Computerized Decision Support\nThe Gorry and Scott-Morton Classical Framework\nComputer Support for Structured Decisions\nComputer Support for Unstructured Decisions\nComputer Support for Semistructured Problems\n1.6 The Concept of Decision Support Systems (DSS)\nDSS as an Umbrella Term\nEvolution of DSS into Business Intelligence\n1.7 A Framework for Business Intelligence (BI)\nDefinitio", "r Semistructured Problems\n1.6 The Concept of Decision Support Systems (DSS)\nDSS as an Umbrella Term\nEvolution of DSS into Business Intelligence\n1.7 A Framework for Business Intelligence (BI)\nDefinitions of BI\nA Brief History of BI\nThe Architecture of BI\n\nTable of Contents\nStyles of BI\nThe Origins and Drivers of BI\nA Multimedia Exercise in Business Intelligence\nApplication Case 1.1 Sabre Helps Its Clients Through Dashboards and Analytics\nThe DSSBI Connection\n1.8 Business Analytics Overview\nDescriptive Analytics\nApplication Case 1.2 Eliminating Inefficiencies at Seattle Childrens Hospital\nApplication Case 1.3 Analysis at the Speed of Thought\nPredictive Analytics\nApplication Case 1.4 Moneyball: Analytics in Sports and Movies\nApplication Case 1.5 Analyzing Athletic Injuries\nPrescriptive Analytics\nApplication Case 1.6 Industrial and Commercial Bank of China (ICBC) Employs Models to\nReconfigure Its Branch Network\nAnalytics Applied to Different Domains\nAnalytics or Data Science?\n1.9 Brief Int", "ics\nApplication Case 1.6 Industrial and Commercial Bank of China (ICBC) Employs Models to\nReconfigure Its Branch Network\nAnalytics Applied to Different Domains\nAnalytics or Data Science?\n1.9 Brief Introduction to Big Data Analytics\nWhat Is Big Data?\nApplication Case 1.7 Gilt Groupes Flash Sales Streamlined by Big Data Analytics\n1.10 Plan of the Book\nPart I: Business Analytics: An Overview\nPart II: Descriptive Analytics\nPart III: Predictive Analytics\nPart IV: Prescriptive Analytics\nPart V: Big Data and Future Directions for Business Analytics\n1.11 Resources, Links, and the Teradata University Network Connection\nResources and Links\nVendors, Products, and Demos\nPeriodicals\nThe Teradata University Network Connection\nThe Books Web Site\nChapter Highlights\nKey Terms\nQuestions for Discussion\nExercises\nEnd-of-Chapter Application Case Nationwide Insurance Used BI to Enhance Customer Service\n\nTable of Contents\nReferences\nChapter 2 Foundations and Technologies for Decision Making\n2.1 Opening Vigne", "ises\nEnd-of-Chapter Application Case Nationwide Insurance Used BI to Enhance Customer Service\n\nTable of Contents\nReferences\nChapter 2 Foundations and Technologies for Decision Making\n2.1 Opening Vignette: Decision Modeling at HP Using Spreadsheets\n2.2 Decision Making: Introduction and Definitions\nCharacteristics of Decision Making\nA Working Definition of Decision Making\nDecision-Making Disciplines\nDecision Style and Decision Makers\n2.3 Phases of the Decision-Making Process\n2.4 Decision Making: The Intelligence Phase\nProblem (or Opportunity) Identification\nApplication Case 2.1 Making Elevators Go Faster!\nProblem Classification\nProblem Decomposition\nProblem Ownership\n2.5 Decision Making: The Design Phase\nModels\nMathematical (Quantitative) Models\nThe Benefits of Models\nSelection of a Principle of Choice\nNormative Models\nSuboptimization\nDescriptive Models\nGood Enough, or Satisficing\nDeveloping (Generating) Alternatives\nMeasuring Outcomes\nRisk\nScenarios\nPossible Scenarios\nErrors in Decision", "e of Choice\nNormative Models\nSuboptimization\nDescriptive Models\nGood Enough, or Satisficing\nDeveloping (Generating) Alternatives\nMeasuring Outcomes\nRisk\nScenarios\nPossible Scenarios\nErrors in Decision Making\n2.6 Decision Making: The Choice Phase\n2.7 Decision Making: The Implementation Phase\n2.8 How Decisions Are Supported\nSupport for the Intelligence Phase\nSupport for the Design Phase\nSupport for the Choice Phase\n\nTable of Contents\nSupport for the Implementation Phase\n2.9 Decision Support Systems: Capabilities\nA DSS Application\n2.10 DSS Classifications\nThe AIS SIGDSS Classification for DSS\nOther DSS Categories\nCustom-Made Systems Versus Ready-Made Systems\n2.11 Components of Decision Support Systems\nThe Data Management Subsystem\nThe Model Management Subsystem\nApplication Case 2.2 Station Casinos Wins by Building Customer Relationships Using Its Data\nApplication Case 2.3 SNAP DSS Helps OneNet MakeTelecommunications Rate Decisions\nThe User Interface Subsystem\nThe Knowledge-Based Managemen", "sinos Wins by Building Customer Relationships Using Its Data\nApplication Case 2.3 SNAP DSS Helps OneNet MakeTelecommunications Rate Decisions\nThe User Interface Subsystem\nThe Knowledge-Based Management Subsystem\nApplication Case 2.4 From a Game Winner to a Doctor!\nChapter Highlights\nKey Terms\nQuestions for Discussion\nExercises\nEnd-of-Chapter Application Case Logistics Optimization in a Major Shipping Company (CSAV)\nReferences\nPart II Descriptive Analytics\nChapter 3 Data Warehousing\n3.1 Opening Vignette: Isle of Capri Casinos Is Winning with Enterprise Data\nWarehouse\n3.2 Data Warehousing Definitions and Concepts\nWhat Is a Data Warehouse?\nA Historical Perspective to Data Warehousing\nCharacteristics of Data Warehousing\nData Marts\nOperational Data Stores\nEnterprise Data Warehouses (EDW)\nMetadata\nApplication Case 3.1 A Better Data Plan: Well-Established TELCOs Leverage Data Warehousing and\nAnalytics to Stay on Top in a Competitive Industry\n3.3 Data Warehousing Process Overview\n\nTable of Con", "ta\nApplication Case 3.1 A Better Data Plan: Well-Established TELCOs Leverage Data Warehousing and\nAnalytics to Stay on Top in a Competitive Industry\n3.3 Data Warehousing Process Overview\n\nTable of Contents\nApplication Case 3.2 Data Warehousing Helps MultiCare Save More Lives\n3.4 Data Warehousing Architectures\nAlternative Data Warehousing Architectures\nWhich Architecture Is the Best?\n3.5 Data Integration and the Extraction, Transformation, and Load (ETL) Processes\nData Integration\nApplication Case 3.3 BP Lubricants Achieves BIGS Success\nExtraction, Transformation, and Load\n3.6 Data Warehouse Development\nApplication Case 3.4 Things Go Better with Cokes Data Warehouse\nData Warehouse Development Approaches\nApplication Case 3.5 Starwood Hotels & Resorts Manages Hotel Profitability with Data Warehousing\nAdditional Data Warehouse Development Considerations\nRepresentation of Data in Data Warehouse\nAnalysis of Data in the Data Warehouse\nOLAP Versus OLTP\nOLAP Operations\n3.7 Data Warehousing Impl", "ehousing\nAdditional Data Warehouse Development Considerations\nRepresentation of Data in Data Warehouse\nAnalysis of Data in the Data Warehouse\nOLAP Versus OLTP\nOLAP Operations\n3.7 Data Warehousing Implementation Issues\nApplication Case 3.6 EDW Helps Connect State Agencies in Michigan\nMassive Data Warehouses and Scalability\n3.8 Real-Time Data Warehousing\nApplication Case 3.7 Egg Plc Fries the Competition in Near Real Time\n3.9 Data Warehouse Administration, Security Issues, and Future Trends\nThe Future of Data Warehousing\n3.10 Resources, Links, and the Teradata University Network Connection\nResources and Links\nCases\nVendors, Products, and Demos\nPeriodicals\nAdditional References\nThe Teradata University Network (TUN) Connection\nChapter Highlights\nKey Terms\nQuestions for Discussion\nExercises\nEnd-of-Chapter Application Case Continental Airlines Flies High with Its Real-Time Data\nWarehouse\n\nTable of Contents\nReferences\nChapter 4 Business Reporting, Visual Analytics, and Business Performance\nMa", "-of-Chapter Application Case Continental Airlines Flies High with Its Real-Time Data\nWarehouse\n\nTable of Contents\nReferences\nChapter 4 Business Reporting, Visual Analytics, and Business Performance\nManagement\n4.1 Opening Vignette:Self-Service Reporting Environment Saves Millions for\nCorporate Customers\n4.2 Business Reporting Definitions and Concepts\nWhat Is a Business Report?\nApplication Case 4.1 Delta Lloyd Group Ensures Accuracy and Efficiency in Financial Reporting\nComponents of the Business Reporting System\nApplication Case 4.2 Flood of Paper Ends at FEMA\n4.3 Data and Information Visualization\nApplication Case 4.3 Tableau Saves Blastrac Thousands of Dollars with Simplified Information\nSharing\nA Brief History of Data Visualization\nApplication Case 4.4 TIBCO Spotfire Provides Dana-Farber Cancer Institute with Unprecedented\nInsight into Cancer Vaccine Clinical Trials\n4.4 Different Types of Charts and Graphs\nBasic Charts and Graphs\nSpecialized Charts and Graphs\n4.5 The Emergence of Dat", " Cancer Institute with Unprecedented\nInsight into Cancer Vaccine Clinical Trials\n4.4 Different Types of Charts and Graphs\nBasic Charts and Graphs\nSpecialized Charts and Graphs\n4.5 The Emergence of Data Visualization and Visual Analytics\nVisual Analytics\nHigh-Powered Visual Analytics Environments\n4.6 Performance Dashboards\nApplication Case 4.5 Dallas Cowboys Score Big with Tableau and Teknion\nDashboard Design\nApplication Case 4.6 Saudi Telecom Company Excels with Information Visualization\nWhat to Look For in a Dashboard\nBest Practices in Dashboard Design\nBenchmark Key Performance Indicators with Industry Standards\nWrap the Dashboard Metrics with Contextual Metadata\nValidate the Dashboard Design by a Usability Specialist\nPrioritize and Rank Alerts/Exceptions Streamed to the Dashboard\nEnrich Dashboard with Business Users Comments\nPresent Information in Three Different Levels\nPick the Right Visual Construct Using Dashboard Design Principles\nProvide for Guided Analytics\n4.7 Business Perform", "Dashboard with Business Users Comments\nPresent Information in Three Different Levels\nPick the Right Visual Construct Using Dashboard Design Principles\nProvide for Guided Analytics\n4.7 Business Performance Management\n\nTable of Contents\nClosed-Loop BPM Cycle \n Application Case 4.7 IBM Cognos Express Helps Mace for Faster\n4.8 Performance Measurement\nKey Performance Indicator (KPI)\nPerformance Measurement System\n4.9 Balanced Scorecards\nThe Four Perspectives\nThe Meaning of Balance in BSC\nDashboards Versus Scorecards\n4.10 Six Sigma as a Performance Measurement System\nThe DMAIC Performance Model\nBalanced Scorecard Versus Six Sigma\nEffective Performance Measurement\nApplication Case 4.8 Expedia.coms Customer Satisfaction Scorecard\nChapter Highlights\nKey Terms\nQuestions for Discussion\nExercises\nEnd-of-Chapter Application Case Smart Business Reporting Helps Healthcare Providers Deliver\nBetter Care\nReferences\nPart III Predictive Analytics\nChapter 5 Data Mining\n5.1 Opening Vignette: Cabelas Reels i", "of-Chapter Application Case Smart Business Reporting Helps Healthcare Providers Deliver\nBetter Care\nReferences\nPart III Predictive Analytics\nChapter 5 Data Mining\n5.1 Opening Vignette: Cabelas Reels in More Customers withAdvanced Analytics and\nData Mining\n5.2 Data Mining Concepts and Applications\nApplication Case 5.1 Smarter Insurance: Infinity P&C ImprovesCustomer Service and Combats Fraud\nwith Predictive Analytics\nDefinitions, Characteristics, and Benefits\nApplication Case 5.2 Harnessing Analytics to Combat Crime:Predictive Analytics Helps Memphis\nPolice Department Pinpoint Crimeand Focus Police Resources\nHow Data Mining Works\nData Mining Versus Statistics\n5.3 Data Mining Applications\nApplication Case 5.3 A Mine on Terrorist Funding\n5.4 Data Mining Process\n\nTable of Contents\nStep 1: Business Understanding\nStep 2: Data Understanding\nStep 3: Data Preparation\nStep 4: Model Building\nApplication Case 5.4 Data Mining in Cancer Research\nStep 5: Testing and Evaluation\nStep 6: Deployment\nOthe", "ness Understanding\nStep 2: Data Understanding\nStep 3: Data Preparation\nStep 4: Model Building\nApplication Case 5.4 Data Mining in Cancer Research\nStep 5: Testing and Evaluation\nStep 6: Deployment\nOther Data Mining Standardized Processes and Methodologies\n5.5 Data Mining Methods\nClassification\nEstimating the True Accuracy of Classification Models\nCluster Analysis for Data Mining\nApplication Case 5.5 2degrees Gets a 1275 Percent Boost in ChurnIdentification\nAssociation Rule Mining\n5.6 Data Mining Software Tools\nApplication Case 5.6 Data Mining Goes to Hollywood: PredictingFinancial Success of Movies\n5.7 Data Mining Privacy Issues, Myths, and Blunders\nData Mining and Privacy Issues\nApplication Case 5.7 Predicting Customer Buying PatternsTheTarget Story\nData Mining Myths and Blunders\nChapter Highlights\nKey Terms\nQuestions for Discussion\nExercises\nEnd-of-Chapter Application Case Macys.com Enhances ItsCustomers Shopping Experience with\nAnalytics\nReferences\nChapter 6 Techniques for Predictive", "ighlights\nKey Terms\nQuestions for Discussion\nExercises\nEnd-of-Chapter Application Case Macys.com Enhances ItsCustomers Shopping Experience with\nAnalytics\nReferences\nChapter 6 Techniques for Predictive Modeling\n6.1 Opening Vignette: Predictive Modeling Helps BetterUnderstand and Manage Complex\nMedicalProcedures\n6.2 Basic Concepts of Neural Networks\nBiological and Artificial Neural Networks\nApplication Case 6.1 Neural Networks Are Helping to Save Lives inthe Mining Industry\nElements of ANN\nNetwork Information Processing\nNeural Network Architectures\nApplication Case 6.2 Predictive Modeling Is Powering the PowerGenerators\n\nTable of Contents\n6.3 Developing Neural NetworkBased Systems\nThe General ANN Learning Process\nBackpropagation\n6.4 Illuminating the Black Box of ANN with SensitivityAnalysis\nApplication Case 6.3 Sensitivity Analysis Reveals Injury SeverityFactors in Traffic Accidents\n6.5 Support Vector Machines\nApplication Case 6.4 Managing Student Retention with PredictiveModeling\nMathem", "Application Case 6.3 Sensitivity Analysis Reveals Injury SeverityFactors in Traffic Accidents\n6.5 Support Vector Machines\nApplication Case 6.4 Managing Student Retention with PredictiveModeling\nMathematical Formulation of SVMs\nPrimal Form\nDual Form\nSoft Margin\nNonlinear Classification\nKernel Trick\n6.6 A Process-Based Approach to the Use of SVM\nSupport Vector Machines Versus Artificial Neural Networks\n6.7 Nearest Neighbor Method for Prediction\nSimilarity Measure: The Distance Metric\nParameter Selection\nApplication Case 6.5 Efficient Image Recognition andCategorization with kNN\nChapter Highlights\nKey Terms\nQuestions for Discussion\nExercises\nEnd-of-Chapter Application Case Coors Improves Beer Flavorswith Neural Networks\nReferences\nChapter 7 Text Analytics, Text Mining, and Sentiment Analysis\n7.1 Opening Vignette: Machine Versus Men on Jeopardy!: TheStory of Watson\n7.2 Text Analytics and Text Mining Concepts andDefinitions\nApplication Case 7.1 Text Mining for Patent Analysis\n7.3 Natural La", "7.1 Opening Vignette: Machine Versus Men on Jeopardy!: TheStory of Watson\n7.2 Text Analytics and Text Mining Concepts andDefinitions\nApplication Case 7.1 Text Mining for Patent Analysis\n7.3 Natural Language Processing\nApplication Case 7.2 Text Mining Improves Hong KongGovernments Ability to Anticipate and\nAddress Public Complaints\n7.4 Text Mining Applications\nMarketing Applications\nSecurity Applications\nApplication Case 7.3 Mining for Lies\n\nTable of Contents\nBiomedical Applications\nAcademic Applications\nApplication Case 7.4 Text Mining and Sentiment Analysis HelpImprove Customer Service Performance\n7.5 Text Mining Process\nTask 1: Establish the Corpus\nTask 2: Create the TermDocument Matrix\nTask 3: Extract the Knowledge\nApplication Case 7.5 Research Literature Survey with TextMining\n7.6 Text Mining Tools\nCommercial Software Tools\nFree Software Tools\nApplication Case 7.6 A Potpourri of Text Mining Case Synopses\n7.7 Sentiment Analysis Overview\nApplication Case 7.7 Whirlpool Achieves Custom", " Mining Tools\nCommercial Software Tools\nFree Software Tools\nApplication Case 7.6 A Potpourri of Text Mining Case Synopses\n7.7 Sentiment Analysis Overview\nApplication Case 7.7 Whirlpool Achieves Customer Loyalty andProduct Success with Text Analytics\n7.8 Sentiment Analysis Applications\n7.9 Sentiment Analysis Process\nMethods for Polarity Identification\nUsing a Lexicon\nUsing a Collection of Training Documents\nIdentifying Semantic Orientation of Sentences and Phrases\nIdentifying Semantic Orientation of Document\n7.10 Sentiment Analysis and Speech Analytics 359How Is It Done?\nApplication Case 7.8 Cutting Through the Confusion: Blue CrossBlue Shield of North Carolina Uses\nNexidias Speech Analytics to EaseMember Experience in Healthcare\nChapter Highlights\nKey Terms\nQuestions for Discussion\nExercises\nEnd-of-Chapter Application Case BBVA Seamlessly Monitorsand Improves Its Online Reputation\nReferences\nChapter 8 Web Analytics, Web Mining, and Social Analytics\n8.1 Opening Vignette: Security First ", "es\nEnd-of-Chapter Application Case BBVA Seamlessly Monitorsand Improves Its Online Reputation\nReferences\nChapter 8 Web Analytics, Web Mining, and Social Analytics\n8.1 Opening Vignette: Security First Insurance Deepens Connection with\nPolicyholders\n8.2 Web Mining Overview\n8.3 Web Content and Web Structure Mining\nApplication Case 8.1 Identifying Extremist Groups with Web Linkand Content Analysis\n\nTable of Contents\n8.4 Search Engines\nAnatomy of a Search Engine\n1. Development Cycle\nWeb Crawler\nDocument Indexer\n2. Response Cycle\nQuery Analyzer\nDocument Matcher/Ranker\nHow Does Google Do It?\nApplication Case 8.2 IGN Increases Search Traffic by 1500 Percent\n8.5 Search Engine Optimization\nMethods for Search Engine Optimization\nApplication Case 8.3 Understanding Why Customers Abandon Shopping Carts Results in $10 Million\nSales Increase\n8.6 Web Usage Mining (Web Analytics)\nWeb Analytics Technologies\nApplication Case 8.4 Allegro Boosts Online Click-Through Rates by 500 Percent with Web Analysis\nWe", "Results in $10 Million\nSales Increase\n8.6 Web Usage Mining (Web Analytics)\nWeb Analytics Technologies\nApplication Case 8.4 Allegro Boosts Online Click-Through Rates by 500 Percent with Web Analysis\nWeb Analytics Metrics\nWeb Site Usability\nTraffic Sources\nVisitor Profiles\nConversion Statistics\n8.7 Web Analytics Maturity Model and Web Analytics Tools\nWeb Analytics Tools\nPutting It All TogetherA Web Site Optimization Ecosystem\nA Framework for Voice of the Customer Strategy\n8.8 Social Analytics and Social Network Analysis\nSocial Network Analysis\nSocial Network Analysis Metrics\nApplication Case 8.5 Social Network Analysis HelpsTelecommunication Firms\nConnections\nDistributions\nSegmentation\n8.9 Social Media Definitions and Concepts\nHow Do People Use Social Media?\nApplication Case 8.6 Measuring the Impact of Social Media at Lollapalooza\n8.10 Social Media Analytics\n\nTable of Contents\nMeasuring the Social Media Impact\nBest Practices in Social Media Analytics\nApplication Case 8.7 eHarmony Uses So", "e Impact of Social Media at Lollapalooza\n8.10 Social Media Analytics\n\nTable of Contents\nMeasuring the Social Media Impact\nBest Practices in Social Media Analytics\nApplication Case 8.7 eHarmony Uses Social Media to Help Take the Mystery Out of Online Dating\nSocial Media Analytics Tools and Vendors\nChapter Highlights\nKey Terms\nQuestions for Discussion\nExercises\nEnd-of-Chapter Application Case Keeping Students on Track with Web and Predictive Analytics\nReferences\nPart IV Prescriptive Analytics\nChapter 9 Model-Based Decision Making: Optimization and Multi-Criteria\nSystems\n9.1 Opening Vignette: Midwest ISO Saves Billions by Better Planning of Power Plant\nOperations and Capacity Planning\n9.2 Decision Support Systems Modeling\nApplication Case 9.1 Optimal Transport for ExxonMobil Downstream Through a DSS\nCurrent Modeling Issues\nApplication Case 9.2 Forecasting/Predictive Analytics Proves to Bea Good Gamble for Harrahs\nCherokee Casino and Hotel\n9.3 Structure of Mathematical Models for Decision ", "h a DSS\nCurrent Modeling Issues\nApplication Case 9.2 Forecasting/Predictive Analytics Proves to Bea Good Gamble for Harrahs\nCherokee Casino and Hotel\n9.3 Structure of Mathematical Models for Decision Support\nThe Components of Decision Support Mathematical Models\nThe Structure of Mathematical Models\n9.4 Certainty, Uncertainty, and Risk\nDecision Making Under Certainty\nDecision Making Under Uncertainty\nDecision Making Under Risk (Risk Analysis)\nApplication Case 9.3 American Airlines UsesShould-Cost Modeling to Assess the Uncertainty of\nBidsfor Shipment Routes\n9.5 Decision Modeling with Spreadsheets\nApplication Case 9.4 Showcase Scheduling at Fred Astaire East Side Dance Studio\n9.6 Mathematical Programming Optimization\nApplication Case 9.5 Spreadsheet Model Helps Assign Medical Residents\nMathematical Programming\nLinear Programming\nModeling in LP: An Example\n\nTable of Contents\nImplementation\n9.7 Multiple Goals, Sensitivity Analysis, What-If Analysis,and Goal Seeking\nMultiple Goals\nSensitivi", "matical Programming\nLinear Programming\nModeling in LP: An Example\n\nTable of Contents\nImplementation\n9.7 Multiple Goals, Sensitivity Analysis, What-If Analysis,and Goal Seeking\nMultiple Goals\nSensitivity Analysis\nWhat-If Analysis\nGoal Seeking\n9.8 Decision Analysis with Decision Tables and Decision Trees\nDecision Tables\nDecision Trees\n9.9 Multi-Criteria Decision Making With Pairwise Comparisons\nThe Analytic Hierarchy Process\nApplication Case 9.6 U.S. HUD Saves the House by Using AHP for Selecting IT Projects\nTutorial on Applying Analytic Hierarchy Process Using Web-HIPRE\nChapter Highlights\nKey Terms\nQuestions for Discussion\nExercises\nEnd-of-Chapter Application Case Pre-Positioning of Emergency Items for CARE International\nReferences\nChapter 10 Modeling and Analysis: Heuristic Search Methods and\nSimulation\n10.1 Opening Vignette: System Dynamics Allows FluorCorporation to Better Plan for\nProject and Change Management\n10.2 Problem-Solving Search Methods\nAnalytical Techniques\nAlgorithms\nBlin", "and\nSimulation\n10.1 Opening Vignette: System Dynamics Allows FluorCorporation to Better Plan for\nProject and Change Management\n10.2 Problem-Solving Search Methods\nAnalytical Techniques\nAlgorithms\nBlind Searching\nHeuristic Searching\nApplication Case 10.1 Chilean Government Uses Heuristics to Make Decisions on School Lunch\nProviders\n10.3 Genetic Algorithms and Developing GA Applications\nExample: The Vector Game\nTerminology of Genetic Algorithms\nHow Do Genetic Algorithms Work?\nLimitations of Genetic Algorithms\nGenetic Algorithm Applications\n10.4 Simulation\n\nTable of Contents\nApplication Case 10.2 Improving Maintenance Decision Making in the Finnish Air Force Through\nSimulation\nApplication Case 10.3 Simulating Effects of Hepatitis B Interventions\nMajor Characteristics of Simulation\nAdvantages of Simulation\nDisadvantages of Simulation\nThe Methodology of Simulation\nSimulation Types\nMonte Carlo Simulation\nDiscrete Event Simulation\n10.5 Visual Interactive Simulation\nConventional Simulation Ina", "of Simulation\nDisadvantages of Simulation\nThe Methodology of Simulation\nSimulation Types\nMonte Carlo Simulation\nDiscrete Event Simulation\n10.5 Visual Interactive Simulation\nConventional Simulation Inadequacies\nVisual Interactive Simulation\nVisual Interactive Models and DSS\nApplication Case 10.4 Improving Job-Shop Scheduling DecisionsThrough RFID: A Simulation-Based\nAssessment\nSimulation Software\n10.6 System Dynamics Modeling\n10.7 Agent-Based Modeling\nApplication Case 10.5 Agent-Based Simulation Helps Analyze Spread of a Pandemic Outbreak\nChapter Highlights\nKey Terms\nQuestions for Discussion\nExercises\nEnd-of-Chapter Application Case HP Applies Management Science Modeling to Optimize Its Supply\nChain and Wins a MajorAward\nReferences\nChapter 11 Automated Decision Systems and Expert Systems\n11.1 Opening Vignette: InterContinental Hotel Group Uses Decision Rules for Optimal\nHotel Room Rates\n11.2 Automated Decision Systems\nApplication Case 11.1 Giant Food Stores Prices the EntireStore\n11.3 T", ".1 Opening Vignette: InterContinental Hotel Group Uses Decision Rules for Optimal\nHotel Room Rates\n11.2 Automated Decision Systems\nApplication Case 11.1 Giant Food Stores Prices the EntireStore\n11.3 The Artificial Intelligence Field\n11.4 Basic Concepts of Expert Systems\nExperts\nExpertise\nFeatures of ES\n\nTable of Contents\nApplication Case 11.2 Expert System Helps in Identifying SportTalents\n11.5 Applications of Expert Systems\nApplication Case 11.3 Expert System Aids in Identification of Chemical, Biological, and\nRadiological Agents\nClassical Applications of ES\nNewer Applications of ES\nAreas for ES Applications\n11.6 Structure of Expert Systems\nKnowledge Acquisition Subsystem\nKnowledge Base\nInference Engine\nUser Interface\nBlackboard (Workplace)\nExplanation Subsystem (Justifier)\nKnowledge-Refining System\nApplication Case 11.4 Diagnosing Heart Diseases by Signal Processing\n11.7 Knowledge Engineering\nKnowledge Acquisition\nKnowledge Verification and Validation\nKnowledge Representation\nInferen", "ning System\nApplication Case 11.4 Diagnosing Heart Diseases by Signal Processing\n11.7 Knowledge Engineering\nKnowledge Acquisition\nKnowledge Verification and Validation\nKnowledge Representation\nInferencing\nExplanation and Justification\n11.8 Problem Areas Suitable for Expert Systems\n11.9 Development of Expert Systems\nDefining the Nature and Scope of the Problem\nIdentifying Proper Experts\nAcquiring Knowledge\nSelecting the Building Tools\nCoding the System\nEvaluating the System\nApplication Case 11.5 Clinical Decision Support System for Tendon Injuries\n11.10 Concluding Remarks\nChapter Highlights\nKey Terms\nQuestions for Discussion\nExercises\nEnd-of-Chapter Application Case Tax Collections Optimization for New York State\n\nTable of Contents\nReferences\nChapter 12 Knowledge Management and Collaborative Systems\n12.1 Opening Vignette: Expertise Transfer System to Train Future Army Personnel\n12.2 Introduction to Knowledge Management\nKnowledge Management Concepts and Definitions\nKnowledge\nExplicit and", "e Systems\n12.1 Opening Vignette: Expertise Transfer System to Train Future Army Personnel\n12.2 Introduction to Knowledge Management\nKnowledge Management Concepts and Definitions\nKnowledge\nExplicit and Tacit Knowledge\n12.3 Approaches to Knowledge Management\nThe Process Approach to Knowledge Management\nThe Practice Approach to Knowledge Management\nHybrid Approaches to Knowledge Management\nKnowledge Repositories\n12.4 Information Technology (IT) in Knowledge Management\nThe KMS Cycle\nComponents of KMS\nTechnologies That Support Knowledge Management\n12.5 Making Decisions in Groups: Characteristics, Process,Benefits, and\nDysfunctions\nCharacteristics of Groupwork\nThe Group Decision-Making Process\nThe Benefits and Limitations of Groupwork\n12.6 Supporting Groupwork with Computerized Systems\nAn Overview of Group Support Systems (GSS)\nGroupware\nTime/Place Framework\n12.7 Tools for Indirect Support of Decision Making\nGroupware Tools\nGroupware\nCollaborative Workflow\nWeb 2.0\nWikis\nCollaborative Network", "ew of Group Support Systems (GSS)\nGroupware\nTime/Place Framework\n12.7 Tools for Indirect Support of Decision Making\nGroupware Tools\nGroupware\nCollaborative Workflow\nWeb 2.0\nWikis\nCollaborative Networks\n12.8 Direct Computerized Support for Decision Making:From Group Decision Support\nSystems to Group SupportSystems\nGroup Decision Support Systems (GDSS)\nGroup Support Systems\n\nTable of Contents\nHow GDSS (or GSS) Improve Groupwork\nFacilities for GDSS\nChapter Highlights\nKey Terms\nQuestions for Discussion\nExercises\nEnd-of-Chapter Application Case Solving Crimes by Sharing Digital Forensic Knowledge\nReferences\nPart V Big Data and Future Directions for Business Analytics\nChapter 13 Big Data and Analytics\n13.1 Opening Vignette: Big Data Meets Big Science at CERN\n13.2 Definition of Big Data\nThe Vs That Define Big Data\nApplication Case 13.1 Big Data Analytics Helps Luxottica ImproveIts Marketing Effectiveness\n13.3 Fundamentals of Big Data Analytics\nBusiness Problems Addressed by Big Data Analytics", "at Define Big Data\nApplication Case 13.1 Big Data Analytics Helps Luxottica ImproveIts Marketing Effectiveness\n13.3 Fundamentals of Big Data Analytics\nBusiness Problems Addressed by Big Data Analytics\nApplication Case 13.2 Top 5 Investment Bank Achieves Single Source of Truth\n13.4 Big Data Technologies\nMapReduce\nWhy Use MapReduce?\nHadoop\nHow Does Hadoop Work?\nHadoop Technical Components\nHadoop: The Pros and Cons\nNoSQL\nApplication Case 13.3 eBays Big Data Solution\n13.5 Data Scientist\nWhere Do Data Scientists Come From?\nApplication Case 13.4 Big Data and Analytics in Politics\n13.6 Big Data and Data Warehousing\nUse Case(s) for Hadoop\nUse Case(s) for Data Warehousing\nThe Gray Areas (Any One of the Two Would Do the Job)\nCoexistence of Hadoop and Data Warehouse\n13.7 Big Data Vendors\nApplication Case 13.5 Dublin City Council Is Leveraging Big Datato Reduce Traffic Congestion\n\nTable of Contents\nApplication Case 13.6 Creditreform Boosts Credit Rating Quality with Big Data Visual Analytics\n13.8 ", "Case 13.5 Dublin City Council Is Leveraging Big Datato Reduce Traffic Congestion\n\nTable of Contents\nApplication Case 13.6 Creditreform Boosts Credit Rating Quality with Big Data Visual Analytics\n13.8 Big Data and Stream Analytics\nStream Analytics Versus Perpetual Analytics\nCritical Event Processing\nData Stream Mining\n13.9 Applications of Stream Analytics\ne-Commerce\nTelecommunications\nApplication Case 13.7 Turning Machine-Generated Streaming Data into Valuable Business Insights\nLaw Enforcement and Cyber Security\nPower Industry\nFinancial Services\nHealth Sciences\nGovernment\nChapter Highlights\nKey Terms\nQuestions for Discussion\nExercises\nEnd-of-Chapter Application Case Discovery Health Turns Big Data into Better Healthcare\nReferences\nChapter 14 Business Analytics: Emerging Trends and Future Impacts\n14.1 Opening Vignette: Oklahoma Gas and Electric Employs Analytics to Promote Smart\nEnergy Use\n14.2 Location-Based Analytics for Organizations\nGeospatial Analytics\nApplication Case 14.1 Great Cl", "pacts\n14.1 Opening Vignette: Oklahoma Gas and Electric Employs Analytics to Promote Smart\nEnergy Use\n14.2 Location-Based Analytics for Organizations\nGeospatial Analytics\nApplication Case 14.1 Great Clips Employs Spatial Analytics to Shave Time in Location Decisions\nA Multimedia Exercise in Analytics Employing Geospatial Analytics\nReal-Time Location Intelligence\nApplication Case 14.2 Quiznos Targets Customers for Its Sandwiches\n14.3 Analytics Applications for Consumers\nApplication Case 14.3 A Life Coach in Your Pocket\n14.4 Recommendation Engines\n14.5 Web 2.0 and Online Social Networking\nRepresentative Characteristics of Web 2.0\nSocial Networking\nA Definition and Basic Information\n\nTable of Contents\nImplications of Business and Enterprise Social Networks\n14.6 Cloud Computing and BI\nService-Oriented DSS\nData-as-a-Service (DaaS)\nInformation-as-a-Service (Information on Demand) (IaaS)\nAnalytics-as-a-Service (AaaS)\n14.7 Impacts of Analytics in Organizations: An Overview\nNew Organizational Un", "riented DSS\nData-as-a-Service (DaaS)\nInformation-as-a-Service (Information on Demand) (IaaS)\nAnalytics-as-a-Service (AaaS)\n14.7 Impacts of Analytics in Organizations: An Overview\nNew Organizational Units\nRestructuring Business Processes and Virtual Teams\nThe Impacts of ADS Systems\nJob Satisfaction\nJob Stress and Anxiety\nAnalytics Impact on Managers Activities and Their Performance\n14.8 Issues of Legality, Privacy, and Ethics\nLegal Issues\nPrivacy\nRecent Technology Issues in Privacy and Analytics\nEthics in Decision Making and Support\n14.9 An Overview of the Analytics Ecosystem\nAnalytics Industry Clusters\nData Infrastructure Providers\nData Warehouse Industry\nMiddleware Industry\nData Aggregators/Distributors\nAnalytics-Focused Software Developers\nReporting/Analytics\nPredictive Analytics\nPrescriptive Analytics\nApplication Developers or System Integrators: Industry Specific or General\nAnalytics User Organizations\nAnalytics Industry Analysts and Influencers\nAcademic Providers and Certification", "ptive Analytics\nApplication Developers or System Integrators: Industry Specific or General\nAnalytics User Organizations\nAnalytics Industry Analysts and Influencers\nAcademic Providers and Certification Agencies\nChapter Highlights\nKey Terms\nQuestions for Discussion\nExercises\nEnd-of-Chapter Application Case Southern States Cooperative Optimizes Its Catalog Campaign\n\nTable of Contents\nReferences\nGlossary\nIndex\n"], "sources": ["Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Applied Predictive Analysis.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Cloud Computing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Natural Language Processing.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Predictive Analysis.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf", "Web and Social Media Analytics.pdf"], "processed_pdfs": ["Web and Social Media Analytics.pdf", "Applied Predictive Analysis.pdf", "Cloud Computing.pdf", "Predictive Analysis.pdf", "Natural Language Processing.pdf"]}